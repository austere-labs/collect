import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -> str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -> str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
            self, output_dir: str, timestamp: str, failed_results: List) -> str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, 'w', encoding='utf-8') as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -> str:
        """Read and return content from input file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -> str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 游댮/游리/游릭 | ... | ... |

Use emojis: 游댮 Critical, 游리 Important, 游릭 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
            self,
            timestamp: str,
            from_file: str, results: LLMRunnerResults) -> Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": []
        }

    def write_successful_results(self,
                                 results: LLMRunnerResults,
                                 output_dir: str,
                                 timestamp: str,
                                 summary: Dict[str, Any]) -> None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(
                result, response_text)

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
            self,
            output_dir: str,
            timestamp: str,
            summary: Dict[str, Any]) -> str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
            self, from_file: str,
            to_file: Optional[str] = None) -> Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results)
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"]) + (1 if "error_file" in summary else 0) + 1
        }

    async def review_diff_from_git(
            self,
            to_file: Optional[str] = None,
            staged_only: bool = True) -> Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ['git', 'diff', '--staged'],
                    capture_output=True, text=True, check=True)
            else:
                result = subprocess.run(
                    ['git', 'diff'],
                    capture_output=True, text=True, check=True)

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception(
                "Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results)
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"]) + (1 if "error_file" in summary else 0) + 1
        }
