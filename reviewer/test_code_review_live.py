import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"
        
        reviewer = CodeReviewer(output_dir=temp_output_dir)
        
        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)
        
        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir
        
        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] >= 1
        assert summary["successful_reviews"] + summary["failed_reviews"] == summary["total_models"]
        assert "timestamp" in summary
        
        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) >= 1  # At least summary file should exist
        
        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1
        
        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, 'r') as f:
            summary_data = json.load(f)
        
        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]
        
        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)
            
            # Verify file contains expected content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                assert "Code Review" in content
                assert "Model:" in content
                assert "Timestamp:" in content
                assert "Duration:" in content
                assert "Generated by" in content
        
        # Print results for visibility
        print(f"\nâœ… Live code review test completed successfully!")
        print(f"ðŸ“Š Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")
        
        if summary["successful_reviews"] > 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")
        
        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow  
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)
        
        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(temp_output_dir, staged_only=False)
            
            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"
            
            print(f"\nâœ… Git diff review completed!")
            print(f"ðŸ“Š Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed")
            
        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)
        
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"
        
        prompt = reviewer.create_code_review_prompt(test_content)
        
        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "ðŸ”´" in prompt and "ðŸŸ¡" in prompt and "ðŸŸ¢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)
        
        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)
        
        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] > 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Should contain actual review content, not just raw API response
            assert len(content) > 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON
            
            print(f"âœ… Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]