# DSPy beats human prompt engineering expertise

Sander Schulhoff, creator of learnprompting.org, documented in **"The Prompt Report: A Systematic Survey of Prompting Techniques"** (arXiv:2406.06608, 2024) that DSPy's automated prompt optimization significantly outperformed his 20 hours of manual prompt engineering in just 10 minutes, achieving an F1 score of nearly 0.6 on a binary classification task. This experience fundamentally changed his perspective on automated prompting, leading him to now recommend DSPy over manual approaches for tasks with ground truth labels. The 80-page systematic survey, co-authored with researchers from OpenAI, Google, Microsoft, Princeton, and Stanford, analyzed 1,565 papers on prompting techniques and represents the most comprehensive academic examination of prompt engineering methods to date.

## The head-to-head comparison that changed everything

Schulhoff spent **20 hours manually crafting prompts** for a binary classification problem, applying the expertise that made him "the OG prompt engineer" who created the first prompt engineering guide two months before ChatGPT's release. DSPy, the automated prompt optimization framework developed at Stanford, generated its initial prompt in **10 minutes and significantly outperformed** Schulhoff's carefully engineered version. After minimal adjustments, the DSPy-generated prompt achieved an **F1 score approaching 0.6**, demonstrating clear superiority over the human-crafted approach.

The comparison emerged as part of Schulhoff's broader investigation into prompting techniques for "The Prompt Report," which systematically evaluated 58 different prompting methods across six problem-solving strategies. Before this experiment, Schulhoff believed manual prompting would remain superior "for quite a while" and that automated approaches were "just sort of too difficult." The DSPy results completely reversed this position. As he stated in a Latent Space podcast interview: "I spent 20 hours prompt engineering for a task and DSPy beat me in 10 minutes. And that's when I changed my mind. I would absolutely recommend using these, DSPy in particular, because it's just so easy to set up."

## DSPy's systematic approach outperforms expert intuition  

DSPy operates by compiling declarative language model calls into self-improving pipelines, automatically generating and refining both examples and explanations through systematic optimization. The framework, created by Omar Khattab and colleagues at Stanford (arXiv:2310.03714), takes a "prompting as programming" approach with a PyTorch-like API that makes prompt optimization accessible to developers without deep prompt engineering expertise. According to the original DSPy paper, the system outperforms standard few-shot prompting by **over 25% for GPT-3.5 and 65% for Llama2-13b-chat**, while beating expert-created demonstrations by **5-46% for GPT-3.5 and 16-40% for Llama2-13b-chat**.

The key advantage lies in DSPy's ability to systematically explore the prompt space rather than relying on human intuition about what makes an effective prompt. While a human engineer iterates based on subjective assessments and pattern recognition, DSPy leverages ground truth labels to objectively optimize prompt performance. This systematic approach proves particularly effective for classification tasks where clear evaluation metrics exist, though Schulhoff notes the limitation that "you really need ground truth labels" for DSPy to work effectively.

## The comprehensive survey that redefined prompt engineering

"The Prompt Report" represents the most extensive academic examination of prompting techniques to date, analyzing **1,565 relevant papers** from an initial collection of 4,247 publications using PRISMA systematic review methodology. The 80+ page survey categorizes prompting approaches into six core strategies: few-shot prompting (most powerful for accuracy), thought generation (including Chain-of-Thought), zero-shot prompting, ensembling, self-criticism, and decomposition. Benchmarking these techniques against ChatGPT using the MMLU dataset revealed that few-shot prompting delivers the strongest accuracy improvements, while popular techniques like role prompting proved "largely ineffective for accuracy."

The paper's authorship reflects its comprehensive scope, with **over 30 co-authors** from leading AI institutions including OpenAI, Google, Microsoft, Princeton, and Stanford. This collaboration brought together diverse perspectives on prompt engineering, from theoretical foundations to practical applications across different model architectures and task domains.

## Conclusion

Schulhoff's documentation of DSPy outperforming his manual prompt engineering marks a pivotal moment in the evolution of AI system development. The findings in "The Prompt Report" demonstrate that automated prompt optimization has crossed a critical threshold where it can consistently outperform human experts, at least for tasks with clear evaluation criteria. This shift from artisanal prompt crafting to systematic optimization suggests that the future of prompt engineering lies not in human creativity but in algorithmic exploration of the vast prompt space. While manual prompting retains value for exploratory tasks without ground truth labels, Schulhoff's experience proves that for many practical applications, **automated tools like DSPy represent not just a time-saving convenience but a path to genuinely superior performance**.