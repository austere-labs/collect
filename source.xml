<?xml version='1.0' encoding='utf-8'?>
<source_code project="collect">
  <file path="test_generate_prompt.py">import pytest
from config import Config
from collect import generate_prompt


@pytest.fixture
def sample_prompt():
    """Sample prompt content for testing."""
    return """Create a helpful AI assistant that can answer programming questions.
The assistant should be knowledgeable about Python, JavaScript, and web development.
It should provide clear explanations and code examples when appropriate."""


class TestGeneratePrompt:

    @pytest.mark.asyncio
    async def test_generate_prompt_basic(self, sample_prompt):
        """Test basic functionality of generate_prompt."""
        # Check if we have required config
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(sample_prompt)

        # Verify we got a string response
        assert isinstance(result, str)
        assert len(result) &gt; 0

        # The generated prompt should contain relevant content
        # Note: We can't predict exact content, but it should be substantial
        assert len(result) &gt; 50  # Should be more than just a few words

    @pytest.mark.asyncio
    async def test_generate_prompt_with_target_model(self, sample_prompt):
        """Test generate_prompt with target_model parameter."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(
            sample_prompt, target_model="claude-3-7-sonnet-20250219"
        )

        assert isinstance(result, str)
        assert len(result) &gt; 0

    @pytest.mark.asyncio
    async def test_generate_prompt_empty_string(self):
        """Test error handling for empty prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("")

    @pytest.mark.asyncio
    async def test_generate_prompt_whitespace_only(self):
        """Test error handling for whitespace-only prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("   \n\t   ")

    @pytest.mark.asyncio
    async def test_generate_prompt_simple_task(self):
        """Test with a simple task description."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        simple_task = "A coding assistant that helps with Python"
        result = await generate_prompt(simple_task)

        assert isinstance(result, str)
        assert len(result) &gt; len(simple_task)  # Should be expanded


if __name__ == "__main__":
    # Run a simple test

    async def manual_test():
        """Manual test function for quick verification."""
        test_prompt = "Create a Python function that validates email addresses."

        try:
            result = await generate_prompt(test_prompt)
            print(f"Input prompt: {test_prompt}")
            print(f"Generated prompt ({len(result)} chars):")
            print("-" * 50)
            print(result)
            print("-" * 50)
        except Exception as e:
            print(f"Error: {e}")

    # Uncomment to run manual test
    # asyncio.run(manual_test())
</file>
  <file path="collect.py">from typing import List
from mcp.server.fastmcp import FastMCP, Context
import tiktoken
import markdownify
import readabilipy.simple_json
from html_to_markdown import convert_to_markdown
from bs4 import BeautifulSoup
from secret_manager import SecretManager
from config import Config
from models.anthropic_mpc import AnthropicMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP
from models.gemini_mcp import GeminiMCP
from fetcher import Fetcher
import pyperclip
from reviewer.code_review import CodeReviewer
import subprocess
import atexit
import time

mcp = FastMCP("Collect")


@mcp.tool()
async def run_code_review(from_file: str, to_file: str = "codereview"):
    """
    Run code review on a diff file using multiple LLM models.

    Args:
        from_file: Path to the file containing the diff/code to review
        to_file: Directory name to write results to (default: "codereview")

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_code(from_file, to_file)


@mcp.tool()
async def run_git_diff_review(to_file: str = "codereview", staged_only: bool = True):
    """
    Run code review on git diff output.

    Args:
        to_file: Directory name to write results to(default: "codereview")
        staged_only: If True, review only staged changes;
        if False, review all changes

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_diff_from_git(to_file, staged_only)


@mcp.tool()
async def fetch_urls(urls: List[str], ctx: Context = None) -&gt; str:
    """
    Fetch content from multiple URLs concurrently and merge the responses.

    Use this tool when you need to:
    - Retrieve content from multiple web pages at once
    - Compare information across multiple sources
    - Gather data from several API endpoints simultaneously
    - Fetch related pages in parallel for efficiency

    Args:
        urls: List of URLs to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Merged content from all URLs as a single string

    Example:
        fetch_urls(["https://api.example.com/users",
                   "https://api.example.com/posts"])
    """
    fetcher = Fetcher(ctx)
    merged_responses = await fetcher.fetch_urls(urls)
    return merged_responses


@mcp.tool()
async def fetch_url(url: str, ctx: Context = None) -&gt; str:
    """
    Fetch raw content from a single URL.

    Use this tool when you need to:
    - Retrieve raw HTML/JSON from a web page or API
    - Get unprocessed content for custom parsing
    - Access web resources programmatically
    - Fetch data before converting to markdown

    Args:
        url: The URL to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Raw content from the URL(HTML, JSON, or plain text)

    Note: For documentation extraction, consider using get_docs instead.
          For markdown conversion, use to_markdown on the result.
    """
    fetcher = Fetcher(ctx)
    return fetcher.get(url)


@mcp.tool()
async def get_docs(url: str, extract_value: str = None, ctx: Context = None) -&gt; str:
    """
    Fetch and extract specific documentation content from web pages.

    Use this tool when users need to:
    - Extract specific sections from documentation websites
    - Get targeted information from technical docs
    - Retrieve API documentation for specific methods/classes
    - Pull configuration examples from documentation
    - Find specific topics within large documentation sites

    Args:
        url: The URL of the documentation page to fetch
        extract_value: Optional. Specific section/topic to extract(e.g., "authentication",
                      "API endpoints", "installation guide"). If not provided, returns
                      the entire page content.
        ctx: MCP context(automatically provided)

    Returns:
        Extracted documentation content as markdown. If extract_value is specified,
        uses Gemini AI to intelligently extract only the relevant section.

    Examples:
        - get_docs("https://docs.python.org/3/", "datetime module")
        - get_docs("https://fastapi.tiangolo.com/", "dependency injection")
        - get_docs("https://react.dev/", "useEffect hook")
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-flash-preview-05-20"
    gemini = GeminiMCP(config, secret_mgr, model=model)

    if extract_value is None:
        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        return response
    else:
        prompt_prefatory = f"""
        # Documentation Extraction Task

        Extract and format the documentation for: **{extract_value} **

        # Instructions:
        - Focus specifically on the requested section/topic
        - Include code examples, parameters, and usage details if present
        - Maintain original formatting and structure
        - If the exact section isn't found, extract the most relevant related content
        - Return only the extracted documentation content, no meta-commentary

        ## Content to extract: {extract_value}
        """

        prompt = prompt_prefatory + "\n\n"
        response = await gemini.build_prompt_from_url(url, prompt, ctx)
        return response.strip()


@mcp.tool()
async def copy_clipboard(text: str) -&gt; str:
    """
    Copy text to the system clipboard.

    Use this tool when users need to:
    - Copy generated code snippets to clipboard
    - Save formatted text for pasting elsewhere
    - Copy API keys, URLs, or configuration values
    - Transfer content between applications

    Args:
        text: The text content to copy to clipboard

    Note: The text will replace any existing clipboard content.
    """
    pyperclip.copy(text)


@mcp.tool()
def strip_html(html: str) -&gt; str:
    """
    Remove all HTML tags and return plain text content.

    Use this tool when you need to:
    - Extract plain text from HTML pages
    - Remove formatting and tags from web content
    - Clean HTML for text analysis
    - Prepare content for non-HTML processing

    Args:
        html: Raw HTML string to process

    Returns:
        Plain text with all HTML tags removed

    Note: This removes ALL formatting. For readable formatting, use to_markdown instead.
    """
    soup = BeautifulSoup(html, "lxml")
    return soup.get_text()


@mcp.tool()
def to_markdown(html: str) -&gt; str:
    """Extract and convert HTML content to markdown using markdownify
    and readabilipy

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown

    """
    html_to_json = readabilipy.simple_json.simple_json_from_html_string(
        html,
        use_readability=True,
    )
    if not html_to_json["content"]:
        return "&lt;error&gt;Page failed to be simplified from HTML to json&lt;/error&gt;"

    return markdownify.markdownify(
        html_to_json["content"],
        heading_style=markdownify.ATX,
    )


def html_to_markdown(html: str) -&gt; str:
    """This uses html-to-markdown library instead of markdownify

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown as a str
    """

    return convert_to_markdown(
        html,
        heading_style="atx",
    )


@mcp.tool()
async def get_anthropic_model_list() -&gt; List[str]:
    """
    Get the list of available Anthropic Claude models.

    Use this tool when you need to:
    - Check which Claude models are available
    - Verify model names before using them
    - List Anthropic's current model offerings
    - Help users choose between Claude models

    Returns:
        List of available Anthropic model names (e.g., ["claude-3-opus", "claude-3-sonnet"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.get_model_list()


@mcp.tool()
async def get_openai_model_list() -&gt; List[str]:
    """
    Get the list of available OpenAI models.

    Use this tool when you need to:
    - Check which GPT models are available
    - Verify OpenAI model names
    - List current OpenAI offerings
    - Help users choose between GPT models

    Returns:
        List of available OpenAI model names (e.g., ["gpt-4", "gpt-3.5-turbo"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    openai_mcp = OpenAIMCP(config, secret_mgr, model="gpt-4o")
    return openai_mcp.get_model_list()


@mcp.tool()
async def get_xai_model_list() -&gt; List[str]:
    """
    Get the list of available XAI (Grok) models.

    Use this tool when you need to:
    - Check which Grok models are available
    - Verify XAI model names
    - List current Grok offerings
    - Help users choose between Grok models

    Returns:
        List of available XAI model names (e.g., ["grok-3", "grok-3-mini"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-mini-fast-latest")
    return xai_mcp.get_model_list()


@mcp.tool()
async def get_gemini_model_list() -&gt; List[dict]:
    """
    Get the list of available Google Gemini models
    (filtered for 2.0 and 2.5 versions).

    Use this tool when you need to:
    - Check which Gemini models are available with their token limits
    - Verify Google AI model names and capabilities
    - List current Gemini 2.0 and 2.5 offerings
    - Help users choose between Gemini models based on token capacity

    Returns:
        List of model dictionaries sorted by token limit (highest first),
        each containing:
        - model_name: The model identifier (e.g., "gemini-2.5-flash")
        - token_window: Input token limit (e.g., 1048576)

    Example return:
        [
            {"model_name": "gemini-2.5-flash", "token_window": 1048576},
            {"model_name": "gemini-2.0-flash", "token_window": 1048576},
            {"model_name": "gemini-2.5-pro", "token_window": 1048576}
        ]
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.5-flash")
    return gemini_mcp.get_model_list()


@mcp.tool()
async def count_openai_tokens(text: str, model: str = "gpt-4") -&gt; int:
    """
    Count tokens in text using OpenAI's tiktoken tokenizer.

    Use this tool when you need to:
    - Check if content fits within OpenAI model limits
    - Estimate API costs for OpenAI models
    - Split content to fit token windows
    - Optimize prompts for token efficiency

    Args:
        text: The text to count tokens for
        model: OpenAI model name (default: "gpt-4")

    Returns:
        Number of tokens in the text for the specified model
    """
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@mcp.tool()
async def count_anthropic_tokens(text: str) -&gt; int:
    """
    Count tokens in text using Anthropic's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Claude model limits
    - Estimate API costs for Anthropic models
    - Split content for Claude's context window
    - Optimize prompts for Claude

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Anthropic models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.count_tokens(text)


@mcp.tool()
async def count_gemini_tokens(text: str) -&gt; int:
    """
    Count tokens in text using Google Gemini's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Gemini model limits
    - Estimate API costs for Google AI models
    - Split content for Gemini's context window
    - Optimize prompts for Gemini

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Gemini models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.0-flash")
    return gemini_mcp.count_tokens(text)


@mcp.tool()
async def count_grok_tokens(text: str) -&gt; int:
    """
    Count tokens in text using XAI Grok's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Grok model limits
    - Estimate API costs for XAI models
    - Split content for Grok's context window
    - Optimize prompts for Grok

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Grok models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-fast-latest")
    return xai_mcp.count_tokens(text)


@mcp.tool()
async def generate_prompt(prompt: str, target_model: str = None) -&gt; str:
    """
    Generate an optimized AI prompt using Anthropic's experimental prompt engineering API.

    This tool leverages Anthropic's closed research preview API to automatically create
    high-quality, structured prompts from simple task descriptions. The API analyzes
    your input and generates professional-grade prompts optimized for Claude models.

    Use this tool when you need to:
    - Transform simple ideas into comprehensive AI prompts
    - Create structured prompts for specific tasks or roles
    - Optimize prompts for better AI responses
    - Generate consistent prompt templates for repeated use
    - Improve prompt clarity and effectiveness

    Args:
        prompt: A brief description of what you want the AI to do.
                Can be as simple as a role description or task summary.
                Examples:
                - "a helpful programming assistant"
                - "a chef for meal planning"
                - "a technical documentation writer"
                - "analyze code for security vulnerabilities"
        target_model: Optional. The specific model to optimize for (e.g., "claude-3-opus").
                     If not specified, generates a general-purpose prompt.

    Returns:
        A professionally crafted prompt ready for use with Claude or other AI models.
        The generated prompt includes appropriate context, instructions, and structure
        to maximize response quality.

    Raises:
        ValueError: If the prompt is empty or only contains whitespace
        RuntimeError: If the API call fails or returns an unexpected response

    Example:
        &gt;&gt;&gt; result = await generate_prompt("a Python code reviewer")
        &gt;&gt;&gt; print(result)
        "You are an expert Python code reviewer with deep knowledge..."

    Note:
        This uses Anthropic's experimental "prompt-tools" API which requires special
        access. The API is in closed research preview and may change without notice.
    """
    try:
        # Validate input
        task_content = prompt.strip()
        if not task_content:
            raise ValueError("Prompt cannot be empty")

        # Set up Anthropic MCP client
        config = Config()
        secret_mgr = SecretManager(config.project_id)
        anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)

        # Call generate_prompt API with new signature
        response = anthropic_mcp.generate_prompt(task_content, target_model)

        # Extract the generated prompt text from the response
        if response.messages and response.messages[0].content:
            return response.messages[0].content[0].text
        else:
            raise ValueError("No prompt generated in response")

    except ValueError:
        # Re-raise ValueError (like empty prompt) without wrapping
        raise
    except Exception as e:
        raise RuntimeError(f"Error generating prompt: {str(e)}")


def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["uv", "run", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for API to initialize
        time.sleep(2)

        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")

            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()

            atexit.register(cleanup_api)

    except Exception as e:
        print(f"Failed to start API server: {e}")

    # Continue with MCP server startup
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
</file>
  <file path="config.py">from dotenv import load_dotenv
import os


class Config:
    # Load .env for configuration
    load_dotenv(".env")

    def __init__(self) -&gt; None:
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.port = os.getenv("PORT")
        self.db_path = os.getenv("DB_PATH")
        self.anthropic_key_path = os.getenv("ANTHROPIC_API_KEY_PATH")
        self.anthropic_model_opus = os.getenv("ANTHROPIC_MODEL_OPUS")
        self.anthropic_model_sonnet = os.getenv("ANTHROPIC_MODEL_SONNET")
        self.gemini_api_key_path = os.getenv("GEMINI_API_KEY_PATH")
        self.gemini_base_url = os.getenv("GEMINI_BASE_URL")
        self.xai_api_key_path = os.getenv("XAI_API_KEY_PATH")
        self.grok_system_prompt = os.getenv("GROK_SYSTEM_PROMPT")
        self.openai_api_key_path = os.getenv("OPENAI_API_KEY_PATH")
        self.openai_default_code_review_model = os.getenv(
            "OPENAI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.gemini_default_code_review_model = os.getenv(
            "GEMINI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.anthropic_default_code_review_model = os.getenv(
            "ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.xai_default_code_review_model = os.getenv("XAI_DEFAULT_CODE_REVIEW_MODEL")

        # GitHub configuration
        self.github_url = os.getenv("GITHUB_URL")

        # Command subdirectories - read as comma-separated string
        command_subdirs_str = os.getenv(
            "COMMAND_SUBDIRS", "archive,go,js,mcp,python,tools"
        )
        self.command_subdirs = [
            subdir.strip() for subdir in command_subdirs_str.split(",")
        ]
</file>
  <file path="test_collect.py">import pytest

from collect import (
    count_anthropic_tokens,
    count_gemini_tokens,
    count_openai_tokens,
    count_grok_tokens,
    get_docs,
)

# The @pytest.mark.parametrize decorator runs the test function
#  test_empty_text_returns_zero three separate times, once for each
#  function in the list. Each time, it passes a different token-counting
#  function to the test as the func parameter.

#  This is useful for testing similar functionality across multiple
#  implementations without duplicating test code. In this case, it verifies
#   that all three token-counting functions return zero when given empty
#  text.


@pytest.mark.asyncio
async def test_openai_hello_token_count():
    result = await count_openai_tokens("hello", model="gpt-3.5-turbo")
    assert result == 1


@pytest.mark.parametrize(
    "func,text",
    [
        (count_openai_tokens, "Hello, world!"),
        (count_gemini_tokens, "Hello, Gemini!"),
        (count_anthropic_tokens, "Hello Claude"),
        (count_grok_tokens, "Hello Grok"),
    ],
)
@pytest.mark.asyncio
async def test_nonempty_text_returns_positive_int(func, text):
    n = await func(text)
    assert isinstance(n, int)
    assert n &gt; 0


@pytest.mark.asyncio
async def test_get_docs_with_extract_value():
    url = "https://docs.python.org/3/library/json.html"
    extract_value = "json.dumps"

    result = await get_docs(url, extract_value)

    assert isinstance(result, str)
    assert len(result) &gt; 0
    assert "json.dumps" in result.lower()

    print(f"Extracted docs for {extract_value}:")
    print(result[:500] + "..." if len(result) &gt; 500 else result)


@pytest.mark.asyncio
async def test_get_docs_without_extract_value():
    url = "https://docs.python.org/3/library/json.html"

    result = await get_docs(url)

    assert isinstance(result, str)
    assert len(result) &gt; 0
    # Should contain raw HTML content when no extraction is performed
    assert "html" in result.lower() or "json" in result.lower()

    print(f"Raw content length: {len(result)}")
    print(result[:200] + "..." if len(result) &gt; 200 else result)
</file>
  <file path="requirements.txt">aiohttp==3.11.11
annotated-types==0.7.0
anthropic==0.50.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.8
distro==1.9.0
docstring-parser==0.16
-e file:///Users/benjaminmetz/python/collect
google-api-core==2.24.2
google-auth==2.39.0
google-cloud-aiplatform==1.91.0
google-cloud-bigquery==3.31.0
google-cloud-core==2.4.3
google-cloud-resource-manager==1.14.2
google-cloud-storage==2.19.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
idna==3.10
iniconfig==2.1.0
jiter==0.9.0
markdown-it-py==3.0.0
mcp==1.7.1
mdurl==0.1.2
numpy==2.2.5
packaging==25.0
pluggy==1.5.0
proto-plus==1.26.1
protobuf==5.29.4
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.11.4
pydantic-core==2.33.2
pydantic-settings==2.9.1
pygments==2.19.1
pyperclip==1.9.0
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
regex==2024.11.6
requests==2.32.3
rich==14.0.0
rsa==4.9.1
sentencepiece==0.2.0
shapely==2.1.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
sse-starlette==2.3.3
starlette==0.46.2
tiktoken==0.9.0
typer==0.15.3
typing-extensions==4.13.2
typing-inspection==0.4.0
urllib3==2.4.0
uvicorn==0.34.2
</file>
  <file path="uv.lock">version = 1
revision = 1
requires-python = "&gt;=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.12.11"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/6b/850a842871ab7be0d00686750d0ee9d8fb8e7be981e4e5700bb6c88f1b8f/aiohttp-3.12.11.tar.gz", hash = "sha256:a5149ae1b11ce4cf8b122846bfa3d7c5f29fe3bfe6745ab21b3eea9615bc5564", size = 7814403 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/ac/15e21c6a17b5183d1617505b125c773f554a56e06be577a289151a8e5ce7/aiohttp-3.12.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:5fadc4b67f972a701805aa501cd9d22cdbeda21f9c9ae85e60678f84b1727a16", size = 694170 },
    { url = "https://files.pythonhosted.org/packages/02/5b/347f8aff5793829b3a31a927bd039ec4f22221a32c459b9d19fe880921e3/aiohttp-3.12.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:144d67c29ae36f052584fc45a363e92798441a5af5762d83037aade3e2aa9dc5", size = 471832 },
    { url = "https://files.pythonhosted.org/packages/4b/e5/9ed82f5b6a2dca30940e90820ce2f8203e15111de464bba0980e2c7e169b/aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6b73299e4bf37d14c6e4ca5ce7087b44914a8d9e1f40faedc271f28d64ec277e", size = 464133 },
    { url = "https://files.pythonhosted.org/packages/3c/8d/edcddc41d4f1157a2536143476070ae66de2b839af3724655c2a6358670a/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1226325e98e6d3cdfdaca639efdc3af8e82cd17287ae393626d1bd60626b0e93", size = 1702942 },
    { url = "https://files.pythonhosted.org/packages/b1/2e/efcb6a35d0646ced659edc3172e8e9384246d2cd0b0f3080fc3c441cb511/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0ecae011f2f779271407f2959877230670de3c48f67e5db9fbafa9fddbfa3a", size = 1684207 },
    { url = "https://files.pythonhosted.org/packages/56/f7/0324c499b7c610633d2f5e8af5457fd3a0584f5f4827bc46b673866596ac/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a8a711883eedcd55f2e1ba218d8224b9f20f1dfac90ffca28e78daf891667e3a", size = 1736275 },
    { url = "https://files.pythonhosted.org/packages/98/0f/b7aa0fd1ed777b5d6fb62c0dcf82effb717e8b51c802067fc3bcb703e003/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2601c1fcd9b67e632548cfd3c760741b31490502f6f3e5e21287678c1c6fa1b2", size = 1785648 },
    { url = "https://files.pythonhosted.org/packages/2c/2a/7defcf31010a2964bf17f6c9d9190e3be889f0c5edc3ff2cdac6e60764d7/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d5b11ea794ee54b33d0d817a1aec0ef0dd2026f070b493bc5a67b7e413b95d4", size = 1707981 },
    { url = "https://files.pythonhosted.org/packages/b6/9e/ff3d9a01f533752e81fd92bfe1301ae5a7bd5a306d752ad54f8bc61570fa/aiohttp-3.12.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:109b3544138ce8a5aca598d5e7ff958699e3e19ee3675d27d5ee9c2e30765a4a", size = 1621683 },
    { url = "https://files.pythonhosted.org/packages/2c/98/446c96927f2e7d2eaea95660a60eb6077771d00df834430cec002cadd96b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b795085d063d24c6d09300c85ddd6b9c49816d5c498b40b6899ca24584e936e4", size = 1674706 },
    { url = "https://files.pythonhosted.org/packages/e1/2a/038cb4af5e58994bc9315d0cb6a906d20ddfffb8eb3d0dfcfe8fe95b1939/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ebcbc113f40e4c9c0f8d2b6b31a2dd2a9768f3fa5f623b7e1285684e24f5159f", size = 1706372 },
    { url = "https://files.pythonhosted.org/packages/28/18/dc16cc7cb9b8baf9308f23ecf1e787d916238d01060bea272d5c29e9aa6b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:590e5d792150d75fa34029d0555b126e65ad50d66818a996303de4af52b65b32", size = 1648967 },
    { url = "https://files.pythonhosted.org/packages/44/f5/f427ef971e00088c7f0f5a4a7e405732e0ce0b87dfc3eec0f1a8c16863d2/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:9c2a4dec596437b02f0c34f92ea799d6e300184a0304c1e54e462af52abeb0a8", size = 1725099 },
    { url = "https://files.pythonhosted.org/packages/d4/0a/34fc018d4e193115b512bc08f6afaf79c23609a6487e47f0d593d1d9df41/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aace119abc495cc4ced8745e3faceb0c22e8202c60b55217405c5f389b569576", size = 1758571 },
    { url = "https://files.pythonhosted.org/packages/b6/69/b466ec346506384a93bcb864ab75a21b6520c64fcc3720ab2056470a657f/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd749731390520a2dc1ce215bcf0ee1018c3e2e3cd834f966a02c0e71ad7d637", size = 1707461 },
    { url = "https://files.pythonhosted.org/packages/f4/fc/3437d3e40581bc7d0816e134fdcae3c7e5c3f21dbdcfbd54402af3973b1c/aiohttp-3.12.11-cp313-cp313-win32.whl", hash = "sha256:65952736356d1fbc9efdd17492dce36e2501f609a14ccb298156e392d3ad8b83", size = 420053 },
    { url = "https://files.pythonhosted.org/packages/6c/cf/cd84df67147c986315c63fef29a6ecadf03bf5528340b8c82eedd988cf57/aiohttp-3.12.11-cp313-cp313-win_amd64.whl", hash = "sha256:854132093e12dd77f5c07975581c42ae51a6a8868dcbbb509c77d1963c3713b7", size = 445988 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.50.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/40/85/4dd9f80da0727c56d7e7f7c627cb724edd9e6df062df6ecc0e90f06e6dbb/anthropic-0.50.0.tar.gz", hash = "sha256:42175ec04ce4ff2fa37cd436710206aadff546ee99d70d974699f59b49adc66f", size = 213021 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/ae/975f97ad5581a9e187a3717e21d79d6c7ad6be926fee9aa8a15b3d9f8f37/anthropic-0.50.0-py3-none-any.whl", hash = "sha256:defbd79327ca2fa61fd7b9eb2f1627dfb1f69c25d49288c52e167ddb84574f80", size = 245291 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "asttokens"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4a/e7/82da0a03e7ba5141f05cce0d302e6eed121ae055e0456ca228bf693984bc/asttokens-3.0.0.tar.gz", hash = "sha256:0dcd8baa8d62b0c1d118b399b2ddba3c4aff271d0d7a9e0d4c1681c79035bbc7", size = 61978 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/8a/c46dcc25341b5bce5472c718902eb3d38600a903b14fa6aeecef3f21a46f/asttokens-3.0.0-py3-none-any.whl", hash = "sha256:e3078351a059199dd5138cb1c706e6430c05eff2ff136af5eb4790f9d28932e2", size = 26918 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285 },
]

[[package]]
name = "black"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "mypy-extensions" },
    { name = "packaging" },
    { name = "pathspec" },
    { name = "platformdirs" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/49/26a7b0f3f35da4b5a65f081943b7bcd22d7002f5f0fb8098ec1ff21cb6ef/black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666", size = 649449 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/87/0edf98916640efa5d0696e1abb0a8357b52e69e82322628f25bf14d263d1/black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f", size = 1650673 },
    { url = "https://files.pythonhosted.org/packages/52/e5/f7bf17207cf87fa6e9b676576749c6b6ed0d70f179a3d812c997870291c3/black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3", size = 1453190 },
    { url = "https://files.pythonhosted.org/packages/e3/ee/adda3d46d4a9120772fae6de454c8495603c37c4c3b9c60f25b1ab6401fe/black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171", size = 1782926 },
    { url = "https://files.pythonhosted.org/packages/cc/64/94eb5f45dcb997d2082f097a3944cfc7fe87e071907f677e80788a2d7b7a/black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18", size = 1442613 },
    { url = "https://files.pythonhosted.org/packages/09/71/54e999902aed72baf26bca0d50781b01838251a462612966e9fc4891eadd/black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717", size = 207646 },
]

[[package]]
name = "cachetools"
version = "5.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/81/3747dad6b14fa2cf53fcf10548cf5aea6913e96fab41a3c198676f8948a5/cachetools-5.5.2.tar.gz", hash = "sha256:1a661caa9175d26759571b2e19580f9d6393969e5dfca11fdb1f947a23e640d4", size = 28380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl", hash = "sha256:d26a22bcc62eb95c3beabd9f1ee5e820d3d2704fe2967cbe350e20c8ffcd3f0a", size = 10080 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "collect"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "anthropic" },
    { name = "beautifulsoup4" },
    { name = "black" },
    { name = "fastapi" },
    { name = "google-ai-generativelanguage" },
    { name = "google-api-python-client" },
    { name = "google-auth-httplib2" },
    { name = "google-cloud-aiplatform", extra = ["tokenization"] },
    { name = "google-cloud-secret-manager" },
    { name = "google-genai" },
    { name = "google-generativeai" },
    { name = "html-to-markdown" },
    { name = "html5lib" },
    { name = "httplib2" },
    { name = "httpx" },
    { name = "ipython" },
    { name = "lxml" },
    { name = "marimo" },
    { name = "markdownify" },
    { name = "mcp", extra = ["cli"] },
    { name = "openai" },
    { name = "pathspec" },
    { name = "pyperclip" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
    { name = "pytest-xdist" },
    { name = "python-json-logger" },
    { name = "readabilipy" },
    { name = "rich" },
    { name = "ruff" },
    { name = "tiktoken" },
    { name = "uvicorn" },
    { name = "yoyo-migrations" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = "&gt;=3.12.11" },
    { name = "anthropic", specifier = "&gt;=0.50.0" },
    { name = "beautifulsoup4", specifier = "&gt;=4.13.4" },
    { name = "black", specifier = "&gt;=25.1.0" },
    { name = "fastapi", specifier = "&gt;=0.116.1" },
    { name = "google-ai-generativelanguage", specifier = "&gt;=0.6.15" },
    { name = "google-api-python-client", specifier = "&gt;=2.169.0" },
    { name = "google-auth-httplib2", specifier = "&gt;=0.2.0" },
    { name = "google-cloud-aiplatform", extras = ["tokenization"], specifier = "&gt;=1.91.0" },
    { name = "google-cloud-secret-manager", specifier = "&gt;=2.23.3" },
    { name = "google-genai", specifier = "&gt;=1.13.0" },
    { name = "google-generativeai", specifier = "&gt;=0.8.5" },
    { name = "html-to-markdown", specifier = "&gt;=1.3.2" },
    { name = "html5lib", specifier = "&gt;=1.1" },
    { name = "httplib2", specifier = "&gt;=0.22.0" },
    { name = "httpx", specifier = "&gt;=0.28.1" },
    { name = "ipython", specifier = "&gt;=9.4.0" },
    { name = "lxml", specifier = "&gt;=5.4.0" },
    { name = "marimo", specifier = "&gt;=0.14.12" },
    { name = "markdownify", specifier = "&gt;=1.1.0" },
    { name = "mcp", extras = ["cli"], specifier = "&gt;=1.7.1" },
    { name = "openai", specifier = "&gt;=1.59.4" },
    { name = "pathspec", specifier = "&gt;=0.12.1" },
    { name = "pyperclip", specifier = "&gt;=1.9.0" },
    { name = "pytest", specifier = "&gt;=8.3.5" },
    { name = "pytest-asyncio", specifier = "&gt;=0.26.0" },
    { name = "pytest-xdist", specifier = "&gt;=3.6.1" },
    { name = "python-json-logger", specifier = "&gt;=3.3.0" },
    { name = "readabilipy", specifier = "&gt;=0.3.0" },
    { name = "rich", specifier = "&gt;=14.0.0" },
    { name = "ruff", specifier = "&gt;=0.11.9" },
    { name = "tiktoken", specifier = "&gt;=0.9.0" },
    { name = "uvicorn", specifier = "&gt;=0.34.2" },
    { name = "yoyo-migrations", specifier = "&gt;=9.0.0" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "decorator"
version = "5.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/fa/6d96a0978d19e17b68d634497769987b16c8f4cd0a7a05048bec693caa6b/decorator-5.2.1.tar.gz", hash = "sha256:65f266143752f734b0a7cc83c46f4618af75b8c5911b00ccb61d0ac9b6da0360", size = 56711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl", hash = "sha256:d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a", size = 9190 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "docstring-parser"
version = "0.16"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/08/12/9c22a58c0b1e29271051222d8906257616da84135af9ed167c9e28f85cb3/docstring_parser-0.16.tar.gz", hash = "sha256:538beabd0af1e2db0146b6bd3caa526c35a34d61af9fd2887f3a8a27a739aa6e", size = 26565 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl", hash = "sha256:bf0a1387354d3691d102edef7ec124f219ef639982d096e26e3b60aeffa90637", size = 36533 },
]

[[package]]
name = "docutils"
version = "0.21.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ae/ed/aefcc8cd0ba62a0560c3c18c33925362d46c6075480bfa4df87b28e169a9/docutils-0.21.2.tar.gz", hash = "sha256:3a6b18732edf182daa3cd12775bbb338cf5691468f91eeeb109deff6ebfa986f", size = 2204444 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/d7/9322c609343d929e75e7e5e6255e614fcc67572cfd083959cdef3b7aad79/docutils-0.21.2-py3-none-any.whl", hash = "sha256:dafca5b9e384f0e419294eb4d2ff9fa826435bf15f15b7bd45723e8ad76811b2", size = 587408 },
]

[[package]]
name = "execnet"
version = "2.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/ff/b4c0dc78fbe20c3e59c0c7334de0c27eb4001a2b2017999af398bf730817/execnet-2.1.1.tar.gz", hash = "sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3", size = 166524 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl", hash = "sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc", size = 40612 },
]

[[package]]
name = "executing"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/50/a9d80c47ff289c611ff12e63f7c5d13942c65d68125160cefd768c73e6e4/executing-2.2.0.tar.gz", hash = "sha256:5d108c028108fe2551d1a7b2e8b713341e2cb4fc0aa7dcf966fa4327a5226755", size = 978693 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7b/8f/c4d9bafc34ad7ad5d8dc16dd1347ee0e507a52c3adb6bfa8887e1c6a26ba/executing-2.2.0-py2.py3-none-any.whl", hash = "sha256:11387150cad388d62750327a53d3339fad4888b39a6fe233c3afbb54ecffd3aa", size = 26702 },
]

[[package]]
name = "fastapi"
version = "0.116.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/78/d7/6c8b3bfe33eeffa208183ec037fee0cce9f7f024089ab1c5d12ef04bd27c/fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143", size = 296485 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565", size = 95631 },
]

[[package]]
name = "frozenlist"
version = "1.6.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5b/bf/a812e2fe6cb3f6c6cfc8d0303bf1742f2286004e5ec41ac8c89cf68cdb54/frozenlist-1.6.2.tar.gz", hash = "sha256:effc641518696471cf4962e8e32050133bc1f7b2851ae8fd0cb8797dd70dc202", size = 43108 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b8/f6/973abfcb8b68f2e8b58071a04ec72f5e1f0acd19dae0d3b7a8abc3d9ab07/frozenlist-1.6.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2ad8851ae1f6695d735f8646bf1e68675871789756f7f7e8dc8224a74eabb9d0", size = 85517 },
    { url = "https://files.pythonhosted.org/packages/c8/d0/ac45f2dcf0afd5f7d57204af8b7516ecbc3599ea681e06f4b25d3845bea8/frozenlist-1.6.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:cd2d5abc0ccd99a2a5b437987f3b1e9c265c1044d2855a09ac68f09bbb8082ca", size = 49916 },
    { url = "https://files.pythonhosted.org/packages/50/cc/99c3f31823630b7411f7c1e83399e91d6b56a5661a5b724935ef5b51f5f5/frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15c33f665faa9b8f8e525b987eeaae6641816e0f6873e8a9c4d224338cebbb55", size = 48107 },
    { url = "https://files.pythonhosted.org/packages/85/4e/38643ce3ee80d222892b694d02c15ea476c4d564493a6fe530347163744e/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3e6c0681783723bb472b6b8304e61ecfcb4c2b11cf7f243d923813c21ae5d2a", size = 255771 },
    { url = "https://files.pythonhosted.org/packages/ca/e6/ceed85a7d5c0f666485384fc393e32353f8088e154a1109e5ef60165d366/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:61bae4d345a26550d0ed9f2c9910ea060f89dbfc642b7b96e9510a95c3a33b3c", size = 252519 },
    { url = "https://files.pythonhosted.org/packages/29/99/9f2e2b90cf918465e3b6ca4eea79e6be53d24fba33937e37d86c3764bbf9/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90e5a84016d0d2fb828f770ede085b5d89155fcb9629b8a3237c960c41c120c3", size = 263348 },
    { url = "https://files.pythonhosted.org/packages/4e/ac/59f3ec4c1b4897186efb4757379915734a48bb16bbc15a9fe0bf0857b679/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55dc289a064c04819d669e6e8a85a1c0416e6c601782093bdc749ae14a2f39da", size = 257858 },
    { url = "https://files.pythonhosted.org/packages/48/4a/19c97510d0c2be1ebaae68383d1b5a256a12a660ca17b0c427b1024d9b92/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b79bcf97ca03c95b044532a4fef6e5ae106a2dd863875b75fde64c553e3f4820", size = 238248 },
    { url = "https://files.pythonhosted.org/packages/ef/64/641aa2b0944fa3d881323948e0d8d6fee746dae03d9023eb510bb80bc46a/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e5e7564d232a782baa3089b25a0d979e2e4d6572d3c7231fcceacc5c22bf0f7", size = 255932 },
    { url = "https://files.pythonhosted.org/packages/6c/f8/5b68d5658fac7332e5d26542a4af0ffc2edca8da8f854f6274882889ee1e/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6fcd8d56880dccdd376afb18f483ab55a0e24036adc9a83c914d4b7bb5729d4e", size = 253329 },
    { url = "https://files.pythonhosted.org/packages/e9/20/379d7a27eb82748b41319bf376bf2c034e7ee11dda94f12b331edcc261ff/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4fbce985c7fe7bafb4d9bf647c835dbe415b465a897b0c79d1bdf0f3fae5fe50", size = 266164 },
    { url = "https://files.pythonhosted.org/packages/13/bd/d7dbf94220020850392cb661bedfdf786398bafae85d1045dd108971d261/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:3bd12d727cd616387d50fe283abebb2db93300c98f8ff1084b68460acd551926", size = 241641 },
    { url = "https://files.pythonhosted.org/packages/a4/70/916fef6284d294077265cd69ad05f228e44f7ed88d9acb690df5a1174049/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:38544cae535ed697960891131731b33bb865b7d197ad62dc380d2dbb1bceff48", size = 261215 },
    { url = "https://files.pythonhosted.org/packages/8f/98/1326a7189fa519692698cddf598f56766b0fea6ac71cddaf64760a055397/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:47396898f98fae5c9b9bb409c3d2cf6106e409730f35a0926aad09dd7acf1ef5", size = 262597 },
    { url = "https://files.pythonhosted.org/packages/f4/d6/0a95ab9289c72e86c37c9b8afe82576556456b6f66a35d242526634130f2/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:d10d835f8ce8571fd555db42d3aef325af903535dad7e6faa7b9c8abe191bffc", size = 258766 },
    { url = "https://files.pythonhosted.org/packages/1b/d0/9e946aabd89ebfcb71ec1371327f0e25d4868cd4439471a6fcb6eaf7b366/frozenlist-1.6.2-cp313-cp313-win32.whl", hash = "sha256:a400fe775a41b6d7a3fef00d88f10cbae4f0074c9804e282013d7797671ba58d", size = 40961 },
    { url = "https://files.pythonhosted.org/packages/43/e9/d714f5eb0fde1413344ded982ae9638307b59651d5c04263af42eb81a315/frozenlist-1.6.2-cp313-cp313-win_amd64.whl", hash = "sha256:cc8b25b321863ed46992558a29bb09b766c41e25f31461666d501be0f893bada", size = 46204 },
    { url = "https://files.pythonhosted.org/packages/f5/7a/8f6dde73862499e60eb390778a1e46b87c1fe3c5722622d731ccda7a173c/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:56de277a0e0ad26a1dcdc99802b4f5becd7fd890807b68e3ecff8ced01d58132", size = 91326 },
    { url = "https://files.pythonhosted.org/packages/79/60/dcdc75edbcf8241e7cb15fced68b3be63f67ff3faaf559c540a7eb63233b/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:9cb386dd69ae91be586aa15cb6f39a19b5f79ffc1511371eca8ff162721c4867", size = 52426 },
    { url = "https://files.pythonhosted.org/packages/64/e6/df2a43ccb2c4f1ea3692aae9a89cfc5dd932a90b7898f98f13ed9e2680a9/frozenlist-1.6.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53835d8a6929c2f16e02616f8b727bd140ce8bf0aeddeafdb290a67c136ca8ad", size = 51460 },
    { url = "https://files.pythonhosted.org/packages/fd/b3/c4f2f7fca9487b25c39bf64535f029316e184072a82f3660ce72defc5421/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cc49f2277e8173abf028d744f8b7d69fe8cc26bffc2de97d47a3b529599fbf50", size = 310270 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/046eb34d8d0fee1a8c9dc91a9ba581283c67a1ace20bcc01c86a53595105/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:65eb9e8a973161bdac5fa06ea6bd261057947adc4f47a7a6ef3d6db30c78c5b4", size = 289062 },
    { url = "https://files.pythonhosted.org/packages/48/7b/80991efaa0aa25e867cf93033c28e9d1310f34f90421eb59eb1f2073d937/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:301eb2f898d863031f8c5a56c88a6c5d976ba11a4a08a1438b96ee3acb5aea80", size = 312202 },
    { url = "https://files.pythonhosted.org/packages/78/6b/6fe30bdababdf82c5b34f0093770c4be6211071e23570721b80b11c9d52a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:207f717fd5e65fddb77d33361ab8fa939f6d89195f11307e073066886b33f2b8", size = 309557 },
    { url = "https://files.pythonhosted.org/packages/9d/ef/b7bf48802fc7d084703ba2173e6a8d0590bea378dcd6a480051c41bddf47/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f83992722642ee0db0333b1dbf205b1a38f97d51a7382eb304ba414d8c3d1e05", size = 282135 },
    { url = "https://files.pythonhosted.org/packages/af/f8/6911a085bce8d0d0df3dfc2560e3e0fb4d6c19ff101014bcf61aa32ba39a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12af99e6023851b36578e5bcc60618b5b30f4650340e29e565cd1936326dbea7", size = 303392 },
    { url = "https://files.pythonhosted.org/packages/9c/5d/b4e0cc6dbd6b9282926a470a919da7c6599ff324ab5268c7ecaff82cb858/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6f01620444a674eaad900a3263574418e99c49e2a5d6e5330753857363b5d59f", size = 309402 },
    { url = "https://files.pythonhosted.org/packages/0f/1b/bf777de3c810e68e8758337fcc97ee8c956376c87aecee9a61ba19a94123/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:82b94c8948341512306ca8ccc702771600b442c6abe5f8ee017e00e452a209e8", size = 312924 },
    { url = "https://files.pythonhosted.org/packages/0e/03/a69b890bc310790fcae61fd3b5be64876811b12db5d50b32e62f65e766bd/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:324a4cf4c220ddb3db1f46ade01e48432c63fa8c26812c710006e7f6cfba4a08", size = 291768 },
    { url = "https://files.pythonhosted.org/packages/70/cc/559386adf987b47c8977c929271d11a72efd92778a0a2f4cc97827a9a25b/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:695284e51458dabb89af7f7dc95c470aa51fd259207aba5378b187909297feef", size = 313305 },
    { url = "https://files.pythonhosted.org/packages/e7/fa/eb0e21730ffccfb2d0d367d863cbaacf8367bdc277b44eabf72f7329ab91/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:9ccbeb1c8dda4f42d0678076aa5cbde941a232be71c67b9d8ca89fbaf395807c", size = 312228 },
    { url = "https://files.pythonhosted.org/packages/d1/c1/8471b67172abc9478ad78c70a3f3a5c4fed6d4bcadc748e1b6dfa06ab2ae/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cbbdf62fcc1864912c592a1ec748fee94f294c6b23215d5e8e9569becb7723ee", size = 309905 },
    { url = "https://files.pythonhosted.org/packages/bb/2c/ee21987c3a175b49d0b827b1e45394a7a5d08c7de5b766ed6d0889d30568/frozenlist-1.6.2-cp313-cp313t-win32.whl", hash = "sha256:76857098ee17258df1a61f934f2bae052b8542c9ea6b187684a737b2e3383a65", size = 44644 },
    { url = "https://files.pythonhosted.org/packages/65/46/fce60f65b1fb17a90c4bf410a5c90cb3b40616cc229e75866f8be97c112c/frozenlist-1.6.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c06a88daba7e891add42f9278cdf7506a49bc04df9b1648be54da1bf1c79b4c6", size = 50607 },
    { url = "https://files.pythonhosted.org/packages/13/be/0ebbb283f2d91b72beaee2d07760b2c47dab875c49c286f5591d3d157198/frozenlist-1.6.2-py3-none-any.whl", hash = "sha256:947abfcc8c42a329bbda6df97a4b9c9cdb4e12c85153b3b57b9d2f02aa5877dc", size = 12582 },
]

[[package]]
name = "google-ai-generativelanguage"
version = "0.6.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/11/d1/48fe5d7a43d278e9f6b5ada810b0a3530bbeac7ed7fcbcd366f932f05316/google_ai_generativelanguage-0.6.15.tar.gz", hash = "sha256:8f6d9dc4c12b065fe2d0289026171acea5183ebf2d0b11cefe12f3821e159ec3", size = 1375443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/a3/67b8a6ff5001a1d8864922f2d6488dc2a14367ceb651bc3f09a947f2f306/google_ai_generativelanguage-0.6.15-py3-none-any.whl", hash = "sha256:5a03ef86377aa184ffef3662ca28f19eeee158733e45d7947982eb953c6ebb6c", size = 1327356 },
]

[[package]]
name = "google-api-core"
version = "2.24.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "googleapis-common-protos" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/09/5c/085bcb872556934bb119e5e09de54daa07873f6866b8f0303c49e72287f7/google_api_core-2.24.2.tar.gz", hash = "sha256:81718493daf06d96d6bc76a91c23874dbf2fac0adbbf542831b805ee6e974696", size = 163516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/95/f472d85adab6e538da2025dfca9e976a0d125cc0af2301f190e77b76e51c/google_api_core-2.24.2-py3-none-any.whl", hash = "sha256:810a63ac95f3c441b7c0e43d344e372887f62ce9071ba972eacf32672e072de9", size = 160061 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
    { name = "grpcio-status" },
]

[[package]]
name = "google-api-python-client"
version = "2.171.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-auth-httplib2" },
    { name = "httplib2" },
    { name = "uritemplate" },
]
sdist = { url = "https://files.pythonhosted.org/packages/35/99/237cd2510aecca9fabb54007e58553274cc43cb3c18512ee1ea574d11b87/google_api_python_client-2.171.0.tar.gz", hash = "sha256:057a5c08d28463c6b9eb89746355de5f14b7ed27a65c11fdbf1d06c66bb66b23", size = 13028937 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/db/c397e3eb3ea18f423855479d0a5852bdc9c3f644e3d4194931fa664a70b4/google_api_python_client-2.171.0-py3-none-any.whl", hash = "sha256:c9c9b76f561e9d9ac14e54a9e2c0842876201d5b96e69e48f967373f0784cbe9", size = 13547393 },
]

[[package]]
name = "google-auth"
version = "2.39.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cachetools" },
    { name = "pyasn1-modules" },
    { name = "rsa" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/8e/8f45c9a32f73e786e954b8f9761c61422955d23c45d1e8c347f9b4b59e8e/google_auth-2.39.0.tar.gz", hash = "sha256:73222d43cdc35a3aeacbfdcaf73142a97839f10de930550d89ebfe1d0a00cde7", size = 274834 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/12/ad37a1ef86006d0a0117fc06a4a00bd461c775356b534b425f00dde208ea/google_auth-2.39.0-py2.py3-none-any.whl", hash = "sha256:0150b6711e97fb9f52fe599f55648950cc4540015565d8fbb31be2ad6e1548a2", size = 212319 },
]

[[package]]
name = "google-auth-httplib2"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "httplib2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/56/be/217a598a818567b28e859ff087f347475c807a5649296fb5a817c58dacef/google-auth-httplib2-0.2.0.tar.gz", hash = "sha256:38aa7badf48f974f1eb9861794e9c0cb2a0511a4ec0679b1f886d108f5640e05", size = 10842 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl", hash = "sha256:b65a0a2123300dd71281a7bf6e64d65a0759287df52729bdd1ae2e47dc311a3d", size = 9253 },
]

[[package]]
name = "google-cloud-aiplatform"
version = "1.91.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "docstring-parser" },
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-bigquery" },
    { name = "google-cloud-resource-manager" },
    { name = "google-cloud-storage" },
    { name = "packaging" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "shapely" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/50/08/5854569782efbbc8efd0aeda3a4486153605104cbab6ac836b2328bae48e/google_cloud_aiplatform-1.91.0.tar.gz", hash = "sha256:b14e5e52b52b6012c7dc253beab34c511fdc53c69b13f436ddb06882c1a92cd7", size = 9102586 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/88/cea8583fadd142e8ef26f8ec14a6ee4d7c69c4e5ab82bea01a077fddddbe/google_cloud_aiplatform-1.91.0-py2.py3-none-any.whl", hash = "sha256:ff8df100c2af692d114a2219d3abbb96110b3e5655f342fdbb6aefad43901b52", size = 7591910 },
]

[package.optional-dependencies]
tokenization = [
    { name = "sentencepiece" },
]

[[package]]
name = "google-cloud-bigquery"
version = "3.31.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-resumable-media" },
    { name = "packaging" },
    { name = "python-dateutil" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/73/91/4c7274f4d5faf13ac000b06353deaf3579575bf0e4bbad07fa68b9f09ba9/google_cloud_bigquery-3.31.0.tar.gz", hash = "sha256:b89dc716dbe4abdb7a4f873f7050100287bc98514e0614c5d54cd6a8e9fb0991", size = 479961 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e8/bc/4cb8c61fc6dd817a4a390b745ec7b305f4578f547a16d09d54c8a790624b/google_cloud_bigquery-3.31.0-py3-none-any.whl", hash = "sha256:97f4a3219854ff01d6a3a57312feecb0b6e13062226b823f867e2d3619c4787b", size = 250099 },
]

[[package]]
name = "google-cloud-core"
version = "2.4.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/b8/2b53838d2acd6ec6168fd284a990c76695e84c65deee79c9f3a4276f6b4f/google_cloud_core-2.4.3.tar.gz", hash = "sha256:1fab62d7102844b278fe6dead3af32408b1df3eb06f5c7e8634cbd40edc4da53", size = 35861 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl", hash = "sha256:5130f9f4c14b4fafdff75c79448f9495cfade0d8775facf1b09c3bf67e027f6e", size = 29348 },
]

[[package]]
name = "google-cloud-resource-manager"
version = "1.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6e/ca/a4648f5038cb94af4b3942815942a03aa9398f9fb0bef55b3f1585b9940d/google_cloud_resource_manager-1.14.2.tar.gz", hash = "sha256:962e2d904c550d7bac48372607904ff7bb3277e3bb4a36d80cc9a37e28e6eb74", size = 446370 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/ea/a92631c358da377af34d3a9682c97af83185c2d66363d5939ab4a1169a7f/google_cloud_resource_manager-1.14.2-py3-none-any.whl", hash = "sha256:d0fa954dedd1d2b8e13feae9099c01b8aac515b648e612834f9942d2795a9900", size = 394344 },
]

[[package]]
name = "google-cloud-secret-manager"
version = "2.24.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/7a/2fa6735ec693d822fe08a76709c4d95d9b5b4c02e83e720497355039d2ee/google_cloud_secret_manager-2.24.0.tar.gz", hash = "sha256:ce573d40ffc2fb7d01719243a94ee17aa243ea642a6ae6c337501e58fbf642b5", size = 269516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/af/db1217cae1809e69a4527ee6293b82a9af2a1fb2313ad110c775e8f3c820/google_cloud_secret_manager-2.24.0-py3-none-any.whl", hash = "sha256:9bea1254827ecc14874bc86c63b899489f8f50bfe1442bfb2517530b30b3a89b", size = 218050 },
]

[[package]]
name = "google-cloud-storage"
version = "2.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-crc32c" },
    { name = "google-resumable-media" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/36/76/4d965702e96bb67976e755bed9828fa50306dca003dbee08b67f41dd265e/google_cloud_storage-2.19.0.tar.gz", hash = "sha256:cd05e9e7191ba6cb68934d8eb76054d9be4562aa89dbc4236feee4d7d51342b2", size = 5535488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/94/6db383d8ee1adf45dc6c73477152b82731fa4c4a46d9c1932cc8757e0fd4/google_cloud_storage-2.19.0-py2.py3-none-any.whl", hash = "sha256:aeb971b5c29cf8ab98445082cbfe7b161a1f48ed275822f59ed3f1524ea54fba", size = 131787 },
]

[[package]]
name = "google-crc32c"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/19/ae/87802e6d9f9d69adfaedfcfd599266bf386a54d0be058b532d04c794f76d/google_crc32c-1.7.1.tar.gz", hash = "sha256:2bff2305f98846f3e825dbeec9ee406f89da7962accdb29356e4eadc251bd472", size = 14495 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/72/b8d785e9184ba6297a8620c8a37cf6e39b81a8ca01bb0796d7cbb28b3386/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_arm64.whl", hash = "sha256:df8b38bdaf1629d62d51be8bdd04888f37c451564c2042d36e5812da9eff3c35", size = 30467 },
    { url = "https://files.pythonhosted.org/packages/34/25/5f18076968212067c4e8ea95bf3b69669f9fc698476e5f5eb97d5b37999f/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_x86_64.whl", hash = "sha256:e42e20a83a29aa2709a0cf271c7f8aefaa23b7ab52e53b322585297bb94d4638", size = 30309 },
    { url = "https://files.pythonhosted.org/packages/92/83/9228fe65bf70e93e419f38bdf6c5ca5083fc6d32886ee79b450ceefd1dbd/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:905a385140bf492ac300026717af339790921f411c0dfd9aa5a9e69a08ed32eb", size = 33133 },
    { url = "https://files.pythonhosted.org/packages/c3/ca/1ea2fd13ff9f8955b85e7956872fdb7050c4ace8a2306a6d177edb9cf7fe/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b211ddaf20f7ebeec5c333448582c224a7c90a9d98826fbab82c0ddc11348e6", size = 32773 },
    { url = "https://files.pythonhosted.org/packages/89/32/a22a281806e3ef21b72db16f948cad22ec68e4bdd384139291e00ff82fe2/google_crc32c-1.7.1-cp313-cp313-win_amd64.whl", hash = "sha256:0f99eaa09a9a7e642a61e06742856eec8b19fc0037832e03f941fe7cf0c8e4db", size = 33475 },
    { url = "https://files.pythonhosted.org/packages/b8/c5/002975aff514e57fc084ba155697a049b3f9b52225ec3bc0f542871dd524/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32d1da0d74ec5634a05f53ef7df18fc646666a25efaaca9fc7dcfd4caf1d98c3", size = 33243 },
    { url = "https://files.pythonhosted.org/packages/61/cb/c585282a03a0cea70fcaa1bf55d5d702d0f2351094d663ec3be1c6c67c52/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e10554d4abc5238823112c2ad7e4560f96c7bf3820b202660373d769d9e6e4c9", size = 32870 },
]

[[package]]
name = "google-genai"
version = "1.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "google-auth" },
    { name = "httpx" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/14/17/8f717f43732ae2b7775f816f0d8f0b39e2a020bbe7ba202f2ddb2f948c3b/google_genai-1.19.0.tar.gz", hash = "sha256:66f5de78075781bfd9e423f1e3592e4240759dfe0ac42ac74a9dcb2c4f662e9d", size = 198000 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c4/ae/64fccdebf5811453ce53b0d5ee23d4f27ef173ef36d3b67dad791a0007aa/google_genai-1.19.0-py3-none-any.whl", hash = "sha256:a2955612e4af8c84f83eb43c1ce4e74e1b714732926d0705e639761938192466", size = 200043 },
]

[[package]]
name = "google-generativeai"
version = "0.8.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-ai-generativelanguage" },
    { name = "google-api-core" },
    { name = "google-api-python-client" },
    { name = "google-auth" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/40/c42ff9ded9f09ec9392879a8e6538a00b2dc185e834a3392917626255419/google_generativeai-0.8.5-py3-none-any.whl", hash = "sha256:22b420817fb263f8ed520b33285f45976d5b21e904da32b80d4fd20c055123a2", size = 155427 },
]

[[package]]
name = "google-resumable-media"
version = "2.7.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-crc32c" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/5a/0efdc02665dca14e0837b62c8a1a93132c264bd02054a15abb2218afe0ae/google_resumable_media-2.7.2.tar.gz", hash = "sha256:5280aed4629f2b60b847b0d42f9857fd4935c11af266744df33d8074cae92fe0", size = 2163099 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl", hash = "sha256:3ce7551e9fe6d99e9a126101d2536612bb73486721951e9562fee0f90c6ababa", size = 81251 },
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/39/24/33db22342cf4a2ea27c9955e6713140fedd51e8b141b5ce5260897020f1a/googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257", size = 145903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/f1/62a193f0227cf15a920390abe675f386dec35f7ae3ffe6da582d3ade42c7/googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8", size = 294530 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
]

[[package]]
name = "grpc-google-iam-v1"
version = "0.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos", extra = ["grpc"] },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/4e/8d0ca3b035e41fe0b3f31ebbb638356af720335e5a11154c330169b40777/grpc_google_iam_v1-0.14.2.tar.gz", hash = "sha256:b3e1fc387a1a329e41672197d0ace9de22c78dd7d215048c4c78712073f7bd20", size = 16259 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/66/6f/dd9b178aee7835b96c2e63715aba6516a9d50f6bebbd1cc1d32c82a2a6c3/grpc_google_iam_v1-0.14.2-py3-none-any.whl", hash = "sha256:a3171468459770907926d56a440b2bb643eec1d7ba215f48f3ecece42b4d8351", size = 19242 },
]

[[package]]
name = "grpcio"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/95/aa11fc09a85d91fbc7dd405dcb2a1e0256989d67bf89fa65ae24b3ba105a/grpcio-1.71.0.tar.gz", hash = "sha256:2b85f7820475ad3edec209d3d89a7909ada16caab05d3f2e08a7e8ae3200a55c", size = 12549828 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/dd/b00cbb45400d06b26126dcfdbdb34bb6c4f28c3ebbd7aea8228679103ef6/grpcio-1.71.0-cp313-cp313-linux_armv7l.whl", hash = "sha256:cebc1b34ba40a312ab480ccdb396ff3c529377a2fce72c45a741f7215bfe8379", size = 5184138 },
    { url = "https://files.pythonhosted.org/packages/ed/0a/4651215983d590ef53aac40ba0e29dda941a02b097892c44fa3357e706e5/grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl", hash = "sha256:85da336e3649a3d2171e82f696b5cad2c6231fdd5bad52616476235681bee5b3", size = 11310747 },
    { url = "https://files.pythonhosted.org/packages/57/a3/149615b247f321e13f60aa512d3509d4215173bdb982c9098d78484de216/grpcio-1.71.0-cp313-cp313-manylinux_2_17_aarch64.whl", hash = "sha256:f9a412f55bb6e8f3bb000e020dbc1e709627dcb3a56f6431fa7076b4c1aab0db", size = 5653991 },
    { url = "https://files.pythonhosted.org/packages/ca/56/29432a3e8d951b5e4e520a40cd93bebaa824a14033ea8e65b0ece1da6167/grpcio-1.71.0-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:47be9584729534660416f6d2a3108aaeac1122f6b5bdbf9fd823e11fe6fbaa29", size = 6312781 },
    { url = "https://files.pythonhosted.org/packages/a3/f8/286e81a62964ceb6ac10b10925261d4871a762d2a763fbf354115f9afc98/grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c9c80ac6091c916db81131d50926a93ab162a7e97e4428ffc186b6e80d6dda4", size = 5910479 },
    { url = "https://files.pythonhosted.org/packages/35/67/d1febb49ec0f599b9e6d4d0d44c2d4afdbed9c3e80deb7587ec788fcf252/grpcio-1.71.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:789d5e2a3a15419374b7b45cd680b1e83bbc1e52b9086e49308e2c0b5bbae6e3", size = 6013262 },
    { url = "https://files.pythonhosted.org/packages/a1/04/f9ceda11755f0104a075ad7163fc0d96e2e3a9fe25ef38adfc74c5790daf/grpcio-1.71.0-cp313-cp313-musllinux_1_1_i686.whl", hash = "sha256:1be857615e26a86d7363e8a163fade914595c81fec962b3d514a4b1e8760467b", size = 6643356 },
    { url = "https://files.pythonhosted.org/packages/fb/ce/236dbc3dc77cf9a9242adcf1f62538734ad64727fabf39e1346ad4bd5c75/grpcio-1.71.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:a76d39b5fafd79ed604c4be0a869ec3581a172a707e2a8d7a4858cb05a5a7637", size = 6186564 },
    { url = "https://files.pythonhosted.org/packages/10/fd/b3348fce9dd4280e221f513dd54024e765b21c348bc475516672da4218e9/grpcio-1.71.0-cp313-cp313-win32.whl", hash = "sha256:74258dce215cb1995083daa17b379a1a5a87d275387b7ffe137f1d5131e2cfbb", size = 3601890 },
    { url = "https://files.pythonhosted.org/packages/be/f8/db5d5f3fc7e296166286c2a397836b8b042f7ad1e11028d82b061701f0f7/grpcio-1.71.0-cp313-cp313-win_amd64.whl", hash = "sha256:22c3bc8d488c039a199f7a003a38cb7635db6656fa96437a8accde8322ce2366", size = 4273308 },
]

[[package]]
name = "grpcio-status"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos" },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d7/53/a911467bece076020456401f55a27415d2d70d3bc2c37af06b44ea41fc5c/grpcio_status-1.71.0.tar.gz", hash = "sha256:11405fed67b68f406b3f3c7c5ae5104a79d2d309666d10d61b152e91d28fb968", size = 13669 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ad/d6/31fbc43ff097d8c4c9fc3df741431b8018f67bf8dfbe6553a555f6e5f675/grpcio_status-1.71.0-py3-none-any.whl", hash = "sha256:843934ef8c09e3e858952887467f8256aac3910c55f077a359a65b2b3cde3e68", size = 14424 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "html-to-markdown"
version = "1.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1d/48/324d3d938e5ff635497965118df510f62725b72e8b378b8710c03b0dd014/html_to_markdown-1.3.3.tar.gz", hash = "sha256:ad4f992d65d96d53e49d0a56a2ae0c52ef606c17592d2d9a87f99e4632a4a9e3", size = 15491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a3/d0/b96f7e3579cada841657e5764bc294bd2abb6c1e1dbcfb88ecf7a63ea5d9/html_to_markdown-1.3.3-py3-none-any.whl", hash = "sha256:09325777400e561d2c5a1569f475f9434e70a6f8ed1b4866bba8d00906136495", size = 14951 },
]

[[package]]
name = "html5lib"
version = "1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
    { name = "webencodings" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ac/b6/b55c3f49042f1df3dcd422b7f224f939892ee94f22abcf503a9b7339eaf2/html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f", size = 272215 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d", size = 112173 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httplib2"
version = "0.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyparsing" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/ad/2371116b22d616c194aa25ec410c9c6c37f23599dcd590502b74db197584/httplib2-0.22.0.tar.gz", hash = "sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81", size = 351116 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl", hash = "sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc", size = 96854 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "zipp" },
]
sdist = { url = "https://files.pythonhosted.org/packages/76/66/650a33bd90f786193e4de4b3ad86ea60b53c89b669a5c7be931fac31cdb0/importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000", size = 56641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd", size = 27656 },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050 },
]

[[package]]
name = "ipython"
version = "9.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "decorator" },
    { name = "ipython-pygments-lexers" },
    { name = "jedi" },
    { name = "matplotlib-inline" },
    { name = "pexpect", marker = "sys_platform != 'emscripten' and sys_platform != 'win32'" },
    { name = "prompt-toolkit" },
    { name = "pygments" },
    { name = "stack-data" },
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/54/80/406f9e3bde1c1fd9bf5a0be9d090f8ae623e401b7670d8f6fdf2ab679891/ipython-9.4.0.tar.gz", hash = "sha256:c033c6d4e7914c3d9768aabe76bbe87ba1dc66a92a05db6bfa1125d81f2ee270", size = 4385338 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/63/f8/0031ee2b906a15a33d6bfc12dd09c3dfa966b3cb5b284ecfb7549e6ac3c4/ipython-9.4.0-py3-none-any.whl", hash = "sha256:25850f025a446d9b359e8d296ba175a36aedd32e83ca9b5060430fe16801f066", size = 611021 },
]

[[package]]
name = "ipython-pygments-lexers"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/4c/5dd1d8af08107f88c7f741ead7a40854b8ac24ddf9ae850afbcf698aa552/ipython_pygments_lexers-1.1.1.tar.gz", hash = "sha256:09c0138009e56b6854f9535736f4171d855c8c08a563a0dcd8022f78355c7e81", size = 8393 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl", hash = "sha256:a9462224a505ade19a605f71f8fa63c2048833ce50abc86768a0d81d876dc81c", size = 8074 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jedi"
version = "0.19.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "parso" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/3a/79a912fbd4d8dd6fbb02bf69afd3bb72cf0c729bb3063c6f4498603db17a/jedi-0.19.2.tar.gz", hash = "sha256:4770dc3de41bde3966b02eb84fbcf557fb33cce26ad23da12c742fb50ecb11f0", size = 1231287 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl", hash = "sha256:a8ef22bde8490f57fe5c7681a3c83cb58874daf72b4784de3cce5b6ef6edb5b9", size = 1572278 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "loro"
version = "1.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a0/32/ce94b1fc342ac90d9ca21bc6e90c727990734a75505cb893b2a71a364faf/loro-1.5.2.tar.gz", hash = "sha256:70e52acb16474f7c1e52aea2a7fe2771516f1e9f73d4edfe40f3193b122402c7", size = 62538 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/09/061e8cecb42f99856580811156d7651d5e8172bb840224c7cd2eb94a8730/loro-1.5.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:dbb94c104e3aba4ea3f1118c72896de978e737bb066a35051bf49895e72540a7", size = 3098320 },
    { url = "https://files.pythonhosted.org/packages/60/6e/96cb1a78869c8ae91e65d73ef4ee9f74bc16fd3baff5a7463f7702687dab/loro-1.5.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:847a10f493399f9b650b588b3d81893dfaa1e45e7091881268094f2b9f7df38b", size = 2882026 },
    { url = "https://files.pythonhosted.org/packages/eb/e7/2a131e3e8072614af1cc2970efc1c30a812eb8b0f5286c7b6b390ae3fc9f/loro-1.5.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:902215b77b35e58286d907e8292f78b014cd9c55a46bc5deb944f555509b7747", size = 3110094 },
    { url = "https://files.pythonhosted.org/packages/8c/63/34efc556a5a7663f045d64b9744c10f7b00386f252fac47c939f1c1795be/loro-1.5.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:19e8c9896348063721ef56631d2275c186faf63f6336079c57f41055c9cc1c30", size = 3202938 },
    { url = "https://files.pythonhosted.org/packages/67/3f/5a37b5f1bec5d633f469754e26bf0ce77a26f7697cd95d0b4a51b9cd90be/loro-1.5.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91e75cd4b26506bb5b564ed24b433147fc8b77e8779b5736bc4f3bfddf270590", size = 3579945 },
    { url = "https://files.pythonhosted.org/packages/78/b3/cd3202d6398524c5e1442688c6825e148eb953aa0de04952fd546c69a398/loro-1.5.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:41e54109599190dede34366476a8f42ae6e9fd7fd439823150e9f70e39d7d54e", size = 3318843 },
    { url = "https://files.pythonhosted.org/packages/a5/65/8ed127c827ed9b540f5660e9c98265702dbfdd71ad59063bd3c799ca0dda/loro-1.5.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd3f330795212f24b9dd710f952f7f7138ba86d6159f524025eb4627641ed4ef", size = 3243417 },
    { url = "https://files.pythonhosted.org/packages/4e/29/6894f6db7a1eb7d5d2936b658b3a26c4ea8ce6b0563dde024b909a63289d/loro-1.5.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ebdd716ce67c182f71a093c552f9a47428f7a3d93b038780bbb0f06779805d0", size = 3511123 },
    { url = "https://files.pythonhosted.org/packages/17/26/230867103d5ec58ef18f8d0bc169a4defb4f865f9969247d4e9c723ae10e/loro-1.5.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a8ac5ff8b697e9a828fe4387da715d78d0f2afcf23bbd76f5089b4122f5e78a3", size = 3256828 },
    { url = "https://files.pythonhosted.org/packages/79/8b/7aed297d9cc236e15674275364e37e938e9335c9dfad49ad35904fa8b1f3/loro-1.5.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3dce7920c45c9c884246898805b270d63550a5dec61d3f33274010c40127a37c", size = 3464838 },
    { url = "https://files.pythonhosted.org/packages/1d/c1/352fd39b61a842dc991bf95aaa75db34b6c353c1a3844da17e01f917deb5/loro-1.5.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:66afec16e22db99f1818906bc7cabda0cb077e0e493882b4c0983a8bc431413d", size = 3502790 },
    { url = "https://files.pythonhosted.org/packages/2c/11/859dfc28b1397d731d2cc710dae0e7cb1cbeb45ab70ec518b4ed4f690a4c/loro-1.5.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9f052715922592f099e9b6553fccb48761c5ad83deefcb0df55effde309eb12d", size = 3414408 },
    { url = "https://files.pythonhosted.org/packages/86/3e/fcd87311399e2eff892fb3a6b6f1d3307a2dfd99811fddf0889bee89d585/loro-1.5.2-cp313-cp313-win32.whl", hash = "sha256:978e9f6b0c9ad8c6b1ab70372eafbe00c41782522b216802cf961a81edd27561", size = 2580638 },
    { url = "https://files.pythonhosted.org/packages/93/06/dd73ca0865630923f18fa4486e66a171a0a26ae8e7541f1c3d93100f1f5b/loro-1.5.2-cp313-cp313-win_amd64.whl", hash = "sha256:3ecebbf9f5f880c6ca9a1628e5f469d3d67b67c1fd50536c52c5f6eae01be549", size = 2743550 },
    { url = "https://files.pythonhosted.org/packages/d2/70/9e5030bb9f1b86520f482605f660e5a192d6f5e56104fee122fe7d3dc72e/loro-1.5.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:354de426d6404cce252fb81be17a1589f1bd47197ba7f730f60fbb52452f49ab", size = 3106619 },
    { url = "https://files.pythonhosted.org/packages/2b/37/43c8e3fa8c6239be1b22c0dfd779a4ab000682dddebc23becd057668c436/loro-1.5.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:18e3b6f07483c5553795fea05c8d318f96c018909dd390c68b81701afb12cac3", size = 3195270 },
    { url = "https://files.pythonhosted.org/packages/b1/d6/8aaa433d08710cb1b95781d56efad366350082798463e35b5a6a4988b160/loro-1.5.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2298b96c5f533807373db27dbf5b10c88f1c5d9e0145feb952e7a813a81af645", size = 3575129 },
    { url = "https://files.pythonhosted.org/packages/51/4e/44425f11da9b5278653c3ca01cdfd4da850f94ead5843d8134043ac825cf/loro-1.5.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0aa8edef791c1b46e19bf86ab17f9dbefc61b8f1fbecc49054d5eb880380d897", size = 3317031 },
    { url = "https://files.pythonhosted.org/packages/3b/ae/af1713c7c3cc91a9d6cc1b812733665875eb30c22e4c9e0e213a9a69b1a2/loro-1.5.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:633c026cbb17c485de40f09aab13362f0c79140913dc67445606e3237092d70f", size = 3251501 },
    { url = "https://files.pythonhosted.org/packages/4b/df/958e8abb78ca47ce06e0088bc5d44b5945ffbd08503936cbc0340b62a5f3/loro-1.5.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:903fed16d40b0373f747ecc398f5b86aaab16c37b4c670f580c2c5301bad4de5", size = 3456858 },
    { url = "https://files.pythonhosted.org/packages/f1/f6/982af3432bde075f1fd3201de0e95f35a868f4e85cee36bb22bb0524b069/loro-1.5.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2f9f77b1f582d86e1a57cdb38a43ea1a5861a6f0d73783335c2efdc3d1dcb793", size = 3494470 },
    { url = "https://files.pythonhosted.org/packages/47/b3/a4725db48fb4c7637076023ccedf7dcb7f24a3d266208f2e2aafb8179861/loro-1.5.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:489230b2716c0a2ad50e205670abed029ba0787c028a62dd31226f7935f5d1fd", size = 3410923 },
]

[[package]]
name = "lxml"
version = "5.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/76/3d/14e82fc7c8fb1b7761f7e748fd47e2ec8276d137b6acfe5a4bb73853e08f/lxml-5.4.0.tar.gz", hash = "sha256:d12832e1dbea4be280b22fd0ea7c9b87f0d8fc51ba06e92dc62d52f804f78ebd", size = 3679479 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/cb/2ba1e9dd953415f58548506fa5549a7f373ae55e80c61c9041b7fd09a38a/lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:773e27b62920199c6197130632c18fb7ead3257fce1ffb7d286912e56ddb79e0", size = 8110086 },
    { url = "https://files.pythonhosted.org/packages/b5/3e/6602a4dca3ae344e8609914d6ab22e52ce42e3e1638c10967568c5c1450d/lxml-5.4.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ce9c671845de9699904b1e9df95acfe8dfc183f2310f163cdaa91a3535af95de", size = 4404613 },
    { url = "https://files.pythonhosted.org/packages/4c/72/bf00988477d3bb452bef9436e45aeea82bb40cdfb4684b83c967c53909c7/lxml-5.4.0-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9454b8d8200ec99a224df8854786262b1bd6461f4280064c807303c642c05e76", size = 5012008 },
    { url = "https://files.pythonhosted.org/packages/92/1f/93e42d93e9e7a44b2d3354c462cd784dbaaf350f7976b5d7c3f85d68d1b1/lxml-5.4.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cccd007d5c95279e529c146d095f1d39ac05139de26c098166c4beb9374b0f4d", size = 4760915 },
    { url = "https://files.pythonhosted.org/packages/45/0b/363009390d0b461cf9976a499e83b68f792e4c32ecef092f3f9ef9c4ba54/lxml-5.4.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0fce1294a0497edb034cb416ad3e77ecc89b313cff7adbee5334e4dc0d11f422", size = 5283890 },
    { url = "https://files.pythonhosted.org/packages/19/dc/6056c332f9378ab476c88e301e6549a0454dbee8f0ae16847414f0eccb74/lxml-5.4.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:24974f774f3a78ac12b95e3a20ef0931795ff04dbb16db81a90c37f589819551", size = 4812644 },
    { url = "https://files.pythonhosted.org/packages/ee/8a/f8c66bbb23ecb9048a46a5ef9b495fd23f7543df642dabeebcb2eeb66592/lxml-5.4.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:497cab4d8254c2a90bf988f162ace2ddbfdd806fce3bda3f581b9d24c852e03c", size = 4921817 },
    { url = "https://files.pythonhosted.org/packages/04/57/2e537083c3f381f83d05d9b176f0d838a9e8961f7ed8ddce3f0217179ce3/lxml-5.4.0-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:e794f698ae4c5084414efea0f5cc9f4ac562ec02d66e1484ff822ef97c2cadff", size = 4753916 },
    { url = "https://files.pythonhosted.org/packages/d8/80/ea8c4072109a350848f1157ce83ccd9439601274035cd045ac31f47f3417/lxml-5.4.0-cp313-cp313-manylinux_2_28_ppc64le.whl", hash = "sha256:2c62891b1ea3094bb12097822b3d44b93fc6c325f2043c4d2736a8ff09e65f60", size = 5289274 },
    { url = "https://files.pythonhosted.org/packages/b3/47/c4be287c48cdc304483457878a3f22999098b9a95f455e3c4bda7ec7fc72/lxml-5.4.0-cp313-cp313-manylinux_2_28_s390x.whl", hash = "sha256:142accb3e4d1edae4b392bd165a9abdee8a3c432a2cca193df995bc3886249c8", size = 4874757 },
    { url = "https://files.pythonhosted.org/packages/2f/04/6ef935dc74e729932e39478e44d8cfe6a83550552eaa072b7c05f6f22488/lxml-5.4.0-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:1a42b3a19346e5601d1b8296ff6ef3d76038058f311902edd574461e9c036982", size = 4947028 },
    { url = "https://files.pythonhosted.org/packages/cb/f9/c33fc8daa373ef8a7daddb53175289024512b6619bc9de36d77dca3df44b/lxml-5.4.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4291d3c409a17febf817259cb37bc62cb7eb398bcc95c1356947e2871911ae61", size = 4834487 },
    { url = "https://files.pythonhosted.org/packages/8d/30/fc92bb595bcb878311e01b418b57d13900f84c2b94f6eca9e5073ea756e6/lxml-5.4.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4f5322cf38fe0e21c2d73901abf68e6329dc02a4994e483adbcf92b568a09a54", size = 5381688 },
    { url = "https://files.pythonhosted.org/packages/43/d1/3ba7bd978ce28bba8e3da2c2e9d5ae3f8f521ad3f0ca6ea4788d086ba00d/lxml-5.4.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0be91891bdb06ebe65122aa6bf3fc94489960cf7e03033c6f83a90863b23c58b", size = 5242043 },
    { url = "https://files.pythonhosted.org/packages/ee/cd/95fa2201041a610c4d08ddaf31d43b98ecc4b1d74b1e7245b1abdab443cb/lxml-5.4.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:15a665ad90054a3d4f397bc40f73948d48e36e4c09f9bcffc7d90c87410e478a", size = 5021569 },
    { url = "https://files.pythonhosted.org/packages/2d/a6/31da006fead660b9512d08d23d31e93ad3477dd47cc42e3285f143443176/lxml-5.4.0-cp313-cp313-win32.whl", hash = "sha256:d5663bc1b471c79f5c833cffbc9b87d7bf13f87e055a5c86c363ccd2348d7e82", size = 3485270 },
    { url = "https://files.pythonhosted.org/packages/fc/14/c115516c62a7d2499781d2d3d7215218c0731b2c940753bf9f9b7b73924d/lxml-5.4.0-cp313-cp313-win_amd64.whl", hash = "sha256:bcb7a1096b4b6b24ce1ac24d4942ad98f983cd3810f9711bcd0293f43a9d8b9f", size = 3814606 },
]

[[package]]
name = "marimo"
version = "0.14.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "docutils" },
    { name = "itsdangerous" },
    { name = "jedi" },
    { name = "loro" },
    { name = "markdown" },
    { name = "narwhals" },
    { name = "packaging" },
    { name = "psutil" },
    { name = "pygments" },
    { name = "pymdown-extensions" },
    { name = "pyyaml" },
    { name = "starlette" },
    { name = "tomlkit" },
    { name = "uvicorn" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0b/6d/8c0bdb68d608561e3039718f171ede292e7da7e7580a51b1f4b2ce6e204f/marimo-0.14.12.tar.gz", hash = "sha256:cf18513e30a5d2e8864930885b674dd89cbc9ad3a5e128b9ecfa48323de6d14f", size = 29622446 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/fa/d802cd61fb4714c17529057dc4b07d48c3e115d0af331907b3d19f5482f6/marimo-0.14.12-py3-none-any.whl", hash = "sha256:154d168ceb8b9f4cc10f8cd9f6299cf0c5d8643b0291370a9e64a88b2f517ed3", size = 30118091 },
]

[[package]]
name = "markdown"
version = "3.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/c2/4ab49206c17f75cb08d6311171f2d65798988db4360c4d1485bd0eedd67c/markdown-3.8.2.tar.gz", hash = "sha256:247b9a70dd12e27f67431ce62523e675b866d254f900c4fe75ce3dda62237c45", size = 362071 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl", hash = "sha256:5c83764dbd4e00bdd94d85a19b8d55ccca20fe35b2e678a1422b380324dd5f24", size = 106827 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "markdownify"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2f/78/c48fed23c7aebc2c16049062e72de1da3220c274de59d28c942acdc9ffb2/markdownify-1.1.0.tar.gz", hash = "sha256:449c0bbbf1401c5112379619524f33b63490a8fa479456d41de9dc9e37560ebd", size = 17127 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/11/b751af7ad41b254a802cf52f7bc1fca7cabe2388132f2ce60a1a6b9b9622/markdownify-1.1.0-py3-none-any.whl", hash = "sha256:32a5a08e9af02c8a6528942224c91b933b4bd2c7d078f9012943776fc313eeef", size = 13901 },
]

[[package]]
name = "matplotlib-inline"
version = "0.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/5b/a36a337438a14116b16480db471ad061c36c3694df7c2084a0da7ba538b7/matplotlib_inline-0.1.7.tar.gz", hash = "sha256:8423b23ec666be3d16e16b60bdd8ac4e86e840ebd1dd11a30b9f117f2fa0ab90", size = 8159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/8e/9ad090d3553c280a8060fbf6e24dc1c0c29704ee7d1c372f0c174aa59285/matplotlib_inline-0.1.7-py3-none-any.whl", hash = "sha256:df192d39a4ff8f21b1895d72e6a13f5fcc5099f00fa84384e0ea28c2cc0653ca", size = 9899 },
]

[[package]]
name = "mcp"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/25/ae/588691c45b38f4fbac07fa3d6d50cea44cc6b35d16ddfdf26e17a0467ab2/mcp-1.7.1.tar.gz", hash = "sha256:eb4f1f53bd717f75dda8a1416e00804b831a8f3c331e23447a03b78f04b43a6e", size = 230903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ae/79/fe0e20c3358997a80911af51bad927b5ea2f343ef95ab092b19c9cc48b59/mcp-1.7.1-py3-none-any.whl", hash = "sha256:f7e6108977db6d03418495426c7ace085ba2341b75197f8727f96f9cfd30057a", size = 100365 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "multidict"
version = "6.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/2f/a3470242707058fe856fe59241eee5635d79087100b7042a867368863a27/multidict-6.4.4.tar.gz", hash = "sha256:69ee9e6ba214b5245031b76233dd95408a0fd57fdb019ddcc1ead4790932a8e8", size = 90183 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/2a/e166d2ffbf4b10131b2d5b0e458f7cee7d986661caceae0de8753042d4b2/multidict-6.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:82ffabefc8d84c2742ad19c37f02cde5ec2a1ee172d19944d380f920a340e4b9", size = 64123 },
    { url = "https://files.pythonhosted.org/packages/8c/96/e200e379ae5b6f95cbae472e0199ea98913f03d8c9a709f42612a432932c/multidict-6.4.4-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6a2f58a66fe2c22615ad26156354005391e26a2f3721c3621504cd87c1ea87bf", size = 38049 },
    { url = "https://files.pythonhosted.org/packages/75/fb/47afd17b83f6a8c7fa863c6d23ac5ba6a0e6145ed8a6bcc8da20b2b2c1d2/multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5883d6ee0fd9d8a48e9174df47540b7545909841ac82354c7ae4cbe9952603bd", size = 37078 },
    { url = "https://files.pythonhosted.org/packages/fa/70/1af3143000eddfb19fd5ca5e78393985ed988ac493bb859800fe0914041f/multidict-6.4.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9abcf56a9511653fa1d052bfc55fbe53dbee8f34e68bd6a5a038731b0ca42d15", size = 224097 },
    { url = "https://files.pythonhosted.org/packages/b1/39/d570c62b53d4fba844e0378ffbcd02ac25ca423d3235047013ba2f6f60f8/multidict-6.4.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:6ed5ae5605d4ad5a049fad2a28bb7193400700ce2f4ae484ab702d1e3749c3f9", size = 230768 },
    { url = "https://files.pythonhosted.org/packages/fd/f8/ed88f2c4d06f752b015933055eb291d9bc184936903752c66f68fb3c95a7/multidict-6.4.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bbfcb60396f9bcfa63e017a180c3105b8c123a63e9d1428a36544e7d37ca9e20", size = 231331 },
    { url = "https://files.pythonhosted.org/packages/9c/6f/8e07cffa32f483ab887b0d56bbd8747ac2c1acd00dc0af6fcf265f4a121e/multidict-6.4.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b0f1987787f5f1e2076b59692352ab29a955b09ccc433c1f6b8e8e18666f608b", size = 230169 },
    { url = "https://files.pythonhosted.org/packages/e6/2b/5dcf173be15e42f330110875a2668ddfc208afc4229097312212dc9c1236/multidict-6.4.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1d0121ccce8c812047d8d43d691a1ad7641f72c4f730474878a5aeae1b8ead8c", size = 222947 },
    { url = "https://files.pythonhosted.org/packages/39/75/4ddcbcebe5ebcd6faa770b629260d15840a5fc07ce8ad295a32e14993726/multidict-6.4.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:83ec4967114295b8afd120a8eec579920c882831a3e4c3331d591a8e5bfbbc0f", size = 215761 },
    { url = "https://files.pythonhosted.org/packages/6a/c9/55e998ae45ff15c5608e384206aa71a11e1b7f48b64d166db400b14a3433/multidict-6.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:995f985e2e268deaf17867801b859a282e0448633f1310e3704b30616d269d69", size = 227605 },
    { url = "https://files.pythonhosted.org/packages/04/49/c2404eac74497503c77071bd2e6f88c7e94092b8a07601536b8dbe99be50/multidict-6.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:d832c608f94b9f92a0ec8b7e949be7792a642b6e535fcf32f3e28fab69eeb046", size = 226144 },
    { url = "https://files.pythonhosted.org/packages/62/c5/0cd0c3c6f18864c40846aa2252cd69d308699cb163e1c0d989ca301684da/multidict-6.4.4-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d21c1212171cf7da703c5b0b7a0e85be23b720818aef502ad187d627316d5645", size = 221100 },
    { url = "https://files.pythonhosted.org/packages/71/7b/f2f3887bea71739a046d601ef10e689528d4f911d84da873b6be9194ffea/multidict-6.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:cbebaa076aaecad3d4bb4c008ecc73b09274c952cf6a1b78ccfd689e51f5a5b0", size = 232731 },
    { url = "https://files.pythonhosted.org/packages/e5/b3/d9de808349df97fa75ec1372758701b5800ebad3c46ae377ad63058fbcc6/multidict-6.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:c93a6fb06cc8e5d3628b2b5fda215a5db01e8f08fc15fadd65662d9b857acbe4", size = 229637 },
    { url = "https://files.pythonhosted.org/packages/5e/57/13207c16b615eb4f1745b44806a96026ef8e1b694008a58226c2d8f5f0a5/multidict-6.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8cd8f81f1310182362fb0c7898145ea9c9b08a71081c5963b40ee3e3cac589b1", size = 225594 },
    { url = "https://files.pythonhosted.org/packages/3a/e4/d23bec2f70221604f5565000632c305fc8f25ba953e8ce2d8a18842b9841/multidict-6.4.4-cp313-cp313-win32.whl", hash = "sha256:3e9f1cd61a0ab857154205fb0b1f3d3ace88d27ebd1409ab7af5096e409614cd", size = 35359 },
    { url = "https://files.pythonhosted.org/packages/a7/7a/cfe1a47632be861b627f46f642c1d031704cc1c0f5c0efbde2ad44aa34bd/multidict-6.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:8ffb40b74400e4455785c2fa37eba434269149ec525fc8329858c862e4b35373", size = 38903 },
    { url = "https://files.pythonhosted.org/packages/68/7b/15c259b0ab49938a0a1c8f3188572802704a779ddb294edc1b2a72252e7c/multidict-6.4.4-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:6a602151dbf177be2450ef38966f4be3467d41a86c6a845070d12e17c858a156", size = 68895 },
    { url = "https://files.pythonhosted.org/packages/f1/7d/168b5b822bccd88142e0a3ce985858fea612404edd228698f5af691020c9/multidict-6.4.4-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d2b9712211b860d123815a80b859075d86a4d54787e247d7fbee9db6832cf1c", size = 40183 },
    { url = "https://files.pythonhosted.org/packages/e0/b7/d4b8d98eb850ef28a4922ba508c31d90715fd9b9da3801a30cea2967130b/multidict-6.4.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:d2fa86af59f8fc1972e121ade052145f6da22758f6996a197d69bb52f8204e7e", size = 39592 },
    { url = "https://files.pythonhosted.org/packages/18/28/a554678898a19583548e742080cf55d169733baf57efc48c2f0273a08583/multidict-6.4.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50855d03e9e4d66eab6947ba688ffb714616f985838077bc4b490e769e48da51", size = 226071 },
    { url = "https://files.pythonhosted.org/packages/ee/dc/7ba6c789d05c310e294f85329efac1bf5b450338d2542498db1491a264df/multidict-6.4.4-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5bce06b83be23225be1905dcdb6b789064fae92499fbc458f59a8c0e68718601", size = 222597 },
    { url = "https://files.pythonhosted.org/packages/24/4f/34eadbbf401b03768dba439be0fb94b0d187facae9142821a3d5599ccb3b/multidict-6.4.4-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:66ed0731f8e5dfd8369a883b6e564aca085fb9289aacabd9decd70568b9a30de", size = 228253 },
    { url = "https://files.pythonhosted.org/packages/c0/e6/493225a3cdb0d8d80d43a94503fc313536a07dae54a3f030d279e629a2bc/multidict-6.4.4-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:329ae97fc2f56f44d91bc47fe0972b1f52d21c4b7a2ac97040da02577e2daca2", size = 226146 },
    { url = "https://files.pythonhosted.org/packages/2f/70/e411a7254dc3bff6f7e6e004303b1b0591358e9f0b7c08639941e0de8bd6/multidict-6.4.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c27e5dcf520923d6474d98b96749e6805f7677e93aaaf62656005b8643f907ab", size = 220585 },
    { url = "https://files.pythonhosted.org/packages/08/8f/beb3ae7406a619100d2b1fb0022c3bb55a8225ab53c5663648ba50dfcd56/multidict-6.4.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:058cc59b9e9b143cc56715e59e22941a5d868c322242278d28123a5d09cdf6b0", size = 212080 },
    { url = "https://files.pythonhosted.org/packages/9c/ec/355124e9d3d01cf8edb072fd14947220f357e1c5bc79c88dff89297e9342/multidict-6.4.4-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:69133376bc9a03f8c47343d33f91f74a99c339e8b58cea90433d8e24bb298031", size = 226558 },
    { url = "https://files.pythonhosted.org/packages/fd/22/d2b95cbebbc2ada3be3812ea9287dcc9712d7f1a012fad041770afddb2ad/multidict-6.4.4-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d6b15c55721b1b115c5ba178c77104123745b1417527ad9641a4c5e2047450f0", size = 212168 },
    { url = "https://files.pythonhosted.org/packages/4d/c5/62bfc0b2f9ce88326dbe7179f9824a939c6c7775b23b95de777267b9725c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:a887b77f51d3d41e6e1a63cf3bc7ddf24de5939d9ff69441387dfefa58ac2e26", size = 217970 },
    { url = "https://files.pythonhosted.org/packages/79/74/977cea1aadc43ff1c75d23bd5bc4768a8fac98c14e5878d6ee8d6bab743c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:632a3bf8f1787f7ef7d3c2f68a7bde5be2f702906f8b5842ad6da9d974d0aab3", size = 226980 },
    { url = "https://files.pythonhosted.org/packages/48/fc/cc4a1a2049df2eb84006607dc428ff237af38e0fcecfdb8a29ca47b1566c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:a145c550900deb7540973c5cdb183b0d24bed6b80bf7bddf33ed8f569082535e", size = 220641 },
    { url = "https://files.pythonhosted.org/packages/3b/6a/a7444d113ab918701988d4abdde373dbdfd2def7bd647207e2bf645c7eac/multidict-6.4.4-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cc5d83c6619ca5c9672cb78b39ed8542f1975a803dee2cda114ff73cbb076edd", size = 221728 },
    { url = "https://files.pythonhosted.org/packages/2b/b0/fdf4c73ad1c55e0f4dbbf2aa59dd37037334091f9a4961646d2b7ac91a86/multidict-6.4.4-cp313-cp313t-win32.whl", hash = "sha256:3312f63261b9df49be9d57aaa6abf53a6ad96d93b24f9cc16cf979956355ce6e", size = 41913 },
    { url = "https://files.pythonhosted.org/packages/8e/92/27989ecca97e542c0d01d05a98a5ae12198a243a9ee12563a0313291511f/multidict-6.4.4-cp313-cp313t-win_amd64.whl", hash = "sha256:ba852168d814b2c73333073e1c7116d9395bea69575a01b0b3c89d2d5a87c8fb", size = 46112 },
    { url = "https://files.pythonhosted.org/packages/84/5d/e17845bb0fa76334477d5de38654d27946d5b5d3695443987a094a71b440/multidict-6.4.4-py3-none-any.whl", hash = "sha256:bd4557071b561a8b3b6075c3ce93cf9bfb6182cb241805c3d66ced3b75eff4ac", size = 10481 },
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/6e/371856a3fb9d31ca8dac321cda606860fa4548858c0cc45d9d1d4ca2628b/mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558", size = 6343 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505", size = 4963 },
]

[[package]]
name = "narwhals"
version = "1.48.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/cd/7395d6c247e821cba6243e9f7ed202fae3fefef643c96581b5ecab927bad/narwhals-1.48.0.tar.gz", hash = "sha256:7243b456cbdb60edb148731a8f9b203f473a373a249ad66c699362508730e63f", size = 515112 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/75/72/5406044d4c251f3d8f78cec05b74839d0332d34c9e94b59120f3697ecf48/narwhals-1.48.0-py3-none-any.whl", hash = "sha256:2bbddc3adeed0c5b15ead8fe61f1d5e459f00c1d2fa60921e52a0f9bdc06077d", size = 376866 },
]

[[package]]
name = "numpy"
version = "2.2.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dc/b2/ce4b867d8cd9c0ee84938ae1e6a6f7926ebf928c9090d036fc3c6a04f946/numpy-2.2.5.tar.gz", hash = "sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291", size = 20273920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e2/a0/0aa7f0f4509a2e07bd7a509042967c2fab635690d4f48c6c7b3afd4f448c/numpy-2.2.5-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4", size = 20935102 },
    { url = "https://files.pythonhosted.org/packages/7e/e4/a6a9f4537542912ec513185396fce52cdd45bdcf3e9d921ab02a93ca5aa9/numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f", size = 14191709 },
    { url = "https://files.pythonhosted.org/packages/be/65/72f3186b6050bbfe9c43cb81f9df59ae63603491d36179cf7a7c8d216758/numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9", size = 5149173 },
    { url = "https://files.pythonhosted.org/packages/e5/e9/83e7a9432378dde5802651307ae5e9ea07bb72b416728202218cd4da2801/numpy-2.2.5-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191", size = 6684502 },
    { url = "https://files.pythonhosted.org/packages/ea/27/b80da6c762394c8ee516b74c1f686fcd16c8f23b14de57ba0cad7349d1d2/numpy-2.2.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372", size = 14084417 },
    { url = "https://files.pythonhosted.org/packages/aa/fc/ebfd32c3e124e6a1043e19c0ab0769818aa69050ce5589b63d05ff185526/numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d", size = 16133807 },
    { url = "https://files.pythonhosted.org/packages/bf/9b/4cc171a0acbe4666f7775cfd21d4eb6bb1d36d3a0431f48a73e9212d2278/numpy-2.2.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7", size = 15575611 },
    { url = "https://files.pythonhosted.org/packages/a3/45/40f4135341850df48f8edcf949cf47b523c404b712774f8855a64c96ef29/numpy-2.2.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73", size = 17895747 },
    { url = "https://files.pythonhosted.org/packages/f8/4c/b32a17a46f0ffbde8cc82df6d3daeaf4f552e346df143e1b188a701a8f09/numpy-2.2.5-cp313-cp313-win32.whl", hash = "sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b", size = 6309594 },
    { url = "https://files.pythonhosted.org/packages/13/ae/72e6276feb9ef06787365b05915bfdb057d01fceb4a43cb80978e518d79b/numpy-2.2.5-cp313-cp313-win_amd64.whl", hash = "sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471", size = 12638356 },
    { url = "https://files.pythonhosted.org/packages/79/56/be8b85a9f2adb688e7ded6324e20149a03541d2b3297c3ffc1a73f46dedb/numpy-2.2.5-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6", size = 20963778 },
    { url = "https://files.pythonhosted.org/packages/ff/77/19c5e62d55bff507a18c3cdff82e94fe174957bad25860a991cac719d3ab/numpy-2.2.5-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba", size = 14207279 },
    { url = "https://files.pythonhosted.org/packages/75/22/aa11f22dc11ff4ffe4e849d9b63bbe8d4ac6d5fae85ddaa67dfe43be3e76/numpy-2.2.5-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133", size = 5199247 },
    { url = "https://files.pythonhosted.org/packages/4f/6c/12d5e760fc62c08eded0394f62039f5a9857f758312bf01632a81d841459/numpy-2.2.5-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376", size = 6711087 },
    { url = "https://files.pythonhosted.org/packages/ef/94/ece8280cf4218b2bee5cec9567629e61e51b4be501e5c6840ceb593db945/numpy-2.2.5-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19", size = 14059964 },
    { url = "https://files.pythonhosted.org/packages/39/41/c5377dac0514aaeec69115830a39d905b1882819c8e65d97fc60e177e19e/numpy-2.2.5-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0", size = 16121214 },
    { url = "https://files.pythonhosted.org/packages/db/54/3b9f89a943257bc8e187145c6bc0eb8e3d615655f7b14e9b490b053e8149/numpy-2.2.5-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a", size = 15575788 },
    { url = "https://files.pythonhosted.org/packages/b1/c4/2e407e85df35b29f79945751b8f8e671057a13a376497d7fb2151ba0d290/numpy-2.2.5-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066", size = 17893672 },
    { url = "https://files.pythonhosted.org/packages/29/7e/d0b44e129d038dba453f00d0e29ebd6eaf2f06055d72b95b9947998aca14/numpy-2.2.5-cp313-cp313t-win32.whl", hash = "sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e", size = 6377102 },
    { url = "https://files.pythonhosted.org/packages/63/be/b85e4aa4bf42c6502851b971f1c326d583fcc68227385f92089cf50a7b45/numpy-2.2.5-cp313-cp313t-win_amd64.whl", hash = "sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8", size = 12750096 },
]

[[package]]
name = "openai"
version = "1.84.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/a3/128caf24e116f48fad3e4d5122cdf84db06c5127911849d51663c66158c8/openai-1.84.0.tar.gz", hash = "sha256:4caa43bdab262cc75680ce1a2322cfc01626204074f7e8d9939ab372acf61698", size = 467066 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/10/f245db006a860dbc1f2e2c8382e0a1762c7753e7971ba43a1dc3f3ec1404/openai-1.84.0-py3-none-any.whl", hash = "sha256:7ec4436c3c933d68dc0f5a0cef0cb3dbc0864a54d62bddaf2ed5f3d521844711", size = 725512 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "parso"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/66/94/68e2e17afaa9169cf6412ab0f28623903be73d1b32e208d9e8e541bb086d/parso-0.8.4.tar.gz", hash = "sha256:eb3a7b58240fb99099a345571deecc0f9540ea5f4dd2fe14c2a99d6b281ab92d", size = 400609 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c6/ac/dac4a63f978e4dcb3c6d3a78c4d8e0192a113d288502a1216950c41b1027/parso-0.8.4-py2.py3-none-any.whl", hash = "sha256:a418670a20291dacd2dddc80c377c5c3791378ee1e8d12bffc35420643d43f18", size = 103650 },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191 },
]

[[package]]
name = "pexpect"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "ptyprocess" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/92/cc564bf6381ff43ce1f4d06852fc19a2f11d180f23dc32d9588bee2f149d/pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f", size = 166450 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523", size = 63772 },
]

[[package]]
name = "platformdirs"
version = "4.3.8"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/8b/3c73abc9c759ecd3f1f7ceff6685840859e8070c4d947c93fae71f6a0bf2/platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc", size = 21362 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/39/979e8e21520d4e47a0bbe349e2713c0aac6f3d853d0e5b34d76206c439aa/platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4", size = 18567 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "proto-plus"
version = "1.26.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f4/ac/87285f15f7cce6d4a008f33f1757fb5a13611ea8914eb58c3d0d26243468/proto_plus-1.26.1.tar.gz", hash = "sha256:21a515a4c4c0088a773899e23c7bbade3d18f9c66c73edd4c7ee3816bc96a012", size = 56142 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl", hash = "sha256:13285478c2dcf2abb829db158e1047e2f1e8d63a077d94263c2b88b043c75a66", size = 50163 },
]

[[package]]
name = "protobuf"
version = "5.29.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/17/7d/b9dca7365f0e2c4fa7c193ff795427cfa6290147e5185ab11ece280a18e7/protobuf-5.29.4.tar.gz", hash = "sha256:4f1dfcd7997b31ef8f53ec82781ff434a28bf71d9102ddde14d076adcfc78c99", size = 424902 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/b2/043a1a1a20edd134563699b0e91862726a0dc9146c090743b6c44d798e75/protobuf-5.29.4-cp310-abi3-win32.whl", hash = "sha256:13eb236f8eb9ec34e63fc8b1d6efd2777d062fa6aaa68268fb67cf77f6839ad7", size = 422709 },
    { url = "https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl", hash = "sha256:bcefcdf3976233f8a502d265eb65ea740c989bacc6c30a58290ed0e519eb4b8d", size = 434506 },
    { url = "https://files.pythonhosted.org/packages/46/de/7c126bbb06aa0f8a7b38aaf8bd746c514d70e6a2a3f6dd460b3b7aad7aae/protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:307ecba1d852ec237e9ba668e087326a67564ef83e45a0189a772ede9e854dd0", size = 417826 },
    { url = "https://files.pythonhosted.org/packages/a2/b5/bade14ae31ba871a139aa45e7a8183d869efe87c34a4850c87b936963261/protobuf-5.29.4-cp38-abi3-manylinux2014_aarch64.whl", hash = "sha256:aec4962f9ea93c431d5714ed1be1c93f13e1a8618e70035ba2b0564d9e633f2e", size = 319574 },
    { url = "https://files.pythonhosted.org/packages/46/88/b01ed2291aae68b708f7d334288ad5fb3e7aa769a9c309c91a0d55cb91b0/protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl", hash = "sha256:d7d3f7d1d5a66ed4942d4fefb12ac4b14a29028b209d4bfb25c68ae172059922", size = 319672 },
    { url = "https://files.pythonhosted.org/packages/12/fb/a586e0c973c95502e054ac5f81f88394f24ccc7982dac19c515acd9e2c93/protobuf-5.29.4-py3-none-any.whl", hash = "sha256:3fde11b505e1597f71b875ef2fc52062b6a9740e5f7c8997ce878b6009145862", size = 172551 },
]

[[package]]
name = "psutil"
version = "7.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2a/80/336820c1ad9286a4ded7e845b2eccfcb27851ab8ac6abece774a6ff4d3de/psutil-7.0.0.tar.gz", hash = "sha256:7be9c3eba38beccb6495ea33afd982a44074b78f28c434a1f51cc07fd315c456", size = 497003 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ed/e6/2d26234410f8b8abdbf891c9da62bee396583f713fb9f3325a4760875d22/psutil-7.0.0-cp36-abi3-macosx_10_9_x86_64.whl", hash = "sha256:101d71dc322e3cffd7cea0650b09b3d08b8e7c4109dd6809fe452dfd00e58b25", size = 238051 },
    { url = "https://files.pythonhosted.org/packages/04/8b/30f930733afe425e3cbfc0e1468a30a18942350c1a8816acfade80c005c4/psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl", hash = "sha256:39db632f6bb862eeccf56660871433e111b6ea58f2caea825571951d4b6aa3da", size = 239535 },
    { url = "https://files.pythonhosted.org/packages/2a/ed/d362e84620dd22876b55389248e522338ed1bf134a5edd3b8231d7207f6d/psutil-7.0.0-cp36-abi3-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1fcee592b4c6f146991ca55919ea3d1f8926497a713ed7faaf8225e174581e91", size = 275004 },
    { url = "https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b1388a4f6875d7e2aff5c4ca1cc16c545ed41dd8bb596cefea80111db353a34", size = 277986 },
    { url = "https://files.pythonhosted.org/packages/eb/a2/709e0fe2f093556c17fbafda93ac032257242cabcc7ff3369e2cb76a97aa/psutil-7.0.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5f098451abc2828f7dc6b58d44b532b22f2088f4999a937557b603ce72b1993", size = 279544 },
    { url = "https://files.pythonhosted.org/packages/50/e6/eecf58810b9d12e6427369784efe814a1eec0f492084ce8eb8f4d89d6d61/psutil-7.0.0-cp37-abi3-win32.whl", hash = "sha256:ba3fcef7523064a6c9da440fc4d6bd07da93ac726b5733c29027d7dc95b39d99", size = 241053 },
    { url = "https://files.pythonhosted.org/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:4cf3d4eb1aa9b348dec30105c55cd9b7d4629285735a102beb4441e38db90553", size = 244885 },
]

[[package]]
name = "ptyprocess"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/20/e5/16ff212c1e452235a90aeb09066144d0c5a6a8c0834397e03f5224495c4e/ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220", size = 70762 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35", size = 13993 },
]

[[package]]
name = "pure-eval"
version = "0.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/05/0a34433a064256a578f1783a10da6df098ceaa4a57bbeaa96a6c0352786b/pure_eval-0.2.3.tar.gz", hash = "sha256:5f4e983f40564c576c7c8635ae88db5956bb2229d7e9237d03b3c0b0190eaf42", size = 19752 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8e/37/efad0257dc6e593a18957422533ff0f87ede7c9c6ea010a2177d738fb82f/pure_eval-0.2.3-py3-none-any.whl", hash = "sha256:1db8e35b67b3d218d818ae653e27f06c3aa420901fa7b081ca98cbedc874e0d0", size = 11842 },
]

[[package]]
name = "pyasn1"
version = "0.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ba/e9/01f1a64245b89f039897cb0130016d79f77d52669aae6ee7b159a6c4c018/pyasn1-0.6.1.tar.gz", hash = "sha256:6f580d2bdd84365380830acf45550f2511469f673cb4a5ae3857a3170128b034", size = 145322 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl", hash = "sha256:0d632f46f2ba09143da3a8afe9e33fb6f92fa2320ab7e886e2d0f7672af84629", size = 83135 },
]

[[package]]
name = "pyasn1-modules"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e9/e6/78ebbb10a8c8e4b61a59249394a4a594c1a7af95593dc933a349c8d00964/pyasn1_modules-0.4.2.tar.gz", hash = "sha256:677091de870a80aae844b1ca6134f54652fa2c8c5a52aa396440ac3106e941e6", size = 307892 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl", hash = "sha256:29253a9207ce32b64c3ac6600edc75368f98473906e8fd1043bd6b5b1de2c14a", size = 181259 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pymdown-extensions"
version = "10.16"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown" },
    { name = "pyyaml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/0a/c06b542ac108bfc73200677309cd9188a3a01b127a63f20cadc18d873d88/pymdown_extensions-10.16.tar.gz", hash = "sha256:71dac4fca63fabeffd3eb9038b756161a33ec6e8d230853d3cecf562155ab3de", size = 853197 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/d4/10bb14004d3c792811e05e21b5e5dcae805aacb739bd12a0540967b99592/pymdown_extensions-10.16-py3-none-any.whl", hash = "sha256:f5dd064a4db588cb2d95229fc4ee63a1b16cc8b4d0e6145c0899ed8723da1df2", size = 266143 },
]

[[package]]
name = "pyparsing"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/22/f1129e69d94ffff626bdb5c835506b3a5b4f3d070f17ea295e12c2c6f60f/pyparsing-3.2.3.tar.gz", hash = "sha256:b9c13f1ab8b3b542f72e28f634bad4de758ab3ce4546e4301970ad6fa77c38be", size = 1088608 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/e7/df2285f3d08fee213f2d041540fa4fc9ca6c2d44cf36d3a035bf2a8d2bcc/pyparsing-3.2.3-py3-none-any.whl", hash = "sha256:a749938e02d6fd0b59b356ca504a24982314bb090c383e3cf201c95ef7e2bfcf", size = 111120 },
]

[[package]]
name = "pyperclip"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz", hash = "sha256:b7de0142ddc81bfc5c7507eea19da920b92252b548b96186caf94a5e2527d310", size = 20961 }

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "pytest-asyncio"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d0/d4/14f53324cb1a6381bef29d698987625d80052bb33932d8e7cbf9b337b17c/pytest_asyncio-1.0.0.tar.gz", hash = "sha256:d15463d13f4456e1ead2594520216b225a16f781e144f8fdf6c5bb4667c48b3f", size = 46960 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/05/ce271016e351fddc8399e546f6e23761967ee09c8c568bbfbecb0c150171/pytest_asyncio-1.0.0-py3-none-any.whl", hash = "sha256:4f024da9f1ef945e680dc68610b52550e36590a67fd31bb3b4943979a1f90ef3", size = 15976 },
]

[[package]]
name = "pytest-xdist"
version = "3.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "execnet" },
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/dc/865845cfe987b21658e871d16e0a24e871e00884c545f246dd8f6f69edda/pytest_xdist-3.7.0.tar.gz", hash = "sha256:f9248c99a7c15b7d2f90715df93610353a485827bc06eefb6566d23f6400f126", size = 87550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/b2/0e802fde6f1c5b2f7ae7e9ad42b83fd4ecebac18a8a8c2f2f14e39dce6e1/pytest_xdist-3.7.0-py3-none-any.whl", hash = "sha256:7d3fbd255998265052435eb9daa4e99b62e6fb9cfb6efd1f858d4d8c0c7f0ca0", size = 46142 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-json-logger"
version = "3.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz", hash = "sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84", size = 16642 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl", hash = "sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7", size = 15163 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "readabilipy"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "html5lib" },
    { name = "lxml" },
    { name = "regex" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b8/e4/260a202516886c2e0cc6e6ae96d1f491792d829098886d9529a2439fbe8e/readabilipy-0.3.0.tar.gz", hash = "sha256:e13313771216953935ac031db4234bdb9725413534bfb3c19dbd6caab0887ae0", size = 35491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dd/46/8a640c6de1a6c6af971f858b2fb178ca5e1db91f223d8ba5f40efe1491e5/readabilipy-0.3.0-py3-none-any.whl", hash = "sha256:d106da0fad11d5fdfcde21f5c5385556bfa8ff0258483037d39ea6b1d6db3943", size = 22158 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "rsa"
version = "4.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/8a/22b7beea3ee0d44b1916c0c1cb0ee3af23b700b6da9f04991899d0c555d4/rsa-4.9.1.tar.gz", hash = "sha256:e7bdbfdb5497da4c07dfd35530e1a902659db6ff241e39d9953cad06ebd0ae75", size = 29034 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl", hash = "sha256:68635866661c6836b8d39430f97a996acbd61bfa49406748ea243539fe239762", size = 34696 },
]

[[package]]
name = "ruff"
version = "0.11.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ed/da/9c6f995903b4d9474b39da91d2d626659af3ff1eeb43e9ae7c119349dba6/ruff-0.11.13.tar.gz", hash = "sha256:26fa247dc68d1d4e72c179e08889a25ac0c7ba4d78aecfc835d49cbfd60bf514", size = 4282054 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7d/ce/a11d381192966e0b4290842cc8d4fac7dc9214ddf627c11c1afff87da29b/ruff-0.11.13-py3-none-linux_armv6l.whl", hash = "sha256:4bdfbf1240533f40042ec00c9e09a3aade6f8c10b6414cf11b519488d2635d46", size = 10292516 },
    { url = "https://files.pythonhosted.org/packages/78/db/87c3b59b0d4e753e40b6a3b4a2642dfd1dcaefbff121ddc64d6c8b47ba00/ruff-0.11.13-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:aef9c9ed1b5ca28bb15c7eac83b8670cf3b20b478195bd49c8d756ba0a36cf48", size = 11106083 },
    { url = "https://files.pythonhosted.org/packages/77/79/d8cec175856ff810a19825d09ce700265f905c643c69f45d2b737e4a470a/ruff-0.11.13-py3-none-macosx_11_0_arm64.whl", hash = "sha256:53b15a9dfdce029c842e9a5aebc3855e9ab7771395979ff85b7c1dedb53ddc2b", size = 10436024 },
    { url = "https://files.pythonhosted.org/packages/8b/5b/f6d94f2980fa1ee854b41568368a2e1252681b9238ab2895e133d303538f/ruff-0.11.13-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ab153241400789138d13f362c43f7edecc0edfffce2afa6a68434000ecd8f69a", size = 10646324 },
    { url = "https://files.pythonhosted.org/packages/6c/9c/b4c2acf24ea4426016d511dfdc787f4ce1ceb835f3c5fbdbcb32b1c63bda/ruff-0.11.13-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:6c51f93029d54a910d3d24f7dd0bb909e31b6cd989a5e4ac513f4eb41629f0dc", size = 10174416 },
    { url = "https://files.pythonhosted.org/packages/f3/10/e2e62f77c65ede8cd032c2ca39c41f48feabedb6e282bfd6073d81bb671d/ruff-0.11.13-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1808b3ed53e1a777c2ef733aca9051dc9bf7c99b26ece15cb59a0320fbdbd629", size = 11724197 },
    { url = "https://files.pythonhosted.org/packages/bb/f0/466fe8469b85c561e081d798c45f8a1d21e0b4a5ef795a1d7f1a9a9ec182/ruff-0.11.13-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:d28ce58b5ecf0f43c1b71edffabe6ed7f245d5336b17805803312ec9bc665933", size = 12511615 },
    { url = "https://files.pythonhosted.org/packages/17/0e/cefe778b46dbd0cbcb03a839946c8f80a06f7968eb298aa4d1a4293f3448/ruff-0.11.13-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:55e4bc3a77842da33c16d55b32c6cac1ec5fb0fbec9c8c513bdce76c4f922165", size = 12117080 },
    { url = "https://files.pythonhosted.org/packages/5d/2c/caaeda564cbe103bed145ea557cb86795b18651b0f6b3ff6a10e84e5a33f/ruff-0.11.13-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:633bf2c6f35678c56ec73189ba6fa19ff1c5e4807a78bf60ef487b9dd272cc71", size = 11326315 },
    { url = "https://files.pythonhosted.org/packages/75/f0/782e7d681d660eda8c536962920c41309e6dd4ebcea9a2714ed5127d44bd/ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ffbc82d70424b275b089166310448051afdc6e914fdab90e08df66c43bb5ca9", size = 11555640 },
    { url = "https://files.pythonhosted.org/packages/5d/d4/3d580c616316c7f07fb3c99dbecfe01fbaea7b6fd9a82b801e72e5de742a/ruff-0.11.13-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:4a9ddd3ec62a9a89578c85842b836e4ac832d4a2e0bfaad3b02243f930ceafcc", size = 10507364 },
    { url = "https://files.pythonhosted.org/packages/5a/dc/195e6f17d7b3ea6b12dc4f3e9de575db7983db187c378d44606e5d503319/ruff-0.11.13-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:d237a496e0778d719efb05058c64d28b757c77824e04ffe8796c7436e26712b7", size = 10141462 },
    { url = "https://files.pythonhosted.org/packages/f4/8e/39a094af6967faa57ecdeacb91bedfb232474ff8c3d20f16a5514e6b3534/ruff-0.11.13-py3-none-musllinux_1_2_i686.whl", hash = "sha256:26816a218ca6ef02142343fd24c70f7cd8c5aa6c203bca284407adf675984432", size = 11121028 },
    { url = "https://files.pythonhosted.org/packages/5a/c0/b0b508193b0e8a1654ec683ebab18d309861f8bd64e3a2f9648b80d392cb/ruff-0.11.13-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:51c3f95abd9331dc5b87c47ac7f376db5616041173826dfd556cfe3d4977f492", size = 11602992 },
    { url = "https://files.pythonhosted.org/packages/7c/91/263e33ab93ab09ca06ce4f8f8547a858cc198072f873ebc9be7466790bae/ruff-0.11.13-py3-none-win32.whl", hash = "sha256:96c27935418e4e8e77a26bb05962817f28b8ef3843a6c6cc49d8783b5507f250", size = 10474944 },
    { url = "https://files.pythonhosted.org/packages/46/f4/7c27734ac2073aae8efb0119cae6931b6fb48017adf048fdf85c19337afc/ruff-0.11.13-py3-none-win_amd64.whl", hash = "sha256:29c3189895a8a6a657b7af4e97d330c8a3afd2c9c8f46c81e2fc5a31866517e3", size = 11548669 },
    { url = "https://files.pythonhosted.org/packages/ec/bf/b273dd11673fed8a6bd46032c0ea2a04b2ac9bfa9c628756a5856ba113b0/ruff-0.11.13-py3-none-win_arm64.whl", hash = "sha256:b4385285e9179d608ff1d2fb9922062663c658605819a6876d8beef0c30b7f3b", size = 10683928 },
]

[[package]]
name = "sentencepiece"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz", hash = "sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843", size = 2632106 }

[[package]]
name = "shapely"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fb/fe/3b0d2f828ffaceadcdcb51b75b9c62d98e62dd95ce575278de35f24a1c20/shapely-2.1.0.tar.gz", hash = "sha256:2cbe90e86fa8fc3ca8af6ffb00a77b246b918c7cf28677b7c21489b678f6b02e", size = 313617 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/77/4e368704b2193e74498473db4461d697cc6083c96f8039367e59009d78bd/shapely-2.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b64423295b563f43a043eb786e7a03200ebe68698e36d2b4b1c39f31dfb50dfb", size = 1830029 },
    { url = "https://files.pythonhosted.org/packages/71/3c/d888597bda680e4de987316b05ca9db07416fa29523beff64f846503302f/shapely-2.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1b5578f45adc25b235b22d1ccb9a0348c8dc36f31983e57ea129a88f96f7b870", size = 1637999 },
    { url = "https://files.pythonhosted.org/packages/03/8d/ee0e23b7ef88fba353c63a81f1f329c77f5703835db7b165e7c0b8b7f839/shapely-2.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1a7e83d383b27f02b684e50ab7f34e511c92e33b6ca164a6a9065705dd64bcb", size = 2929348 },
    { url = "https://files.pythonhosted.org/packages/d1/a7/5c9cb413e4e2ce52c16be717e94abd40ce91b1f8974624d5d56154c5d40b/shapely-2.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:942031eb4d8f7b3b22f43ba42c09c7aa3d843aa10d5cc1619fe816e923b66e55", size = 3048973 },
    { url = "https://files.pythonhosted.org/packages/84/23/45b90c0bd2157b238490ca56ef2eedf959d3514c7d05475f497a2c88b6d9/shapely-2.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d2843c456a2e5627ee6271800f07277c0d2652fb287bf66464571a057dbc00b3", size = 3873148 },
    { url = "https://files.pythonhosted.org/packages/c0/bc/ed7d5d37f5395166042576f0c55a12d7e56102799464ba7ea3a72a38c769/shapely-2.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8c4b17469b7f39a5e6a7cfea79f38ae08a275427f41fe8b48c372e1449147908", size = 4052655 },
    { url = "https://files.pythonhosted.org/packages/c0/8f/a1dafbb10d20d1c569f2db3fb1235488f624dafe8469e8ce65356800ba31/shapely-2.1.0-cp313-cp313-win32.whl", hash = "sha256:30e967abd08fce49513d4187c01b19f139084019f33bec0673e8dbeb557c45e4", size = 1526600 },
    { url = "https://files.pythonhosted.org/packages/e3/f0/9f8cdf2258d7aed742459cea51c70d184de92f5d2d6f5f7f1ded90a18c31/shapely-2.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:1dc8d4364483a14aba4c844b7bd16a6fa3728887e2c33dfa1afa34a3cf4d08a5", size = 1707115 },
    { url = "https://files.pythonhosted.org/packages/75/ed/32952df461753a65b3e5d24c8efb361d3a80aafaef0b70d419063f6f2c11/shapely-2.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:673e073fea099d1c82f666fb7ab0a00a77eff2999130a69357ce11941260d855", size = 1824847 },
    { url = "https://files.pythonhosted.org/packages/ff/b9/2284de512af30b02f93ddcdd2e5c79834a3cf47fa3ca11b0f74396feb046/shapely-2.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:6d1513f915a56de67659fe2047c1ad5ff0f8cbff3519d1e74fced69c9cb0e7da", size = 1631035 },
    { url = "https://files.pythonhosted.org/packages/35/16/a59f252a7e736b73008f10d0950ffeeb0d5953be7c0bdffd39a02a6ba310/shapely-2.1.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0d6a7043178890b9e028d80496ff4c79dc7629bff4d78a2f25323b661756bab8", size = 2968639 },
    { url = "https://files.pythonhosted.org/packages/a5/0a/6a20eca7b0092cfa243117e8e145a58631a4833a0a519ec9b445172e83a0/shapely-2.1.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb638378dc3d76f7e85b67d7e2bb1366811912430ac9247ac00c127c2b444cdc", size = 3055713 },
    { url = "https://files.pythonhosted.org/packages/fb/44/eeb0c7583b1453d1cf7a319a1d738e08f98a5dc993fa1ef3c372983e4cb5/shapely-2.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:737124e87d91d616acf9a911f74ac55e05db02a43a6a7245b3d663817b876055", size = 3890478 },
    { url = "https://files.pythonhosted.org/packages/5d/6e/37ff3c6af1d408cacb0a7d7bfea7b8ab163a5486e35acb08997eae9d8756/shapely-2.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e6c229e7bb87aae5df82fa00b6718987a43ec168cc5affe095cca59d233f314", size = 4036148 },
    { url = "https://files.pythonhosted.org/packages/c8/6a/8c0b7de3aeb5014a23f06c5e9d3c7852ebcf0d6b00fe660b93261e310e24/shapely-2.1.0-cp313-cp313t-win32.whl", hash = "sha256:a9580bda119b1f42f955aa8e52382d5c73f7957e0203bc0c0c60084846f3db94", size = 1535993 },
    { url = "https://files.pythonhosted.org/packages/a8/91/ae80359a58409d52e4d62c7eacc7eb3ddee4b9135f1db884b6a43cf2e174/shapely-2.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e8ff4e5cfd799ba5b6f37b5d5527dbd85b4a47c65b6d459a03d0962d2a9d4d10", size = 1717777 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677 },
]

[[package]]
name = "sqlparse"
version = "0.5.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e5/40/edede8dd6977b0d3da179a342c198ed100dd2aba4be081861ee5911e4da4/sqlparse-0.5.3.tar.gz", hash = "sha256:09f67787f56a0b16ecdbde1bfc7f5d9c3371ca683cfeaa8e6ff60b4807ec9272", size = 84999 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl", hash = "sha256:cf2196ed3418f3ba5de6af7e82c694a9fbdbfecccdfc72e281548517081f16ca", size = 44415 },
]

[[package]]
name = "sse-starlette"
version = "2.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/35/7d8d94eb0474352d55f60f80ebc30f7e59441a29e18886a6425f0bccd0d3/sse_starlette-2.3.3.tar.gz", hash = "sha256:fdd47c254aad42907cfd5c5b83e2282be15be6c51197bf1a9b70b8e990522072", size = 17499 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/20/52fdb5ebb158294b0adb5662235dd396fc7e47aa31c293978d8d8942095a/sse_starlette-2.3.3-py3-none-any.whl", hash = "sha256:8b0a0ced04a329ff7341b01007580dd8cf71331cc21c0ccea677d500618da1e0", size = 10235 },
]

[[package]]
name = "stack-data"
version = "0.6.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "asttokens" },
    { name = "executing" },
    { name = "pure-eval" },
]
sdist = { url = "https://files.pythonhosted.org/packages/28/e3/55dcc2cfbc3ca9c29519eb6884dd1415ecb53b0e934862d3559ddcb7e20b/stack_data-0.6.3.tar.gz", hash = "sha256:836a778de4fec4dcd1dcd89ed8abff8a221f58308462e1c4aa2a3cf30148f0b9", size = 44707 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl", hash = "sha256:d5558e0c25a4cb0853cddad3d77da9891a08cb85dd9f9f91b9f8cd66e511e695", size = 24521 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tabulate"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ec/fe/802052aecb21e3797b8f7902564ab6ea0d60ff8ca23952079064155d1ae1/tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c", size = 81090 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f", size = 35252 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tomlkit"
version = "0.13.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/18/0bbf3884e9eaa38819ebe46a7bd25dcd56b67434402b66a58c4b8e552575/tomlkit-0.13.3.tar.gz", hash = "sha256:430cf247ee57df2b94ee3fbe588e71d362a941ebb545dec29b53961d61add2a1", size = 185207 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl", hash = "sha256:c89c649d79ee40629a9fda55f8ace8c6a1b42deb912b2a8fd8d942ddadb606b0", size = 38901 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "traitlets"
version = "5.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/eb/79/72064e6a701c2183016abbbfedaba506d81e30e232a68c9f0d6f6fcd1574/traitlets-5.14.3.tar.gz", hash = "sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7", size = 161621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/c0/8f5d070730d7836adc9c9b6408dec68c6ced86b304a9b26a14df072a6e8c/traitlets-5.14.3-py3-none-any.whl", hash = "sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f", size = 85359 },
]

[[package]]
name = "typer"
version = "0.15.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/1a/5f36851f439884bcfe8539f6a20ff7516e7b60f319bbaf69a90dc35cc2eb/typer-0.15.3.tar.gz", hash = "sha256:818873625d0569653438316567861899f7e9972f2e6e0c16dab608345ced713c", size = 101641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/48/20/9d953de6f4367163d23ec823200eb3ecb0050a2609691e512c8b95827a9b/typer-0.15.3-py3-none-any.whl", hash = "sha256:c86a65ad77ca531f03de08d1b9cb67cd09ad02ddddf4b34745b5008f43b239bd", size = 45253 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uritemplate"
version = "4.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/60/f174043244c5306c9988380d2cb10009f91563fc4b31293d27e17201af56/uritemplate-4.2.0.tar.gz", hash = "sha256:480c2ed180878955863323eea31b0ede668795de182617fef9c6ca09e6ec9d0e", size = 33267 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/99/3ae339466c9183ea5b8ae87b34c0b897eda475d2aec2307cae60e5cd4f29/uritemplate-4.2.0-py3-none-any.whl", hash = "sha256:962201ba1c4edcab02e60f9a0d3821e82dfc5d2d6662a21abd533879bdb8a686", size = 11488 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166 },
]

[[package]]
name = "webencodings"
version = "0.5.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/02/ae6ceac1baeda530866a85075641cec12989bd8d31af6d5ab4a3e8c92f47/webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923", size = 9721 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78", size = 11774 },
]

[[package]]
name = "websockets"
version = "15.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/e6/26d09fab466b7ca9c7737474c52be4f76a40301b08362eb2dbc19dcc16c1/websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee", size = 177016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/9f/51f0cf64471a9d2b4d0fc6c534f323b664e7095640c34562f5182e5a7195/websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931", size = 175440 },
    { url = "https://files.pythonhosted.org/packages/8a/05/aa116ec9943c718905997412c5989f7ed671bc0188ee2ba89520e8765d7b/websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675", size = 173098 },
    { url = "https://files.pythonhosted.org/packages/ff/0b/33cef55ff24f2d92924923c99926dcce78e7bd922d649467f0eda8368923/websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151", size = 173329 },
    { url = "https://files.pythonhosted.org/packages/31/1d/063b25dcc01faa8fada1469bdf769de3768b7044eac9d41f734fd7b6ad6d/websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22", size = 183111 },
    { url = "https://files.pythonhosted.org/packages/93/53/9a87ee494a51bf63e4ec9241c1ccc4f7c2f45fff85d5bde2ff74fcb68b9e/websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f", size = 182054 },
    { url = "https://files.pythonhosted.org/packages/ff/b2/83a6ddf56cdcbad4e3d841fcc55d6ba7d19aeb89c50f24dd7e859ec0805f/websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8", size = 182496 },
    { url = "https://files.pythonhosted.org/packages/98/41/e7038944ed0abf34c45aa4635ba28136f06052e08fc2168520bb8b25149f/websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375", size = 182829 },
    { url = "https://files.pythonhosted.org/packages/e0/17/de15b6158680c7623c6ef0db361da965ab25d813ae54fcfeae2e5b9ef910/websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d", size = 182217 },
    { url = "https://files.pythonhosted.org/packages/33/2b/1f168cb6041853eef0362fb9554c3824367c5560cbdaad89ac40f8c2edfc/websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4", size = 182195 },
    { url = "https://files.pythonhosted.org/packages/86/eb/20b6cdf273913d0ad05a6a14aed4b9a85591c18a987a3d47f20fa13dcc47/websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa", size = 176393 },
    { url = "https://files.pythonhosted.org/packages/1b/6c/c65773d6cab416a64d191d6ee8a8b1c68a09970ea6909d16965d26bfed1e/websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561", size = 176837 },
    { url = "https://files.pythonhosted.org/packages/fa/a8/5b41e0da817d64113292ab1f8247140aac61cbf6cfd085d6a0fa77f4984f/websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f", size = 169743 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]

[[package]]
name = "yoyo-migrations"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "importlib-metadata" },
    { name = "sqlparse" },
    { name = "tabulate" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/8c/5d/9ef7f808ea955eca9f08043c65bdc81a4694e784c978b24ad72022974a97/yoyo_migrations-9.0.0-py3-none-any.whl", hash = "sha256:fc65d3a6d9449c1c54d64ff2ff98e32a27da356057c60e3471010bfb19ede081", size = 49002 },
]

[[package]]
name = "zipp"
version = "3.23.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e3/02/0f2892c661036d50ede074e376733dca2ae7c6eb617489437771209d4180/zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166", size = 25547 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e", size = 10276 },
]
</file>
  <file path="prompt_service_notebook.py">import marimo

__generated_with = "0.14.12"
app = marimo.App(width="medium")


@app.cell
def _():
    from repository.database import SQLite3Database
    from repository.prompt_service import PromptService
    from repository.prompt_models import (
        Prompt,
        PromptType,
        PromptPlanStatus,
        CmdCategory,
    )

    return (
        CmdCategory,
        Prompt,
        PromptPlanStatus,
        PromptService,
        PromptType,
        SQLite3Database,
    )


@app.cell
def _(PromptService, SQLite3Database):
    db = SQLite3Database("data/collect.db")
    with db.get_connection() as conn:
        ps = PromptService(conn)
    return (ps,)


@app.cell
def _(ps):
    cmds = ps.load_cmds_from_disk()
    plans = ps.load_plans_from_disk()
    return cmds, plans


@app.cell
def _(cmds):
    print(f"Num cmds: {len(cmds.loaded_prompts)}\n")
    for cmd in cmds.loaded_prompts:
        print(cmd.name)
    return


@app.cell
def _(plans):
    print(f"Num plans: {len(plans.loaded_prompts)}\n")
    for plan in plans.loaded_prompts:
        print(plan.name)
    return


@app.cell
def _():
    db_name = "collect_completed_add_claude_sdk_processing.md"
    result = db_name.split("_")
    print(result)
    return db_name, result


@app.cell
def _(result):
    print(f"project name: {result[0]}")
    print(f"plan status: {result[1]}")
    return


@app.cell
def _(result):
    namelist = result[2:]
    print(namelist)
    return (namelist,)


@app.cell
def _(namelist):
    newname = ""
    for word in namelist:
        if not word.endswith(".md"):
            newname = newname + word + "_"
        else:
            newname = newname + word
    print(newname)
    return


@app.cell
def _(db_name):
    print(db_name.split("_")[2:])
    return


@app.cell
def _(PromptType):
    def parse_db_name(db_name: str, prompt_type: PromptType) -&gt; str:
        ls = db_name.split("_")
        filename = ""
        if prompt_type == PromptType.PLAN:
            project = ls[0]
            plan_status = ls[1]
            print(f"project: {project}")
            print(f"plan status: {plan_status}")

            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

        if prompt_type == PromptType.CMD:
            cmd_dir = ls[0]
            print(f"cmd/dir: {cmd_dir}")
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

    return (parse_db_name,)


@app.cell
def _(PromptType, db_name, parse_db_name):
    parse_db_name(db_name, PromptType.PLAN)
    return


@app.cell
def _(PromptType, parse_db_name):
    parse_db_name("tools_create_database.md", PromptType.CMD)
    return


@app.cell
def _(CmdCategory, Prompt, PromptPlanStatus, PromptType, ps):
    def new_cmd_prompt(prompt_content: str) -&gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
</file>
  <file path="Makefile">PROJECT_NAME := collect

marimo:
	uv run marimo edit

.PHONY: movetools
movetools:
	./movetools

.PHONY: buildsrc
buildsrc: 
	./tools/buildsrc

.PHONY: ensuregithub
ensuregithub:
	./tools/ensure-github-url

lint:
	ruff check .

format:
	black .

test: 
	uv run pytest -v -s -n auto

test-fast:
	uv run pytest -v -n auto -m "not slow"

test-slow:
	uv run pytest -v -s -m slow

test-single:
	uv run pytest -v -s

check: 
	make lint
	make format
	make movetools
	make ensuregithub
	make buildsrc

migrate:
	uv run yoyo apply --config yoyo.ini --batch


</file>
  <file path=".mcp.json">{
	"mcpServers": {
		"collect": {
			"command": "/Users/benjaminmetz/.local/bin/uv",
			"args": [
				"--directory",
				"/Users/benjaminmetz/python/collect",
				"run",
				"collect.py"
			]
		}
	}
}
</file>
  <file path="pyproject.toml">[project]
name = "collect"
version = "0.1.0"
description = "development toolkit for all the things.."
readme = "README.md"
requires-python = "&gt;=3.13"
dependencies = [
    "aiohttp&gt;=3.12.11",
    "anthropic&gt;=0.50.0",
    "beautifulsoup4&gt;=4.13.4",
    "black&gt;=25.1.0",
    "fastapi&gt;=0.116.1",
    "google-ai-generativelanguage&gt;=0.6.15",
    "google-api-python-client&gt;=2.169.0",
    "google-auth-httplib2&gt;=0.2.0",
    "google-cloud-aiplatform[tokenization]&gt;=1.91.0",
    "google-cloud-secret-manager&gt;=2.23.3",
    "google-genai&gt;=1.13.0",
    "google-generativeai&gt;=0.8.5",
    "html-to-markdown&gt;=1.3.2",
    "html5lib&gt;=1.1",
    "httplib2&gt;=0.22.0",
    "httpx&gt;=0.28.1",
    "ipython&gt;=9.4.0",
    "lxml&gt;=5.4.0",
    "marimo&gt;=0.14.12",
    "markdownify&gt;=1.1.0",
    "mcp[cli]&gt;=1.7.1",
    "openai&gt;=1.59.4",
    "pathspec&gt;=0.12.1",
    "pyperclip&gt;=1.9.0",
    "pytest&gt;=8.3.5",
    "pytest-asyncio&gt;=0.26.0",
    "pytest-xdist&gt;=3.6.1",
    "python-json-logger&gt;=3.3.0",
    "readabilipy&gt;=0.3.0",
    "rich&gt;=14.0.0",
    "ruff&gt;=0.11.9",
    "tiktoken&gt;=0.9.0",
    "uvicorn&gt;=0.34.2",
    "yoyo-migrations&gt;=9.0.0",
]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."]
filterwarnings = [
    "ignore::UserWarning:google.auth._default"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests"
]
# Parallel test execution settings
addopts = [
    "--strict-markers",  # Ensure all marks are registered
    "--tb=short",       # Shorter traceback format
    "--dist=worksteal", # Better work distribution for uneven test times
]
</file>
  <file path="initial_load.py">#!/usr/bin/env python3
"""
Initial load script for loading plans and commands from disk into database
Uses PromptService to load from .claude/commands,
.gemini/commands, and _docs/plans
"""
import sys
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import PromptLoadResult, PromptCreateResult


def load_commands_to_db(service: PromptService) -&gt; PromptLoadResult:
    """
    Load commands from .claude and .gemini directories and save to database
    This is primarily for an initial load or to clean and restart from disk

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📁 Loading commands from disk...")

    # Load commands from filesystem
    cmd_result: PromptLoadResult = service.load_cmds_from_disk()

    if cmd_result.errors:
        print(
            f"⚠️  Found {len(cmd_result.errors)
                           } errors while loading commands:"
        )
        for error in cmd_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not cmd_result.loaded_prompts:
        print("ℹ️  No commands found to load")
        return cmd_result

    print(f"Found: {len(cmd_result.loaded_prompts)} to save to database")

    # Save commands to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        cmd_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save command: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (cmd_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=cmd_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def load_plans_to_db(service: PromptService) -&gt; PromptLoadResult:
    """Load plans from _docs/plans directories and save to database

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📋 Loading plans from disk...")

    # Load plans from filesystem
    plan_result: PromptLoadResult = service.load_plans_from_disk()

    if plan_result.errors:
        print(
            f"⚠️  Found {len(plan_result.errors)
                           } errors while loading plans:"
        )
        for error in plan_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not plan_result.loaded_prompts:
        print("ℹ️  No plans found to load")
        return plan_result

    print(
        f"📄 Found {len(plan_result.loaded_prompts)
                     } plans to save to database"
    )

    # Save plans to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        plan_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save plan: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (plan_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=plan_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def main():
    """Main function to orchestrate the complete loading process"""
    print("🚀 Starting initial data load from disk to database...")
    print("=" * 60)

    try:
        # Initialize database connection
        database = SQLite3Database("data/collect.db")

        with database.get_connection() as conn:
            # Create PromptService instance
            service = PromptService(conn)

            # Track overall statistics
            total_loaded = 0
            total_errors = 0

            # Load commands
            cmd_result: PromptLoadResult = load_commands_to_db(service)
            cmd_loaded = len(cmd_result.loaded_prompts)
            cmd_errors = len(cmd_result.errors) if cmd_result.errors else 0

            total_loaded += cmd_loaded
            total_errors += cmd_errors

            print(f"✅ Commands: {cmd_loaded} loaded, {cmd_errors} errors")
            print()

            # Load plans
            plan_result: PromptLoadResult = load_plans_to_db(service)
            plan_loaded = len(plan_result.loaded_prompts)
            plan_errors = len(plan_result.errors) if plan_result.errors else 0

            total_loaded += plan_loaded
            total_errors += plan_errors

            print(f"✅ Plans: {plan_loaded} loaded, {plan_errors} errors")
            print()

            # Print final summary
            print("=" * 60)
            print("📊 FINAL SUMMARY:")
            print(f"   ✅ Total items loaded: {total_loaded}")
            print(f"   ❌ Total errors: {total_errors}")

            if total_errors == 0:
                print("🎉 All data loaded successfully!")
            else:
                print(
                    f"⚠️  Completed with {
                      total_errors} errors - check output above"
                )

    except Exception as e:
        print(f"💥 Fatal error during loading process: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>
  <file path=".sync_cache.json">{
  ".claude/commands/go/go_build_endpoint_test.md": {
    "sha": "bb6aa9bffab635d3455caf7cfa59fc4f43036aac",
    "path": ".claude/commands/go/go_build_endpoint_test.md",
    "last_synced": 1754870996.5877829,
    "converted": true
  },
  ".claude/commands/archive/build_context.md": {
    "sha": "d976db85985179f771925095a73d98142d3ab30b",
    "path": ".claude/commands/archive/build_context.md",
    "last_synced": 1754870996.598152,
    "converted": true
  },
  ".claude/commands/commit.md": {
    "sha": "891c52b5b372e6fffd161dcf9b55930755d9fbf3",
    "path": ".claude/commands/commit.md",
    "last_synced": 1754870996.614573,
    "converted": true
  },
  ".claude/commands/go/create_go_structs.md": {
    "sha": "56a5e5dc2abb511b98b2e1147d428551610aa49e",
    "path": ".claude/commands/go/create_go_structs.md",
    "last_synced": 1754870996.625781,
    "converted": true
  },
  ".claude/commands/diff_code_review.md": {
    "sha": "6ef1818de14d6d1e994d9fd4841bc5486c4d6f20",
    "path": ".claude/commands/diff_code_review.md",
    "last_synced": 1754870996.635034,
    "converted": true
  },
  ".claude/commands/convert_to_toml.md": {
    "sha": "7bdd42b1ffeee07bd06ed5b4b97e99381c8052e9",
    "path": ".claude/commands/convert_to_toml.md",
    "last_synced": 1754870996.642501,
    "converted": true
  },
  ".claude/commands/create_checklist_3.md": {
    "sha": "27f4fa9841078c71d9490d9ef2c967970c053bbf",
    "path": ".claude/commands/create_checklist_3.md",
    "last_synced": 1754870996.688289,
    "converted": true
  },
  ".claude/commands/go/create_go_structsV1.md": {
    "sha": "d7e28d7eb8dee775f9416574d8f06ef11ca42158",
    "path": ".claude/commands/go/create_go_structsV1.md",
    "last_synced": 1754870996.692773,
    "converted": true
  },
  ".claude/commands/go/go_update_config.md": {
    "sha": "3cbaf390b5abd45230dde54705958bd873f16ebc",
    "path": ".claude/commands/go/go_update_config.md",
    "last_synced": 1754871010.753632,
    "converted": true
  },
  ".claude/commands/mcp/copy_to_clipboard.md": {
    "sha": "aad8af9420d4a80933aaf12accfe7a8450679f32",
    "path": ".claude/commands/mcp/copy_to_clipboard.md",
    "last_synced": 1754871011.429687,
    "converted": true
  },
  ".claude/commands/mcp/get_docs.md": {
    "sha": "9e27863768032379a1798cf6da1a881720ed65bf",
    "path": ".claude/commands/mcp/get_docs.md",
    "last_synced": 1754871012.238699,
    "converted": true
  },
  ".claude/commands/model_code_review.md": {
    "sha": "8bc222f49f8587a01362d06ff6475cb8e46ace2e",
    "path": ".claude/commands/model_code_review.md",
    "last_synced": 1754871023.324787,
    "converted": true
  },
  ".claude/commands/pr.md": {
    "sha": "0e8a152f358b515f158d5025428db18847101fc0",
    "path": ".claude/commands/pr.md",
    "last_synced": 1754871024.7506702,
    "converted": true
  },
  ".claude/commands/prime_webapp.md": {
    "sha": "c6f4cfe53f2aa92e1ac5661138989c9f9ff3ec42",
    "path": ".claude/commands/prime_webapp.md",
    "last_synced": 1754871038.662535,
    "converted": true
  },
  ".claude/commands/python/python_update_config.md": {
    "sha": "8562257de3a622c1aefb06be4c78a4ce9c580e2a",
    "path": ".claude/commands/python/python_update_config.md",
    "last_synced": 1754871042.126036,
    "converted": true
  },
  ".claude/commands/read.md": {
    "sha": "524139e35449b4f5c0a88b206699cec5d2f1409b",
    "path": ".claude/commands/read.md",
    "last_synced": 1754871043.723397,
    "converted": true
  },
  ".claude/commands/response_in_markdown.md": {
    "sha": "a540be858d83e4b65df8dd2a23e3656bbdce8ee7",
    "path": ".claude/commands/response_in_markdown.md",
    "last_synced": 1754871049.784717,
    "converted": true
  },
  ".claude/commands/runplan.md": {
    "sha": "08d9b2d8c3bd445c55170adec7e80b9d3733e27a",
    "path": ".claude/commands/runplan.md",
    "last_synced": 1754871056.6670442,
    "converted": true
  },
  ".claude/commands/test_runner.md": {
    "sha": "28e088b2d155b9eca391add1bc75cdf701a3a4d8",
    "path": ".claude/commands/test_runner.md",
    "last_synced": 1754871059.413824,
    "converted": true
  },
  ".claude/commands/tools/create_database.md": {
    "sha": "fcfeeb2b78e39ecf0a1b8b4a85735dd83fa1c8f1",
    "path": ".claude/commands/tools/create_database.md",
    "last_synced": 1754871070.783872,
    "converted": true
  },
  ".claude/commands/tools/extract.md": {
    "sha": "e8946fb23beb24cd8e8689c48cd21b155dfa10a6",
    "path": ".claude/commands/tools/extract.md",
    "last_synced": 1754871074.061621,
    "converted": true
  }
}</file>
  <file path="llmrunner.py">import asyncio
from datetime import datetime

from pydantic import BaseModel
from typing import Dict, Union, List, Optional, Any

from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP
from models.gemini_mcp import GeminiMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP


class ModelsToMCP(BaseModel):
    model_config = {"arbitrary_types_allowed": True}
    models_to_mcp: Dict[str, Union[GeminiMCP, AnthropicMCP, OpenAIMCP, XaiMCP]]


class ModelResult(BaseModel):
    model: str
    timestamp: str
    success: bool
    actual_model: Optional[str] = None
    duration_seconds: Optional[float] = None
    response: Optional[Any] = None
    error: Optional[str] = None


class LLMRunnerResults(BaseModel):
    successful_results: List[ModelResult]
    failed_results: List[ModelResult]
    total_models: int
    success_count: int
    failure_count: int


async def llmrunner(prompt: str, models_to_mcp: ModelsToMCP) -&gt; LLMRunnerResults:

    async def call_model(model_name: str) -&gt; dict:
        try:
            start_time = datetime.now()
            iso_time = start_time.isoformat()

            mcp_instance = models_to_mcp.models_to_mcp[model_name]
            print(f"sending to --&gt; {model_name} : at -&gt; {iso_time}")
            response = mcp_instance.send_message(prompt, model=model_name)
            end_time = datetime.now()

            result = ModelResult(
                model=model_name,
                actual_model=model_name,
                timestamp=iso_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                response=response,
                success=True,
            )

            return result

        except Exception as e:
            error_result = ModelResult(
                success=False,
                error=str(e),
                model=model_name,
                timestamp=datetime.now().isoformat(),
            )

            return error_result

    print(
        f"starting runner for: {
          len(models_to_mcp.models_to_mcp.keys())} models -&gt;"
    )
    tasks = [call_model(model) for model in models_to_mcp.models_to_mcp.keys()]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful_results = [
        r for r in results if isinstance(r, ModelResult) and r.success
    ]

    failed_results = [
        r for r in results if isinstance(r, ModelResult) and not r.success
    ]

    return LLMRunnerResults(
        successful_results=successful_results,
        failed_results=failed_results,
        total_models=len(models_to_mcp.models_to_mcp),
        success_count=len(successful_results),
        failure_count=len(failed_results),
    )


def code_review_models_to_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    anthropic_model = config.anthropic_default_code_review_model
    gemini_model = config.gemini_default_code_review_model
    xai_model = config.xai_default_code_review_model
    openai_model = config.openai_default_code_review_model

    gemini_mcp = GeminiMCP(config, secret_mgr, gemini_model)
    openai_mcp = OpenAIMCP(config, secret_mgr, openai_model)
    xai_mcp = XaiMCP(config, secret_mgr, xai_model)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, anthropic_model)

    model_mcps = {
        gemini_model: gemini_mcp,
        openai_model: openai_mcp,
        xai_model: xai_mcp,
        anthropic_model: anthropic_mcp,
    }

    return ModelsToMCP(models_to_mcp=model_mcps)
</file>
  <file path="README.md">**Collect** is a command-line toolkit built with Python that functions as an MCP (Model Context Protocol) server. It is designed to assist with AI-driven development by providing tools to fetch web content, process it, and coordinate analysis across multiple AI models.

*   **Multi-Model Integration**: Interact with models from Google (Gemini), Anthropic (Claude), OpenAI (GPT), and XAI (Grok) through a single interface.
*   **Content Processing**: Fetch content from URLs and convert HTML to clean markdown or plain text.
*   **Code &amp; Diff Analysis**: Perform code reviews on files or git diffs using any of the integrated AI models.
*   **Secure Configuration**: Utilizes Google Cloud Secret Manager for API key storage.
*   **Prompt Management**: A version-controlled system for managing and synchronizing prompts between the local filesystem and a SQLite database.
*   **Token Utilities**: Tools to count token usage for various models to manage costs and context windows.

### MCP Server Configuration

#### For Claude Code

To enable Claude Code to use the `collect` MCP server, create a `.mcp.json` file in your project's root directory:

1.  **Create the Configuration File**: In the root of your project where you want to use the collect tools, create a file named `.mcp.json`.
2.  **Add Configuration**: Add the following JSON configuration:

```json
{
  "mcpServers": {
    "collect": {
      "command": "/path/to/.local/bin/uv",
      "args": [
        "--directory",
        "/path/to/collect",
        "run",
        "collect.py"
      ]
    }
  }
}
```

Replace `/path/to/.local/bin/uv` with the full path to your `uv` binary (you can find this with `which uv`), and `/path/to/collect` with the full path to your collect repository.

#### For Gemini CLI

To enable the Gemini CLI to automatically start the `collect` MCP server, you need to configure a `.gemini/settings.json` file in your project's root directory:

1.  **Create the Directory**: If it doesn't already exist, create a `.gemini` directory in the root of the `collect` project.
2.  **Create the Settings File**: Inside the `.gemini` directory, create a file named `settings.json`.
3.  **Add Configuration**: Paste the following JSON configuration into the `settings.json` file.

```json
{
  "mcpServers": {
    "collect": {
      "command": "uv",
      "args": [
        "run",
        "python",
        "collect.py"
      ],
      "workingDirectory": "/Users/benjaminmetz/python/collect",
      "enabled": true
    }
  }
}
```

This configuration tells the Gemini CLI how to launch the `collect` server, specifying the command, arguments, and working directory.

### Command Category System

The command category system dynamically creates categories based on subdirectories configured in the `.env` file. This approach allows for easy extension of command categories without code changes.

#### How Categories Are Created

1. **Configuration**: Command subdirectories are defined in the `.env` file:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
   ```

2. **Dynamic Enum Generation**: The `create_cmd_category_enum()` function in `repository/prompt_models.py` reads the `COMMAND_SUBDIRS` from the `.env` file via the `Config` class and dynamically creates a `CmdCategory` enum at runtime.

3. **Directory Management**: When the `PromptService` initializes, the `cmd_check_dirs()` function in `repository/prompt_service.py`:
   - Reads the subdirectory list from the config
   - Checks for the existence of each configured subdirectory under both `.claude/commands/` and `.gemini/commands/`
   - Automatically creates any missing directories
   - Each subdirectory becomes a valid command category

4. **Category Assignment**: When loading commands from disk:
   - Files directly in `.claude/commands/` or `.gemini/commands/` are assigned the `UNCATEGORIZED` category
   - Files in subdirectories are assigned the category matching the subdirectory name
   - The category is stored as part of the prompt's metadata in the database

#### Adding New Categories

To add new command categories:
1. Update the `COMMAND_SUBDIRS` line in the `.env` file with your new category
2. The system will automatically create the directories and recognize them as valid categories on the next run
3. Commands placed in those directories will be tagged with the new category

#### Example

To add a "rust" category:
1. Edit `.env`:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools,rust
   ```
2. Restart the service or run the prompt service
3. The system will create:
   - `.claude/commands/rust/`
   - `.gemini/commands/rust/`
4. Any `.md` files placed in these directories will be categorized as "rust" commands

#### Current Directory Structure
Based on the `.env` configuration (`COMMAND_SUBDIRS=archive,go,js,mcp,python,tools`), the directory structure is:

```
.claude/
└── commands/
    ├── archive/          # Archived commands
    ├── go/               # Go-specific commands
    ├── js/               # JavaScript commands
    ├── mcp/              # MCP server commands
    ├── python/           # Python-specific commands
    └── tools/            # Tool-related commands

.gemini/
└── commands/
    ├── archive/
    ├── go/
    ├── js/
    ├── mcp/
    ├── python/
    └── tools/
```

Note: Files placed directly in `.claude/commands/` or `.gemini/commands/` (not in subdirectories) are automatically assigned the `UNCATEGORIZED` category.

### Prompt Management System

The project includes a system for managing prompts **that is very much under construction**. Prompts are categorized as either **Commands** (`CMD`) or **Plans** (`PLAN`). This system, located in the `repository/` directory, uses a SQLite database to store and version prompts, while also synchronizing them with the local filesystem.

*   **Core Components**:
    *   `prompt_service.py`: The main service class that orchestrates loading, saving, versioning, and flattening prompts.
    *   `prompt_models.py`: Defines the Pydantic data models for prompts, including `Prompt`, `PromptData`, and various status enums like `PromptType` and `PromptPlanStatus`.
    *   `database.py`: Manages the connection to the `collect.db` SQLite database.
    *   `20250727_01_create-prompt-tables.sql`: The database migration file that defines the schema for the `prompt` and `prompt_history` tables.

*   **Synchronization Workflow**:
    1.  **Loading from Disk**: The `PromptService` can load prompts from predefined directories (`.claude/commands`, `.gemini/commands`, and `_docs/plans`).
    2.  **Database Persistence**: Loaded prompts are saved to the SQLite database. The service checks for existing prompts by name. If a prompt already exists and its content has changed (verified via a SHA256 hash), a new version is created in the `prompt_history` table, and the main `prompt` table is updated.
    3.  **Flattening to Disk**: The service can "flatten" the prompts from the database back to the filesystem, ensuring that the local files are consistent with the database state. This is useful for maintaining a clear and organized prompt library.

*   **Versioning**:
    *   Every time a prompt's content is updated, its `version` number is incremented.
    *   A complete record of all versions is stored in the `prompt_history` table, including a timestamp and a change summary. This allows for a full audit trail of how a prompt has evolved.

This system ensures that prompts are treated as version-controlled assets within the project, providing a structured and auditable way to manage the instructions given to the AI models.
</file>
  <file path="api.py">#!/usr/bin/env python

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api import prompt_api_router
from config import Config
import uvicorn

import sys
import logging
from pythonjsonlogger.json import JsonFormatter
from contextlib import asynccontextmanager


# Configure JSON logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(stream=sys.stdout)
handler.setFormatter(JsonFormatter())
handler.setLevel(logging.INFO)

logger.addHandler(handler)

# Load configuration
config = Config()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting prompt API service...")
    app.state.db_path = config.db_path
    app.state.config = config
    logger.info(f"Database path set to: {app.state.db_path}")
    logger.info(f"Service running on port: {config.port}")

    yield

    # Shutdown
    logger.info("Shutting down prompt API service...")


app = FastAPI(
    title="Prompt Service API",
    description="HTTP API for managing prompts and plans",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

app.include_router(prompt_api_router, tags=["prompt_api"])


def main():
    uvicorn.run(app, host="0.0.0.0", port=int(config.port))


if __name__ == "__main__":
    main()
</file>
  <file path="yoyo.ini">[DEFAULT]
sources = migrations
database = sqlite:///data/collect.db
batch_mode = on
verbosity = 0

</file>
  <file path="secret_manager.py">from google.cloud import secretmanager


class SecretManager:
    def __init__(
        self,
        project_id: str,
    ) -&gt; None:
        self.project_id = project_id
        self.gcp_client = secretmanager.SecretManagerServiceClient()

    def get_secret(self, secret_name: str) -&gt; str:
        response = self.gcp_client.access_secret_version(request={"name": secret_name})
        return response.payload.data.decode("UTF-8").strip()
</file>
  <file path=".gitignore"># Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
venv/
env/

# Environment variables
.env.local
.env.*.local

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Test coverage
.coverage
htmlcov/
.pytest_cache/
.tox/

# Temporary files
:w
*.tmp
*.temp

# Python version management
.python-version

# UV lock file (optional - some prefer to track this)
# uv.lock
</file>
  <file path=".env">PORT=8081
GCP_PROJECT_ID=482777410016
DB_PATH=data/collect.db
ANTHROPIC_API_KEY_PATH=projects/482777410016/secrets/AnthropicMCP/versions/1
ANTHROPIC_MODEL_OPUS=claude-opus-4-20250514
ANTHROPIC_MODEL_SONNET=claude-sonnet-4-20250514
GEMINI_API_KEY_PATH=projects/482777410016/secrets/GeminiTest/versions/1
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/
XAI_API_KEY_PATH=projects/482777410016/secrets/XAI_API_KEY_ELEPHNT/versions/1
GROK_SYSTEM_PROMPT=You are a helpful assistant that can answer questions and help with tasks.
OPENAI_API_KEY_PATH=projects/482777410016/secrets/OpenAIMCP/versions/1

# default code review models for running the code review loop
OPENAI_DEFAULT_CODE_REVIEW_MODEL=o3-mini-2025-01-31
GEMINI_DEFAULT_CODE_REVIEW_MODEL=gemini-2.5-flash-preview-05-20
ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL=claude-opus-4-20250514
XAI_DEFAULT_CODE_REVIEW_MODEL=grok-3-mini-fast-latest

# Command subdirectories
COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
GITHUB_URL=https://github.com/austere-labs/collect
</file>
  <file path="filterModel.md"># Gemini Model Filtering Implementation Guide

## Overview

This document describes how to filter Gemini models by version number (2.0, 2.5, etc.) and extract input token limits from the API response.

## Current State

The `GeminiMCP.get_model_list()` method has been updated to return the full API response instead of just model names:

```python
def get_model_list(self) -&gt; Dict:
    # ... API call logic ...
    model_data = response.json()
    return model_data  # Returns full response with all model metadata
```

## Implementation Plan

### 1. Add Filtering Methods to GeminiMCP Class

Add these methods to the `GeminiMCP` class in `models/gemini_mcp.py`:

```python
def filter_models_by_version(self, versions: list[str]) -&gt; list[dict]:
    """
    Filter models by version numbers and include token limits.
    
    Args:
        versions: List of version strings (e.g., ['2.0', '2.5'])
    
    Returns:
        List of dicts with model info including inputTokenLimit
    """
    all_models = self.get_model_list()
    filtered_models = []
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Check if model matches any requested version
        for version in versions:
            if version in model_name:
                model_info = {
                    'name': model_name,
                    'displayName': model.get('displayName', ''),
                    'inputTokenLimit': model.get('inputTokenLimit', 0),
                    'outputTokenLimit': model.get('outputTokenLimit', 0),
                    'description': model.get('description', ''),
                    'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
                }
                filtered_models.append(model_info)
                break
    
    return filtered_models

def get_models_with_token_info(self) -&gt; list[dict]:
    """
    Get all models with their token limit information.
    
    Returns:
        List of models sorted by inputTokenLimit (descending)
    """

    all_models = self.get_model_list()
    models_with_tokens = []

    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Only include models with token limit info
        if input_limit &gt; 0:
            models_with_tokens.append({
                'name': model_name,
                'inputTokenLimit': input_limit,
                'outputTokenLimit': model.get('outputTokenLimit', 0)
            })
    
    # Sort by input token limit (highest first)
    models_with_tokens.sort(key=lambda x: x['inputTokenLimit'], reverse=True)
    return models_with_tokens
```

### 2. Advanced Filtering Function (Standalone)

For more complex filtering needs, you can use this standalone function:

```python
def filter_gemini_models(models_data: dict, 
                        versions: list[str] = None,
                        min_input_tokens: int = None,
                        max_input_tokens: int = None,
                        generation_methods: list[str] = None) -&gt; list[dict]:
    """
    Advanced filtering with multiple criteria.
    
    Args:
        models_data: Response from get_model_list()
        versions: Filter by version numbers (optional)
        min_input_tokens: Minimum inputTokenLimit (optional)
        max_input_tokens: Maximum inputTokenLimit (optional)
        generation_methods: Required generation methods (optional)
    
    Returns:
        Filtered list of model information
    """
    filtered_models = []
    
    for model in models_data.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Apply version filter
        if versions:
            if not any(ver in model_name for ver in versions):
                continue
        
        # Apply token limit filters
        if min_input_tokens and input_limit &lt; min_input_tokens:
            continue
        if max_input_tokens and input_limit &gt; max_input_tokens:
            continue
        
        # Apply generation method filter
        if generation_methods:
            supported_methods = model.get('supportedGenerationMethods', [])
            if not all(method in supported_methods for method in generation_methods):
                continue
        
        # Model passed all filters
        model_info = {
            'name': model_name,
            'displayName': model.get('displayName', ''),
            'inputTokenLimit': input_limit,
            'outputTokenLimit': model.get('outputTokenLimit', 0),
            'description': model.get('description', ''),
            'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
        }
        filtered_models.append(model_info)
    
    return filtered_models
```

## Usage Examples

### Basic Version Filtering

```python
def test_filter_by_version(gemini_mcp):
    # Get models for versions 2.0 and 2.5
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    print(f"Found {len(filtered)} models:")
    for model in filtered:
        print(f"- {model['name']}: {model['inputTokenLimit']:,} input tokens")
```

### Get Models with Token Info

```python
def test_models_with_tokens(gemini_mcp):
    models = gemini_mcp.get_models_with_token_info()
    
    print("Models by input token limit:")
    for model in models[:10]:  # Top 10 models
        print(f"- {model['name']}: {model['inputTokenLimit']:,} tokens")
```

### Advanced Filtering

```python
def test_advanced_filtering(gemini_mcp):
    all_models = gemini_mcp.get_model_list()
    
    # Find 2.5 models with at least 100k input tokens
    filtered = filter_gemini_models(
        all_models,
        versions=['2.5'],
        min_input_tokens=100000,
        generation_methods=['generateContent']
    )
    
    print("High-capacity 2.5 models:")
    for model in filtered:
        print(f"- {model['name']}")
        print(f"  Input limit: {model['inputTokenLimit']:,}")
        print(f"  Output limit: {model['outputTokenLimit']:,}")
```

### Grouping Models by Version

```python
def group_models_by_version(gemini_mcp):
    from collections import defaultdict
    
    all_models = gemini_mcp.get_model_list()
    version_groups = defaultdict(list)
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Extract version pattern
        if '2.5' in model_name:
            version = '2.5'
        elif '2.0' in model_name:
            version = '2.0'
        elif '1.5' in model_name:
            version = '1.5'
        elif '1.0' in model_name:
            version = '1.0'
        else:
            version = 'other'
        
        version_groups[version].append({
            'name': model_name,
            'inputTokenLimit': model.get('inputTokenLimit', 0)
        })
    
    # Display grouped results
    for version, models in sorted(version_groups.items()):
        print(f"\nVersion {version} ({len(models)} models):")
        for model in sorted(models, key=lambda x: x['inputTokenLimit'], reverse=True)[:3]:
            print(f"  - {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Expected Output Format

When filtering models, you'll get results like:

```
Found 15 models:
- gemini-2.5-pro: 2,000,000 input tokens
- gemini-2.5-flash: 1,000,000 input tokens
- gemini-2.5-flash-preview-05-20: 1,000,000 input tokens
- gemini-2.0-flash: 32,768 input tokens
- gemini-2.0-flash-exp: 32,768 input tokens
- gemini-2.0-pro-exp: 32,768 input tokens
```

## API Response Structure

The Gemini API returns model data in this format:

```json
{
  "models": [
    {
      "name": "models/gemini-2.5-flash",
      "displayName": "Gemini 2.5 Flash",
      "description": "Fast and versatile multimodal model",
      "inputTokenLimit": 1000000,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ]
    }
    // ... more models
  ]
}
```

## Testing the Implementation

Add this test to `models/test_gemini_mcp.py`:

```python
def test_filter_models_by_version(gemini_mcp):
    # Test filtering for 2.0 and 2.5 versions
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    assert len(filtered) &gt; 0
    assert all('2.0' in m['name'] or '2.5' in m['name'] for m in filtered)
    assert all('inputTokenLimit' in m for m in filtered)
    
    # Print results for verification
    print(f"\nFound {len(filtered)} models for versions 2.0 and 2.5:")
    for model in sorted(filtered, key=lambda x: x['inputTokenLimit'], reverse=True):
        print(f"  {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Notes

1. **Token Limits**: Not all models return `inputTokenLimit`. Handle missing values gracefully.
2. **Model Names**: The API returns full names like "models/gemini-2.5-flash". We extract just the model part.
3. **Sorting**: Consider sorting results by token limit, name, or version for consistent output.
4. **Caching**: For production use, consider caching the model list as it doesn't change frequently.
</file>
  <file path="test_llmrunner.py">import pytest

from llmrunner import (
    llmrunner,
    code_review_models_to_mcp,
    ModelResult,
    LLMRunnerResults,
)


@pytest.fixture
def models_to_mcp():
    return code_review_models_to_mcp()


@pytest.mark.asyncio
async def test_llmrunner(models_to_mcp):
    prompt = "What is 2 + 2?"
    result = await llmrunner(prompt, models_to_mcp)

    assert isinstance(result, LLMRunnerResults)
    assert isinstance(result.successful_results, list)
    assert isinstance(result.failed_results, list)
    assert isinstance(result.total_models, int)
    assert isinstance(result.success_count, int)
    assert isinstance(result.failure_count, int)

    assert result.total_models == len(models_to_mcp.models_to_mcp)
    assert result.success_count + result.failure_count == result.total_models

    for success_result in result.successful_results:
        assert isinstance(success_result, ModelResult)
        assert success_result.success is True
        assert success_result.model is not None
        assert success_result.timestamp is not None
        assert success_result.response is not None
        assert success_result.duration_seconds is not None

    for failed_result in result.failed_results:
        assert isinstance(failed_result, ModelResult)
        assert failed_result.success is False
        assert failed_result.model is not None
        assert failed_result.timestamp is not None
        assert failed_result.error is not None

    print(f"Total models: {result.total_models}")
    print(f"Successful: {result.success_count}")
    print(f"Failed: {result.failure_count}")

    for failed_result in result.failed_results:
        print(
            f"Failed model: {
              failed_result.model} - Error: {failed_result.error}"
        )

    for success_result in result.successful_results:
        print(f"Successful model: {success_result.model}")
</file>
  <file path="GEMINI.md"># Gemini Code Assistant Context

This document provides context for the Gemini Code Assistant to understand the project structure, conventions, and important files.

## Project Overview

This project is a Python-based MCP (Model Context Protocol) server named "Collect". Its primary purpose is to fetch web content, process it, and facilitate multi-model AI analysis workflows. It provides a unified interface to interact with various AI models (OpenAI, Anthropic, Gemini, XAI) for tasks like code review. The server is built using the `mcp` library and exposes several tools for fetching URLs, converting HTML to markdown, counting tokens, and more. It also includes a database layer using SQLite for data persistence.

**Key Technologies:**

*   **Programming Language:** Python
*   **Framework:** `mcp` (Model Context Protocol)
*   **Key Libraries:** 
    *   `httpx` for asynchronous HTTP requests.
    *   `anthropic`, `openai`, `google-cloud-aiplatform` for interacting with various LLMs.
    *   `readabilipy`, `markdownify`, `beautifulsoup4` for HTML processing.
    *   `pyperclip` for clipboard integration.
    *   `yoyo-migrations` for database schema management.
*   **Package Manager:** `uv`
*   **Testing:** `pytest` with `pytest-asyncio` and `pytest-xdist` for parallel testing.
*   **Linting/Formatting:** `ruff` and `black`.
*   **Database:** SQLite.

## Building and Running

*   **Install Dependencies:** `uv sync`
*   **Run the Server:** `python collect.py`
*   **Run Tests:** `uv run pytest -v -s -n auto`
*   **Run Linter:** `ruff check .`
*   **Run Formatter:** `black .`
*   **Apply Database Migrations:** `uv run yoyo apply --config yoyo.ini --batch`

## Development Conventions

*   **Testing:** Tests are written using `pytest` and are located in files like `test_collect.py`. Asynchronous functions are tested using `@pytest.mark.asyncio`. The project uses `pytest-xdist` for parallel test execution.
*   **Linting and Formatting:** The project uses `ruff` for linting and `black` for formatting. These are run via the `Makefile`.
*   **Configuration:** Project configuration is managed in `config.py`, which loads environment variables from a `.env` file.
*   **Secrets Management:** API keys and other secrets are managed through Google Cloud Secret Manager, as indicated in `secret_manager.py` and `config.py`.
*   **Database:** The project uses SQLite for its database. The database connection logic is in `repository/database.py`. Migrations are handled by `yoyo-migrations`.

## Key Files

*   **`collect.py`:** The main entry point of the MCP server. It defines the available tools, such as `fetch_urls`, `run_code_review`, and `to_markdown`.
*   **`pyproject.toml`:** Defines the project's dependencies and development tool configurations.
*   **`Makefile`:** Provides convenient commands for common development tasks like testing, linting, and formatting.
*   **`config.py`:** Handles the project's configuration by loading environment variables from a `.env` file.
*   **`reviewer/code_review.py`:** Contains the logic for the code review functionality. It takes a diff file, sends it to multiple LLMs, and then formats and saves the results.
*   **`models/`:** This directory contains modules for interacting with different AI models (e.g., `anthropic_mpc.py`, `openai_mpc.py`).
*   **`repository/database.py`:** Contains the logic for connecting to the SQLite database.
*   **`migrations/`:** This directory contains the SQL migration files for the database schema.
</file>
  <file path="movetools">#!/bin/bash

# Enhanced setup script to copy tools to user bin directory with colorful output
# This script should be run from the collect project home directory

# Color definitions for enhanced output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly BOLD='\033[1m'
readonly NC='\033[0m' # No Color

# Set default target directory (configurable)
TARGET_DIR=${1:-~/bin}

# Expand tilde to home directory
TARGET_DIR="${TARGET_DIR/#\~/$HOME}"

# Get the directory where this script is located (project home)
PROJECT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )"
TOOLS_DIR="$PROJECT_DIR/tools"

# Enhanced printing functions
print_header() {
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${WHITE}  🔧 MOVETOOLS - Tool Installation Script${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
}

print_info() {
    echo -e "${CYAN}ℹ${NC}  $1"
}

print_success() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC}  $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_step() {
    echo -e "\n${BOLD}${PURPLE}▶${NC} ${BOLD}$1${NC}"
}

print_item() {
    echo -e "   ${GREEN}•${NC} $1"
}

# Function to display usage
usage() {
    print_header
    echo -e "${BOLD}USAGE:${NC}"
    echo -e "  $0 [target_directory]"
    echo ""
    echo -e "${BOLD}DESCRIPTION:${NC}"
    echo -e "  ${BOLD}MOVETOOLS${NC} is a comprehensive installation and backup script for the collect project."
    echo -e "  It performs two main functions:"
    echo ""
    echo -e "  ${BOLD}1. Tool Installation:${NC}"
    echo -e "     • Copies all executable tools from the tools/ directory to your bin directory"
    echo -e "     • Makes all copied tools executable with proper permissions"
    echo -e "     • Validates PATH configuration and provides setup guidance"
    echo -e "     • Provides colorful visual feedback throughout the process"
    echo ""
    echo -e "  ${BOLD}2. Dotfiles Backup:${NC}"
    echo -e "     • Backs up your .zshrc configuration to the project's dotfiles/ directory"
    echo -e "     • Copies your Ghostty terminal configuration (~/.config/ghostty)"
    echo -e "     • Backs up your Neovim init.lua configuration (~/.config/nvim/init.lua)"
    echo -e "     • Creates organized dotfiles structure for version control"
    echo ""
    echo -e "${BOLD}ARGUMENTS:${NC}"
    echo -e "  ${CYAN}target_directory${NC}    Optional. Directory to install tools (default: ~/bin)"
    echo ""
    echo -e "${BOLD}OPTIONS:${NC}"
    echo -e "  ${YELLOW}--llm${NC}              Display comprehensive usage information (LLM-friendly)"
    echo -e "  ${YELLOW}--help, -h${NC}         Display this usage information"
    echo ""
    echo -e "${BOLD}FEATURES:${NC}"
    echo -e "  • Enhanced colorful terminal output with status indicators"
    echo -e "  • Automatic directory creation with proper error handling"
    echo -e "  • Tool validation and permission management"
    echo -e "  • PATH verification with configuration suggestions"
    echo -e "  • Comprehensive dotfiles backup across multiple applications"
    echo -e "  • Installation summary with detailed feedback"
    echo ""
    echo -e "${BOLD}EXAMPLES:${NC}"
    echo -e "  ${GREEN}$0${NC}                 # Install tools to ~/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 ~/.local/bin${NC}    # Install tools to ~/.local/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 --llm${NC}           # Show comprehensive help for AI assistants"
    echo -e "  ${GREEN}$0 --help${NC}          # Show this help message"
    echo ""
    echo -e "${BOLD}DIRECTORY STRUCTURE:${NC}"
    echo -e "  ${CYAN}tools/${NC}              # Source directory containing executable tools"
    echo -e "  ${CYAN}dotfiles/zshrc${NC}      # Backed up shell configuration"
    echo -e "  ${CYAN}dotfiles/ghostty/${NC}   # Backed up terminal configuration"
    echo -e "  ${CYAN}dotfiles/nvim/${NC}      # Backed up Neovim configuration"
    echo ""
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    exit 0
}

# Check for help flags
if [ $# -eq 1 ] &amp;&amp; ([ "$1" = "--llm" ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]); then
    usage
fi

# Display header
print_header

# Display configuration
print_step "Configuration"
print_info "Project directory: ${BOLD}$PROJECT_DIR${NC}"
print_info "Tools directory: ${BOLD}$TOOLS_DIR${NC}"
print_info "Target directory: ${BOLD}$TARGET_DIR${NC}"

# Check if tools directory exists
print_step "Validation"
if [ ! -d "$TOOLS_DIR" ]; then
    print_error "Tools directory not found at $TOOLS_DIR"
    exit 1
fi
print_success "Tools directory found"

# Create target directory if it doesn't exist
if [ ! -d "$TARGET_DIR" ]; then
    print_info "Creating target directory..."
    mkdir -p "$TARGET_DIR"
    if [ $? -eq 0 ]; then
        print_success "Target directory created"
    else
        print_error "Failed to create target directory"
        exit 1
    fi
else
    print_success "Target directory exists"
fi

# Copy all files from tools directory
print_step "Copying Tools"
tool_count=0
copied_tools=()

for tool in "$TOOLS_DIR"/*; do
    if [ -f "$tool" ]; then
        tool_name=$(basename "$tool")
        # Skip CLAUDE.md file
        if [ "$tool_name" = "CLAUDE.md" ]; then
            continue
        fi
        if cp "$tool" "$TARGET_DIR/" 2&gt;/dev/null; then
            print_item "Copied ${BOLD}$tool_name${NC}"
            copied_tools+=("$tool_name")
            ((tool_count++))
        else
            print_error "Failed to copy $tool_name"
        fi
    fi
done

if [ $tool_count -eq 0 ]; then
    print_warning "No tools found in $TOOLS_DIR"
    exit 0
fi

# Make all copied tools executable
print_step "Setting Permissions"
executable_count=0
for tool_name in "${copied_tools[@]}"; do
    tool_path="$TARGET_DIR/$tool_name"
    if chmod u+x "$tool_path" 2&gt;/dev/null; then
        print_item "Made ${BOLD}$tool_name${NC} executable"
        ((executable_count++))
    else
        print_error "Failed to make $tool_name executable"
    fi
done

# Display completion summary
print_step "Summary"
print_success "Installation complete!"
print_info "${BOLD}$tool_count${NC} tools copied and ${BOLD}$executable_count${NC} made executable"
print_info "Tools are now available in: ${BOLD}$TARGET_DIR${NC}"

# List installed tools
if [ ${#copied_tools[@]} -gt 0 ]; then
    echo ""
    print_info "${BOLD}Installed tools:${NC}"
    for tool_name in "${copied_tools[@]}"; do
        echo -e "   ${GREEN}▸${NC} ${BOLD}$tool_name${NC}"
    done
fi

# Check if target directory is in PATH
echo ""
if [[ ":$PATH:" != *":$TARGET_DIR:"* ]]; then
    print_warning "Target directory is not in your PATH"
    echo -e "${YELLOW}💡${NC} To use these tools from anywhere, add this line to your ${BOLD}~/.bashrc${NC} or ${BOLD}~/.zshrc${NC}:"
    echo -e "   ${CYAN}export PATH=\"$TARGET_DIR:\$PATH\"${NC}"
else
    print_success "Target directory is already in your PATH"
fi

# Copy dotfiles section
print_step "Copying Dotfiles"
DOTFILES_DIR="$PROJECT_DIR/dotfiles"

# Ensure dotfiles directory exists
if [ ! -d "$DOTFILES_DIR" ]; then
    print_info "Creating dotfiles directory..."
    mkdir -p "$DOTFILES_DIR"
    if [ $? -eq 0 ]; then
        print_success "Dotfiles directory created"
    else
        print_error "Failed to create dotfiles directory"
    fi
else
    print_success "Dotfiles directory exists"
fi

# Copy .zshrc
if [ -f "$HOME/.zshrc" ]; then
    if cp "$HOME/.zshrc" "$DOTFILES_DIR/.zshrc" 2&gt;/dev/null; then
        print_item "Copied ${BOLD}.zshrc${NC} from home directory"
    else
        print_error "Failed to copy .zshrc"
    fi
else
    print_warning ".zshrc not found in home directory"
fi

# Copy ghostty config
GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"
if [ -d "$GHOSTTY_CONFIG_DIR" ]; then
    # Create ghostty subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/ghostty"
    if cp -r "$GHOSTTY_CONFIG_DIR"/* "$DOTFILES_DIR/ghostty/" 2&gt;/dev/null; then
        print_item "Copied ${BOLD}ghostty config${NC} from ~/.config/ghostty"
    else
        print_error "Failed to copy ghostty config"
    fi
else
    print_warning "Ghostty config directory not found at ~/.config/ghostty"
fi

# Copy nvim init.lua
NVIM_CONFIG="$HOME/.config/nvim/init.lua"
if [ -f "$NVIM_CONFIG" ]; then
    # Create nvim subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/nvim"
    if cp "$NVIM_CONFIG" "$DOTFILES_DIR/nvim/init.lua" 2&gt;/dev/null; then
        print_item "Copied ${BOLD}nvim init.lua${NC} from ~/.config/nvim"
    else
        print_error "Failed to copy nvim init.lua"
    fi
else
    print_warning "Nvim init.lua not found at ~/.config/nvim/init.lua"
fi

echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}${GREEN}🎉 Setup completed successfully!${NC}"
echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
</file>
  <file path="source.xml">&lt;?xml version='1.0' encoding='utf-8'?&gt;
&lt;source_code project="collect"&gt;
  &lt;file path="test_generate_prompt.py"&gt;import pytest
from config import Config
from collect import generate_prompt


@pytest.fixture
def sample_prompt():
    """Sample prompt content for testing."""
    return """Create a helpful AI assistant that can answer programming questions.
The assistant should be knowledgeable about Python, JavaScript, and web development.
It should provide clear explanations and code examples when appropriate."""


class TestGeneratePrompt:

    @pytest.mark.asyncio
    async def test_generate_prompt_basic(self, sample_prompt):
        """Test basic functionality of generate_prompt."""
        # Check if we have required config
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(sample_prompt)

        # Verify we got a string response
        assert isinstance(result, str)
        assert len(result) &amp;gt; 0

        # The generated prompt should contain relevant content
        # Note: We can't predict exact content, but it should be substantial
        assert len(result) &amp;gt; 50  # Should be more than just a few words

    @pytest.mark.asyncio
    async def test_generate_prompt_with_target_model(self, sample_prompt):
        """Test generate_prompt with target_model parameter."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(
            sample_prompt, target_model="claude-3-7-sonnet-20250219"
        )

        assert isinstance(result, str)
        assert len(result) &amp;gt; 0

    @pytest.mark.asyncio
    async def test_generate_prompt_empty_string(self):
        """Test error handling for empty prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("")

    @pytest.mark.asyncio
    async def test_generate_prompt_whitespace_only(self):
        """Test error handling for whitespace-only prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("   \n\t   ")

    @pytest.mark.asyncio
    async def test_generate_prompt_simple_task(self):
        """Test with a simple task description."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        simple_task = "A coding assistant that helps with Python"
        result = await generate_prompt(simple_task)

        assert isinstance(result, str)
        assert len(result) &amp;gt; len(simple_task)  # Should be expanded


if __name__ == "__main__":
    # Run a simple test

    async def manual_test():
        """Manual test function for quick verification."""
        test_prompt = "Create a Python function that validates email addresses."

        try:
            result = await generate_prompt(test_prompt)
            print(f"Input prompt: {test_prompt}")
            print(f"Generated prompt ({len(result)} chars):")
            print("-" * 50)
            print(result)
            print("-" * 50)
        except Exception as e:
            print(f"Error: {e}")

    # Uncomment to run manual test
    # asyncio.run(manual_test())
&lt;/file&gt;
  &lt;file path="collect.py"&gt;from typing import List
from mcp.server.fastmcp import FastMCP, Context
import tiktoken
import markdownify
import readabilipy.simple_json
from html_to_markdown import convert_to_markdown
from bs4 import BeautifulSoup
from secret_manager import SecretManager
from config import Config
from models.anthropic_mpc import AnthropicMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP
from models.gemini_mcp import GeminiMCP
from fetcher import Fetcher
import pyperclip
from reviewer.code_review import CodeReviewer
import subprocess
import atexit
import time

mcp = FastMCP("Collect")


@mcp.tool()
async def run_code_review(from_file: str, to_file: str = "codereview"):
    """
    Run code review on a diff file using multiple LLM models.

    Args:
        from_file: Path to the file containing the diff/code to review
        to_file: Directory name to write results to (default: "codereview")

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_code(from_file, to_file)


@mcp.tool()
async def run_git_diff_review(to_file: str = "codereview", staged_only: bool = True):
    """
    Run code review on git diff output.

    Args:
        to_file: Directory name to write results to(default: "codereview")
        staged_only: If True, review only staged changes;
        if False, review all changes

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_diff_from_git(to_file, staged_only)


@mcp.tool()
async def fetch_urls(urls: List[str], ctx: Context = None) -&amp;gt; str:
    """
    Fetch content from multiple URLs concurrently and merge the responses.

    Use this tool when you need to:
    - Retrieve content from multiple web pages at once
    - Compare information across multiple sources
    - Gather data from several API endpoints simultaneously
    - Fetch related pages in parallel for efficiency

    Args:
        urls: List of URLs to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Merged content from all URLs as a single string

    Example:
        fetch_urls(["https://api.example.com/users",
                   "https://api.example.com/posts"])
    """
    fetcher = Fetcher(ctx)
    merged_responses = await fetcher.fetch_urls(urls)
    return merged_responses


@mcp.tool()
async def fetch_url(url: str, ctx: Context = None) -&amp;gt; str:
    """
    Fetch raw content from a single URL.

    Use this tool when you need to:
    - Retrieve raw HTML/JSON from a web page or API
    - Get unprocessed content for custom parsing
    - Access web resources programmatically
    - Fetch data before converting to markdown

    Args:
        url: The URL to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Raw content from the URL(HTML, JSON, or plain text)

    Note: For documentation extraction, consider using get_docs instead.
          For markdown conversion, use to_markdown on the result.
    """
    fetcher = Fetcher(ctx)
    return fetcher.get(url)


@mcp.tool()
async def get_docs(url: str, extract_value: str = None, ctx: Context = None) -&amp;gt; str:
    """
    Fetch and extract specific documentation content from web pages.

    Use this tool when users need to:
    - Extract specific sections from documentation websites
    - Get targeted information from technical docs
    - Retrieve API documentation for specific methods/classes
    - Pull configuration examples from documentation
    - Find specific topics within large documentation sites

    Args:
        url: The URL of the documentation page to fetch
        extract_value: Optional. Specific section/topic to extract(e.g., "authentication",
                      "API endpoints", "installation guide"). If not provided, returns
                      the entire page content.
        ctx: MCP context(automatically provided)

    Returns:
        Extracted documentation content as markdown. If extract_value is specified,
        uses Gemini AI to intelligently extract only the relevant section.

    Examples:
        - get_docs("https://docs.python.org/3/", "datetime module")
        - get_docs("https://fastapi.tiangolo.com/", "dependency injection")
        - get_docs("https://react.dev/", "useEffect hook")
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-flash-preview-05-20"
    gemini = GeminiMCP(config, secret_mgr, model=model)

    if extract_value is None:
        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        return response
    else:
        prompt_prefatory = f"""
        # Documentation Extraction Task

        Extract and format the documentation for: **{extract_value} **

        # Instructions:
        - Focus specifically on the requested section/topic
        - Include code examples, parameters, and usage details if present
        - Maintain original formatting and structure
        - If the exact section isn't found, extract the most relevant related content
        - Return only the extracted documentation content, no meta-commentary

        ## Content to extract: {extract_value}
        """

        prompt = prompt_prefatory + "\n\n"
        response = await gemini.build_prompt_from_url(url, prompt, ctx)
        return response.strip()


@mcp.tool()
async def copy_clipboard(text: str) -&amp;gt; str:
    """
    Copy text to the system clipboard.

    Use this tool when users need to:
    - Copy generated code snippets to clipboard
    - Save formatted text for pasting elsewhere
    - Copy API keys, URLs, or configuration values
    - Transfer content between applications

    Args:
        text: The text content to copy to clipboard

    Note: The text will replace any existing clipboard content.
    """
    pyperclip.copy(text)


@mcp.tool()
def strip_html(html: str) -&amp;gt; str:
    """
    Remove all HTML tags and return plain text content.

    Use this tool when you need to:
    - Extract plain text from HTML pages
    - Remove formatting and tags from web content
    - Clean HTML for text analysis
    - Prepare content for non-HTML processing

    Args:
        html: Raw HTML string to process

    Returns:
        Plain text with all HTML tags removed

    Note: This removes ALL formatting. For readable formatting, use to_markdown instead.
    """
    soup = BeautifulSoup(html, "lxml")
    return soup.get_text()


@mcp.tool()
def to_markdown(html: str) -&amp;gt; str:
    """Extract and convert HTML content to markdown using markdownify
    and readabilipy

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown

    """
    html_to_json = readabilipy.simple_json.simple_json_from_html_string(
        html,
        use_readability=True,
    )
    if not html_to_json["content"]:
        return "&amp;lt;error&amp;gt;Page failed to be simplified from HTML to json&amp;lt;/error&amp;gt;"

    return markdownify.markdownify(
        html_to_json["content"],
        heading_style=markdownify.ATX,
    )


def html_to_markdown(html: str) -&amp;gt; str:
    """This uses html-to-markdown library instead of markdownify

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown as a str
    """

    return convert_to_markdown(
        html,
        heading_style="atx",
    )


@mcp.tool()
async def get_anthropic_model_list() -&amp;gt; List[str]:
    """
    Get the list of available Anthropic Claude models.

    Use this tool when you need to:
    - Check which Claude models are available
    - Verify model names before using them
    - List Anthropic's current model offerings
    - Help users choose between Claude models

    Returns:
        List of available Anthropic model names (e.g., ["claude-3-opus", "claude-3-sonnet"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.get_model_list()


@mcp.tool()
async def get_openai_model_list() -&amp;gt; List[str]:
    """
    Get the list of available OpenAI models.

    Use this tool when you need to:
    - Check which GPT models are available
    - Verify OpenAI model names
    - List current OpenAI offerings
    - Help users choose between GPT models

    Returns:
        List of available OpenAI model names (e.g., ["gpt-4", "gpt-3.5-turbo"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    openai_mcp = OpenAIMCP(config, secret_mgr, model="gpt-4o")
    return openai_mcp.get_model_list()


@mcp.tool()
async def get_xai_model_list() -&amp;gt; List[str]:
    """
    Get the list of available XAI (Grok) models.

    Use this tool when you need to:
    - Check which Grok models are available
    - Verify XAI model names
    - List current Grok offerings
    - Help users choose between Grok models

    Returns:
        List of available XAI model names (e.g., ["grok-3", "grok-3-mini"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-mini-fast-latest")
    return xai_mcp.get_model_list()


@mcp.tool()
async def get_gemini_model_list() -&amp;gt; List[dict]:
    """
    Get the list of available Google Gemini models
    (filtered for 2.0 and 2.5 versions).

    Use this tool when you need to:
    - Check which Gemini models are available with their token limits
    - Verify Google AI model names and capabilities
    - List current Gemini 2.0 and 2.5 offerings
    - Help users choose between Gemini models based on token capacity

    Returns:
        List of model dictionaries sorted by token limit (highest first),
        each containing:
        - model_name: The model identifier (e.g., "gemini-2.5-flash")
        - token_window: Input token limit (e.g., 1048576)

    Example return:
        [
            {"model_name": "gemini-2.5-flash", "token_window": 1048576},
            {"model_name": "gemini-2.0-flash", "token_window": 1048576},
            {"model_name": "gemini-2.5-pro", "token_window": 1048576}
        ]
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.5-flash")
    return gemini_mcp.get_model_list()


@mcp.tool()
async def count_openai_tokens(text: str, model: str = "gpt-4") -&amp;gt; int:
    """
    Count tokens in text using OpenAI's tiktoken tokenizer.

    Use this tool when you need to:
    - Check if content fits within OpenAI model limits
    - Estimate API costs for OpenAI models
    - Split content to fit token windows
    - Optimize prompts for token efficiency

    Args:
        text: The text to count tokens for
        model: OpenAI model name (default: "gpt-4")

    Returns:
        Number of tokens in the text for the specified model
    """
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@mcp.tool()
async def count_anthropic_tokens(text: str) -&amp;gt; int:
    """
    Count tokens in text using Anthropic's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Claude model limits
    - Estimate API costs for Anthropic models
    - Split content for Claude's context window
    - Optimize prompts for Claude

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Anthropic models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.count_tokens(text)


@mcp.tool()
async def count_gemini_tokens(text: str) -&amp;gt; int:
    """
    Count tokens in text using Google Gemini's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Gemini model limits
    - Estimate API costs for Google AI models
    - Split content for Gemini's context window
    - Optimize prompts for Gemini

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Gemini models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.0-flash")
    return gemini_mcp.count_tokens(text)


@mcp.tool()
async def count_grok_tokens(text: str) -&amp;gt; int:
    """
    Count tokens in text using XAI Grok's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Grok model limits
    - Estimate API costs for XAI models
    - Split content for Grok's context window
    - Optimize prompts for Grok

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Grok models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-fast-latest")
    return xai_mcp.count_tokens(text)


@mcp.tool()
async def generate_prompt(prompt: str, target_model: str = None) -&amp;gt; str:
    """
    Generate an optimized AI prompt using Anthropic's experimental prompt engineering API.

    This tool leverages Anthropic's closed research preview API to automatically create
    high-quality, structured prompts from simple task descriptions. The API analyzes
    your input and generates professional-grade prompts optimized for Claude models.

    Use this tool when you need to:
    - Transform simple ideas into comprehensive AI prompts
    - Create structured prompts for specific tasks or roles
    - Optimize prompts for better AI responses
    - Generate consistent prompt templates for repeated use
    - Improve prompt clarity and effectiveness

    Args:
        prompt: A brief description of what you want the AI to do.
                Can be as simple as a role description or task summary.
                Examples:
                - "a helpful programming assistant"
                - "a chef for meal planning"
                - "a technical documentation writer"
                - "analyze code for security vulnerabilities"
        target_model: Optional. The specific model to optimize for (e.g., "claude-3-opus").
                     If not specified, generates a general-purpose prompt.

    Returns:
        A professionally crafted prompt ready for use with Claude or other AI models.
        The generated prompt includes appropriate context, instructions, and structure
        to maximize response quality.

    Raises:
        ValueError: If the prompt is empty or only contains whitespace
        RuntimeError: If the API call fails or returns an unexpected response

    Example:
        &amp;gt;&amp;gt;&amp;gt; result = await generate_prompt("a Python code reviewer")
        &amp;gt;&amp;gt;&amp;gt; print(result)
        "You are an expert Python code reviewer with deep knowledge..."

    Note:
        This uses Anthropic's experimental "prompt-tools" API which requires special
        access. The API is in closed research preview and may change without notice.
    """
    try:
        # Validate input
        task_content = prompt.strip()
        if not task_content:
            raise ValueError("Prompt cannot be empty")

        # Set up Anthropic MCP client
        config = Config()
        secret_mgr = SecretManager(config.project_id)
        anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)

        # Call generate_prompt API with new signature
        response = anthropic_mcp.generate_prompt(task_content, target_model)

        # Extract the generated prompt text from the response
        if response.messages and response.messages[0].content:
            return response.messages[0].content[0].text
        else:
            raise ValueError("No prompt generated in response")

    except ValueError:
        # Re-raise ValueError (like empty prompt) without wrapping
        raise
    except Exception as e:
        raise RuntimeError(f"Error generating prompt: {str(e)}")


def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["uv", "run", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for API to initialize
        time.sleep(2)

        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")

            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()

            atexit.register(cleanup_api)

    except Exception as e:
        print(f"Failed to start API server: {e}")

    # Continue with MCP server startup
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
&lt;/file&gt;
  &lt;file path="config.py"&gt;from dotenv import load_dotenv
import os


class Config:
    # Load .env for configuration
    load_dotenv(".env")

    def __init__(self) -&amp;gt; None:
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.port = os.getenv("PORT")
        self.db_path = os.getenv("DB_PATH")
        self.anthropic_key_path = os.getenv("ANTHROPIC_API_KEY_PATH")
        self.anthropic_model_opus = os.getenv("ANTHROPIC_MODEL_OPUS")
        self.anthropic_model_sonnet = os.getenv("ANTHROPIC_MODEL_SONNET")
        self.gemini_api_key_path = os.getenv("GEMINI_API_KEY_PATH")
        self.gemini_base_url = os.getenv("GEMINI_BASE_URL")
        self.xai_api_key_path = os.getenv("XAI_API_KEY_PATH")
        self.grok_system_prompt = os.getenv("GROK_SYSTEM_PROMPT")
        self.openai_api_key_path = os.getenv("OPENAI_API_KEY_PATH")
        self.openai_default_code_review_model = os.getenv(
            "OPENAI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.gemini_default_code_review_model = os.getenv(
            "GEMINI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.anthropic_default_code_review_model = os.getenv(
            "ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.xai_default_code_review_model = os.getenv("XAI_DEFAULT_CODE_REVIEW_MODEL")

        # GitHub configuration
        self.github_url = os.getenv("GITHUB_URL")

        # Command subdirectories - read as comma-separated string
        command_subdirs_str = os.getenv(
            "COMMAND_SUBDIRS", "archive,go,js,mcp,python,tools"
        )
        self.command_subdirs = [
            subdir.strip() for subdir in command_subdirs_str.split(",")
        ]
&lt;/file&gt;
  &lt;file path="test_collect.py"&gt;import pytest

from collect import (
    count_anthropic_tokens,
    count_gemini_tokens,
    count_openai_tokens,
    count_grok_tokens,
    get_docs,
)

# The @pytest.mark.parametrize decorator runs the test function
#  test_empty_text_returns_zero three separate times, once for each
#  function in the list. Each time, it passes a different token-counting
#  function to the test as the func parameter.

#  This is useful for testing similar functionality across multiple
#  implementations without duplicating test code. In this case, it verifies
#   that all three token-counting functions return zero when given empty
#  text.


@pytest.mark.asyncio
async def test_openai_hello_token_count():
    result = await count_openai_tokens("hello", model="gpt-3.5-turbo")
    assert result == 1


@pytest.mark.parametrize(
    "func,text",
    [
        (count_openai_tokens, "Hello, world!"),
        (count_gemini_tokens, "Hello, Gemini!"),
        (count_anthropic_tokens, "Hello Claude"),
        (count_grok_tokens, "Hello Grok"),
    ],
)
@pytest.mark.asyncio
async def test_nonempty_text_returns_positive_int(func, text):
    n = await func(text)
    assert isinstance(n, int)
    assert n &amp;gt; 0


@pytest.mark.asyncio
async def test_get_docs_with_extract_value():
    url = "https://docs.python.org/3/library/json.html"
    extract_value = "json.dumps"

    result = await get_docs(url, extract_value)

    assert isinstance(result, str)
    assert len(result) &amp;gt; 0
    assert "json.dumps" in result.lower()

    print(f"Extracted docs for {extract_value}:")
    print(result[:500] + "..." if len(result) &amp;gt; 500 else result)


@pytest.mark.asyncio
async def test_get_docs_without_extract_value():
    url = "https://docs.python.org/3/library/json.html"

    result = await get_docs(url)

    assert isinstance(result, str)
    assert len(result) &amp;gt; 0
    # Should contain raw HTML content when no extraction is performed
    assert "html" in result.lower() or "json" in result.lower()

    print(f"Raw content length: {len(result)}")
    print(result[:200] + "..." if len(result) &amp;gt; 200 else result)
&lt;/file&gt;
  &lt;file path="requirements.txt"&gt;aiohttp==3.11.11
annotated-types==0.7.0
anthropic==0.50.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.8
distro==1.9.0
docstring-parser==0.16
-e file:///Users/benjaminmetz/python/collect
google-api-core==2.24.2
google-auth==2.39.0
google-cloud-aiplatform==1.91.0
google-cloud-bigquery==3.31.0
google-cloud-core==2.4.3
google-cloud-resource-manager==1.14.2
google-cloud-storage==2.19.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
idna==3.10
iniconfig==2.1.0
jiter==0.9.0
markdown-it-py==3.0.0
mcp==1.7.1
mdurl==0.1.2
numpy==2.2.5
packaging==25.0
pluggy==1.5.0
proto-plus==1.26.1
protobuf==5.29.4
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.11.4
pydantic-core==2.33.2
pydantic-settings==2.9.1
pygments==2.19.1
pyperclip==1.9.0
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
regex==2024.11.6
requests==2.32.3
rich==14.0.0
rsa==4.9.1
sentencepiece==0.2.0
shapely==2.1.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
sse-starlette==2.3.3
starlette==0.46.2
tiktoken==0.9.0
typer==0.15.3
typing-extensions==4.13.2
typing-inspection==0.4.0
urllib3==2.4.0
uvicorn==0.34.2
&lt;/file&gt;
  &lt;file path="uv.lock"&gt;version = 1
revision = 1
requires-python = "&amp;gt;=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.12.11"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/6b/850a842871ab7be0d00686750d0ee9d8fb8e7be981e4e5700bb6c88f1b8f/aiohttp-3.12.11.tar.gz", hash = "sha256:a5149ae1b11ce4cf8b122846bfa3d7c5f29fe3bfe6745ab21b3eea9615bc5564", size = 7814403 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/ac/15e21c6a17b5183d1617505b125c773f554a56e06be577a289151a8e5ce7/aiohttp-3.12.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:5fadc4b67f972a701805aa501cd9d22cdbeda21f9c9ae85e60678f84b1727a16", size = 694170 },
    { url = "https://files.pythonhosted.org/packages/02/5b/347f8aff5793829b3a31a927bd039ec4f22221a32c459b9d19fe880921e3/aiohttp-3.12.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:144d67c29ae36f052584fc45a363e92798441a5af5762d83037aade3e2aa9dc5", size = 471832 },
    { url = "https://files.pythonhosted.org/packages/4b/e5/9ed82f5b6a2dca30940e90820ce2f8203e15111de464bba0980e2c7e169b/aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6b73299e4bf37d14c6e4ca5ce7087b44914a8d9e1f40faedc271f28d64ec277e", size = 464133 },
    { url = "https://files.pythonhosted.org/packages/3c/8d/edcddc41d4f1157a2536143476070ae66de2b839af3724655c2a6358670a/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1226325e98e6d3cdfdaca639efdc3af8e82cd17287ae393626d1bd60626b0e93", size = 1702942 },
    { url = "https://files.pythonhosted.org/packages/b1/2e/efcb6a35d0646ced659edc3172e8e9384246d2cd0b0f3080fc3c441cb511/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0ecae011f2f779271407f2959877230670de3c48f67e5db9fbafa9fddbfa3a", size = 1684207 },
    { url = "https://files.pythonhosted.org/packages/56/f7/0324c499b7c610633d2f5e8af5457fd3a0584f5f4827bc46b673866596ac/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a8a711883eedcd55f2e1ba218d8224b9f20f1dfac90ffca28e78daf891667e3a", size = 1736275 },
    { url = "https://files.pythonhosted.org/packages/98/0f/b7aa0fd1ed777b5d6fb62c0dcf82effb717e8b51c802067fc3bcb703e003/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2601c1fcd9b67e632548cfd3c760741b31490502f6f3e5e21287678c1c6fa1b2", size = 1785648 },
    { url = "https://files.pythonhosted.org/packages/2c/2a/7defcf31010a2964bf17f6c9d9190e3be889f0c5edc3ff2cdac6e60764d7/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d5b11ea794ee54b33d0d817a1aec0ef0dd2026f070b493bc5a67b7e413b95d4", size = 1707981 },
    { url = "https://files.pythonhosted.org/packages/b6/9e/ff3d9a01f533752e81fd92bfe1301ae5a7bd5a306d752ad54f8bc61570fa/aiohttp-3.12.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:109b3544138ce8a5aca598d5e7ff958699e3e19ee3675d27d5ee9c2e30765a4a", size = 1621683 },
    { url = "https://files.pythonhosted.org/packages/2c/98/446c96927f2e7d2eaea95660a60eb6077771d00df834430cec002cadd96b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b795085d063d24c6d09300c85ddd6b9c49816d5c498b40b6899ca24584e936e4", size = 1674706 },
    { url = "https://files.pythonhosted.org/packages/e1/2a/038cb4af5e58994bc9315d0cb6a906d20ddfffb8eb3d0dfcfe8fe95b1939/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ebcbc113f40e4c9c0f8d2b6b31a2dd2a9768f3fa5f623b7e1285684e24f5159f", size = 1706372 },
    { url = "https://files.pythonhosted.org/packages/28/18/dc16cc7cb9b8baf9308f23ecf1e787d916238d01060bea272d5c29e9aa6b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:590e5d792150d75fa34029d0555b126e65ad50d66818a996303de4af52b65b32", size = 1648967 },
    { url = "https://files.pythonhosted.org/packages/44/f5/f427ef971e00088c7f0f5a4a7e405732e0ce0b87dfc3eec0f1a8c16863d2/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:9c2a4dec596437b02f0c34f92ea799d6e300184a0304c1e54e462af52abeb0a8", size = 1725099 },
    { url = "https://files.pythonhosted.org/packages/d4/0a/34fc018d4e193115b512bc08f6afaf79c23609a6487e47f0d593d1d9df41/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aace119abc495cc4ced8745e3faceb0c22e8202c60b55217405c5f389b569576", size = 1758571 },
    { url = "https://files.pythonhosted.org/packages/b6/69/b466ec346506384a93bcb864ab75a21b6520c64fcc3720ab2056470a657f/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd749731390520a2dc1ce215bcf0ee1018c3e2e3cd834f966a02c0e71ad7d637", size = 1707461 },
    { url = "https://files.pythonhosted.org/packages/f4/fc/3437d3e40581bc7d0816e134fdcae3c7e5c3f21dbdcfbd54402af3973b1c/aiohttp-3.12.11-cp313-cp313-win32.whl", hash = "sha256:65952736356d1fbc9efdd17492dce36e2501f609a14ccb298156e392d3ad8b83", size = 420053 },
    { url = "https://files.pythonhosted.org/packages/6c/cf/cd84df67147c986315c63fef29a6ecadf03bf5528340b8c82eedd988cf57/aiohttp-3.12.11-cp313-cp313-win_amd64.whl", hash = "sha256:854132093e12dd77f5c07975581c42ae51a6a8868dcbbb509c77d1963c3713b7", size = 445988 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.50.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/40/85/4dd9f80da0727c56d7e7f7c627cb724edd9e6df062df6ecc0e90f06e6dbb/anthropic-0.50.0.tar.gz", hash = "sha256:42175ec04ce4ff2fa37cd436710206aadff546ee99d70d974699f59b49adc66f", size = 213021 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/ae/975f97ad5581a9e187a3717e21d79d6c7ad6be926fee9aa8a15b3d9f8f37/anthropic-0.50.0-py3-none-any.whl", hash = "sha256:defbd79327ca2fa61fd7b9eb2f1627dfb1f69c25d49288c52e167ddb84574f80", size = 245291 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "asttokens"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4a/e7/82da0a03e7ba5141f05cce0d302e6eed121ae055e0456ca228bf693984bc/asttokens-3.0.0.tar.gz", hash = "sha256:0dcd8baa8d62b0c1d118b399b2ddba3c4aff271d0d7a9e0d4c1681c79035bbc7", size = 61978 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/8a/c46dcc25341b5bce5472c718902eb3d38600a903b14fa6aeecef3f21a46f/asttokens-3.0.0-py3-none-any.whl", hash = "sha256:e3078351a059199dd5138cb1c706e6430c05eff2ff136af5eb4790f9d28932e2", size = 26918 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285 },
]

[[package]]
name = "black"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "mypy-extensions" },
    { name = "packaging" },
    { name = "pathspec" },
    { name = "platformdirs" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/49/26a7b0f3f35da4b5a65f081943b7bcd22d7002f5f0fb8098ec1ff21cb6ef/black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666", size = 649449 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/87/0edf98916640efa5d0696e1abb0a8357b52e69e82322628f25bf14d263d1/black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f", size = 1650673 },
    { url = "https://files.pythonhosted.org/packages/52/e5/f7bf17207cf87fa6e9b676576749c6b6ed0d70f179a3d812c997870291c3/black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3", size = 1453190 },
    { url = "https://files.pythonhosted.org/packages/e3/ee/adda3d46d4a9120772fae6de454c8495603c37c4c3b9c60f25b1ab6401fe/black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171", size = 1782926 },
    { url = "https://files.pythonhosted.org/packages/cc/64/94eb5f45dcb997d2082f097a3944cfc7fe87e071907f677e80788a2d7b7a/black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18", size = 1442613 },
    { url = "https://files.pythonhosted.org/packages/09/71/54e999902aed72baf26bca0d50781b01838251a462612966e9fc4891eadd/black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717", size = 207646 },
]

[[package]]
name = "cachetools"
version = "5.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/81/3747dad6b14fa2cf53fcf10548cf5aea6913e96fab41a3c198676f8948a5/cachetools-5.5.2.tar.gz", hash = "sha256:1a661caa9175d26759571b2e19580f9d6393969e5dfca11fdb1f947a23e640d4", size = 28380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl", hash = "sha256:d26a22bcc62eb95c3beabd9f1ee5e820d3d2704fe2967cbe350e20c8ffcd3f0a", size = 10080 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "collect"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "anthropic" },
    { name = "beautifulsoup4" },
    { name = "black" },
    { name = "fastapi" },
    { name = "google-ai-generativelanguage" },
    { name = "google-api-python-client" },
    { name = "google-auth-httplib2" },
    { name = "google-cloud-aiplatform", extra = ["tokenization"] },
    { name = "google-cloud-secret-manager" },
    { name = "google-genai" },
    { name = "google-generativeai" },
    { name = "html-to-markdown" },
    { name = "html5lib" },
    { name = "httplib2" },
    { name = "httpx" },
    { name = "ipython" },
    { name = "lxml" },
    { name = "marimo" },
    { name = "markdownify" },
    { name = "mcp", extra = ["cli"] },
    { name = "openai" },
    { name = "pathspec" },
    { name = "pyperclip" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
    { name = "pytest-xdist" },
    { name = "python-json-logger" },
    { name = "readabilipy" },
    { name = "rich" },
    { name = "ruff" },
    { name = "tiktoken" },
    { name = "uvicorn" },
    { name = "yoyo-migrations" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = "&amp;gt;=3.12.11" },
    { name = "anthropic", specifier = "&amp;gt;=0.50.0" },
    { name = "beautifulsoup4", specifier = "&amp;gt;=4.13.4" },
    { name = "black", specifier = "&amp;gt;=25.1.0" },
    { name = "fastapi", specifier = "&amp;gt;=0.116.1" },
    { name = "google-ai-generativelanguage", specifier = "&amp;gt;=0.6.15" },
    { name = "google-api-python-client", specifier = "&amp;gt;=2.169.0" },
    { name = "google-auth-httplib2", specifier = "&amp;gt;=0.2.0" },
    { name = "google-cloud-aiplatform", extras = ["tokenization"], specifier = "&amp;gt;=1.91.0" },
    { name = "google-cloud-secret-manager", specifier = "&amp;gt;=2.23.3" },
    { name = "google-genai", specifier = "&amp;gt;=1.13.0" },
    { name = "google-generativeai", specifier = "&amp;gt;=0.8.5" },
    { name = "html-to-markdown", specifier = "&amp;gt;=1.3.2" },
    { name = "html5lib", specifier = "&amp;gt;=1.1" },
    { name = "httplib2", specifier = "&amp;gt;=0.22.0" },
    { name = "httpx", specifier = "&amp;gt;=0.28.1" },
    { name = "ipython", specifier = "&amp;gt;=9.4.0" },
    { name = "lxml", specifier = "&amp;gt;=5.4.0" },
    { name = "marimo", specifier = "&amp;gt;=0.14.12" },
    { name = "markdownify", specifier = "&amp;gt;=1.1.0" },
    { name = "mcp", extras = ["cli"], specifier = "&amp;gt;=1.7.1" },
    { name = "openai", specifier = "&amp;gt;=1.59.4" },
    { name = "pathspec", specifier = "&amp;gt;=0.12.1" },
    { name = "pyperclip", specifier = "&amp;gt;=1.9.0" },
    { name = "pytest", specifier = "&amp;gt;=8.3.5" },
    { name = "pytest-asyncio", specifier = "&amp;gt;=0.26.0" },
    { name = "pytest-xdist", specifier = "&amp;gt;=3.6.1" },
    { name = "python-json-logger", specifier = "&amp;gt;=3.3.0" },
    { name = "readabilipy", specifier = "&amp;gt;=0.3.0" },
    { name = "rich", specifier = "&amp;gt;=14.0.0" },
    { name = "ruff", specifier = "&amp;gt;=0.11.9" },
    { name = "tiktoken", specifier = "&amp;gt;=0.9.0" },
    { name = "uvicorn", specifier = "&amp;gt;=0.34.2" },
    { name = "yoyo-migrations", specifier = "&amp;gt;=9.0.0" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "decorator"
version = "5.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/fa/6d96a0978d19e17b68d634497769987b16c8f4cd0a7a05048bec693caa6b/decorator-5.2.1.tar.gz", hash = "sha256:65f266143752f734b0a7cc83c46f4618af75b8c5911b00ccb61d0ac9b6da0360", size = 56711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl", hash = "sha256:d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a", size = 9190 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "docstring-parser"
version = "0.16"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/08/12/9c22a58c0b1e29271051222d8906257616da84135af9ed167c9e28f85cb3/docstring_parser-0.16.tar.gz", hash = "sha256:538beabd0af1e2db0146b6bd3caa526c35a34d61af9fd2887f3a8a27a739aa6e", size = 26565 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl", hash = "sha256:bf0a1387354d3691d102edef7ec124f219ef639982d096e26e3b60aeffa90637", size = 36533 },
]

[[package]]
name = "docutils"
version = "0.21.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ae/ed/aefcc8cd0ba62a0560c3c18c33925362d46c6075480bfa4df87b28e169a9/docutils-0.21.2.tar.gz", hash = "sha256:3a6b18732edf182daa3cd12775bbb338cf5691468f91eeeb109deff6ebfa986f", size = 2204444 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/d7/9322c609343d929e75e7e5e6255e614fcc67572cfd083959cdef3b7aad79/docutils-0.21.2-py3-none-any.whl", hash = "sha256:dafca5b9e384f0e419294eb4d2ff9fa826435bf15f15b7bd45723e8ad76811b2", size = 587408 },
]

[[package]]
name = "execnet"
version = "2.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/ff/b4c0dc78fbe20c3e59c0c7334de0c27eb4001a2b2017999af398bf730817/execnet-2.1.1.tar.gz", hash = "sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3", size = 166524 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl", hash = "sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc", size = 40612 },
]

[[package]]
name = "executing"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/50/a9d80c47ff289c611ff12e63f7c5d13942c65d68125160cefd768c73e6e4/executing-2.2.0.tar.gz", hash = "sha256:5d108c028108fe2551d1a7b2e8b713341e2cb4fc0aa7dcf966fa4327a5226755", size = 978693 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7b/8f/c4d9bafc34ad7ad5d8dc16dd1347ee0e507a52c3adb6bfa8887e1c6a26ba/executing-2.2.0-py2.py3-none-any.whl", hash = "sha256:11387150cad388d62750327a53d3339fad4888b39a6fe233c3afbb54ecffd3aa", size = 26702 },
]

[[package]]
name = "fastapi"
version = "0.116.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/78/d7/6c8b3bfe33eeffa208183ec037fee0cce9f7f024089ab1c5d12ef04bd27c/fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143", size = 296485 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565", size = 95631 },
]

[[package]]
name = "frozenlist"
version = "1.6.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5b/bf/a812e2fe6cb3f6c6cfc8d0303bf1742f2286004e5ec41ac8c89cf68cdb54/frozenlist-1.6.2.tar.gz", hash = "sha256:effc641518696471cf4962e8e32050133bc1f7b2851ae8fd0cb8797dd70dc202", size = 43108 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b8/f6/973abfcb8b68f2e8b58071a04ec72f5e1f0acd19dae0d3b7a8abc3d9ab07/frozenlist-1.6.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2ad8851ae1f6695d735f8646bf1e68675871789756f7f7e8dc8224a74eabb9d0", size = 85517 },
    { url = "https://files.pythonhosted.org/packages/c8/d0/ac45f2dcf0afd5f7d57204af8b7516ecbc3599ea681e06f4b25d3845bea8/frozenlist-1.6.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:cd2d5abc0ccd99a2a5b437987f3b1e9c265c1044d2855a09ac68f09bbb8082ca", size = 49916 },
    { url = "https://files.pythonhosted.org/packages/50/cc/99c3f31823630b7411f7c1e83399e91d6b56a5661a5b724935ef5b51f5f5/frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15c33f665faa9b8f8e525b987eeaae6641816e0f6873e8a9c4d224338cebbb55", size = 48107 },
    { url = "https://files.pythonhosted.org/packages/85/4e/38643ce3ee80d222892b694d02c15ea476c4d564493a6fe530347163744e/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3e6c0681783723bb472b6b8304e61ecfcb4c2b11cf7f243d923813c21ae5d2a", size = 255771 },
    { url = "https://files.pythonhosted.org/packages/ca/e6/ceed85a7d5c0f666485384fc393e32353f8088e154a1109e5ef60165d366/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:61bae4d345a26550d0ed9f2c9910ea060f89dbfc642b7b96e9510a95c3a33b3c", size = 252519 },
    { url = "https://files.pythonhosted.org/packages/29/99/9f2e2b90cf918465e3b6ca4eea79e6be53d24fba33937e37d86c3764bbf9/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90e5a84016d0d2fb828f770ede085b5d89155fcb9629b8a3237c960c41c120c3", size = 263348 },
    { url = "https://files.pythonhosted.org/packages/4e/ac/59f3ec4c1b4897186efb4757379915734a48bb16bbc15a9fe0bf0857b679/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55dc289a064c04819d669e6e8a85a1c0416e6c601782093bdc749ae14a2f39da", size = 257858 },
    { url = "https://files.pythonhosted.org/packages/48/4a/19c97510d0c2be1ebaae68383d1b5a256a12a660ca17b0c427b1024d9b92/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b79bcf97ca03c95b044532a4fef6e5ae106a2dd863875b75fde64c553e3f4820", size = 238248 },
    { url = "https://files.pythonhosted.org/packages/ef/64/641aa2b0944fa3d881323948e0d8d6fee746dae03d9023eb510bb80bc46a/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e5e7564d232a782baa3089b25a0d979e2e4d6572d3c7231fcceacc5c22bf0f7", size = 255932 },
    { url = "https://files.pythonhosted.org/packages/6c/f8/5b68d5658fac7332e5d26542a4af0ffc2edca8da8f854f6274882889ee1e/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6fcd8d56880dccdd376afb18f483ab55a0e24036adc9a83c914d4b7bb5729d4e", size = 253329 },
    { url = "https://files.pythonhosted.org/packages/e9/20/379d7a27eb82748b41319bf376bf2c034e7ee11dda94f12b331edcc261ff/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4fbce985c7fe7bafb4d9bf647c835dbe415b465a897b0c79d1bdf0f3fae5fe50", size = 266164 },
    { url = "https://files.pythonhosted.org/packages/13/bd/d7dbf94220020850392cb661bedfdf786398bafae85d1045dd108971d261/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:3bd12d727cd616387d50fe283abebb2db93300c98f8ff1084b68460acd551926", size = 241641 },
    { url = "https://files.pythonhosted.org/packages/a4/70/916fef6284d294077265cd69ad05f228e44f7ed88d9acb690df5a1174049/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:38544cae535ed697960891131731b33bb865b7d197ad62dc380d2dbb1bceff48", size = 261215 },
    { url = "https://files.pythonhosted.org/packages/8f/98/1326a7189fa519692698cddf598f56766b0fea6ac71cddaf64760a055397/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:47396898f98fae5c9b9bb409c3d2cf6106e409730f35a0926aad09dd7acf1ef5", size = 262597 },
    { url = "https://files.pythonhosted.org/packages/f4/d6/0a95ab9289c72e86c37c9b8afe82576556456b6f66a35d242526634130f2/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:d10d835f8ce8571fd555db42d3aef325af903535dad7e6faa7b9c8abe191bffc", size = 258766 },
    { url = "https://files.pythonhosted.org/packages/1b/d0/9e946aabd89ebfcb71ec1371327f0e25d4868cd4439471a6fcb6eaf7b366/frozenlist-1.6.2-cp313-cp313-win32.whl", hash = "sha256:a400fe775a41b6d7a3fef00d88f10cbae4f0074c9804e282013d7797671ba58d", size = 40961 },
    { url = "https://files.pythonhosted.org/packages/43/e9/d714f5eb0fde1413344ded982ae9638307b59651d5c04263af42eb81a315/frozenlist-1.6.2-cp313-cp313-win_amd64.whl", hash = "sha256:cc8b25b321863ed46992558a29bb09b766c41e25f31461666d501be0f893bada", size = 46204 },
    { url = "https://files.pythonhosted.org/packages/f5/7a/8f6dde73862499e60eb390778a1e46b87c1fe3c5722622d731ccda7a173c/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:56de277a0e0ad26a1dcdc99802b4f5becd7fd890807b68e3ecff8ced01d58132", size = 91326 },
    { url = "https://files.pythonhosted.org/packages/79/60/dcdc75edbcf8241e7cb15fced68b3be63f67ff3faaf559c540a7eb63233b/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:9cb386dd69ae91be586aa15cb6f39a19b5f79ffc1511371eca8ff162721c4867", size = 52426 },
    { url = "https://files.pythonhosted.org/packages/64/e6/df2a43ccb2c4f1ea3692aae9a89cfc5dd932a90b7898f98f13ed9e2680a9/frozenlist-1.6.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53835d8a6929c2f16e02616f8b727bd140ce8bf0aeddeafdb290a67c136ca8ad", size = 51460 },
    { url = "https://files.pythonhosted.org/packages/fd/b3/c4f2f7fca9487b25c39bf64535f029316e184072a82f3660ce72defc5421/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cc49f2277e8173abf028d744f8b7d69fe8cc26bffc2de97d47a3b529599fbf50", size = 310270 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/046eb34d8d0fee1a8c9dc91a9ba581283c67a1ace20bcc01c86a53595105/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:65eb9e8a973161bdac5fa06ea6bd261057947adc4f47a7a6ef3d6db30c78c5b4", size = 289062 },
    { url = "https://files.pythonhosted.org/packages/48/7b/80991efaa0aa25e867cf93033c28e9d1310f34f90421eb59eb1f2073d937/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:301eb2f898d863031f8c5a56c88a6c5d976ba11a4a08a1438b96ee3acb5aea80", size = 312202 },
    { url = "https://files.pythonhosted.org/packages/78/6b/6fe30bdababdf82c5b34f0093770c4be6211071e23570721b80b11c9d52a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:207f717fd5e65fddb77d33361ab8fa939f6d89195f11307e073066886b33f2b8", size = 309557 },
    { url = "https://files.pythonhosted.org/packages/9d/ef/b7bf48802fc7d084703ba2173e6a8d0590bea378dcd6a480051c41bddf47/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f83992722642ee0db0333b1dbf205b1a38f97d51a7382eb304ba414d8c3d1e05", size = 282135 },
    { url = "https://files.pythonhosted.org/packages/af/f8/6911a085bce8d0d0df3dfc2560e3e0fb4d6c19ff101014bcf61aa32ba39a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12af99e6023851b36578e5bcc60618b5b30f4650340e29e565cd1936326dbea7", size = 303392 },
    { url = "https://files.pythonhosted.org/packages/9c/5d/b4e0cc6dbd6b9282926a470a919da7c6599ff324ab5268c7ecaff82cb858/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6f01620444a674eaad900a3263574418e99c49e2a5d6e5330753857363b5d59f", size = 309402 },
    { url = "https://files.pythonhosted.org/packages/0f/1b/bf777de3c810e68e8758337fcc97ee8c956376c87aecee9a61ba19a94123/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:82b94c8948341512306ca8ccc702771600b442c6abe5f8ee017e00e452a209e8", size = 312924 },
    { url = "https://files.pythonhosted.org/packages/0e/03/a69b890bc310790fcae61fd3b5be64876811b12db5d50b32e62f65e766bd/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:324a4cf4c220ddb3db1f46ade01e48432c63fa8c26812c710006e7f6cfba4a08", size = 291768 },
    { url = "https://files.pythonhosted.org/packages/70/cc/559386adf987b47c8977c929271d11a72efd92778a0a2f4cc97827a9a25b/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:695284e51458dabb89af7f7dc95c470aa51fd259207aba5378b187909297feef", size = 313305 },
    { url = "https://files.pythonhosted.org/packages/e7/fa/eb0e21730ffccfb2d0d367d863cbaacf8367bdc277b44eabf72f7329ab91/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:9ccbeb1c8dda4f42d0678076aa5cbde941a232be71c67b9d8ca89fbaf395807c", size = 312228 },
    { url = "https://files.pythonhosted.org/packages/d1/c1/8471b67172abc9478ad78c70a3f3a5c4fed6d4bcadc748e1b6dfa06ab2ae/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cbbdf62fcc1864912c592a1ec748fee94f294c6b23215d5e8e9569becb7723ee", size = 309905 },
    { url = "https://files.pythonhosted.org/packages/bb/2c/ee21987c3a175b49d0b827b1e45394a7a5d08c7de5b766ed6d0889d30568/frozenlist-1.6.2-cp313-cp313t-win32.whl", hash = "sha256:76857098ee17258df1a61f934f2bae052b8542c9ea6b187684a737b2e3383a65", size = 44644 },
    { url = "https://files.pythonhosted.org/packages/65/46/fce60f65b1fb17a90c4bf410a5c90cb3b40616cc229e75866f8be97c112c/frozenlist-1.6.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c06a88daba7e891add42f9278cdf7506a49bc04df9b1648be54da1bf1c79b4c6", size = 50607 },
    { url = "https://files.pythonhosted.org/packages/13/be/0ebbb283f2d91b72beaee2d07760b2c47dab875c49c286f5591d3d157198/frozenlist-1.6.2-py3-none-any.whl", hash = "sha256:947abfcc8c42a329bbda6df97a4b9c9cdb4e12c85153b3b57b9d2f02aa5877dc", size = 12582 },
]

[[package]]
name = "google-ai-generativelanguage"
version = "0.6.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/11/d1/48fe5d7a43d278e9f6b5ada810b0a3530bbeac7ed7fcbcd366f932f05316/google_ai_generativelanguage-0.6.15.tar.gz", hash = "sha256:8f6d9dc4c12b065fe2d0289026171acea5183ebf2d0b11cefe12f3821e159ec3", size = 1375443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/a3/67b8a6ff5001a1d8864922f2d6488dc2a14367ceb651bc3f09a947f2f306/google_ai_generativelanguage-0.6.15-py3-none-any.whl", hash = "sha256:5a03ef86377aa184ffef3662ca28f19eeee158733e45d7947982eb953c6ebb6c", size = 1327356 },
]

[[package]]
name = "google-api-core"
version = "2.24.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "googleapis-common-protos" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/09/5c/085bcb872556934bb119e5e09de54daa07873f6866b8f0303c49e72287f7/google_api_core-2.24.2.tar.gz", hash = "sha256:81718493daf06d96d6bc76a91c23874dbf2fac0adbbf542831b805ee6e974696", size = 163516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/95/f472d85adab6e538da2025dfca9e976a0d125cc0af2301f190e77b76e51c/google_api_core-2.24.2-py3-none-any.whl", hash = "sha256:810a63ac95f3c441b7c0e43d344e372887f62ce9071ba972eacf32672e072de9", size = 160061 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
    { name = "grpcio-status" },
]

[[package]]
name = "google-api-python-client"
version = "2.171.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-auth-httplib2" },
    { name = "httplib2" },
    { name = "uritemplate" },
]
sdist = { url = "https://files.pythonhosted.org/packages/35/99/237cd2510aecca9fabb54007e58553274cc43cb3c18512ee1ea574d11b87/google_api_python_client-2.171.0.tar.gz", hash = "sha256:057a5c08d28463c6b9eb89746355de5f14b7ed27a65c11fdbf1d06c66bb66b23", size = 13028937 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/db/c397e3eb3ea18f423855479d0a5852bdc9c3f644e3d4194931fa664a70b4/google_api_python_client-2.171.0-py3-none-any.whl", hash = "sha256:c9c9b76f561e9d9ac14e54a9e2c0842876201d5b96e69e48f967373f0784cbe9", size = 13547393 },
]

[[package]]
name = "google-auth"
version = "2.39.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cachetools" },
    { name = "pyasn1-modules" },
    { name = "rsa" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/8e/8f45c9a32f73e786e954b8f9761c61422955d23c45d1e8c347f9b4b59e8e/google_auth-2.39.0.tar.gz", hash = "sha256:73222d43cdc35a3aeacbfdcaf73142a97839f10de930550d89ebfe1d0a00cde7", size = 274834 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/12/ad37a1ef86006d0a0117fc06a4a00bd461c775356b534b425f00dde208ea/google_auth-2.39.0-py2.py3-none-any.whl", hash = "sha256:0150b6711e97fb9f52fe599f55648950cc4540015565d8fbb31be2ad6e1548a2", size = 212319 },
]

[[package]]
name = "google-auth-httplib2"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "httplib2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/56/be/217a598a818567b28e859ff087f347475c807a5649296fb5a817c58dacef/google-auth-httplib2-0.2.0.tar.gz", hash = "sha256:38aa7badf48f974f1eb9861794e9c0cb2a0511a4ec0679b1f886d108f5640e05", size = 10842 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl", hash = "sha256:b65a0a2123300dd71281a7bf6e64d65a0759287df52729bdd1ae2e47dc311a3d", size = 9253 },
]

[[package]]
name = "google-cloud-aiplatform"
version = "1.91.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "docstring-parser" },
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-bigquery" },
    { name = "google-cloud-resource-manager" },
    { name = "google-cloud-storage" },
    { name = "packaging" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "shapely" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/50/08/5854569782efbbc8efd0aeda3a4486153605104cbab6ac836b2328bae48e/google_cloud_aiplatform-1.91.0.tar.gz", hash = "sha256:b14e5e52b52b6012c7dc253beab34c511fdc53c69b13f436ddb06882c1a92cd7", size = 9102586 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/88/cea8583fadd142e8ef26f8ec14a6ee4d7c69c4e5ab82bea01a077fddddbe/google_cloud_aiplatform-1.91.0-py2.py3-none-any.whl", hash = "sha256:ff8df100c2af692d114a2219d3abbb96110b3e5655f342fdbb6aefad43901b52", size = 7591910 },
]

[package.optional-dependencies]
tokenization = [
    { name = "sentencepiece" },
]

[[package]]
name = "google-cloud-bigquery"
version = "3.31.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-resumable-media" },
    { name = "packaging" },
    { name = "python-dateutil" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/73/91/4c7274f4d5faf13ac000b06353deaf3579575bf0e4bbad07fa68b9f09ba9/google_cloud_bigquery-3.31.0.tar.gz", hash = "sha256:b89dc716dbe4abdb7a4f873f7050100287bc98514e0614c5d54cd6a8e9fb0991", size = 479961 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e8/bc/4cb8c61fc6dd817a4a390b745ec7b305f4578f547a16d09d54c8a790624b/google_cloud_bigquery-3.31.0-py3-none-any.whl", hash = "sha256:97f4a3219854ff01d6a3a57312feecb0b6e13062226b823f867e2d3619c4787b", size = 250099 },
]

[[package]]
name = "google-cloud-core"
version = "2.4.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/b8/2b53838d2acd6ec6168fd284a990c76695e84c65deee79c9f3a4276f6b4f/google_cloud_core-2.4.3.tar.gz", hash = "sha256:1fab62d7102844b278fe6dead3af32408b1df3eb06f5c7e8634cbd40edc4da53", size = 35861 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl", hash = "sha256:5130f9f4c14b4fafdff75c79448f9495cfade0d8775facf1b09c3bf67e027f6e", size = 29348 },
]

[[package]]
name = "google-cloud-resource-manager"
version = "1.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6e/ca/a4648f5038cb94af4b3942815942a03aa9398f9fb0bef55b3f1585b9940d/google_cloud_resource_manager-1.14.2.tar.gz", hash = "sha256:962e2d904c550d7bac48372607904ff7bb3277e3bb4a36d80cc9a37e28e6eb74", size = 446370 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/ea/a92631c358da377af34d3a9682c97af83185c2d66363d5939ab4a1169a7f/google_cloud_resource_manager-1.14.2-py3-none-any.whl", hash = "sha256:d0fa954dedd1d2b8e13feae9099c01b8aac515b648e612834f9942d2795a9900", size = 394344 },
]

[[package]]
name = "google-cloud-secret-manager"
version = "2.24.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/7a/2fa6735ec693d822fe08a76709c4d95d9b5b4c02e83e720497355039d2ee/google_cloud_secret_manager-2.24.0.tar.gz", hash = "sha256:ce573d40ffc2fb7d01719243a94ee17aa243ea642a6ae6c337501e58fbf642b5", size = 269516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/af/db1217cae1809e69a4527ee6293b82a9af2a1fb2313ad110c775e8f3c820/google_cloud_secret_manager-2.24.0-py3-none-any.whl", hash = "sha256:9bea1254827ecc14874bc86c63b899489f8f50bfe1442bfb2517530b30b3a89b", size = 218050 },
]

[[package]]
name = "google-cloud-storage"
version = "2.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-crc32c" },
    { name = "google-resumable-media" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/36/76/4d965702e96bb67976e755bed9828fa50306dca003dbee08b67f41dd265e/google_cloud_storage-2.19.0.tar.gz", hash = "sha256:cd05e9e7191ba6cb68934d8eb76054d9be4562aa89dbc4236feee4d7d51342b2", size = 5535488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/94/6db383d8ee1adf45dc6c73477152b82731fa4c4a46d9c1932cc8757e0fd4/google_cloud_storage-2.19.0-py2.py3-none-any.whl", hash = "sha256:aeb971b5c29cf8ab98445082cbfe7b161a1f48ed275822f59ed3f1524ea54fba", size = 131787 },
]

[[package]]
name = "google-crc32c"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/19/ae/87802e6d9f9d69adfaedfcfd599266bf386a54d0be058b532d04c794f76d/google_crc32c-1.7.1.tar.gz", hash = "sha256:2bff2305f98846f3e825dbeec9ee406f89da7962accdb29356e4eadc251bd472", size = 14495 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/72/b8d785e9184ba6297a8620c8a37cf6e39b81a8ca01bb0796d7cbb28b3386/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_arm64.whl", hash = "sha256:df8b38bdaf1629d62d51be8bdd04888f37c451564c2042d36e5812da9eff3c35", size = 30467 },
    { url = "https://files.pythonhosted.org/packages/34/25/5f18076968212067c4e8ea95bf3b69669f9fc698476e5f5eb97d5b37999f/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_x86_64.whl", hash = "sha256:e42e20a83a29aa2709a0cf271c7f8aefaa23b7ab52e53b322585297bb94d4638", size = 30309 },
    { url = "https://files.pythonhosted.org/packages/92/83/9228fe65bf70e93e419f38bdf6c5ca5083fc6d32886ee79b450ceefd1dbd/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:905a385140bf492ac300026717af339790921f411c0dfd9aa5a9e69a08ed32eb", size = 33133 },
    { url = "https://files.pythonhosted.org/packages/c3/ca/1ea2fd13ff9f8955b85e7956872fdb7050c4ace8a2306a6d177edb9cf7fe/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b211ddaf20f7ebeec5c333448582c224a7c90a9d98826fbab82c0ddc11348e6", size = 32773 },
    { url = "https://files.pythonhosted.org/packages/89/32/a22a281806e3ef21b72db16f948cad22ec68e4bdd384139291e00ff82fe2/google_crc32c-1.7.1-cp313-cp313-win_amd64.whl", hash = "sha256:0f99eaa09a9a7e642a61e06742856eec8b19fc0037832e03f941fe7cf0c8e4db", size = 33475 },
    { url = "https://files.pythonhosted.org/packages/b8/c5/002975aff514e57fc084ba155697a049b3f9b52225ec3bc0f542871dd524/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32d1da0d74ec5634a05f53ef7df18fc646666a25efaaca9fc7dcfd4caf1d98c3", size = 33243 },
    { url = "https://files.pythonhosted.org/packages/61/cb/c585282a03a0cea70fcaa1bf55d5d702d0f2351094d663ec3be1c6c67c52/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e10554d4abc5238823112c2ad7e4560f96c7bf3820b202660373d769d9e6e4c9", size = 32870 },
]

[[package]]
name = "google-genai"
version = "1.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "google-auth" },
    { name = "httpx" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/14/17/8f717f43732ae2b7775f816f0d8f0b39e2a020bbe7ba202f2ddb2f948c3b/google_genai-1.19.0.tar.gz", hash = "sha256:66f5de78075781bfd9e423f1e3592e4240759dfe0ac42ac74a9dcb2c4f662e9d", size = 198000 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c4/ae/64fccdebf5811453ce53b0d5ee23d4f27ef173ef36d3b67dad791a0007aa/google_genai-1.19.0-py3-none-any.whl", hash = "sha256:a2955612e4af8c84f83eb43c1ce4e74e1b714732926d0705e639761938192466", size = 200043 },
]

[[package]]
name = "google-generativeai"
version = "0.8.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-ai-generativelanguage" },
    { name = "google-api-core" },
    { name = "google-api-python-client" },
    { name = "google-auth" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/40/c42ff9ded9f09ec9392879a8e6538a00b2dc185e834a3392917626255419/google_generativeai-0.8.5-py3-none-any.whl", hash = "sha256:22b420817fb263f8ed520b33285f45976d5b21e904da32b80d4fd20c055123a2", size = 155427 },
]

[[package]]
name = "google-resumable-media"
version = "2.7.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-crc32c" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/5a/0efdc02665dca14e0837b62c8a1a93132c264bd02054a15abb2218afe0ae/google_resumable_media-2.7.2.tar.gz", hash = "sha256:5280aed4629f2b60b847b0d42f9857fd4935c11af266744df33d8074cae92fe0", size = 2163099 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl", hash = "sha256:3ce7551e9fe6d99e9a126101d2536612bb73486721951e9562fee0f90c6ababa", size = 81251 },
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/39/24/33db22342cf4a2ea27c9955e6713140fedd51e8b141b5ce5260897020f1a/googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257", size = 145903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/f1/62a193f0227cf15a920390abe675f386dec35f7ae3ffe6da582d3ade42c7/googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8", size = 294530 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
]

[[package]]
name = "grpc-google-iam-v1"
version = "0.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos", extra = ["grpc"] },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/4e/8d0ca3b035e41fe0b3f31ebbb638356af720335e5a11154c330169b40777/grpc_google_iam_v1-0.14.2.tar.gz", hash = "sha256:b3e1fc387a1a329e41672197d0ace9de22c78dd7d215048c4c78712073f7bd20", size = 16259 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/66/6f/dd9b178aee7835b96c2e63715aba6516a9d50f6bebbd1cc1d32c82a2a6c3/grpc_google_iam_v1-0.14.2-py3-none-any.whl", hash = "sha256:a3171468459770907926d56a440b2bb643eec1d7ba215f48f3ecece42b4d8351", size = 19242 },
]

[[package]]
name = "grpcio"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/95/aa11fc09a85d91fbc7dd405dcb2a1e0256989d67bf89fa65ae24b3ba105a/grpcio-1.71.0.tar.gz", hash = "sha256:2b85f7820475ad3edec209d3d89a7909ada16caab05d3f2e08a7e8ae3200a55c", size = 12549828 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/dd/b00cbb45400d06b26126dcfdbdb34bb6c4f28c3ebbd7aea8228679103ef6/grpcio-1.71.0-cp313-cp313-linux_armv7l.whl", hash = "sha256:cebc1b34ba40a312ab480ccdb396ff3c529377a2fce72c45a741f7215bfe8379", size = 5184138 },
    { url = "https://files.pythonhosted.org/packages/ed/0a/4651215983d590ef53aac40ba0e29dda941a02b097892c44fa3357e706e5/grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl", hash = "sha256:85da336e3649a3d2171e82f696b5cad2c6231fdd5bad52616476235681bee5b3", size = 11310747 },
    { url = "https://files.pythonhosted.org/packages/57/a3/149615b247f321e13f60aa512d3509d4215173bdb982c9098d78484de216/grpcio-1.71.0-cp313-cp313-manylinux_2_17_aarch64.whl", hash = "sha256:f9a412f55bb6e8f3bb000e020dbc1e709627dcb3a56f6431fa7076b4c1aab0db", size = 5653991 },
    { url = "https://files.pythonhosted.org/packages/ca/56/29432a3e8d951b5e4e520a40cd93bebaa824a14033ea8e65b0ece1da6167/grpcio-1.71.0-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:47be9584729534660416f6d2a3108aaeac1122f6b5bdbf9fd823e11fe6fbaa29", size = 6312781 },
    { url = "https://files.pythonhosted.org/packages/a3/f8/286e81a62964ceb6ac10b10925261d4871a762d2a763fbf354115f9afc98/grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c9c80ac6091c916db81131d50926a93ab162a7e97e4428ffc186b6e80d6dda4", size = 5910479 },
    { url = "https://files.pythonhosted.org/packages/35/67/d1febb49ec0f599b9e6d4d0d44c2d4afdbed9c3e80deb7587ec788fcf252/grpcio-1.71.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:789d5e2a3a15419374b7b45cd680b1e83bbc1e52b9086e49308e2c0b5bbae6e3", size = 6013262 },
    { url = "https://files.pythonhosted.org/packages/a1/04/f9ceda11755f0104a075ad7163fc0d96e2e3a9fe25ef38adfc74c5790daf/grpcio-1.71.0-cp313-cp313-musllinux_1_1_i686.whl", hash = "sha256:1be857615e26a86d7363e8a163fade914595c81fec962b3d514a4b1e8760467b", size = 6643356 },
    { url = "https://files.pythonhosted.org/packages/fb/ce/236dbc3dc77cf9a9242adcf1f62538734ad64727fabf39e1346ad4bd5c75/grpcio-1.71.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:a76d39b5fafd79ed604c4be0a869ec3581a172a707e2a8d7a4858cb05a5a7637", size = 6186564 },
    { url = "https://files.pythonhosted.org/packages/10/fd/b3348fce9dd4280e221f513dd54024e765b21c348bc475516672da4218e9/grpcio-1.71.0-cp313-cp313-win32.whl", hash = "sha256:74258dce215cb1995083daa17b379a1a5a87d275387b7ffe137f1d5131e2cfbb", size = 3601890 },
    { url = "https://files.pythonhosted.org/packages/be/f8/db5d5f3fc7e296166286c2a397836b8b042f7ad1e11028d82b061701f0f7/grpcio-1.71.0-cp313-cp313-win_amd64.whl", hash = "sha256:22c3bc8d488c039a199f7a003a38cb7635db6656fa96437a8accde8322ce2366", size = 4273308 },
]

[[package]]
name = "grpcio-status"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos" },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d7/53/a911467bece076020456401f55a27415d2d70d3bc2c37af06b44ea41fc5c/grpcio_status-1.71.0.tar.gz", hash = "sha256:11405fed67b68f406b3f3c7c5ae5104a79d2d309666d10d61b152e91d28fb968", size = 13669 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ad/d6/31fbc43ff097d8c4c9fc3df741431b8018f67bf8dfbe6553a555f6e5f675/grpcio_status-1.71.0-py3-none-any.whl", hash = "sha256:843934ef8c09e3e858952887467f8256aac3910c55f077a359a65b2b3cde3e68", size = 14424 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "html-to-markdown"
version = "1.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1d/48/324d3d938e5ff635497965118df510f62725b72e8b378b8710c03b0dd014/html_to_markdown-1.3.3.tar.gz", hash = "sha256:ad4f992d65d96d53e49d0a56a2ae0c52ef606c17592d2d9a87f99e4632a4a9e3", size = 15491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a3/d0/b96f7e3579cada841657e5764bc294bd2abb6c1e1dbcfb88ecf7a63ea5d9/html_to_markdown-1.3.3-py3-none-any.whl", hash = "sha256:09325777400e561d2c5a1569f475f9434e70a6f8ed1b4866bba8d00906136495", size = 14951 },
]

[[package]]
name = "html5lib"
version = "1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
    { name = "webencodings" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ac/b6/b55c3f49042f1df3dcd422b7f224f939892ee94f22abcf503a9b7339eaf2/html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f", size = 272215 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d", size = 112173 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httplib2"
version = "0.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyparsing" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/ad/2371116b22d616c194aa25ec410c9c6c37f23599dcd590502b74db197584/httplib2-0.22.0.tar.gz", hash = "sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81", size = 351116 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl", hash = "sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc", size = 96854 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "zipp" },
]
sdist = { url = "https://files.pythonhosted.org/packages/76/66/650a33bd90f786193e4de4b3ad86ea60b53c89b669a5c7be931fac31cdb0/importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000", size = 56641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd", size = 27656 },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050 },
]

[[package]]
name = "ipython"
version = "9.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "decorator" },
    { name = "ipython-pygments-lexers" },
    { name = "jedi" },
    { name = "matplotlib-inline" },
    { name = "pexpect", marker = "sys_platform != 'emscripten' and sys_platform != 'win32'" },
    { name = "prompt-toolkit" },
    { name = "pygments" },
    { name = "stack-data" },
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/54/80/406f9e3bde1c1fd9bf5a0be9d090f8ae623e401b7670d8f6fdf2ab679891/ipython-9.4.0.tar.gz", hash = "sha256:c033c6d4e7914c3d9768aabe76bbe87ba1dc66a92a05db6bfa1125d81f2ee270", size = 4385338 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/63/f8/0031ee2b906a15a33d6bfc12dd09c3dfa966b3cb5b284ecfb7549e6ac3c4/ipython-9.4.0-py3-none-any.whl", hash = "sha256:25850f025a446d9b359e8d296ba175a36aedd32e83ca9b5060430fe16801f066", size = 611021 },
]

[[package]]
name = "ipython-pygments-lexers"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/4c/5dd1d8af08107f88c7f741ead7a40854b8ac24ddf9ae850afbcf698aa552/ipython_pygments_lexers-1.1.1.tar.gz", hash = "sha256:09c0138009e56b6854f9535736f4171d855c8c08a563a0dcd8022f78355c7e81", size = 8393 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl", hash = "sha256:a9462224a505ade19a605f71f8fa63c2048833ce50abc86768a0d81d876dc81c", size = 8074 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jedi"
version = "0.19.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "parso" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/3a/79a912fbd4d8dd6fbb02bf69afd3bb72cf0c729bb3063c6f4498603db17a/jedi-0.19.2.tar.gz", hash = "sha256:4770dc3de41bde3966b02eb84fbcf557fb33cce26ad23da12c742fb50ecb11f0", size = 1231287 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl", hash = "sha256:a8ef22bde8490f57fe5c7681a3c83cb58874daf72b4784de3cce5b6ef6edb5b9", size = 1572278 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "loro"
version = "1.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a0/32/ce94b1fc342ac90d9ca21bc6e90c727990734a75505cb893b2a71a364faf/loro-1.5.2.tar.gz", hash = "sha256:70e52acb16474f7c1e52aea2a7fe2771516f1e9f73d4edfe40f3193b122402c7", size = 62538 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/09/061e8cecb42f99856580811156d7651d5e8172bb840224c7cd2eb94a8730/loro-1.5.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:dbb94c104e3aba4ea3f1118c72896de978e737bb066a35051bf49895e72540a7", size = 3098320 },
    { url = "https://files.pythonhosted.org/packages/60/6e/96cb1a78869c8ae91e65d73ef4ee9f74bc16fd3baff5a7463f7702687dab/loro-1.5.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:847a10f493399f9b650b588b3d81893dfaa1e45e7091881268094f2b9f7df38b", size = 2882026 },
    { url = "https://files.pythonhosted.org/packages/eb/e7/2a131e3e8072614af1cc2970efc1c30a812eb8b0f5286c7b6b390ae3fc9f/loro-1.5.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:902215b77b35e58286d907e8292f78b014cd9c55a46bc5deb944f555509b7747", size = 3110094 },
    { url = "https://files.pythonhosted.org/packages/8c/63/34efc556a5a7663f045d64b9744c10f7b00386f252fac47c939f1c1795be/loro-1.5.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:19e8c9896348063721ef56631d2275c186faf63f6336079c57f41055c9cc1c30", size = 3202938 },
    { url = "https://files.pythonhosted.org/packages/67/3f/5a37b5f1bec5d633f469754e26bf0ce77a26f7697cd95d0b4a51b9cd90be/loro-1.5.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91e75cd4b26506bb5b564ed24b433147fc8b77e8779b5736bc4f3bfddf270590", size = 3579945 },
    { url = "https://files.pythonhosted.org/packages/78/b3/cd3202d6398524c5e1442688c6825e148eb953aa0de04952fd546c69a398/loro-1.5.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:41e54109599190dede34366476a8f42ae6e9fd7fd439823150e9f70e39d7d54e", size = 3318843 },
    { url = "https://files.pythonhosted.org/packages/a5/65/8ed127c827ed9b540f5660e9c98265702dbfdd71ad59063bd3c799ca0dda/loro-1.5.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd3f330795212f24b9dd710f952f7f7138ba86d6159f524025eb4627641ed4ef", size = 3243417 },
    { url = "https://files.pythonhosted.org/packages/4e/29/6894f6db7a1eb7d5d2936b658b3a26c4ea8ce6b0563dde024b909a63289d/loro-1.5.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ebdd716ce67c182f71a093c552f9a47428f7a3d93b038780bbb0f06779805d0", size = 3511123 },
    { url = "https://files.pythonhosted.org/packages/17/26/230867103d5ec58ef18f8d0bc169a4defb4f865f9969247d4e9c723ae10e/loro-1.5.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a8ac5ff8b697e9a828fe4387da715d78d0f2afcf23bbd76f5089b4122f5e78a3", size = 3256828 },
    { url = "https://files.pythonhosted.org/packages/79/8b/7aed297d9cc236e15674275364e37e938e9335c9dfad49ad35904fa8b1f3/loro-1.5.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3dce7920c45c9c884246898805b270d63550a5dec61d3f33274010c40127a37c", size = 3464838 },
    { url = "https://files.pythonhosted.org/packages/1d/c1/352fd39b61a842dc991bf95aaa75db34b6c353c1a3844da17e01f917deb5/loro-1.5.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:66afec16e22db99f1818906bc7cabda0cb077e0e493882b4c0983a8bc431413d", size = 3502790 },
    { url = "https://files.pythonhosted.org/packages/2c/11/859dfc28b1397d731d2cc710dae0e7cb1cbeb45ab70ec518b4ed4f690a4c/loro-1.5.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9f052715922592f099e9b6553fccb48761c5ad83deefcb0df55effde309eb12d", size = 3414408 },
    { url = "https://files.pythonhosted.org/packages/86/3e/fcd87311399e2eff892fb3a6b6f1d3307a2dfd99811fddf0889bee89d585/loro-1.5.2-cp313-cp313-win32.whl", hash = "sha256:978e9f6b0c9ad8c6b1ab70372eafbe00c41782522b216802cf961a81edd27561", size = 2580638 },
    { url = "https://files.pythonhosted.org/packages/93/06/dd73ca0865630923f18fa4486e66a171a0a26ae8e7541f1c3d93100f1f5b/loro-1.5.2-cp313-cp313-win_amd64.whl", hash = "sha256:3ecebbf9f5f880c6ca9a1628e5f469d3d67b67c1fd50536c52c5f6eae01be549", size = 2743550 },
    { url = "https://files.pythonhosted.org/packages/d2/70/9e5030bb9f1b86520f482605f660e5a192d6f5e56104fee122fe7d3dc72e/loro-1.5.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:354de426d6404cce252fb81be17a1589f1bd47197ba7f730f60fbb52452f49ab", size = 3106619 },
    { url = "https://files.pythonhosted.org/packages/2b/37/43c8e3fa8c6239be1b22c0dfd779a4ab000682dddebc23becd057668c436/loro-1.5.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:18e3b6f07483c5553795fea05c8d318f96c018909dd390c68b81701afb12cac3", size = 3195270 },
    { url = "https://files.pythonhosted.org/packages/b1/d6/8aaa433d08710cb1b95781d56efad366350082798463e35b5a6a4988b160/loro-1.5.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2298b96c5f533807373db27dbf5b10c88f1c5d9e0145feb952e7a813a81af645", size = 3575129 },
    { url = "https://files.pythonhosted.org/packages/51/4e/44425f11da9b5278653c3ca01cdfd4da850f94ead5843d8134043ac825cf/loro-1.5.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0aa8edef791c1b46e19bf86ab17f9dbefc61b8f1fbecc49054d5eb880380d897", size = 3317031 },
    { url = "https://files.pythonhosted.org/packages/3b/ae/af1713c7c3cc91a9d6cc1b812733665875eb30c22e4c9e0e213a9a69b1a2/loro-1.5.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:633c026cbb17c485de40f09aab13362f0c79140913dc67445606e3237092d70f", size = 3251501 },
    { url = "https://files.pythonhosted.org/packages/4b/df/958e8abb78ca47ce06e0088bc5d44b5945ffbd08503936cbc0340b62a5f3/loro-1.5.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:903fed16d40b0373f747ecc398f5b86aaab16c37b4c670f580c2c5301bad4de5", size = 3456858 },
    { url = "https://files.pythonhosted.org/packages/f1/f6/982af3432bde075f1fd3201de0e95f35a868f4e85cee36bb22bb0524b069/loro-1.5.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2f9f77b1f582d86e1a57cdb38a43ea1a5861a6f0d73783335c2efdc3d1dcb793", size = 3494470 },
    { url = "https://files.pythonhosted.org/packages/47/b3/a4725db48fb4c7637076023ccedf7dcb7f24a3d266208f2e2aafb8179861/loro-1.5.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:489230b2716c0a2ad50e205670abed029ba0787c028a62dd31226f7935f5d1fd", size = 3410923 },
]

[[package]]
name = "lxml"
version = "5.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/76/3d/14e82fc7c8fb1b7761f7e748fd47e2ec8276d137b6acfe5a4bb73853e08f/lxml-5.4.0.tar.gz", hash = "sha256:d12832e1dbea4be280b22fd0ea7c9b87f0d8fc51ba06e92dc62d52f804f78ebd", size = 3679479 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/cb/2ba1e9dd953415f58548506fa5549a7f373ae55e80c61c9041b7fd09a38a/lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:773e27b62920199c6197130632c18fb7ead3257fce1ffb7d286912e56ddb79e0", size = 8110086 },
    { url = "https://files.pythonhosted.org/packages/b5/3e/6602a4dca3ae344e8609914d6ab22e52ce42e3e1638c10967568c5c1450d/lxml-5.4.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ce9c671845de9699904b1e9df95acfe8dfc183f2310f163cdaa91a3535af95de", size = 4404613 },
    { url = "https://files.pythonhosted.org/packages/4c/72/bf00988477d3bb452bef9436e45aeea82bb40cdfb4684b83c967c53909c7/lxml-5.4.0-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9454b8d8200ec99a224df8854786262b1bd6461f4280064c807303c642c05e76", size = 5012008 },
    { url = "https://files.pythonhosted.org/packages/92/1f/93e42d93e9e7a44b2d3354c462cd784dbaaf350f7976b5d7c3f85d68d1b1/lxml-5.4.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cccd007d5c95279e529c146d095f1d39ac05139de26c098166c4beb9374b0f4d", size = 4760915 },
    { url = "https://files.pythonhosted.org/packages/45/0b/363009390d0b461cf9976a499e83b68f792e4c32ecef092f3f9ef9c4ba54/lxml-5.4.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0fce1294a0497edb034cb416ad3e77ecc89b313cff7adbee5334e4dc0d11f422", size = 5283890 },
    { url = "https://files.pythonhosted.org/packages/19/dc/6056c332f9378ab476c88e301e6549a0454dbee8f0ae16847414f0eccb74/lxml-5.4.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:24974f774f3a78ac12b95e3a20ef0931795ff04dbb16db81a90c37f589819551", size = 4812644 },
    { url = "https://files.pythonhosted.org/packages/ee/8a/f8c66bbb23ecb9048a46a5ef9b495fd23f7543df642dabeebcb2eeb66592/lxml-5.4.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:497cab4d8254c2a90bf988f162ace2ddbfdd806fce3bda3f581b9d24c852e03c", size = 4921817 },
    { url = "https://files.pythonhosted.org/packages/04/57/2e537083c3f381f83d05d9b176f0d838a9e8961f7ed8ddce3f0217179ce3/lxml-5.4.0-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:e794f698ae4c5084414efea0f5cc9f4ac562ec02d66e1484ff822ef97c2cadff", size = 4753916 },
    { url = "https://files.pythonhosted.org/packages/d8/80/ea8c4072109a350848f1157ce83ccd9439601274035cd045ac31f47f3417/lxml-5.4.0-cp313-cp313-manylinux_2_28_ppc64le.whl", hash = "sha256:2c62891b1ea3094bb12097822b3d44b93fc6c325f2043c4d2736a8ff09e65f60", size = 5289274 },
    { url = "https://files.pythonhosted.org/packages/b3/47/c4be287c48cdc304483457878a3f22999098b9a95f455e3c4bda7ec7fc72/lxml-5.4.0-cp313-cp313-manylinux_2_28_s390x.whl", hash = "sha256:142accb3e4d1edae4b392bd165a9abdee8a3c432a2cca193df995bc3886249c8", size = 4874757 },
    { url = "https://files.pythonhosted.org/packages/2f/04/6ef935dc74e729932e39478e44d8cfe6a83550552eaa072b7c05f6f22488/lxml-5.4.0-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:1a42b3a19346e5601d1b8296ff6ef3d76038058f311902edd574461e9c036982", size = 4947028 },
    { url = "https://files.pythonhosted.org/packages/cb/f9/c33fc8daa373ef8a7daddb53175289024512b6619bc9de36d77dca3df44b/lxml-5.4.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4291d3c409a17febf817259cb37bc62cb7eb398bcc95c1356947e2871911ae61", size = 4834487 },
    { url = "https://files.pythonhosted.org/packages/8d/30/fc92bb595bcb878311e01b418b57d13900f84c2b94f6eca9e5073ea756e6/lxml-5.4.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4f5322cf38fe0e21c2d73901abf68e6329dc02a4994e483adbcf92b568a09a54", size = 5381688 },
    { url = "https://files.pythonhosted.org/packages/43/d1/3ba7bd978ce28bba8e3da2c2e9d5ae3f8f521ad3f0ca6ea4788d086ba00d/lxml-5.4.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0be91891bdb06ebe65122aa6bf3fc94489960cf7e03033c6f83a90863b23c58b", size = 5242043 },
    { url = "https://files.pythonhosted.org/packages/ee/cd/95fa2201041a610c4d08ddaf31d43b98ecc4b1d74b1e7245b1abdab443cb/lxml-5.4.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:15a665ad90054a3d4f397bc40f73948d48e36e4c09f9bcffc7d90c87410e478a", size = 5021569 },
    { url = "https://files.pythonhosted.org/packages/2d/a6/31da006fead660b9512d08d23d31e93ad3477dd47cc42e3285f143443176/lxml-5.4.0-cp313-cp313-win32.whl", hash = "sha256:d5663bc1b471c79f5c833cffbc9b87d7bf13f87e055a5c86c363ccd2348d7e82", size = 3485270 },
    { url = "https://files.pythonhosted.org/packages/fc/14/c115516c62a7d2499781d2d3d7215218c0731b2c940753bf9f9b7b73924d/lxml-5.4.0-cp313-cp313-win_amd64.whl", hash = "sha256:bcb7a1096b4b6b24ce1ac24d4942ad98f983cd3810f9711bcd0293f43a9d8b9f", size = 3814606 },
]

[[package]]
name = "marimo"
version = "0.14.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "docutils" },
    { name = "itsdangerous" },
    { name = "jedi" },
    { name = "loro" },
    { name = "markdown" },
    { name = "narwhals" },
    { name = "packaging" },
    { name = "psutil" },
    { name = "pygments" },
    { name = "pymdown-extensions" },
    { name = "pyyaml" },
    { name = "starlette" },
    { name = "tomlkit" },
    { name = "uvicorn" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0b/6d/8c0bdb68d608561e3039718f171ede292e7da7e7580a51b1f4b2ce6e204f/marimo-0.14.12.tar.gz", hash = "sha256:cf18513e30a5d2e8864930885b674dd89cbc9ad3a5e128b9ecfa48323de6d14f", size = 29622446 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/fa/d802cd61fb4714c17529057dc4b07d48c3e115d0af331907b3d19f5482f6/marimo-0.14.12-py3-none-any.whl", hash = "sha256:154d168ceb8b9f4cc10f8cd9f6299cf0c5d8643b0291370a9e64a88b2f517ed3", size = 30118091 },
]

[[package]]
name = "markdown"
version = "3.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/c2/4ab49206c17f75cb08d6311171f2d65798988db4360c4d1485bd0eedd67c/markdown-3.8.2.tar.gz", hash = "sha256:247b9a70dd12e27f67431ce62523e675b866d254f900c4fe75ce3dda62237c45", size = 362071 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl", hash = "sha256:5c83764dbd4e00bdd94d85a19b8d55ccca20fe35b2e678a1422b380324dd5f24", size = 106827 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "markdownify"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2f/78/c48fed23c7aebc2c16049062e72de1da3220c274de59d28c942acdc9ffb2/markdownify-1.1.0.tar.gz", hash = "sha256:449c0bbbf1401c5112379619524f33b63490a8fa479456d41de9dc9e37560ebd", size = 17127 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/11/b751af7ad41b254a802cf52f7bc1fca7cabe2388132f2ce60a1a6b9b9622/markdownify-1.1.0-py3-none-any.whl", hash = "sha256:32a5a08e9af02c8a6528942224c91b933b4bd2c7d078f9012943776fc313eeef", size = 13901 },
]

[[package]]
name = "matplotlib-inline"
version = "0.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/5b/a36a337438a14116b16480db471ad061c36c3694df7c2084a0da7ba538b7/matplotlib_inline-0.1.7.tar.gz", hash = "sha256:8423b23ec666be3d16e16b60bdd8ac4e86e840ebd1dd11a30b9f117f2fa0ab90", size = 8159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/8e/9ad090d3553c280a8060fbf6e24dc1c0c29704ee7d1c372f0c174aa59285/matplotlib_inline-0.1.7-py3-none-any.whl", hash = "sha256:df192d39a4ff8f21b1895d72e6a13f5fcc5099f00fa84384e0ea28c2cc0653ca", size = 9899 },
]

[[package]]
name = "mcp"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/25/ae/588691c45b38f4fbac07fa3d6d50cea44cc6b35d16ddfdf26e17a0467ab2/mcp-1.7.1.tar.gz", hash = "sha256:eb4f1f53bd717f75dda8a1416e00804b831a8f3c331e23447a03b78f04b43a6e", size = 230903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ae/79/fe0e20c3358997a80911af51bad927b5ea2f343ef95ab092b19c9cc48b59/mcp-1.7.1-py3-none-any.whl", hash = "sha256:f7e6108977db6d03418495426c7ace085ba2341b75197f8727f96f9cfd30057a", size = 100365 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "multidict"
version = "6.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/2f/a3470242707058fe856fe59241eee5635d79087100b7042a867368863a27/multidict-6.4.4.tar.gz", hash = "sha256:69ee9e6ba214b5245031b76233dd95408a0fd57fdb019ddcc1ead4790932a8e8", size = 90183 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/2a/e166d2ffbf4b10131b2d5b0e458f7cee7d986661caceae0de8753042d4b2/multidict-6.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:82ffabefc8d84c2742ad19c37f02cde5ec2a1ee172d19944d380f920a340e4b9", size = 64123 },
    { url = "https://files.pythonhosted.org/packages/8c/96/e200e379ae5b6f95cbae472e0199ea98913f03d8c9a709f42612a432932c/multidict-6.4.4-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6a2f58a66fe2c22615ad26156354005391e26a2f3721c3621504cd87c1ea87bf", size = 38049 },
    { url = "https://files.pythonhosted.org/packages/75/fb/47afd17b83f6a8c7fa863c6d23ac5ba6a0e6145ed8a6bcc8da20b2b2c1d2/multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5883d6ee0fd9d8a48e9174df47540b7545909841ac82354c7ae4cbe9952603bd", size = 37078 },
    { url = "https://files.pythonhosted.org/packages/fa/70/1af3143000eddfb19fd5ca5e78393985ed988ac493bb859800fe0914041f/multidict-6.4.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9abcf56a9511653fa1d052bfc55fbe53dbee8f34e68bd6a5a038731b0ca42d15", size = 224097 },
    { url = "https://files.pythonhosted.org/packages/b1/39/d570c62b53d4fba844e0378ffbcd02ac25ca423d3235047013ba2f6f60f8/multidict-6.4.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:6ed5ae5605d4ad5a049fad2a28bb7193400700ce2f4ae484ab702d1e3749c3f9", size = 230768 },
    { url = "https://files.pythonhosted.org/packages/fd/f8/ed88f2c4d06f752b015933055eb291d9bc184936903752c66f68fb3c95a7/multidict-6.4.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bbfcb60396f9bcfa63e017a180c3105b8c123a63e9d1428a36544e7d37ca9e20", size = 231331 },
    { url = "https://files.pythonhosted.org/packages/9c/6f/8e07cffa32f483ab887b0d56bbd8747ac2c1acd00dc0af6fcf265f4a121e/multidict-6.4.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b0f1987787f5f1e2076b59692352ab29a955b09ccc433c1f6b8e8e18666f608b", size = 230169 },
    { url = "https://files.pythonhosted.org/packages/e6/2b/5dcf173be15e42f330110875a2668ddfc208afc4229097312212dc9c1236/multidict-6.4.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1d0121ccce8c812047d8d43d691a1ad7641f72c4f730474878a5aeae1b8ead8c", size = 222947 },
    { url = "https://files.pythonhosted.org/packages/39/75/4ddcbcebe5ebcd6faa770b629260d15840a5fc07ce8ad295a32e14993726/multidict-6.4.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:83ec4967114295b8afd120a8eec579920c882831a3e4c3331d591a8e5bfbbc0f", size = 215761 },
    { url = "https://files.pythonhosted.org/packages/6a/c9/55e998ae45ff15c5608e384206aa71a11e1b7f48b64d166db400b14a3433/multidict-6.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:995f985e2e268deaf17867801b859a282e0448633f1310e3704b30616d269d69", size = 227605 },
    { url = "https://files.pythonhosted.org/packages/04/49/c2404eac74497503c77071bd2e6f88c7e94092b8a07601536b8dbe99be50/multidict-6.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:d832c608f94b9f92a0ec8b7e949be7792a642b6e535fcf32f3e28fab69eeb046", size = 226144 },
    { url = "https://files.pythonhosted.org/packages/62/c5/0cd0c3c6f18864c40846aa2252cd69d308699cb163e1c0d989ca301684da/multidict-6.4.4-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d21c1212171cf7da703c5b0b7a0e85be23b720818aef502ad187d627316d5645", size = 221100 },
    { url = "https://files.pythonhosted.org/packages/71/7b/f2f3887bea71739a046d601ef10e689528d4f911d84da873b6be9194ffea/multidict-6.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:cbebaa076aaecad3d4bb4c008ecc73b09274c952cf6a1b78ccfd689e51f5a5b0", size = 232731 },
    { url = "https://files.pythonhosted.org/packages/e5/b3/d9de808349df97fa75ec1372758701b5800ebad3c46ae377ad63058fbcc6/multidict-6.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:c93a6fb06cc8e5d3628b2b5fda215a5db01e8f08fc15fadd65662d9b857acbe4", size = 229637 },
    { url = "https://files.pythonhosted.org/packages/5e/57/13207c16b615eb4f1745b44806a96026ef8e1b694008a58226c2d8f5f0a5/multidict-6.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8cd8f81f1310182362fb0c7898145ea9c9b08a71081c5963b40ee3e3cac589b1", size = 225594 },
    { url = "https://files.pythonhosted.org/packages/3a/e4/d23bec2f70221604f5565000632c305fc8f25ba953e8ce2d8a18842b9841/multidict-6.4.4-cp313-cp313-win32.whl", hash = "sha256:3e9f1cd61a0ab857154205fb0b1f3d3ace88d27ebd1409ab7af5096e409614cd", size = 35359 },
    { url = "https://files.pythonhosted.org/packages/a7/7a/cfe1a47632be861b627f46f642c1d031704cc1c0f5c0efbde2ad44aa34bd/multidict-6.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:8ffb40b74400e4455785c2fa37eba434269149ec525fc8329858c862e4b35373", size = 38903 },
    { url = "https://files.pythonhosted.org/packages/68/7b/15c259b0ab49938a0a1c8f3188572802704a779ddb294edc1b2a72252e7c/multidict-6.4.4-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:6a602151dbf177be2450ef38966f4be3467d41a86c6a845070d12e17c858a156", size = 68895 },
    { url = "https://files.pythonhosted.org/packages/f1/7d/168b5b822bccd88142e0a3ce985858fea612404edd228698f5af691020c9/multidict-6.4.4-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d2b9712211b860d123815a80b859075d86a4d54787e247d7fbee9db6832cf1c", size = 40183 },
    { url = "https://files.pythonhosted.org/packages/e0/b7/d4b8d98eb850ef28a4922ba508c31d90715fd9b9da3801a30cea2967130b/multidict-6.4.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:d2fa86af59f8fc1972e121ade052145f6da22758f6996a197d69bb52f8204e7e", size = 39592 },
    { url = "https://files.pythonhosted.org/packages/18/28/a554678898a19583548e742080cf55d169733baf57efc48c2f0273a08583/multidict-6.4.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50855d03e9e4d66eab6947ba688ffb714616f985838077bc4b490e769e48da51", size = 226071 },
    { url = "https://files.pythonhosted.org/packages/ee/dc/7ba6c789d05c310e294f85329efac1bf5b450338d2542498db1491a264df/multidict-6.4.4-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5bce06b83be23225be1905dcdb6b789064fae92499fbc458f59a8c0e68718601", size = 222597 },
    { url = "https://files.pythonhosted.org/packages/24/4f/34eadbbf401b03768dba439be0fb94b0d187facae9142821a3d5599ccb3b/multidict-6.4.4-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:66ed0731f8e5dfd8369a883b6e564aca085fb9289aacabd9decd70568b9a30de", size = 228253 },
    { url = "https://files.pythonhosted.org/packages/c0/e6/493225a3cdb0d8d80d43a94503fc313536a07dae54a3f030d279e629a2bc/multidict-6.4.4-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:329ae97fc2f56f44d91bc47fe0972b1f52d21c4b7a2ac97040da02577e2daca2", size = 226146 },
    { url = "https://files.pythonhosted.org/packages/2f/70/e411a7254dc3bff6f7e6e004303b1b0591358e9f0b7c08639941e0de8bd6/multidict-6.4.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c27e5dcf520923d6474d98b96749e6805f7677e93aaaf62656005b8643f907ab", size = 220585 },
    { url = "https://files.pythonhosted.org/packages/08/8f/beb3ae7406a619100d2b1fb0022c3bb55a8225ab53c5663648ba50dfcd56/multidict-6.4.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:058cc59b9e9b143cc56715e59e22941a5d868c322242278d28123a5d09cdf6b0", size = 212080 },
    { url = "https://files.pythonhosted.org/packages/9c/ec/355124e9d3d01cf8edb072fd14947220f357e1c5bc79c88dff89297e9342/multidict-6.4.4-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:69133376bc9a03f8c47343d33f91f74a99c339e8b58cea90433d8e24bb298031", size = 226558 },
    { url = "https://files.pythonhosted.org/packages/fd/22/d2b95cbebbc2ada3be3812ea9287dcc9712d7f1a012fad041770afddb2ad/multidict-6.4.4-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d6b15c55721b1b115c5ba178c77104123745b1417527ad9641a4c5e2047450f0", size = 212168 },
    { url = "https://files.pythonhosted.org/packages/4d/c5/62bfc0b2f9ce88326dbe7179f9824a939c6c7775b23b95de777267b9725c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:a887b77f51d3d41e6e1a63cf3bc7ddf24de5939d9ff69441387dfefa58ac2e26", size = 217970 },
    { url = "https://files.pythonhosted.org/packages/79/74/977cea1aadc43ff1c75d23bd5bc4768a8fac98c14e5878d6ee8d6bab743c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:632a3bf8f1787f7ef7d3c2f68a7bde5be2f702906f8b5842ad6da9d974d0aab3", size = 226980 },
    { url = "https://files.pythonhosted.org/packages/48/fc/cc4a1a2049df2eb84006607dc428ff237af38e0fcecfdb8a29ca47b1566c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:a145c550900deb7540973c5cdb183b0d24bed6b80bf7bddf33ed8f569082535e", size = 220641 },
    { url = "https://files.pythonhosted.org/packages/3b/6a/a7444d113ab918701988d4abdde373dbdfd2def7bd647207e2bf645c7eac/multidict-6.4.4-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cc5d83c6619ca5c9672cb78b39ed8542f1975a803dee2cda114ff73cbb076edd", size = 221728 },
    { url = "https://files.pythonhosted.org/packages/2b/b0/fdf4c73ad1c55e0f4dbbf2aa59dd37037334091f9a4961646d2b7ac91a86/multidict-6.4.4-cp313-cp313t-win32.whl", hash = "sha256:3312f63261b9df49be9d57aaa6abf53a6ad96d93b24f9cc16cf979956355ce6e", size = 41913 },
    { url = "https://files.pythonhosted.org/packages/8e/92/27989ecca97e542c0d01d05a98a5ae12198a243a9ee12563a0313291511f/multidict-6.4.4-cp313-cp313t-win_amd64.whl", hash = "sha256:ba852168d814b2c73333073e1c7116d9395bea69575a01b0b3c89d2d5a87c8fb", size = 46112 },
    { url = "https://files.pythonhosted.org/packages/84/5d/e17845bb0fa76334477d5de38654d27946d5b5d3695443987a094a71b440/multidict-6.4.4-py3-none-any.whl", hash = "sha256:bd4557071b561a8b3b6075c3ce93cf9bfb6182cb241805c3d66ced3b75eff4ac", size = 10481 },
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/6e/371856a3fb9d31ca8dac321cda606860fa4548858c0cc45d9d1d4ca2628b/mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558", size = 6343 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505", size = 4963 },
]

[[package]]
name = "narwhals"
version = "1.48.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/cd/7395d6c247e821cba6243e9f7ed202fae3fefef643c96581b5ecab927bad/narwhals-1.48.0.tar.gz", hash = "sha256:7243b456cbdb60edb148731a8f9b203f473a373a249ad66c699362508730e63f", size = 515112 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/75/72/5406044d4c251f3d8f78cec05b74839d0332d34c9e94b59120f3697ecf48/narwhals-1.48.0-py3-none-any.whl", hash = "sha256:2bbddc3adeed0c5b15ead8fe61f1d5e459f00c1d2fa60921e52a0f9bdc06077d", size = 376866 },
]

[[package]]
name = "numpy"
version = "2.2.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dc/b2/ce4b867d8cd9c0ee84938ae1e6a6f7926ebf928c9090d036fc3c6a04f946/numpy-2.2.5.tar.gz", hash = "sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291", size = 20273920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e2/a0/0aa7f0f4509a2e07bd7a509042967c2fab635690d4f48c6c7b3afd4f448c/numpy-2.2.5-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4", size = 20935102 },
    { url = "https://files.pythonhosted.org/packages/7e/e4/a6a9f4537542912ec513185396fce52cdd45bdcf3e9d921ab02a93ca5aa9/numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f", size = 14191709 },
    { url = "https://files.pythonhosted.org/packages/be/65/72f3186b6050bbfe9c43cb81f9df59ae63603491d36179cf7a7c8d216758/numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9", size = 5149173 },
    { url = "https://files.pythonhosted.org/packages/e5/e9/83e7a9432378dde5802651307ae5e9ea07bb72b416728202218cd4da2801/numpy-2.2.5-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191", size = 6684502 },
    { url = "https://files.pythonhosted.org/packages/ea/27/b80da6c762394c8ee516b74c1f686fcd16c8f23b14de57ba0cad7349d1d2/numpy-2.2.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372", size = 14084417 },
    { url = "https://files.pythonhosted.org/packages/aa/fc/ebfd32c3e124e6a1043e19c0ab0769818aa69050ce5589b63d05ff185526/numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d", size = 16133807 },
    { url = "https://files.pythonhosted.org/packages/bf/9b/4cc171a0acbe4666f7775cfd21d4eb6bb1d36d3a0431f48a73e9212d2278/numpy-2.2.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7", size = 15575611 },
    { url = "https://files.pythonhosted.org/packages/a3/45/40f4135341850df48f8edcf949cf47b523c404b712774f8855a64c96ef29/numpy-2.2.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73", size = 17895747 },
    { url = "https://files.pythonhosted.org/packages/f8/4c/b32a17a46f0ffbde8cc82df6d3daeaf4f552e346df143e1b188a701a8f09/numpy-2.2.5-cp313-cp313-win32.whl", hash = "sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b", size = 6309594 },
    { url = "https://files.pythonhosted.org/packages/13/ae/72e6276feb9ef06787365b05915bfdb057d01fceb4a43cb80978e518d79b/numpy-2.2.5-cp313-cp313-win_amd64.whl", hash = "sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471", size = 12638356 },
    { url = "https://files.pythonhosted.org/packages/79/56/be8b85a9f2adb688e7ded6324e20149a03541d2b3297c3ffc1a73f46dedb/numpy-2.2.5-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6", size = 20963778 },
    { url = "https://files.pythonhosted.org/packages/ff/77/19c5e62d55bff507a18c3cdff82e94fe174957bad25860a991cac719d3ab/numpy-2.2.5-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba", size = 14207279 },
    { url = "https://files.pythonhosted.org/packages/75/22/aa11f22dc11ff4ffe4e849d9b63bbe8d4ac6d5fae85ddaa67dfe43be3e76/numpy-2.2.5-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133", size = 5199247 },
    { url = "https://files.pythonhosted.org/packages/4f/6c/12d5e760fc62c08eded0394f62039f5a9857f758312bf01632a81d841459/numpy-2.2.5-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376", size = 6711087 },
    { url = "https://files.pythonhosted.org/packages/ef/94/ece8280cf4218b2bee5cec9567629e61e51b4be501e5c6840ceb593db945/numpy-2.2.5-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19", size = 14059964 },
    { url = "https://files.pythonhosted.org/packages/39/41/c5377dac0514aaeec69115830a39d905b1882819c8e65d97fc60e177e19e/numpy-2.2.5-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0", size = 16121214 },
    { url = "https://files.pythonhosted.org/packages/db/54/3b9f89a943257bc8e187145c6bc0eb8e3d615655f7b14e9b490b053e8149/numpy-2.2.5-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a", size = 15575788 },
    { url = "https://files.pythonhosted.org/packages/b1/c4/2e407e85df35b29f79945751b8f8e671057a13a376497d7fb2151ba0d290/numpy-2.2.5-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066", size = 17893672 },
    { url = "https://files.pythonhosted.org/packages/29/7e/d0b44e129d038dba453f00d0e29ebd6eaf2f06055d72b95b9947998aca14/numpy-2.2.5-cp313-cp313t-win32.whl", hash = "sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e", size = 6377102 },
    { url = "https://files.pythonhosted.org/packages/63/be/b85e4aa4bf42c6502851b971f1c326d583fcc68227385f92089cf50a7b45/numpy-2.2.5-cp313-cp313t-win_amd64.whl", hash = "sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8", size = 12750096 },
]

[[package]]
name = "openai"
version = "1.84.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/a3/128caf24e116f48fad3e4d5122cdf84db06c5127911849d51663c66158c8/openai-1.84.0.tar.gz", hash = "sha256:4caa43bdab262cc75680ce1a2322cfc01626204074f7e8d9939ab372acf61698", size = 467066 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/10/f245db006a860dbc1f2e2c8382e0a1762c7753e7971ba43a1dc3f3ec1404/openai-1.84.0-py3-none-any.whl", hash = "sha256:7ec4436c3c933d68dc0f5a0cef0cb3dbc0864a54d62bddaf2ed5f3d521844711", size = 725512 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "parso"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/66/94/68e2e17afaa9169cf6412ab0f28623903be73d1b32e208d9e8e541bb086d/parso-0.8.4.tar.gz", hash = "sha256:eb3a7b58240fb99099a345571deecc0f9540ea5f4dd2fe14c2a99d6b281ab92d", size = 400609 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c6/ac/dac4a63f978e4dcb3c6d3a78c4d8e0192a113d288502a1216950c41b1027/parso-0.8.4-py2.py3-none-any.whl", hash = "sha256:a418670a20291dacd2dddc80c377c5c3791378ee1e8d12bffc35420643d43f18", size = 103650 },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191 },
]

[[package]]
name = "pexpect"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "ptyprocess" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/92/cc564bf6381ff43ce1f4d06852fc19a2f11d180f23dc32d9588bee2f149d/pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f", size = 166450 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523", size = 63772 },
]

[[package]]
name = "platformdirs"
version = "4.3.8"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/8b/3c73abc9c759ecd3f1f7ceff6685840859e8070c4d947c93fae71f6a0bf2/platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc", size = 21362 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/39/979e8e21520d4e47a0bbe349e2713c0aac6f3d853d0e5b34d76206c439aa/platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4", size = 18567 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "proto-plus"
version = "1.26.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f4/ac/87285f15f7cce6d4a008f33f1757fb5a13611ea8914eb58c3d0d26243468/proto_plus-1.26.1.tar.gz", hash = "sha256:21a515a4c4c0088a773899e23c7bbade3d18f9c66c73edd4c7ee3816bc96a012", size = 56142 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl", hash = "sha256:13285478c2dcf2abb829db158e1047e2f1e8d63a077d94263c2b88b043c75a66", size = 50163 },
]

[[package]]
name = "protobuf"
version = "5.29.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/17/7d/b9dca7365f0e2c4fa7c193ff795427cfa6290147e5185ab11ece280a18e7/protobuf-5.29.4.tar.gz", hash = "sha256:4f1dfcd7997b31ef8f53ec82781ff434a28bf71d9102ddde14d076adcfc78c99", size = 424902 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/b2/043a1a1a20edd134563699b0e91862726a0dc9146c090743b6c44d798e75/protobuf-5.29.4-cp310-abi3-win32.whl", hash = "sha256:13eb236f8eb9ec34e63fc8b1d6efd2777d062fa6aaa68268fb67cf77f6839ad7", size = 422709 },
    { url = "https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl", hash = "sha256:bcefcdf3976233f8a502d265eb65ea740c989bacc6c30a58290ed0e519eb4b8d", size = 434506 },
    { url = "https://files.pythonhosted.org/packages/46/de/7c126bbb06aa0f8a7b38aaf8bd746c514d70e6a2a3f6dd460b3b7aad7aae/protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:307ecba1d852ec237e9ba668e087326a67564ef83e45a0189a772ede9e854dd0", size = 417826 },
    { url = "https://files.pythonhosted.org/packages/a2/b5/bade14ae31ba871a139aa45e7a8183d869efe87c34a4850c87b936963261/protobuf-5.29.4-cp38-abi3-manylinux2014_aarch64.whl", hash = "sha256:aec4962f9ea93c431d5714ed1be1c93f13e1a8618e70035ba2b0564d9e633f2e", size = 319574 },
    { url = "https://files.pythonhosted.org/packages/46/88/b01ed2291aae68b708f7d334288ad5fb3e7aa769a9c309c91a0d55cb91b0/protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl", hash = "sha256:d7d3f7d1d5a66ed4942d4fefb12ac4b14a29028b209d4bfb25c68ae172059922", size = 319672 },
    { url = "https://files.pythonhosted.org/packages/12/fb/a586e0c973c95502e054ac5f81f88394f24ccc7982dac19c515acd9e2c93/protobuf-5.29.4-py3-none-any.whl", hash = "sha256:3fde11b505e1597f71b875ef2fc52062b6a9740e5f7c8997ce878b6009145862", size = 172551 },
]

[[package]]
name = "psutil"
version = "7.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2a/80/336820c1ad9286a4ded7e845b2eccfcb27851ab8ac6abece774a6ff4d3de/psutil-7.0.0.tar.gz", hash = "sha256:7be9c3eba38beccb6495ea33afd982a44074b78f28c434a1f51cc07fd315c456", size = 497003 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ed/e6/2d26234410f8b8abdbf891c9da62bee396583f713fb9f3325a4760875d22/psutil-7.0.0-cp36-abi3-macosx_10_9_x86_64.whl", hash = "sha256:101d71dc322e3cffd7cea0650b09b3d08b8e7c4109dd6809fe452dfd00e58b25", size = 238051 },
    { url = "https://files.pythonhosted.org/packages/04/8b/30f930733afe425e3cbfc0e1468a30a18942350c1a8816acfade80c005c4/psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl", hash = "sha256:39db632f6bb862eeccf56660871433e111b6ea58f2caea825571951d4b6aa3da", size = 239535 },
    { url = "https://files.pythonhosted.org/packages/2a/ed/d362e84620dd22876b55389248e522338ed1bf134a5edd3b8231d7207f6d/psutil-7.0.0-cp36-abi3-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1fcee592b4c6f146991ca55919ea3d1f8926497a713ed7faaf8225e174581e91", size = 275004 },
    { url = "https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b1388a4f6875d7e2aff5c4ca1cc16c545ed41dd8bb596cefea80111db353a34", size = 277986 },
    { url = "https://files.pythonhosted.org/packages/eb/a2/709e0fe2f093556c17fbafda93ac032257242cabcc7ff3369e2cb76a97aa/psutil-7.0.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5f098451abc2828f7dc6b58d44b532b22f2088f4999a937557b603ce72b1993", size = 279544 },
    { url = "https://files.pythonhosted.org/packages/50/e6/eecf58810b9d12e6427369784efe814a1eec0f492084ce8eb8f4d89d6d61/psutil-7.0.0-cp37-abi3-win32.whl", hash = "sha256:ba3fcef7523064a6c9da440fc4d6bd07da93ac726b5733c29027d7dc95b39d99", size = 241053 },
    { url = "https://files.pythonhosted.org/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:4cf3d4eb1aa9b348dec30105c55cd9b7d4629285735a102beb4441e38db90553", size = 244885 },
]

[[package]]
name = "ptyprocess"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/20/e5/16ff212c1e452235a90aeb09066144d0c5a6a8c0834397e03f5224495c4e/ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220", size = 70762 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35", size = 13993 },
]

[[package]]
name = "pure-eval"
version = "0.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/05/0a34433a064256a578f1783a10da6df098ceaa4a57bbeaa96a6c0352786b/pure_eval-0.2.3.tar.gz", hash = "sha256:5f4e983f40564c576c7c8635ae88db5956bb2229d7e9237d03b3c0b0190eaf42", size = 19752 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8e/37/efad0257dc6e593a18957422533ff0f87ede7c9c6ea010a2177d738fb82f/pure_eval-0.2.3-py3-none-any.whl", hash = "sha256:1db8e35b67b3d218d818ae653e27f06c3aa420901fa7b081ca98cbedc874e0d0", size = 11842 },
]

[[package]]
name = "pyasn1"
version = "0.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ba/e9/01f1a64245b89f039897cb0130016d79f77d52669aae6ee7b159a6c4c018/pyasn1-0.6.1.tar.gz", hash = "sha256:6f580d2bdd84365380830acf45550f2511469f673cb4a5ae3857a3170128b034", size = 145322 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl", hash = "sha256:0d632f46f2ba09143da3a8afe9e33fb6f92fa2320ab7e886e2d0f7672af84629", size = 83135 },
]

[[package]]
name = "pyasn1-modules"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e9/e6/78ebbb10a8c8e4b61a59249394a4a594c1a7af95593dc933a349c8d00964/pyasn1_modules-0.4.2.tar.gz", hash = "sha256:677091de870a80aae844b1ca6134f54652fa2c8c5a52aa396440ac3106e941e6", size = 307892 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl", hash = "sha256:29253a9207ce32b64c3ac6600edc75368f98473906e8fd1043bd6b5b1de2c14a", size = 181259 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pymdown-extensions"
version = "10.16"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown" },
    { name = "pyyaml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/0a/c06b542ac108bfc73200677309cd9188a3a01b127a63f20cadc18d873d88/pymdown_extensions-10.16.tar.gz", hash = "sha256:71dac4fca63fabeffd3eb9038b756161a33ec6e8d230853d3cecf562155ab3de", size = 853197 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/d4/10bb14004d3c792811e05e21b5e5dcae805aacb739bd12a0540967b99592/pymdown_extensions-10.16-py3-none-any.whl", hash = "sha256:f5dd064a4db588cb2d95229fc4ee63a1b16cc8b4d0e6145c0899ed8723da1df2", size = 266143 },
]

[[package]]
name = "pyparsing"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/22/f1129e69d94ffff626bdb5c835506b3a5b4f3d070f17ea295e12c2c6f60f/pyparsing-3.2.3.tar.gz", hash = "sha256:b9c13f1ab8b3b542f72e28f634bad4de758ab3ce4546e4301970ad6fa77c38be", size = 1088608 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/e7/df2285f3d08fee213f2d041540fa4fc9ca6c2d44cf36d3a035bf2a8d2bcc/pyparsing-3.2.3-py3-none-any.whl", hash = "sha256:a749938e02d6fd0b59b356ca504a24982314bb090c383e3cf201c95ef7e2bfcf", size = 111120 },
]

[[package]]
name = "pyperclip"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz", hash = "sha256:b7de0142ddc81bfc5c7507eea19da920b92252b548b96186caf94a5e2527d310", size = 20961 }

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "pytest-asyncio"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d0/d4/14f53324cb1a6381bef29d698987625d80052bb33932d8e7cbf9b337b17c/pytest_asyncio-1.0.0.tar.gz", hash = "sha256:d15463d13f4456e1ead2594520216b225a16f781e144f8fdf6c5bb4667c48b3f", size = 46960 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/05/ce271016e351fddc8399e546f6e23761967ee09c8c568bbfbecb0c150171/pytest_asyncio-1.0.0-py3-none-any.whl", hash = "sha256:4f024da9f1ef945e680dc68610b52550e36590a67fd31bb3b4943979a1f90ef3", size = 15976 },
]

[[package]]
name = "pytest-xdist"
version = "3.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "execnet" },
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/dc/865845cfe987b21658e871d16e0a24e871e00884c545f246dd8f6f69edda/pytest_xdist-3.7.0.tar.gz", hash = "sha256:f9248c99a7c15b7d2f90715df93610353a485827bc06eefb6566d23f6400f126", size = 87550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/b2/0e802fde6f1c5b2f7ae7e9ad42b83fd4ecebac18a8a8c2f2f14e39dce6e1/pytest_xdist-3.7.0-py3-none-any.whl", hash = "sha256:7d3fbd255998265052435eb9daa4e99b62e6fb9cfb6efd1f858d4d8c0c7f0ca0", size = 46142 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-json-logger"
version = "3.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz", hash = "sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84", size = 16642 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl", hash = "sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7", size = 15163 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "readabilipy"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "html5lib" },
    { name = "lxml" },
    { name = "regex" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b8/e4/260a202516886c2e0cc6e6ae96d1f491792d829098886d9529a2439fbe8e/readabilipy-0.3.0.tar.gz", hash = "sha256:e13313771216953935ac031db4234bdb9725413534bfb3c19dbd6caab0887ae0", size = 35491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dd/46/8a640c6de1a6c6af971f858b2fb178ca5e1db91f223d8ba5f40efe1491e5/readabilipy-0.3.0-py3-none-any.whl", hash = "sha256:d106da0fad11d5fdfcde21f5c5385556bfa8ff0258483037d39ea6b1d6db3943", size = 22158 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "rsa"
version = "4.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/8a/22b7beea3ee0d44b1916c0c1cb0ee3af23b700b6da9f04991899d0c555d4/rsa-4.9.1.tar.gz", hash = "sha256:e7bdbfdb5497da4c07dfd35530e1a902659db6ff241e39d9953cad06ebd0ae75", size = 29034 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl", hash = "sha256:68635866661c6836b8d39430f97a996acbd61bfa49406748ea243539fe239762", size = 34696 },
]

[[package]]
name = "ruff"
version = "0.11.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ed/da/9c6f995903b4d9474b39da91d2d626659af3ff1eeb43e9ae7c119349dba6/ruff-0.11.13.tar.gz", hash = "sha256:26fa247dc68d1d4e72c179e08889a25ac0c7ba4d78aecfc835d49cbfd60bf514", size = 4282054 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7d/ce/a11d381192966e0b4290842cc8d4fac7dc9214ddf627c11c1afff87da29b/ruff-0.11.13-py3-none-linux_armv6l.whl", hash = "sha256:4bdfbf1240533f40042ec00c9e09a3aade6f8c10b6414cf11b519488d2635d46", size = 10292516 },
    { url = "https://files.pythonhosted.org/packages/78/db/87c3b59b0d4e753e40b6a3b4a2642dfd1dcaefbff121ddc64d6c8b47ba00/ruff-0.11.13-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:aef9c9ed1b5ca28bb15c7eac83b8670cf3b20b478195bd49c8d756ba0a36cf48", size = 11106083 },
    { url = "https://files.pythonhosted.org/packages/77/79/d8cec175856ff810a19825d09ce700265f905c643c69f45d2b737e4a470a/ruff-0.11.13-py3-none-macosx_11_0_arm64.whl", hash = "sha256:53b15a9dfdce029c842e9a5aebc3855e9ab7771395979ff85b7c1dedb53ddc2b", size = 10436024 },
    { url = "https://files.pythonhosted.org/packages/8b/5b/f6d94f2980fa1ee854b41568368a2e1252681b9238ab2895e133d303538f/ruff-0.11.13-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ab153241400789138d13f362c43f7edecc0edfffce2afa6a68434000ecd8f69a", size = 10646324 },
    { url = "https://files.pythonhosted.org/packages/6c/9c/b4c2acf24ea4426016d511dfdc787f4ce1ceb835f3c5fbdbcb32b1c63bda/ruff-0.11.13-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:6c51f93029d54a910d3d24f7dd0bb909e31b6cd989a5e4ac513f4eb41629f0dc", size = 10174416 },
    { url = "https://files.pythonhosted.org/packages/f3/10/e2e62f77c65ede8cd032c2ca39c41f48feabedb6e282bfd6073d81bb671d/ruff-0.11.13-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1808b3ed53e1a777c2ef733aca9051dc9bf7c99b26ece15cb59a0320fbdbd629", size = 11724197 },
    { url = "https://files.pythonhosted.org/packages/bb/f0/466fe8469b85c561e081d798c45f8a1d21e0b4a5ef795a1d7f1a9a9ec182/ruff-0.11.13-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:d28ce58b5ecf0f43c1b71edffabe6ed7f245d5336b17805803312ec9bc665933", size = 12511615 },
    { url = "https://files.pythonhosted.org/packages/17/0e/cefe778b46dbd0cbcb03a839946c8f80a06f7968eb298aa4d1a4293f3448/ruff-0.11.13-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:55e4bc3a77842da33c16d55b32c6cac1ec5fb0fbec9c8c513bdce76c4f922165", size = 12117080 },
    { url = "https://files.pythonhosted.org/packages/5d/2c/caaeda564cbe103bed145ea557cb86795b18651b0f6b3ff6a10e84e5a33f/ruff-0.11.13-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:633bf2c6f35678c56ec73189ba6fa19ff1c5e4807a78bf60ef487b9dd272cc71", size = 11326315 },
    { url = "https://files.pythonhosted.org/packages/75/f0/782e7d681d660eda8c536962920c41309e6dd4ebcea9a2714ed5127d44bd/ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ffbc82d70424b275b089166310448051afdc6e914fdab90e08df66c43bb5ca9", size = 11555640 },
    { url = "https://files.pythonhosted.org/packages/5d/d4/3d580c616316c7f07fb3c99dbecfe01fbaea7b6fd9a82b801e72e5de742a/ruff-0.11.13-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:4a9ddd3ec62a9a89578c85842b836e4ac832d4a2e0bfaad3b02243f930ceafcc", size = 10507364 },
    { url = "https://files.pythonhosted.org/packages/5a/dc/195e6f17d7b3ea6b12dc4f3e9de575db7983db187c378d44606e5d503319/ruff-0.11.13-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:d237a496e0778d719efb05058c64d28b757c77824e04ffe8796c7436e26712b7", size = 10141462 },
    { url = "https://files.pythonhosted.org/packages/f4/8e/39a094af6967faa57ecdeacb91bedfb232474ff8c3d20f16a5514e6b3534/ruff-0.11.13-py3-none-musllinux_1_2_i686.whl", hash = "sha256:26816a218ca6ef02142343fd24c70f7cd8c5aa6c203bca284407adf675984432", size = 11121028 },
    { url = "https://files.pythonhosted.org/packages/5a/c0/b0b508193b0e8a1654ec683ebab18d309861f8bd64e3a2f9648b80d392cb/ruff-0.11.13-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:51c3f95abd9331dc5b87c47ac7f376db5616041173826dfd556cfe3d4977f492", size = 11602992 },
    { url = "https://files.pythonhosted.org/packages/7c/91/263e33ab93ab09ca06ce4f8f8547a858cc198072f873ebc9be7466790bae/ruff-0.11.13-py3-none-win32.whl", hash = "sha256:96c27935418e4e8e77a26bb05962817f28b8ef3843a6c6cc49d8783b5507f250", size = 10474944 },
    { url = "https://files.pythonhosted.org/packages/46/f4/7c27734ac2073aae8efb0119cae6931b6fb48017adf048fdf85c19337afc/ruff-0.11.13-py3-none-win_amd64.whl", hash = "sha256:29c3189895a8a6a657b7af4e97d330c8a3afd2c9c8f46c81e2fc5a31866517e3", size = 11548669 },
    { url = "https://files.pythonhosted.org/packages/ec/bf/b273dd11673fed8a6bd46032c0ea2a04b2ac9bfa9c628756a5856ba113b0/ruff-0.11.13-py3-none-win_arm64.whl", hash = "sha256:b4385285e9179d608ff1d2fb9922062663c658605819a6876d8beef0c30b7f3b", size = 10683928 },
]

[[package]]
name = "sentencepiece"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz", hash = "sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843", size = 2632106 }

[[package]]
name = "shapely"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fb/fe/3b0d2f828ffaceadcdcb51b75b9c62d98e62dd95ce575278de35f24a1c20/shapely-2.1.0.tar.gz", hash = "sha256:2cbe90e86fa8fc3ca8af6ffb00a77b246b918c7cf28677b7c21489b678f6b02e", size = 313617 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/77/4e368704b2193e74498473db4461d697cc6083c96f8039367e59009d78bd/shapely-2.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b64423295b563f43a043eb786e7a03200ebe68698e36d2b4b1c39f31dfb50dfb", size = 1830029 },
    { url = "https://files.pythonhosted.org/packages/71/3c/d888597bda680e4de987316b05ca9db07416fa29523beff64f846503302f/shapely-2.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1b5578f45adc25b235b22d1ccb9a0348c8dc36f31983e57ea129a88f96f7b870", size = 1637999 },
    { url = "https://files.pythonhosted.org/packages/03/8d/ee0e23b7ef88fba353c63a81f1f329c77f5703835db7b165e7c0b8b7f839/shapely-2.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1a7e83d383b27f02b684e50ab7f34e511c92e33b6ca164a6a9065705dd64bcb", size = 2929348 },
    { url = "https://files.pythonhosted.org/packages/d1/a7/5c9cb413e4e2ce52c16be717e94abd40ce91b1f8974624d5d56154c5d40b/shapely-2.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:942031eb4d8f7b3b22f43ba42c09c7aa3d843aa10d5cc1619fe816e923b66e55", size = 3048973 },
    { url = "https://files.pythonhosted.org/packages/84/23/45b90c0bd2157b238490ca56ef2eedf959d3514c7d05475f497a2c88b6d9/shapely-2.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d2843c456a2e5627ee6271800f07277c0d2652fb287bf66464571a057dbc00b3", size = 3873148 },
    { url = "https://files.pythonhosted.org/packages/c0/bc/ed7d5d37f5395166042576f0c55a12d7e56102799464ba7ea3a72a38c769/shapely-2.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8c4b17469b7f39a5e6a7cfea79f38ae08a275427f41fe8b48c372e1449147908", size = 4052655 },
    { url = "https://files.pythonhosted.org/packages/c0/8f/a1dafbb10d20d1c569f2db3fb1235488f624dafe8469e8ce65356800ba31/shapely-2.1.0-cp313-cp313-win32.whl", hash = "sha256:30e967abd08fce49513d4187c01b19f139084019f33bec0673e8dbeb557c45e4", size = 1526600 },
    { url = "https://files.pythonhosted.org/packages/e3/f0/9f8cdf2258d7aed742459cea51c70d184de92f5d2d6f5f7f1ded90a18c31/shapely-2.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:1dc8d4364483a14aba4c844b7bd16a6fa3728887e2c33dfa1afa34a3cf4d08a5", size = 1707115 },
    { url = "https://files.pythonhosted.org/packages/75/ed/32952df461753a65b3e5d24c8efb361d3a80aafaef0b70d419063f6f2c11/shapely-2.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:673e073fea099d1c82f666fb7ab0a00a77eff2999130a69357ce11941260d855", size = 1824847 },
    { url = "https://files.pythonhosted.org/packages/ff/b9/2284de512af30b02f93ddcdd2e5c79834a3cf47fa3ca11b0f74396feb046/shapely-2.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:6d1513f915a56de67659fe2047c1ad5ff0f8cbff3519d1e74fced69c9cb0e7da", size = 1631035 },
    { url = "https://files.pythonhosted.org/packages/35/16/a59f252a7e736b73008f10d0950ffeeb0d5953be7c0bdffd39a02a6ba310/shapely-2.1.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0d6a7043178890b9e028d80496ff4c79dc7629bff4d78a2f25323b661756bab8", size = 2968639 },
    { url = "https://files.pythonhosted.org/packages/a5/0a/6a20eca7b0092cfa243117e8e145a58631a4833a0a519ec9b445172e83a0/shapely-2.1.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb638378dc3d76f7e85b67d7e2bb1366811912430ac9247ac00c127c2b444cdc", size = 3055713 },
    { url = "https://files.pythonhosted.org/packages/fb/44/eeb0c7583b1453d1cf7a319a1d738e08f98a5dc993fa1ef3c372983e4cb5/shapely-2.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:737124e87d91d616acf9a911f74ac55e05db02a43a6a7245b3d663817b876055", size = 3890478 },
    { url = "https://files.pythonhosted.org/packages/5d/6e/37ff3c6af1d408cacb0a7d7bfea7b8ab163a5486e35acb08997eae9d8756/shapely-2.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e6c229e7bb87aae5df82fa00b6718987a43ec168cc5affe095cca59d233f314", size = 4036148 },
    { url = "https://files.pythonhosted.org/packages/c8/6a/8c0b7de3aeb5014a23f06c5e9d3c7852ebcf0d6b00fe660b93261e310e24/shapely-2.1.0-cp313-cp313t-win32.whl", hash = "sha256:a9580bda119b1f42f955aa8e52382d5c73f7957e0203bc0c0c60084846f3db94", size = 1535993 },
    { url = "https://files.pythonhosted.org/packages/a8/91/ae80359a58409d52e4d62c7eacc7eb3ddee4b9135f1db884b6a43cf2e174/shapely-2.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e8ff4e5cfd799ba5b6f37b5d5527dbd85b4a47c65b6d459a03d0962d2a9d4d10", size = 1717777 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677 },
]

[[package]]
name = "sqlparse"
version = "0.5.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e5/40/edede8dd6977b0d3da179a342c198ed100dd2aba4be081861ee5911e4da4/sqlparse-0.5.3.tar.gz", hash = "sha256:09f67787f56a0b16ecdbde1bfc7f5d9c3371ca683cfeaa8e6ff60b4807ec9272", size = 84999 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl", hash = "sha256:cf2196ed3418f3ba5de6af7e82c694a9fbdbfecccdfc72e281548517081f16ca", size = 44415 },
]

[[package]]
name = "sse-starlette"
version = "2.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/35/7d8d94eb0474352d55f60f80ebc30f7e59441a29e18886a6425f0bccd0d3/sse_starlette-2.3.3.tar.gz", hash = "sha256:fdd47c254aad42907cfd5c5b83e2282be15be6c51197bf1a9b70b8e990522072", size = 17499 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/20/52fdb5ebb158294b0adb5662235dd396fc7e47aa31c293978d8d8942095a/sse_starlette-2.3.3-py3-none-any.whl", hash = "sha256:8b0a0ced04a329ff7341b01007580dd8cf71331cc21c0ccea677d500618da1e0", size = 10235 },
]

[[package]]
name = "stack-data"
version = "0.6.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "asttokens" },
    { name = "executing" },
    { name = "pure-eval" },
]
sdist = { url = "https://files.pythonhosted.org/packages/28/e3/55dcc2cfbc3ca9c29519eb6884dd1415ecb53b0e934862d3559ddcb7e20b/stack_data-0.6.3.tar.gz", hash = "sha256:836a778de4fec4dcd1dcd89ed8abff8a221f58308462e1c4aa2a3cf30148f0b9", size = 44707 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl", hash = "sha256:d5558e0c25a4cb0853cddad3d77da9891a08cb85dd9f9f91b9f8cd66e511e695", size = 24521 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tabulate"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ec/fe/802052aecb21e3797b8f7902564ab6ea0d60ff8ca23952079064155d1ae1/tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c", size = 81090 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f", size = 35252 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tomlkit"
version = "0.13.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/18/0bbf3884e9eaa38819ebe46a7bd25dcd56b67434402b66a58c4b8e552575/tomlkit-0.13.3.tar.gz", hash = "sha256:430cf247ee57df2b94ee3fbe588e71d362a941ebb545dec29b53961d61add2a1", size = 185207 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl", hash = "sha256:c89c649d79ee40629a9fda55f8ace8c6a1b42deb912b2a8fd8d942ddadb606b0", size = 38901 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "traitlets"
version = "5.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/eb/79/72064e6a701c2183016abbbfedaba506d81e30e232a68c9f0d6f6fcd1574/traitlets-5.14.3.tar.gz", hash = "sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7", size = 161621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/c0/8f5d070730d7836adc9c9b6408dec68c6ced86b304a9b26a14df072a6e8c/traitlets-5.14.3-py3-none-any.whl", hash = "sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f", size = 85359 },
]

[[package]]
name = "typer"
version = "0.15.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/1a/5f36851f439884bcfe8539f6a20ff7516e7b60f319bbaf69a90dc35cc2eb/typer-0.15.3.tar.gz", hash = "sha256:818873625d0569653438316567861899f7e9972f2e6e0c16dab608345ced713c", size = 101641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/48/20/9d953de6f4367163d23ec823200eb3ecb0050a2609691e512c8b95827a9b/typer-0.15.3-py3-none-any.whl", hash = "sha256:c86a65ad77ca531f03de08d1b9cb67cd09ad02ddddf4b34745b5008f43b239bd", size = 45253 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uritemplate"
version = "4.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/60/f174043244c5306c9988380d2cb10009f91563fc4b31293d27e17201af56/uritemplate-4.2.0.tar.gz", hash = "sha256:480c2ed180878955863323eea31b0ede668795de182617fef9c6ca09e6ec9d0e", size = 33267 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/99/3ae339466c9183ea5b8ae87b34c0b897eda475d2aec2307cae60e5cd4f29/uritemplate-4.2.0-py3-none-any.whl", hash = "sha256:962201ba1c4edcab02e60f9a0d3821e82dfc5d2d6662a21abd533879bdb8a686", size = 11488 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166 },
]

[[package]]
name = "webencodings"
version = "0.5.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/02/ae6ceac1baeda530866a85075641cec12989bd8d31af6d5ab4a3e8c92f47/webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923", size = 9721 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78", size = 11774 },
]

[[package]]
name = "websockets"
version = "15.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/e6/26d09fab466b7ca9c7737474c52be4f76a40301b08362eb2dbc19dcc16c1/websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee", size = 177016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/9f/51f0cf64471a9d2b4d0fc6c534f323b664e7095640c34562f5182e5a7195/websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931", size = 175440 },
    { url = "https://files.pythonhosted.org/packages/8a/05/aa116ec9943c718905997412c5989f7ed671bc0188ee2ba89520e8765d7b/websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675", size = 173098 },
    { url = "https://files.pythonhosted.org/packages/ff/0b/33cef55ff24f2d92924923c99926dcce78e7bd922d649467f0eda8368923/websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151", size = 173329 },
    { url = "https://files.pythonhosted.org/packages/31/1d/063b25dcc01faa8fada1469bdf769de3768b7044eac9d41f734fd7b6ad6d/websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22", size = 183111 },
    { url = "https://files.pythonhosted.org/packages/93/53/9a87ee494a51bf63e4ec9241c1ccc4f7c2f45fff85d5bde2ff74fcb68b9e/websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f", size = 182054 },
    { url = "https://files.pythonhosted.org/packages/ff/b2/83a6ddf56cdcbad4e3d841fcc55d6ba7d19aeb89c50f24dd7e859ec0805f/websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8", size = 182496 },
    { url = "https://files.pythonhosted.org/packages/98/41/e7038944ed0abf34c45aa4635ba28136f06052e08fc2168520bb8b25149f/websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375", size = 182829 },
    { url = "https://files.pythonhosted.org/packages/e0/17/de15b6158680c7623c6ef0db361da965ab25d813ae54fcfeae2e5b9ef910/websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d", size = 182217 },
    { url = "https://files.pythonhosted.org/packages/33/2b/1f168cb6041853eef0362fb9554c3824367c5560cbdaad89ac40f8c2edfc/websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4", size = 182195 },
    { url = "https://files.pythonhosted.org/packages/86/eb/20b6cdf273913d0ad05a6a14aed4b9a85591c18a987a3d47f20fa13dcc47/websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa", size = 176393 },
    { url = "https://files.pythonhosted.org/packages/1b/6c/c65773d6cab416a64d191d6ee8a8b1c68a09970ea6909d16965d26bfed1e/websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561", size = 176837 },
    { url = "https://files.pythonhosted.org/packages/fa/a8/5b41e0da817d64113292ab1f8247140aac61cbf6cfd085d6a0fa77f4984f/websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f", size = 169743 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]

[[package]]
name = "yoyo-migrations"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "importlib-metadata" },
    { name = "sqlparse" },
    { name = "tabulate" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/8c/5d/9ef7f808ea955eca9f08043c65bdc81a4694e784c978b24ad72022974a97/yoyo_migrations-9.0.0-py3-none-any.whl", hash = "sha256:fc65d3a6d9449c1c54d64ff2ff98e32a27da356057c60e3471010bfb19ede081", size = 49002 },
]

[[package]]
name = "zipp"
version = "3.23.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e3/02/0f2892c661036d50ede074e376733dca2ae7c6eb617489437771209d4180/zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166", size = 25547 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e", size = 10276 },
]
&lt;/file&gt;
  &lt;file path="prompt_service_notebook.py"&gt;import marimo

__generated_with = "0.14.12"
app = marimo.App(width="medium")


@app.cell
def _():
    from repository.database import SQLite3Database
    from repository.prompt_service import PromptService
    from repository.prompt_models import (
        Prompt,
        PromptType,
        PromptPlanStatus,
        CmdCategory,
    )

    return (
        CmdCategory,
        Prompt,
        PromptPlanStatus,
        PromptService,
        PromptType,
        SQLite3Database,
    )


@app.cell
def _(PromptService, SQLite3Database):
    db = SQLite3Database("data/collect.db")
    with db.get_connection() as conn:
        ps = PromptService(conn)
    return (ps,)


@app.cell
def _(ps):
    cmds = ps.load_cmds_from_disk()
    plans = ps.load_plans_from_disk()
    return cmds, plans


@app.cell
def _(cmds):
    print(f"Num cmds: {len(cmds.loaded_prompts)}\n")
    for cmd in cmds.loaded_prompts:
        print(cmd.name)
    return


@app.cell
def _(plans):
    print(f"Num plans: {len(plans.loaded_prompts)}\n")
    for plan in plans.loaded_prompts:
        print(plan.name)
    return


@app.cell
def _():
    db_name = "collect_completed_add_claude_sdk_processing.md"
    result = db_name.split("_")
    print(result)
    return db_name, result


@app.cell
def _(result):
    print(f"project name: {result[0]}")
    print(f"plan status: {result[1]}")
    return


@app.cell
def _(result):
    namelist = result[2:]
    print(namelist)
    return (namelist,)


@app.cell
def _(namelist):
    newname = ""
    for word in namelist:
        if not word.endswith(".md"):
            newname = newname + word + "_"
        else:
            newname = newname + word
    print(newname)
    return


@app.cell
def _(db_name):
    print(db_name.split("_")[2:])
    return


@app.cell
def _(PromptType):
    def parse_db_name(db_name: str, prompt_type: PromptType) -&amp;gt; str:
        ls = db_name.split("_")
        filename = ""
        if prompt_type == PromptType.PLAN:
            project = ls[0]
            plan_status = ls[1]
            print(f"project: {project}")
            print(f"plan status: {plan_status}")

            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

        if prompt_type == PromptType.CMD:
            cmd_dir = ls[0]
            print(f"cmd/dir: {cmd_dir}")
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

    return (parse_db_name,)


@app.cell
def _(PromptType, db_name, parse_db_name):
    parse_db_name(db_name, PromptType.PLAN)
    return


@app.cell
def _(PromptType, parse_db_name):
    parse_db_name("tools_create_database.md", PromptType.CMD)
    return


@app.cell
def _(CmdCategory, Prompt, PromptPlanStatus, PromptType, ps):
    def new_cmd_prompt(prompt_content: str) -&amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
&lt;/file&gt;
  &lt;file path="Makefile"&gt;PROJECT_NAME := collect

marimo:
	uv run marimo edit

.PHONY: movetools
movetools:
	./movetools

.PHONY: buildsrc
buildsrc: 
	./tools/buildsrc

.PHONY: ensuregithub
ensuregithub:
	./tools/ensure-github-url

lint:
	ruff check .

format:
	black .

test: 
	uv run pytest -v -s -n auto

test-fast:
	uv run pytest -v -n auto -m "not slow"

test-slow:
	uv run pytest -v -s -m slow

test-single:
	uv run pytest -v -s

check: 
	make lint
	make format
	make movetools
	make ensuregithub
	make buildsrc

migrate:
	uv run yoyo apply --config yoyo.ini --batch


&lt;/file&gt;
  &lt;file path=".mcp.json"&gt;{
	"mcpServers": {
		"collect": {
			"command": "/Users/benjaminmetz/.local/bin/uv",
			"args": [
				"--directory",
				"/Users/benjaminmetz/python/collect",
				"run",
				"collect.py"
			]
		}
	}
}
&lt;/file&gt;
  &lt;file path="pyproject.toml"&gt;[project]
name = "collect"
version = "0.1.0"
description = "development toolkit for all the things.."
readme = "README.md"
requires-python = "&amp;gt;=3.13"
dependencies = [
    "aiohttp&amp;gt;=3.12.11",
    "anthropic&amp;gt;=0.50.0",
    "beautifulsoup4&amp;gt;=4.13.4",
    "black&amp;gt;=25.1.0",
    "fastapi&amp;gt;=0.116.1",
    "google-ai-generativelanguage&amp;gt;=0.6.15",
    "google-api-python-client&amp;gt;=2.169.0",
    "google-auth-httplib2&amp;gt;=0.2.0",
    "google-cloud-aiplatform[tokenization]&amp;gt;=1.91.0",
    "google-cloud-secret-manager&amp;gt;=2.23.3",
    "google-genai&amp;gt;=1.13.0",
    "google-generativeai&amp;gt;=0.8.5",
    "html-to-markdown&amp;gt;=1.3.2",
    "html5lib&amp;gt;=1.1",
    "httplib2&amp;gt;=0.22.0",
    "httpx&amp;gt;=0.28.1",
    "ipython&amp;gt;=9.4.0",
    "lxml&amp;gt;=5.4.0",
    "marimo&amp;gt;=0.14.12",
    "markdownify&amp;gt;=1.1.0",
    "mcp[cli]&amp;gt;=1.7.1",
    "openai&amp;gt;=1.59.4",
    "pathspec&amp;gt;=0.12.1",
    "pyperclip&amp;gt;=1.9.0",
    "pytest&amp;gt;=8.3.5",
    "pytest-asyncio&amp;gt;=0.26.0",
    "pytest-xdist&amp;gt;=3.6.1",
    "python-json-logger&amp;gt;=3.3.0",
    "readabilipy&amp;gt;=0.3.0",
    "rich&amp;gt;=14.0.0",
    "ruff&amp;gt;=0.11.9",
    "tiktoken&amp;gt;=0.9.0",
    "uvicorn&amp;gt;=0.34.2",
    "yoyo-migrations&amp;gt;=9.0.0",
]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."]
filterwarnings = [
    "ignore::UserWarning:google.auth._default"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests"
]
# Parallel test execution settings
addopts = [
    "--strict-markers",  # Ensure all marks are registered
    "--tb=short",       # Shorter traceback format
    "--dist=worksteal", # Better work distribution for uneven test times
]
&lt;/file&gt;
  &lt;file path="initial_load.py"&gt;#!/usr/bin/env python3
"""
Initial load script for loading plans and commands from disk into database
Uses PromptService to load from .claude/commands,
.gemini/commands, and _docs/plans
"""
import sys
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import PromptLoadResult, PromptCreateResult


def load_commands_to_db(service: PromptService) -&amp;gt; PromptLoadResult:
    """
    Load commands from .claude and .gemini directories and save to database
    This is primarily for an initial load or to clean and restart from disk

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📁 Loading commands from disk...")

    # Load commands from filesystem
    cmd_result: PromptLoadResult = service.load_cmds_from_disk()

    if cmd_result.errors:
        print(
            f"⚠️  Found {len(cmd_result.errors)
                           } errors while loading commands:"
        )
        for error in cmd_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not cmd_result.loaded_prompts:
        print("ℹ️  No commands found to load")
        return cmd_result

    print(f"Found: {len(cmd_result.loaded_prompts)} to save to database")

    # Save commands to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        cmd_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save command: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (cmd_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=cmd_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def load_plans_to_db(service: PromptService) -&amp;gt; PromptLoadResult:
    """Load plans from _docs/plans directories and save to database

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📋 Loading plans from disk...")

    # Load plans from filesystem
    plan_result: PromptLoadResult = service.load_plans_from_disk()

    if plan_result.errors:
        print(
            f"⚠️  Found {len(plan_result.errors)
                           } errors while loading plans:"
        )
        for error in plan_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not plan_result.loaded_prompts:
        print("ℹ️  No plans found to load")
        return plan_result

    print(
        f"📄 Found {len(plan_result.loaded_prompts)
                     } plans to save to database"
    )

    # Save plans to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        plan_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save plan: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (plan_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=plan_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def main():
    """Main function to orchestrate the complete loading process"""
    print("🚀 Starting initial data load from disk to database...")
    print("=" * 60)

    try:
        # Initialize database connection
        database = SQLite3Database("data/collect.db")

        with database.get_connection() as conn:
            # Create PromptService instance
            service = PromptService(conn)

            # Track overall statistics
            total_loaded = 0
            total_errors = 0

            # Load commands
            cmd_result: PromptLoadResult = load_commands_to_db(service)
            cmd_loaded = len(cmd_result.loaded_prompts)
            cmd_errors = len(cmd_result.errors) if cmd_result.errors else 0

            total_loaded += cmd_loaded
            total_errors += cmd_errors

            print(f"✅ Commands: {cmd_loaded} loaded, {cmd_errors} errors")
            print()

            # Load plans
            plan_result: PromptLoadResult = load_plans_to_db(service)
            plan_loaded = len(plan_result.loaded_prompts)
            plan_errors = len(plan_result.errors) if plan_result.errors else 0

            total_loaded += plan_loaded
            total_errors += plan_errors

            print(f"✅ Plans: {plan_loaded} loaded, {plan_errors} errors")
            print()

            # Print final summary
            print("=" * 60)
            print("📊 FINAL SUMMARY:")
            print(f"   ✅ Total items loaded: {total_loaded}")
            print(f"   ❌ Total errors: {total_errors}")

            if total_errors == 0:
                print("🎉 All data loaded successfully!")
            else:
                print(
                    f"⚠️  Completed with {
                      total_errors} errors - check output above"
                )

    except Exception as e:
        print(f"💥 Fatal error during loading process: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        sys.exit(1)


if __name__ == "__main__":
    main()
&lt;/file&gt;
  &lt;file path=".sync_cache.json"&gt;{
  ".claude/commands/go/go_build_endpoint_test.md": {
    "sha": "bb6aa9bffab635d3455caf7cfa59fc4f43036aac",
    "path": ".claude/commands/go/go_build_endpoint_test.md",
    "last_synced": 1754870996.5877829,
    "converted": true
  },
  ".claude/commands/archive/build_context.md": {
    "sha": "d976db85985179f771925095a73d98142d3ab30b",
    "path": ".claude/commands/archive/build_context.md",
    "last_synced": 1754870996.598152,
    "converted": true
  },
  ".claude/commands/commit.md": {
    "sha": "891c52b5b372e6fffd161dcf9b55930755d9fbf3",
    "path": ".claude/commands/commit.md",
    "last_synced": 1754870996.614573,
    "converted": true
  },
  ".claude/commands/go/create_go_structs.md": {
    "sha": "56a5e5dc2abb511b98b2e1147d428551610aa49e",
    "path": ".claude/commands/go/create_go_structs.md",
    "last_synced": 1754870996.625781,
    "converted": true
  },
  ".claude/commands/diff_code_review.md": {
    "sha": "6ef1818de14d6d1e994d9fd4841bc5486c4d6f20",
    "path": ".claude/commands/diff_code_review.md",
    "last_synced": 1754870996.635034,
    "converted": true
  },
  ".claude/commands/convert_to_toml.md": {
    "sha": "7bdd42b1ffeee07bd06ed5b4b97e99381c8052e9",
    "path": ".claude/commands/convert_to_toml.md",
    "last_synced": 1754870996.642501,
    "converted": true
  },
  ".claude/commands/create_checklist_3.md": {
    "sha": "27f4fa9841078c71d9490d9ef2c967970c053bbf",
    "path": ".claude/commands/create_checklist_3.md",
    "last_synced": 1754870996.688289,
    "converted": true
  },
  ".claude/commands/go/create_go_structsV1.md": {
    "sha": "d7e28d7eb8dee775f9416574d8f06ef11ca42158",
    "path": ".claude/commands/go/create_go_structsV1.md",
    "last_synced": 1754870996.692773,
    "converted": true
  },
  ".claude/commands/go/go_update_config.md": {
    "sha": "3cbaf390b5abd45230dde54705958bd873f16ebc",
    "path": ".claude/commands/go/go_update_config.md",
    "last_synced": 1754871010.753632,
    "converted": true
  },
  ".claude/commands/mcp/copy_to_clipboard.md": {
    "sha": "aad8af9420d4a80933aaf12accfe7a8450679f32",
    "path": ".claude/commands/mcp/copy_to_clipboard.md",
    "last_synced": 1754871011.429687,
    "converted": true
  },
  ".claude/commands/mcp/get_docs.md": {
    "sha": "9e27863768032379a1798cf6da1a881720ed65bf",
    "path": ".claude/commands/mcp/get_docs.md",
    "last_synced": 1754871012.238699,
    "converted": true
  },
  ".claude/commands/model_code_review.md": {
    "sha": "8bc222f49f8587a01362d06ff6475cb8e46ace2e",
    "path": ".claude/commands/model_code_review.md",
    "last_synced": 1754871023.324787,
    "converted": true
  },
  ".claude/commands/pr.md": {
    "sha": "0e8a152f358b515f158d5025428db18847101fc0",
    "path": ".claude/commands/pr.md",
    "last_synced": 1754871024.7506702,
    "converted": true
  },
  ".claude/commands/prime_webapp.md": {
    "sha": "c6f4cfe53f2aa92e1ac5661138989c9f9ff3ec42",
    "path": ".claude/commands/prime_webapp.md",
    "last_synced": 1754871038.662535,
    "converted": true
  },
  ".claude/commands/python/python_update_config.md": {
    "sha": "8562257de3a622c1aefb06be4c78a4ce9c580e2a",
    "path": ".claude/commands/python/python_update_config.md",
    "last_synced": 1754871042.126036,
    "converted": true
  },
  ".claude/commands/read.md": {
    "sha": "524139e35449b4f5c0a88b206699cec5d2f1409b",
    "path": ".claude/commands/read.md",
    "last_synced": 1754871043.723397,
    "converted": true
  },
  ".claude/commands/response_in_markdown.md": {
    "sha": "a540be858d83e4b65df8dd2a23e3656bbdce8ee7",
    "path": ".claude/commands/response_in_markdown.md",
    "last_synced": 1754871049.784717,
    "converted": true
  },
  ".claude/commands/runplan.md": {
    "sha": "08d9b2d8c3bd445c55170adec7e80b9d3733e27a",
    "path": ".claude/commands/runplan.md",
    "last_synced": 1754871056.6670442,
    "converted": true
  },
  ".claude/commands/test_runner.md": {
    "sha": "28e088b2d155b9eca391add1bc75cdf701a3a4d8",
    "path": ".claude/commands/test_runner.md",
    "last_synced": 1754871059.413824,
    "converted": true
  },
  ".claude/commands/tools/create_database.md": {
    "sha": "fcfeeb2b78e39ecf0a1b8b4a85735dd83fa1c8f1",
    "path": ".claude/commands/tools/create_database.md",
    "last_synced": 1754871070.783872,
    "converted": true
  },
  ".claude/commands/tools/extract.md": {
    "sha": "e8946fb23beb24cd8e8689c48cd21b155dfa10a6",
    "path": ".claude/commands/tools/extract.md",
    "last_synced": 1754871074.061621,
    "converted": true
  }
}&lt;/file&gt;
  &lt;file path="llmrunner.py"&gt;import asyncio
from datetime import datetime

from pydantic import BaseModel
from typing import Dict, Union, List, Optional, Any

from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP
from models.gemini_mcp import GeminiMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP


class ModelsToMCP(BaseModel):
    model_config = {"arbitrary_types_allowed": True}
    models_to_mcp: Dict[str, Union[GeminiMCP, AnthropicMCP, OpenAIMCP, XaiMCP]]


class ModelResult(BaseModel):
    model: str
    timestamp: str
    success: bool
    actual_model: Optional[str] = None
    duration_seconds: Optional[float] = None
    response: Optional[Any] = None
    error: Optional[str] = None


class LLMRunnerResults(BaseModel):
    successful_results: List[ModelResult]
    failed_results: List[ModelResult]
    total_models: int
    success_count: int
    failure_count: int


async def llmrunner(prompt: str, models_to_mcp: ModelsToMCP) -&amp;gt; LLMRunnerResults:

    async def call_model(model_name: str) -&amp;gt; dict:
        try:
            start_time = datetime.now()
            iso_time = start_time.isoformat()

            mcp_instance = models_to_mcp.models_to_mcp[model_name]
            print(f"sending to --&amp;gt; {model_name} : at -&amp;gt; {iso_time}")
            response = mcp_instance.send_message(prompt, model=model_name)
            end_time = datetime.now()

            result = ModelResult(
                model=model_name,
                actual_model=model_name,
                timestamp=iso_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                response=response,
                success=True,
            )

            return result

        except Exception as e:
            error_result = ModelResult(
                success=False,
                error=str(e),
                model=model_name,
                timestamp=datetime.now().isoformat(),
            )

            return error_result

    print(
        f"starting runner for: {
          len(models_to_mcp.models_to_mcp.keys())} models -&amp;gt;"
    )
    tasks = [call_model(model) for model in models_to_mcp.models_to_mcp.keys()]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful_results = [
        r for r in results if isinstance(r, ModelResult) and r.success
    ]

    failed_results = [
        r for r in results if isinstance(r, ModelResult) and not r.success
    ]

    return LLMRunnerResults(
        successful_results=successful_results,
        failed_results=failed_results,
        total_models=len(models_to_mcp.models_to_mcp),
        success_count=len(successful_results),
        failure_count=len(failed_results),
    )


def code_review_models_to_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    anthropic_model = config.anthropic_default_code_review_model
    gemini_model = config.gemini_default_code_review_model
    xai_model = config.xai_default_code_review_model
    openai_model = config.openai_default_code_review_model

    gemini_mcp = GeminiMCP(config, secret_mgr, gemini_model)
    openai_mcp = OpenAIMCP(config, secret_mgr, openai_model)
    xai_mcp = XaiMCP(config, secret_mgr, xai_model)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, anthropic_model)

    model_mcps = {
        gemini_model: gemini_mcp,
        openai_model: openai_mcp,
        xai_model: xai_mcp,
        anthropic_model: anthropic_mcp,
    }

    return ModelsToMCP(models_to_mcp=model_mcps)
&lt;/file&gt;
  &lt;file path="README.md"&gt;**Collect** is a command-line toolkit built with Python that functions as an MCP (Model Context Protocol) server. It is designed to assist with AI-driven development by providing tools to fetch web content, process it, and coordinate analysis across multiple AI models.

*   **Multi-Model Integration**: Interact with models from Google (Gemini), Anthropic (Claude), OpenAI (GPT), and XAI (Grok) through a single interface.
*   **Content Processing**: Fetch content from URLs and convert HTML to clean markdown or plain text.
*   **Code &amp;amp; Diff Analysis**: Perform code reviews on files or git diffs using any of the integrated AI models.
*   **Secure Configuration**: Utilizes Google Cloud Secret Manager for API key storage.
*   **Prompt Management**: A version-controlled system for managing and synchronizing prompts between the local filesystem and a SQLite database.
*   **Token Utilities**: Tools to count token usage for various models to manage costs and context windows.

### MCP Server Configuration

#### For Claude Code

To enable Claude Code to use the `collect` MCP server, create a `.mcp.json` file in your project's root directory:

1.  **Create the Configuration File**: In the root of your project where you want to use the collect tools, create a file named `.mcp.json`.
2.  **Add Configuration**: Add the following JSON configuration:

```json
{
  "mcpServers": {
    "collect": {
      "command": "/path/to/.local/bin/uv",
      "args": [
        "--directory",
        "/path/to/collect",
        "run",
        "collect.py"
      ]
    }
  }
}
```

Replace `/path/to/.local/bin/uv` with the full path to your `uv` binary (you can find this with `which uv`), and `/path/to/collect` with the full path to your collect repository.

#### For Gemini CLI

To enable the Gemini CLI to automatically start the `collect` MCP server, you need to configure a `.gemini/settings.json` file in your project's root directory:

1.  **Create the Directory**: If it doesn't already exist, create a `.gemini` directory in the root of the `collect` project.
2.  **Create the Settings File**: Inside the `.gemini` directory, create a file named `settings.json`.
3.  **Add Configuration**: Paste the following JSON configuration into the `settings.json` file.

```json
{
  "mcpServers": {
    "collect": {
      "command": "uv",
      "args": [
        "run",
        "python",
        "collect.py"
      ],
      "workingDirectory": "/Users/benjaminmetz/python/collect",
      "enabled": true
    }
  }
}
```

This configuration tells the Gemini CLI how to launch the `collect` server, specifying the command, arguments, and working directory.

### Command Category System

The command category system dynamically creates categories based on subdirectories configured in the `.env` file. This approach allows for easy extension of command categories without code changes.

#### How Categories Are Created

1. **Configuration**: Command subdirectories are defined in the `.env` file:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
   ```

2. **Dynamic Enum Generation**: The `create_cmd_category_enum()` function in `repository/prompt_models.py` reads the `COMMAND_SUBDIRS` from the `.env` file via the `Config` class and dynamically creates a `CmdCategory` enum at runtime.

3. **Directory Management**: When the `PromptService` initializes, the `cmd_check_dirs()` function in `repository/prompt_service.py`:
   - Reads the subdirectory list from the config
   - Checks for the existence of each configured subdirectory under both `.claude/commands/` and `.gemini/commands/`
   - Automatically creates any missing directories
   - Each subdirectory becomes a valid command category

4. **Category Assignment**: When loading commands from disk:
   - Files directly in `.claude/commands/` or `.gemini/commands/` are assigned the `UNCATEGORIZED` category
   - Files in subdirectories are assigned the category matching the subdirectory name
   - The category is stored as part of the prompt's metadata in the database

#### Adding New Categories

To add new command categories:
1. Update the `COMMAND_SUBDIRS` line in the `.env` file with your new category
2. The system will automatically create the directories and recognize them as valid categories on the next run
3. Commands placed in those directories will be tagged with the new category

#### Example

To add a "rust" category:
1. Edit `.env`:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools,rust
   ```
2. Restart the service or run the prompt service
3. The system will create:
   - `.claude/commands/rust/`
   - `.gemini/commands/rust/`
4. Any `.md` files placed in these directories will be categorized as "rust" commands

#### Current Directory Structure
Based on the `.env` configuration (`COMMAND_SUBDIRS=archive,go,js,mcp,python,tools`), the directory structure is:

```
.claude/
└── commands/
    ├── archive/          # Archived commands
    ├── go/               # Go-specific commands
    ├── js/               # JavaScript commands
    ├── mcp/              # MCP server commands
    ├── python/           # Python-specific commands
    └── tools/            # Tool-related commands

.gemini/
└── commands/
    ├── archive/
    ├── go/
    ├── js/
    ├── mcp/
    ├── python/
    └── tools/
```

Note: Files placed directly in `.claude/commands/` or `.gemini/commands/` (not in subdirectories) are automatically assigned the `UNCATEGORIZED` category.

### Prompt Management System

The project includes a system for managing prompts **that is very much under construction**. Prompts are categorized as either **Commands** (`CMD`) or **Plans** (`PLAN`). This system, located in the `repository/` directory, uses a SQLite database to store and version prompts, while also synchronizing them with the local filesystem.

*   **Core Components**:
    *   `prompt_service.py`: The main service class that orchestrates loading, saving, versioning, and flattening prompts.
    *   `prompt_models.py`: Defines the Pydantic data models for prompts, including `Prompt`, `PromptData`, and various status enums like `PromptType` and `PromptPlanStatus`.
    *   `database.py`: Manages the connection to the `collect.db` SQLite database.
    *   `20250727_01_create-prompt-tables.sql`: The database migration file that defines the schema for the `prompt` and `prompt_history` tables.

*   **Synchronization Workflow**:
    1.  **Loading from Disk**: The `PromptService` can load prompts from predefined directories (`.claude/commands`, `.gemini/commands`, and `_docs/plans`).
    2.  **Database Persistence**: Loaded prompts are saved to the SQLite database. The service checks for existing prompts by name. If a prompt already exists and its content has changed (verified via a SHA256 hash), a new version is created in the `prompt_history` table, and the main `prompt` table is updated.
    3.  **Flattening to Disk**: The service can "flatten" the prompts from the database back to the filesystem, ensuring that the local files are consistent with the database state. This is useful for maintaining a clear and organized prompt library.

*   **Versioning**:
    *   Every time a prompt's content is updated, its `version` number is incremented.
    *   A complete record of all versions is stored in the `prompt_history` table, including a timestamp and a change summary. This allows for a full audit trail of how a prompt has evolved.

This system ensures that prompts are treated as version-controlled assets within the project, providing a structured and auditable way to manage the instructions given to the AI models.
&lt;/file&gt;
  &lt;file path="api.py"&gt;#!/usr/bin/env python

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api import prompt_api_router
from config import Config
import uvicorn

import sys
import logging
from pythonjsonlogger.json import JsonFormatter
from contextlib import asynccontextmanager


# Configure JSON logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(stream=sys.stdout)
handler.setFormatter(JsonFormatter())
handler.setLevel(logging.INFO)

logger.addHandler(handler)

# Load configuration
config = Config()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting prompt API service...")
    app.state.db_path = config.db_path
    app.state.config = config
    logger.info(f"Database path set to: {app.state.db_path}")
    logger.info(f"Service running on port: {config.port}")

    yield

    # Shutdown
    logger.info("Shutting down prompt API service...")


app = FastAPI(
    title="Prompt Service API",
    description="HTTP API for managing prompts and plans",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

app.include_router(prompt_api_router, tags=["prompt_api"])


def main():
    uvicorn.run(app, host="0.0.0.0", port=int(config.port))


if __name__ == "__main__":
    main()
&lt;/file&gt;
  &lt;file path="yoyo.ini"&gt;[DEFAULT]
sources = migrations
database = sqlite:///data/collect.db
batch_mode = on
verbosity = 0

&lt;/file&gt;
  &lt;file path="secret_manager.py"&gt;from google.cloud import secretmanager


class SecretManager:
    def __init__(
        self,
        project_id: str,
    ) -&amp;gt; None:
        self.project_id = project_id
        self.gcp_client = secretmanager.SecretManagerServiceClient()

    def get_secret(self, secret_name: str) -&amp;gt; str:
        response = self.gcp_client.access_secret_version(request={"name": secret_name})
        return response.payload.data.decode("UTF-8").strip()
&lt;/file&gt;
  &lt;file path=".gitignore"&gt;# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
venv/
env/

# Environment variables
.env.local
.env.*.local

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Test coverage
.coverage
htmlcov/
.pytest_cache/
.tox/

# Temporary files
:w
*.tmp
*.temp

# Python version management
.python-version

# UV lock file (optional - some prefer to track this)
# uv.lock
&lt;/file&gt;
  &lt;file path=".env"&gt;PORT=8081
GCP_PROJECT_ID=482777410016
DB_PATH=data/collect.db
ANTHROPIC_API_KEY_PATH=projects/482777410016/secrets/AnthropicMCP/versions/1
ANTHROPIC_MODEL_OPUS=claude-opus-4-20250514
ANTHROPIC_MODEL_SONNET=claude-sonnet-4-20250514
GEMINI_API_KEY_PATH=projects/482777410016/secrets/GeminiTest/versions/1
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/
XAI_API_KEY_PATH=projects/482777410016/secrets/XAI_API_KEY_ELEPHNT/versions/1
GROK_SYSTEM_PROMPT=You are a helpful assistant that can answer questions and help with tasks.
OPENAI_API_KEY_PATH=projects/482777410016/secrets/OpenAIMCP/versions/1

# default code review models for running the code review loop
OPENAI_DEFAULT_CODE_REVIEW_MODEL=o3-mini-2025-01-31
GEMINI_DEFAULT_CODE_REVIEW_MODEL=gemini-2.5-flash-preview-05-20
ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL=claude-opus-4-20250514
XAI_DEFAULT_CODE_REVIEW_MODEL=grok-3-mini-fast-latest

# Command subdirectories
COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
GITHUB_URL=https://github.com/austere-labs/collect
&lt;/file&gt;
  &lt;file path="filterModel.md"&gt;# Gemini Model Filtering Implementation Guide

## Overview

This document describes how to filter Gemini models by version number (2.0, 2.5, etc.) and extract input token limits from the API response.

## Current State

The `GeminiMCP.get_model_list()` method has been updated to return the full API response instead of just model names:

```python
def get_model_list(self) -&amp;gt; Dict:
    # ... API call logic ...
    model_data = response.json()
    return model_data  # Returns full response with all model metadata
```

## Implementation Plan

### 1. Add Filtering Methods to GeminiMCP Class

Add these methods to the `GeminiMCP` class in `models/gemini_mcp.py`:

```python
def filter_models_by_version(self, versions: list[str]) -&amp;gt; list[dict]:
    """
    Filter models by version numbers and include token limits.
    
    Args:
        versions: List of version strings (e.g., ['2.0', '2.5'])
    
    Returns:
        List of dicts with model info including inputTokenLimit
    """
    all_models = self.get_model_list()
    filtered_models = []
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Check if model matches any requested version
        for version in versions:
            if version in model_name:
                model_info = {
                    'name': model_name,
                    'displayName': model.get('displayName', ''),
                    'inputTokenLimit': model.get('inputTokenLimit', 0),
                    'outputTokenLimit': model.get('outputTokenLimit', 0),
                    'description': model.get('description', ''),
                    'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
                }
                filtered_models.append(model_info)
                break
    
    return filtered_models

def get_models_with_token_info(self) -&amp;gt; list[dict]:
    """
    Get all models with their token limit information.
    
    Returns:
        List of models sorted by inputTokenLimit (descending)
    """

    all_models = self.get_model_list()
    models_with_tokens = []

    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Only include models with token limit info
        if input_limit &amp;gt; 0:
            models_with_tokens.append({
                'name': model_name,
                'inputTokenLimit': input_limit,
                'outputTokenLimit': model.get('outputTokenLimit', 0)
            })
    
    # Sort by input token limit (highest first)
    models_with_tokens.sort(key=lambda x: x['inputTokenLimit'], reverse=True)
    return models_with_tokens
```

### 2. Advanced Filtering Function (Standalone)

For more complex filtering needs, you can use this standalone function:

```python
def filter_gemini_models(models_data: dict, 
                        versions: list[str] = None,
                        min_input_tokens: int = None,
                        max_input_tokens: int = None,
                        generation_methods: list[str] = None) -&amp;gt; list[dict]:
    """
    Advanced filtering with multiple criteria.
    
    Args:
        models_data: Response from get_model_list()
        versions: Filter by version numbers (optional)
        min_input_tokens: Minimum inputTokenLimit (optional)
        max_input_tokens: Maximum inputTokenLimit (optional)
        generation_methods: Required generation methods (optional)
    
    Returns:
        Filtered list of model information
    """
    filtered_models = []
    
    for model in models_data.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Apply version filter
        if versions:
            if not any(ver in model_name for ver in versions):
                continue
        
        # Apply token limit filters
        if min_input_tokens and input_limit &amp;lt; min_input_tokens:
            continue
        if max_input_tokens and input_limit &amp;gt; max_input_tokens:
            continue
        
        # Apply generation method filter
        if generation_methods:
            supported_methods = model.get('supportedGenerationMethods', [])
            if not all(method in supported_methods for method in generation_methods):
                continue
        
        # Model passed all filters
        model_info = {
            'name': model_name,
            'displayName': model.get('displayName', ''),
            'inputTokenLimit': input_limit,
            'outputTokenLimit': model.get('outputTokenLimit', 0),
            'description': model.get('description', ''),
            'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
        }
        filtered_models.append(model_info)
    
    return filtered_models
```

## Usage Examples

### Basic Version Filtering

```python
def test_filter_by_version(gemini_mcp):
    # Get models for versions 2.0 and 2.5
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    print(f"Found {len(filtered)} models:")
    for model in filtered:
        print(f"- {model['name']}: {model['inputTokenLimit']:,} input tokens")
```

### Get Models with Token Info

```python
def test_models_with_tokens(gemini_mcp):
    models = gemini_mcp.get_models_with_token_info()
    
    print("Models by input token limit:")
    for model in models[:10]:  # Top 10 models
        print(f"- {model['name']}: {model['inputTokenLimit']:,} tokens")
```

### Advanced Filtering

```python
def test_advanced_filtering(gemini_mcp):
    all_models = gemini_mcp.get_model_list()
    
    # Find 2.5 models with at least 100k input tokens
    filtered = filter_gemini_models(
        all_models,
        versions=['2.5'],
        min_input_tokens=100000,
        generation_methods=['generateContent']
    )
    
    print("High-capacity 2.5 models:")
    for model in filtered:
        print(f"- {model['name']}")
        print(f"  Input limit: {model['inputTokenLimit']:,}")
        print(f"  Output limit: {model['outputTokenLimit']:,}")
```

### Grouping Models by Version

```python
def group_models_by_version(gemini_mcp):
    from collections import defaultdict
    
    all_models = gemini_mcp.get_model_list()
    version_groups = defaultdict(list)
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Extract version pattern
        if '2.5' in model_name:
            version = '2.5'
        elif '2.0' in model_name:
            version = '2.0'
        elif '1.5' in model_name:
            version = '1.5'
        elif '1.0' in model_name:
            version = '1.0'
        else:
            version = 'other'
        
        version_groups[version].append({
            'name': model_name,
            'inputTokenLimit': model.get('inputTokenLimit', 0)
        })
    
    # Display grouped results
    for version, models in sorted(version_groups.items()):
        print(f"\nVersion {version} ({len(models)} models):")
        for model in sorted(models, key=lambda x: x['inputTokenLimit'], reverse=True)[:3]:
            print(f"  - {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Expected Output Format

When filtering models, you'll get results like:

```
Found 15 models:
- gemini-2.5-pro: 2,000,000 input tokens
- gemini-2.5-flash: 1,000,000 input tokens
- gemini-2.5-flash-preview-05-20: 1,000,000 input tokens
- gemini-2.0-flash: 32,768 input tokens
- gemini-2.0-flash-exp: 32,768 input tokens
- gemini-2.0-pro-exp: 32,768 input tokens
```

## API Response Structure

The Gemini API returns model data in this format:

```json
{
  "models": [
    {
      "name": "models/gemini-2.5-flash",
      "displayName": "Gemini 2.5 Flash",
      "description": "Fast and versatile multimodal model",
      "inputTokenLimit": 1000000,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ]
    }
    // ... more models
  ]
}
```

## Testing the Implementation

Add this test to `models/test_gemini_mcp.py`:

```python
def test_filter_models_by_version(gemini_mcp):
    # Test filtering for 2.0 and 2.5 versions
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    assert len(filtered) &amp;gt; 0
    assert all('2.0' in m['name'] or '2.5' in m['name'] for m in filtered)
    assert all('inputTokenLimit' in m for m in filtered)
    
    # Print results for verification
    print(f"\nFound {len(filtered)} models for versions 2.0 and 2.5:")
    for model in sorted(filtered, key=lambda x: x['inputTokenLimit'], reverse=True):
        print(f"  {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Notes

1. **Token Limits**: Not all models return `inputTokenLimit`. Handle missing values gracefully.
2. **Model Names**: The API returns full names like "models/gemini-2.5-flash". We extract just the model part.
3. **Sorting**: Consider sorting results by token limit, name, or version for consistent output.
4. **Caching**: For production use, consider caching the model list as it doesn't change frequently.
&lt;/file&gt;
  &lt;file path="test_llmrunner.py"&gt;import pytest

from llmrunner import (
    llmrunner,
    code_review_models_to_mcp,
    ModelResult,
    LLMRunnerResults,
)


@pytest.fixture
def models_to_mcp():
    return code_review_models_to_mcp()


@pytest.mark.asyncio
async def test_llmrunner(models_to_mcp):
    prompt = "What is 2 + 2?"
    result = await llmrunner(prompt, models_to_mcp)

    assert isinstance(result, LLMRunnerResults)
    assert isinstance(result.successful_results, list)
    assert isinstance(result.failed_results, list)
    assert isinstance(result.total_models, int)
    assert isinstance(result.success_count, int)
    assert isinstance(result.failure_count, int)

    assert result.total_models == len(models_to_mcp.models_to_mcp)
    assert result.success_count + result.failure_count == result.total_models

    for success_result in result.successful_results:
        assert isinstance(success_result, ModelResult)
        assert success_result.success is True
        assert success_result.model is not None
        assert success_result.timestamp is not None
        assert success_result.response is not None
        assert success_result.duration_seconds is not None

    for failed_result in result.failed_results:
        assert isinstance(failed_result, ModelResult)
        assert failed_result.success is False
        assert failed_result.model is not None
        assert failed_result.timestamp is not None
        assert failed_result.error is not None

    print(f"Total models: {result.total_models}")
    print(f"Successful: {result.success_count}")
    print(f"Failed: {result.failure_count}")

    for failed_result in result.failed_results:
        print(
            f"Failed model: {
              failed_result.model} - Error: {failed_result.error}"
        )

    for success_result in result.successful_results:
        print(f"Successful model: {success_result.model}")
&lt;/file&gt;
  &lt;file path="GEMINI.md"&gt;# Gemini Code Assistant Context

This document provides context for the Gemini Code Assistant to understand the project structure, conventions, and important files.

## Project Overview

This project is a Python-based MCP (Model Context Protocol) server named "Collect". Its primary purpose is to fetch web content, process it, and facilitate multi-model AI analysis workflows. It provides a unified interface to interact with various AI models (OpenAI, Anthropic, Gemini, XAI) for tasks like code review. The server is built using the `mcp` library and exposes several tools for fetching URLs, converting HTML to markdown, counting tokens, and more. It also includes a database layer using SQLite for data persistence.

**Key Technologies:**

*   **Programming Language:** Python
*   **Framework:** `mcp` (Model Context Protocol)
*   **Key Libraries:** 
    *   `httpx` for asynchronous HTTP requests.
    *   `anthropic`, `openai`, `google-cloud-aiplatform` for interacting with various LLMs.
    *   `readabilipy`, `markdownify`, `beautifulsoup4` for HTML processing.
    *   `pyperclip` for clipboard integration.
    *   `yoyo-migrations` for database schema management.
*   **Package Manager:** `uv`
*   **Testing:** `pytest` with `pytest-asyncio` and `pytest-xdist` for parallel testing.
*   **Linting/Formatting:** `ruff` and `black`.
*   **Database:** SQLite.

## Building and Running

*   **Install Dependencies:** `uv sync`
*   **Run the Server:** `python collect.py`
*   **Run Tests:** `uv run pytest -v -s -n auto`
*   **Run Linter:** `ruff check .`
*   **Run Formatter:** `black .`
*   **Apply Database Migrations:** `uv run yoyo apply --config yoyo.ini --batch`

## Development Conventions

*   **Testing:** Tests are written using `pytest` and are located in files like `test_collect.py`. Asynchronous functions are tested using `@pytest.mark.asyncio`. The project uses `pytest-xdist` for parallel test execution.
*   **Linting and Formatting:** The project uses `ruff` for linting and `black` for formatting. These are run via the `Makefile`.
*   **Configuration:** Project configuration is managed in `config.py`, which loads environment variables from a `.env` file.
*   **Secrets Management:** API keys and other secrets are managed through Google Cloud Secret Manager, as indicated in `secret_manager.py` and `config.py`.
*   **Database:** The project uses SQLite for its database. The database connection logic is in `repository/database.py`. Migrations are handled by `yoyo-migrations`.

## Key Files

*   **`collect.py`:** The main entry point of the MCP server. It defines the available tools, such as `fetch_urls`, `run_code_review`, and `to_markdown`.
*   **`pyproject.toml`:** Defines the project's dependencies and development tool configurations.
*   **`Makefile`:** Provides convenient commands for common development tasks like testing, linting, and formatting.
*   **`config.py`:** Handles the project's configuration by loading environment variables from a `.env` file.
*   **`reviewer/code_review.py`:** Contains the logic for the code review functionality. It takes a diff file, sends it to multiple LLMs, and then formats and saves the results.
*   **`models/`:** This directory contains modules for interacting with different AI models (e.g., `anthropic_mpc.py`, `openai_mpc.py`).
*   **`repository/database.py`:** Contains the logic for connecting to the SQLite database.
*   **`migrations/`:** This directory contains the SQL migration files for the database schema.
&lt;/file&gt;
  &lt;file path="movetools"&gt;#!/bin/bash

# Enhanced setup script to copy tools to user bin directory with colorful output
# This script should be run from the collect project home directory

# Color definitions for enhanced output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly BOLD='\033[1m'
readonly NC='\033[0m' # No Color

# Set default target directory (configurable)
TARGET_DIR=${1:-~/bin}

# Expand tilde to home directory
TARGET_DIR="${TARGET_DIR/#\~/$HOME}"

# Get the directory where this script is located (project home)
PROJECT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;amp;&amp;amp; pwd )"
TOOLS_DIR="$PROJECT_DIR/tools"

# Enhanced printing functions
print_header() {
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${WHITE}  🔧 MOVETOOLS - Tool Installation Script${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
}

print_info() {
    echo -e "${CYAN}ℹ${NC}  $1"
}

print_success() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC}  $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_step() {
    echo -e "\n${BOLD}${PURPLE}▶${NC} ${BOLD}$1${NC}"
}

print_item() {
    echo -e "   ${GREEN}•${NC} $1"
}

# Function to display usage
usage() {
    print_header
    echo -e "${BOLD}USAGE:${NC}"
    echo -e "  $0 [target_directory]"
    echo ""
    echo -e "${BOLD}DESCRIPTION:${NC}"
    echo -e "  ${BOLD}MOVETOOLS${NC} is a comprehensive installation and backup script for the collect project."
    echo -e "  It performs two main functions:"
    echo ""
    echo -e "  ${BOLD}1. Tool Installation:${NC}"
    echo -e "     • Copies all executable tools from the tools/ directory to your bin directory"
    echo -e "     • Makes all copied tools executable with proper permissions"
    echo -e "     • Validates PATH configuration and provides setup guidance"
    echo -e "     • Provides colorful visual feedback throughout the process"
    echo ""
    echo -e "  ${BOLD}2. Dotfiles Backup:${NC}"
    echo -e "     • Backs up your .zshrc configuration to the project's dotfiles/ directory"
    echo -e "     • Copies your Ghostty terminal configuration (~/.config/ghostty)"
    echo -e "     • Backs up your Neovim init.lua configuration (~/.config/nvim/init.lua)"
    echo -e "     • Creates organized dotfiles structure for version control"
    echo ""
    echo -e "${BOLD}ARGUMENTS:${NC}"
    echo -e "  ${CYAN}target_directory${NC}    Optional. Directory to install tools (default: ~/bin)"
    echo ""
    echo -e "${BOLD}OPTIONS:${NC}"
    echo -e "  ${YELLOW}--llm${NC}              Display comprehensive usage information (LLM-friendly)"
    echo -e "  ${YELLOW}--help, -h${NC}         Display this usage information"
    echo ""
    echo -e "${BOLD}FEATURES:${NC}"
    echo -e "  • Enhanced colorful terminal output with status indicators"
    echo -e "  • Automatic directory creation with proper error handling"
    echo -e "  • Tool validation and permission management"
    echo -e "  • PATH verification with configuration suggestions"
    echo -e "  • Comprehensive dotfiles backup across multiple applications"
    echo -e "  • Installation summary with detailed feedback"
    echo ""
    echo -e "${BOLD}EXAMPLES:${NC}"
    echo -e "  ${GREEN}$0${NC}                 # Install tools to ~/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 ~/.local/bin${NC}    # Install tools to ~/.local/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 --llm${NC}           # Show comprehensive help for AI assistants"
    echo -e "  ${GREEN}$0 --help${NC}          # Show this help message"
    echo ""
    echo -e "${BOLD}DIRECTORY STRUCTURE:${NC}"
    echo -e "  ${CYAN}tools/${NC}              # Source directory containing executable tools"
    echo -e "  ${CYAN}dotfiles/zshrc${NC}      # Backed up shell configuration"
    echo -e "  ${CYAN}dotfiles/ghostty/${NC}   # Backed up terminal configuration"
    echo -e "  ${CYAN}dotfiles/nvim/${NC}      # Backed up Neovim configuration"
    echo ""
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    exit 0
}

# Check for help flags
if [ $# -eq 1 ] &amp;amp;&amp;amp; ([ "$1" = "--llm" ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]); then
    usage
fi

# Display header
print_header

# Display configuration
print_step "Configuration"
print_info "Project directory: ${BOLD}$PROJECT_DIR${NC}"
print_info "Tools directory: ${BOLD}$TOOLS_DIR${NC}"
print_info "Target directory: ${BOLD}$TARGET_DIR${NC}"

# Check if tools directory exists
print_step "Validation"
if [ ! -d "$TOOLS_DIR" ]; then
    print_error "Tools directory not found at $TOOLS_DIR"
    exit 1
fi
print_success "Tools directory found"

# Create target directory if it doesn't exist
if [ ! -d "$TARGET_DIR" ]; then
    print_info "Creating target directory..."
    mkdir -p "$TARGET_DIR"
    if [ $? -eq 0 ]; then
        print_success "Target directory created"
    else
        print_error "Failed to create target directory"
        exit 1
    fi
else
    print_success "Target directory exists"
fi

# Copy all files from tools directory
print_step "Copying Tools"
tool_count=0
copied_tools=()

for tool in "$TOOLS_DIR"/*; do
    if [ -f "$tool" ]; then
        tool_name=$(basename "$tool")
        # Skip CLAUDE.md file
        if [ "$tool_name" = "CLAUDE.md" ]; then
            continue
        fi
        if cp "$tool" "$TARGET_DIR/" 2&amp;gt;/dev/null; then
            print_item "Copied ${BOLD}$tool_name${NC}"
            copied_tools+=("$tool_name")
            ((tool_count++))
        else
            print_error "Failed to copy $tool_name"
        fi
    fi
done

if [ $tool_count -eq 0 ]; then
    print_warning "No tools found in $TOOLS_DIR"
    exit 0
fi

# Make all copied tools executable
print_step "Setting Permissions"
executable_count=0
for tool_name in "${copied_tools[@]}"; do
    tool_path="$TARGET_DIR/$tool_name"
    if chmod u+x "$tool_path" 2&amp;gt;/dev/null; then
        print_item "Made ${BOLD}$tool_name${NC} executable"
        ((executable_count++))
    else
        print_error "Failed to make $tool_name executable"
    fi
done

# Display completion summary
print_step "Summary"
print_success "Installation complete!"
print_info "${BOLD}$tool_count${NC} tools copied and ${BOLD}$executable_count${NC} made executable"
print_info "Tools are now available in: ${BOLD}$TARGET_DIR${NC}"

# List installed tools
if [ ${#copied_tools[@]} -gt 0 ]; then
    echo ""
    print_info "${BOLD}Installed tools:${NC}"
    for tool_name in "${copied_tools[@]}"; do
        echo -e "   ${GREEN}▸${NC} ${BOLD}$tool_name${NC}"
    done
fi

# Check if target directory is in PATH
echo ""
if [[ ":$PATH:" != *":$TARGET_DIR:"* ]]; then
    print_warning "Target directory is not in your PATH"
    echo -e "${YELLOW}💡${NC} To use these tools from anywhere, add this line to your ${BOLD}~/.bashrc${NC} or ${BOLD}~/.zshrc${NC}:"
    echo -e "   ${CYAN}export PATH=\"$TARGET_DIR:\$PATH\"${NC}"
else
    print_success "Target directory is already in your PATH"
fi

# Copy dotfiles section
print_step "Copying Dotfiles"
DOTFILES_DIR="$PROJECT_DIR/dotfiles"

# Ensure dotfiles directory exists
if [ ! -d "$DOTFILES_DIR" ]; then
    print_info "Creating dotfiles directory..."
    mkdir -p "$DOTFILES_DIR"
    if [ $? -eq 0 ]; then
        print_success "Dotfiles directory created"
    else
        print_error "Failed to create dotfiles directory"
    fi
else
    print_success "Dotfiles directory exists"
fi

# Copy .zshrc
if [ -f "$HOME/.zshrc" ]; then
    if cp "$HOME/.zshrc" "$DOTFILES_DIR/.zshrc" 2&amp;gt;/dev/null; then
        print_item "Copied ${BOLD}.zshrc${NC} from home directory"
    else
        print_error "Failed to copy .zshrc"
    fi
else
    print_warning ".zshrc not found in home directory"
fi

# Copy ghostty config
GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"
if [ -d "$GHOSTTY_CONFIG_DIR" ]; then
    # Create ghostty subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/ghostty"
    if cp -r "$GHOSTTY_CONFIG_DIR"/* "$DOTFILES_DIR/ghostty/" 2&amp;gt;/dev/null; then
        print_item "Copied ${BOLD}ghostty config${NC} from ~/.config/ghostty"
    else
        print_error "Failed to copy ghostty config"
    fi
else
    print_warning "Ghostty config directory not found at ~/.config/ghostty"
fi

# Copy nvim init.lua
NVIM_CONFIG="$HOME/.config/nvim/init.lua"
if [ -f "$NVIM_CONFIG" ]; then
    # Create nvim subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/nvim"
    if cp "$NVIM_CONFIG" "$DOTFILES_DIR/nvim/init.lua" 2&amp;gt;/dev/null; then
        print_item "Copied ${BOLD}nvim init.lua${NC} from ~/.config/nvim"
    else
        print_error "Failed to copy nvim init.lua"
    fi
else
    print_warning "Nvim init.lua not found at ~/.config/nvim/init.lua"
fi

echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}${GREEN}🎉 Setup completed successfully!${NC}"
echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
&lt;/file&gt;
  &lt;file path="source.xml"&gt;&amp;lt;?xml version='1.0' encoding='utf-8'?&amp;gt;
&amp;lt;source_code project="collect"&amp;gt;
  &amp;lt;file path="test_generate_prompt.py"&amp;gt;import pytest
from config import Config
from collect import generate_prompt


@pytest.fixture
def sample_prompt():
    """Sample prompt content for testing."""
    return """Create a helpful AI assistant that can answer programming questions.
The assistant should be knowledgeable about Python, JavaScript, and web development.
It should provide clear explanations and code examples when appropriate."""


class TestGeneratePrompt:

    @pytest.mark.asyncio
    async def test_generate_prompt_basic(self, sample_prompt):
        """Test basic functionality of generate_prompt."""
        # Check if we have required config
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(sample_prompt)

        # Verify we got a string response
        assert isinstance(result, str)
        assert len(result) &amp;amp;gt; 0

        # The generated prompt should contain relevant content
        # Note: We can't predict exact content, but it should be substantial
        assert len(result) &amp;amp;gt; 50  # Should be more than just a few words

    @pytest.mark.asyncio
    async def test_generate_prompt_with_target_model(self, sample_prompt):
        """Test generate_prompt with target_model parameter."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(
            sample_prompt, target_model="claude-3-7-sonnet-20250219"
        )

        assert isinstance(result, str)
        assert len(result) &amp;amp;gt; 0

    @pytest.mark.asyncio
    async def test_generate_prompt_empty_string(self):
        """Test error handling for empty prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("")

    @pytest.mark.asyncio
    async def test_generate_prompt_whitespace_only(self):
        """Test error handling for whitespace-only prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("   \n\t   ")

    @pytest.mark.asyncio
    async def test_generate_prompt_simple_task(self):
        """Test with a simple task description."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        simple_task = "A coding assistant that helps with Python"
        result = await generate_prompt(simple_task)

        assert isinstance(result, str)
        assert len(result) &amp;amp;gt; len(simple_task)  # Should be expanded


if __name__ == "__main__":
    # Run a simple test

    async def manual_test():
        """Manual test function for quick verification."""
        test_prompt = "Create a Python function that validates email addresses."

        try:
            result = await generate_prompt(test_prompt)
            print(f"Input prompt: {test_prompt}")
            print(f"Generated prompt ({len(result)} chars):")
            print("-" * 50)
            print(result)
            print("-" * 50)
        except Exception as e:
            print(f"Error: {e}")

    # Uncomment to run manual test
    # asyncio.run(manual_test())
&amp;lt;/file&amp;gt;
  &amp;lt;file path="collect.py"&amp;gt;from typing import List
from mcp.server.fastmcp import FastMCP, Context
import tiktoken
import markdownify
import readabilipy.simple_json
from html_to_markdown import convert_to_markdown
from bs4 import BeautifulSoup
from secret_manager import SecretManager
from config import Config
from models.anthropic_mpc import AnthropicMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP
from models.gemini_mcp import GeminiMCP
from fetcher import Fetcher
import pyperclip
from reviewer.code_review import CodeReviewer
import subprocess
import atexit
import time

mcp = FastMCP("Collect")


@mcp.tool()
async def run_code_review(from_file: str, to_file: str = "codereview"):
    """
    Run code review on a diff file using multiple LLM models.

    Args:
        from_file: Path to the file containing the diff/code to review
        to_file: Directory name to write results to (default: "codereview")

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_code(from_file, to_file)


@mcp.tool()
async def run_git_diff_review(to_file: str = "codereview", staged_only: bool = True):
    """
    Run code review on git diff output.

    Args:
        to_file: Directory name to write results to(default: "codereview")
        staged_only: If True, review only staged changes;
        if False, review all changes

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_diff_from_git(to_file, staged_only)


@mcp.tool()
async def fetch_urls(urls: List[str], ctx: Context = None) -&amp;amp;gt; str:
    """
    Fetch content from multiple URLs concurrently and merge the responses.

    Use this tool when you need to:
    - Retrieve content from multiple web pages at once
    - Compare information across multiple sources
    - Gather data from several API endpoints simultaneously
    - Fetch related pages in parallel for efficiency

    Args:
        urls: List of URLs to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Merged content from all URLs as a single string

    Example:
        fetch_urls(["https://api.example.com/users",
                   "https://api.example.com/posts"])
    """
    fetcher = Fetcher(ctx)
    merged_responses = await fetcher.fetch_urls(urls)
    return merged_responses


@mcp.tool()
async def fetch_url(url: str, ctx: Context = None) -&amp;amp;gt; str:
    """
    Fetch raw content from a single URL.

    Use this tool when you need to:
    - Retrieve raw HTML/JSON from a web page or API
    - Get unprocessed content for custom parsing
    - Access web resources programmatically
    - Fetch data before converting to markdown

    Args:
        url: The URL to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Raw content from the URL(HTML, JSON, or plain text)

    Note: For documentation extraction, consider using get_docs instead.
          For markdown conversion, use to_markdown on the result.
    """
    fetcher = Fetcher(ctx)
    return fetcher.get(url)


@mcp.tool()
async def get_docs(url: str, extract_value: str = None, ctx: Context = None) -&amp;amp;gt; str:
    """
    Fetch and extract specific documentation content from web pages.

    Use this tool when users need to:
    - Extract specific sections from documentation websites
    - Get targeted information from technical docs
    - Retrieve API documentation for specific methods/classes
    - Pull configuration examples from documentation
    - Find specific topics within large documentation sites

    Args:
        url: The URL of the documentation page to fetch
        extract_value: Optional. Specific section/topic to extract(e.g., "authentication",
                      "API endpoints", "installation guide"). If not provided, returns
                      the entire page content.
        ctx: MCP context(automatically provided)

    Returns:
        Extracted documentation content as markdown. If extract_value is specified,
        uses Gemini AI to intelligently extract only the relevant section.

    Examples:
        - get_docs("https://docs.python.org/3/", "datetime module")
        - get_docs("https://fastapi.tiangolo.com/", "dependency injection")
        - get_docs("https://react.dev/", "useEffect hook")
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-flash-preview-05-20"
    gemini = GeminiMCP(config, secret_mgr, model=model)

    if extract_value is None:
        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        return response
    else:
        prompt_prefatory = f"""
        # Documentation Extraction Task

        Extract and format the documentation for: **{extract_value} **

        # Instructions:
        - Focus specifically on the requested section/topic
        - Include code examples, parameters, and usage details if present
        - Maintain original formatting and structure
        - If the exact section isn't found, extract the most relevant related content
        - Return only the extracted documentation content, no meta-commentary

        ## Content to extract: {extract_value}
        """

        prompt = prompt_prefatory + "\n\n"
        response = await gemini.build_prompt_from_url(url, prompt, ctx)
        return response.strip()


@mcp.tool()
async def copy_clipboard(text: str) -&amp;amp;gt; str:
    """
    Copy text to the system clipboard.

    Use this tool when users need to:
    - Copy generated code snippets to clipboard
    - Save formatted text for pasting elsewhere
    - Copy API keys, URLs, or configuration values
    - Transfer content between applications

    Args:
        text: The text content to copy to clipboard

    Note: The text will replace any existing clipboard content.
    """
    pyperclip.copy(text)


@mcp.tool()
def strip_html(html: str) -&amp;amp;gt; str:
    """
    Remove all HTML tags and return plain text content.

    Use this tool when you need to:
    - Extract plain text from HTML pages
    - Remove formatting and tags from web content
    - Clean HTML for text analysis
    - Prepare content for non-HTML processing

    Args:
        html: Raw HTML string to process

    Returns:
        Plain text with all HTML tags removed

    Note: This removes ALL formatting. For readable formatting, use to_markdown instead.
    """
    soup = BeautifulSoup(html, "lxml")
    return soup.get_text()


@mcp.tool()
def to_markdown(html: str) -&amp;amp;gt; str:
    """Extract and convert HTML content to markdown using markdownify
    and readabilipy

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown

    """
    html_to_json = readabilipy.simple_json.simple_json_from_html_string(
        html,
        use_readability=True,
    )
    if not html_to_json["content"]:
        return "&amp;amp;lt;error&amp;amp;gt;Page failed to be simplified from HTML to json&amp;amp;lt;/error&amp;amp;gt;"

    return markdownify.markdownify(
        html_to_json["content"],
        heading_style=markdownify.ATX,
    )


def html_to_markdown(html: str) -&amp;amp;gt; str:
    """This uses html-to-markdown library instead of markdownify

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown as a str
    """

    return convert_to_markdown(
        html,
        heading_style="atx",
    )


@mcp.tool()
async def get_anthropic_model_list() -&amp;amp;gt; List[str]:
    """
    Get the list of available Anthropic Claude models.

    Use this tool when you need to:
    - Check which Claude models are available
    - Verify model names before using them
    - List Anthropic's current model offerings
    - Help users choose between Claude models

    Returns:
        List of available Anthropic model names (e.g., ["claude-3-opus", "claude-3-sonnet"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.get_model_list()


@mcp.tool()
async def get_openai_model_list() -&amp;amp;gt; List[str]:
    """
    Get the list of available OpenAI models.

    Use this tool when you need to:
    - Check which GPT models are available
    - Verify OpenAI model names
    - List current OpenAI offerings
    - Help users choose between GPT models

    Returns:
        List of available OpenAI model names (e.g., ["gpt-4", "gpt-3.5-turbo"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    openai_mcp = OpenAIMCP(config, secret_mgr, model="gpt-4o")
    return openai_mcp.get_model_list()


@mcp.tool()
async def get_xai_model_list() -&amp;amp;gt; List[str]:
    """
    Get the list of available XAI (Grok) models.

    Use this tool when you need to:
    - Check which Grok models are available
    - Verify XAI model names
    - List current Grok offerings
    - Help users choose between Grok models

    Returns:
        List of available XAI model names (e.g., ["grok-3", "grok-3-mini"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-mini-fast-latest")
    return xai_mcp.get_model_list()


@mcp.tool()
async def get_gemini_model_list() -&amp;amp;gt; List[dict]:
    """
    Get the list of available Google Gemini models
    (filtered for 2.0 and 2.5 versions).

    Use this tool when you need to:
    - Check which Gemini models are available with their token limits
    - Verify Google AI model names and capabilities
    - List current Gemini 2.0 and 2.5 offerings
    - Help users choose between Gemini models based on token capacity

    Returns:
        List of model dictionaries sorted by token limit (highest first),
        each containing:
        - model_name: The model identifier (e.g., "gemini-2.5-flash")
        - token_window: Input token limit (e.g., 1048576)

    Example return:
        [
            {"model_name": "gemini-2.5-flash", "token_window": 1048576},
            {"model_name": "gemini-2.0-flash", "token_window": 1048576},
            {"model_name": "gemini-2.5-pro", "token_window": 1048576}
        ]
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.5-flash")
    return gemini_mcp.get_model_list()


@mcp.tool()
async def count_openai_tokens(text: str, model: str = "gpt-4") -&amp;amp;gt; int:
    """
    Count tokens in text using OpenAI's tiktoken tokenizer.

    Use this tool when you need to:
    - Check if content fits within OpenAI model limits
    - Estimate API costs for OpenAI models
    - Split content to fit token windows
    - Optimize prompts for token efficiency

    Args:
        text: The text to count tokens for
        model: OpenAI model name (default: "gpt-4")

    Returns:
        Number of tokens in the text for the specified model
    """
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@mcp.tool()
async def count_anthropic_tokens(text: str) -&amp;amp;gt; int:
    """
    Count tokens in text using Anthropic's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Claude model limits
    - Estimate API costs for Anthropic models
    - Split content for Claude's context window
    - Optimize prompts for Claude

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Anthropic models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.count_tokens(text)


@mcp.tool()
async def count_gemini_tokens(text: str) -&amp;amp;gt; int:
    """
    Count tokens in text using Google Gemini's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Gemini model limits
    - Estimate API costs for Google AI models
    - Split content for Gemini's context window
    - Optimize prompts for Gemini

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Gemini models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.0-flash")
    return gemini_mcp.count_tokens(text)


@mcp.tool()
async def count_grok_tokens(text: str) -&amp;amp;gt; int:
    """
    Count tokens in text using XAI Grok's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Grok model limits
    - Estimate API costs for XAI models
    - Split content for Grok's context window
    - Optimize prompts for Grok

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Grok models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-fast-latest")
    return xai_mcp.count_tokens(text)


@mcp.tool()
async def generate_prompt(prompt: str, target_model: str = None) -&amp;amp;gt; str:
    """
    Generate an optimized AI prompt using Anthropic's experimental prompt engineering API.

    This tool leverages Anthropic's closed research preview API to automatically create
    high-quality, structured prompts from simple task descriptions. The API analyzes
    your input and generates professional-grade prompts optimized for Claude models.

    Use this tool when you need to:
    - Transform simple ideas into comprehensive AI prompts
    - Create structured prompts for specific tasks or roles
    - Optimize prompts for better AI responses
    - Generate consistent prompt templates for repeated use
    - Improve prompt clarity and effectiveness

    Args:
        prompt: A brief description of what you want the AI to do.
                Can be as simple as a role description or task summary.
                Examples:
                - "a helpful programming assistant"
                - "a chef for meal planning"
                - "a technical documentation writer"
                - "analyze code for security vulnerabilities"
        target_model: Optional. The specific model to optimize for (e.g., "claude-3-opus").
                     If not specified, generates a general-purpose prompt.

    Returns:
        A professionally crafted prompt ready for use with Claude or other AI models.
        The generated prompt includes appropriate context, instructions, and structure
        to maximize response quality.

    Raises:
        ValueError: If the prompt is empty or only contains whitespace
        RuntimeError: If the API call fails or returns an unexpected response

    Example:
        &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; result = await generate_prompt("a Python code reviewer")
        &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; print(result)
        "You are an expert Python code reviewer with deep knowledge..."

    Note:
        This uses Anthropic's experimental "prompt-tools" API which requires special
        access. The API is in closed research preview and may change without notice.
    """
    try:
        # Validate input
        task_content = prompt.strip()
        if not task_content:
            raise ValueError("Prompt cannot be empty")

        # Set up Anthropic MCP client
        config = Config()
        secret_mgr = SecretManager(config.project_id)
        anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)

        # Call generate_prompt API with new signature
        response = anthropic_mcp.generate_prompt(task_content, target_model)

        # Extract the generated prompt text from the response
        if response.messages and response.messages[0].content:
            return response.messages[0].content[0].text
        else:
            raise ValueError("No prompt generated in response")

    except ValueError:
        # Re-raise ValueError (like empty prompt) without wrapping
        raise
    except Exception as e:
        raise RuntimeError(f"Error generating prompt: {str(e)}")


def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["uv", "run", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for API to initialize
        time.sleep(2)

        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")

            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()

            atexit.register(cleanup_api)

    except Exception as e:
        print(f"Failed to start API server: {e}")

    # Continue with MCP server startup
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
&amp;lt;/file&amp;gt;
  &amp;lt;file path="config.py"&amp;gt;from dotenv import load_dotenv
import os


class Config:
    # Load .env for configuration
    load_dotenv(".env")

    def __init__(self) -&amp;amp;gt; None:
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.port = os.getenv("PORT")
        self.db_path = os.getenv("DB_PATH")
        self.anthropic_key_path = os.getenv("ANTHROPIC_API_KEY_PATH")
        self.anthropic_model_opus = os.getenv("ANTHROPIC_MODEL_OPUS")
        self.anthropic_model_sonnet = os.getenv("ANTHROPIC_MODEL_SONNET")
        self.gemini_api_key_path = os.getenv("GEMINI_API_KEY_PATH")
        self.gemini_base_url = os.getenv("GEMINI_BASE_URL")
        self.xai_api_key_path = os.getenv("XAI_API_KEY_PATH")
        self.grok_system_prompt = os.getenv("GROK_SYSTEM_PROMPT")
        self.openai_api_key_path = os.getenv("OPENAI_API_KEY_PATH")
        self.openai_default_code_review_model = os.getenv(
            "OPENAI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.gemini_default_code_review_model = os.getenv(
            "GEMINI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.anthropic_default_code_review_model = os.getenv(
            "ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.xai_default_code_review_model = os.getenv("XAI_DEFAULT_CODE_REVIEW_MODEL")

        # GitHub configuration
        self.github_url = os.getenv("GITHUB_URL")

        # Command subdirectories - read as comma-separated string
        command_subdirs_str = os.getenv(
            "COMMAND_SUBDIRS", "archive,go,js,mcp,python,tools"
        )
        self.command_subdirs = [
            subdir.strip() for subdir in command_subdirs_str.split(",")
        ]
&amp;lt;/file&amp;gt;
  &amp;lt;file path="test_collect.py"&amp;gt;import pytest

from collect import (
    count_anthropic_tokens,
    count_gemini_tokens,
    count_openai_tokens,
    count_grok_tokens,
    get_docs,
)

# The @pytest.mark.parametrize decorator runs the test function
#  test_empty_text_returns_zero three separate times, once for each
#  function in the list. Each time, it passes a different token-counting
#  function to the test as the func parameter.

#  This is useful for testing similar functionality across multiple
#  implementations without duplicating test code. In this case, it verifies
#   that all three token-counting functions return zero when given empty
#  text.


@pytest.mark.asyncio
async def test_openai_hello_token_count():
    result = await count_openai_tokens("hello", model="gpt-3.5-turbo")
    assert result == 1


@pytest.mark.parametrize(
    "func,text",
    [
        (count_openai_tokens, "Hello, world!"),
        (count_gemini_tokens, "Hello, Gemini!"),
        (count_anthropic_tokens, "Hello Claude"),
        (count_grok_tokens, "Hello Grok"),
    ],
)
@pytest.mark.asyncio
async def test_nonempty_text_returns_positive_int(func, text):
    n = await func(text)
    assert isinstance(n, int)
    assert n &amp;amp;gt; 0


@pytest.mark.asyncio
async def test_get_docs_with_extract_value():
    url = "https://docs.python.org/3/library/json.html"
    extract_value = "json.dumps"

    result = await get_docs(url, extract_value)

    assert isinstance(result, str)
    assert len(result) &amp;amp;gt; 0
    assert "json.dumps" in result.lower()

    print(f"Extracted docs for {extract_value}:")
    print(result[:500] + "..." if len(result) &amp;amp;gt; 500 else result)


@pytest.mark.asyncio
async def test_get_docs_without_extract_value():
    url = "https://docs.python.org/3/library/json.html"

    result = await get_docs(url)

    assert isinstance(result, str)
    assert len(result) &amp;amp;gt; 0
    # Should contain raw HTML content when no extraction is performed
    assert "html" in result.lower() or "json" in result.lower()

    print(f"Raw content length: {len(result)}")
    print(result[:200] + "..." if len(result) &amp;amp;gt; 200 else result)
&amp;lt;/file&amp;gt;
  &amp;lt;file path="requirements.txt"&amp;gt;aiohttp==3.11.11
annotated-types==0.7.0
anthropic==0.50.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.8
distro==1.9.0
docstring-parser==0.16
-e file:///Users/benjaminmetz/python/collect
google-api-core==2.24.2
google-auth==2.39.0
google-cloud-aiplatform==1.91.0
google-cloud-bigquery==3.31.0
google-cloud-core==2.4.3
google-cloud-resource-manager==1.14.2
google-cloud-storage==2.19.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
idna==3.10
iniconfig==2.1.0
jiter==0.9.0
markdown-it-py==3.0.0
mcp==1.7.1
mdurl==0.1.2
numpy==2.2.5
packaging==25.0
pluggy==1.5.0
proto-plus==1.26.1
protobuf==5.29.4
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.11.4
pydantic-core==2.33.2
pydantic-settings==2.9.1
pygments==2.19.1
pyperclip==1.9.0
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
regex==2024.11.6
requests==2.32.3
rich==14.0.0
rsa==4.9.1
sentencepiece==0.2.0
shapely==2.1.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
sse-starlette==2.3.3
starlette==0.46.2
tiktoken==0.9.0
typer==0.15.3
typing-extensions==4.13.2
typing-inspection==0.4.0
urllib3==2.4.0
uvicorn==0.34.2
&amp;lt;/file&amp;gt;
  &amp;lt;file path="uv.lock"&amp;gt;version = 1
revision = 1
requires-python = "&amp;amp;gt;=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.12.11"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/6b/850a842871ab7be0d00686750d0ee9d8fb8e7be981e4e5700bb6c88f1b8f/aiohttp-3.12.11.tar.gz", hash = "sha256:a5149ae1b11ce4cf8b122846bfa3d7c5f29fe3bfe6745ab21b3eea9615bc5564", size = 7814403 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/ac/15e21c6a17b5183d1617505b125c773f554a56e06be577a289151a8e5ce7/aiohttp-3.12.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:5fadc4b67f972a701805aa501cd9d22cdbeda21f9c9ae85e60678f84b1727a16", size = 694170 },
    { url = "https://files.pythonhosted.org/packages/02/5b/347f8aff5793829b3a31a927bd039ec4f22221a32c459b9d19fe880921e3/aiohttp-3.12.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:144d67c29ae36f052584fc45a363e92798441a5af5762d83037aade3e2aa9dc5", size = 471832 },
    { url = "https://files.pythonhosted.org/packages/4b/e5/9ed82f5b6a2dca30940e90820ce2f8203e15111de464bba0980e2c7e169b/aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6b73299e4bf37d14c6e4ca5ce7087b44914a8d9e1f40faedc271f28d64ec277e", size = 464133 },
    { url = "https://files.pythonhosted.org/packages/3c/8d/edcddc41d4f1157a2536143476070ae66de2b839af3724655c2a6358670a/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1226325e98e6d3cdfdaca639efdc3af8e82cd17287ae393626d1bd60626b0e93", size = 1702942 },
    { url = "https://files.pythonhosted.org/packages/b1/2e/efcb6a35d0646ced659edc3172e8e9384246d2cd0b0f3080fc3c441cb511/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0ecae011f2f779271407f2959877230670de3c48f67e5db9fbafa9fddbfa3a", size = 1684207 },
    { url = "https://files.pythonhosted.org/packages/56/f7/0324c499b7c610633d2f5e8af5457fd3a0584f5f4827bc46b673866596ac/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a8a711883eedcd55f2e1ba218d8224b9f20f1dfac90ffca28e78daf891667e3a", size = 1736275 },
    { url = "https://files.pythonhosted.org/packages/98/0f/b7aa0fd1ed777b5d6fb62c0dcf82effb717e8b51c802067fc3bcb703e003/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2601c1fcd9b67e632548cfd3c760741b31490502f6f3e5e21287678c1c6fa1b2", size = 1785648 },
    { url = "https://files.pythonhosted.org/packages/2c/2a/7defcf31010a2964bf17f6c9d9190e3be889f0c5edc3ff2cdac6e60764d7/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d5b11ea794ee54b33d0d817a1aec0ef0dd2026f070b493bc5a67b7e413b95d4", size = 1707981 },
    { url = "https://files.pythonhosted.org/packages/b6/9e/ff3d9a01f533752e81fd92bfe1301ae5a7bd5a306d752ad54f8bc61570fa/aiohttp-3.12.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:109b3544138ce8a5aca598d5e7ff958699e3e19ee3675d27d5ee9c2e30765a4a", size = 1621683 },
    { url = "https://files.pythonhosted.org/packages/2c/98/446c96927f2e7d2eaea95660a60eb6077771d00df834430cec002cadd96b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b795085d063d24c6d09300c85ddd6b9c49816d5c498b40b6899ca24584e936e4", size = 1674706 },
    { url = "https://files.pythonhosted.org/packages/e1/2a/038cb4af5e58994bc9315d0cb6a906d20ddfffb8eb3d0dfcfe8fe95b1939/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ebcbc113f40e4c9c0f8d2b6b31a2dd2a9768f3fa5f623b7e1285684e24f5159f", size = 1706372 },
    { url = "https://files.pythonhosted.org/packages/28/18/dc16cc7cb9b8baf9308f23ecf1e787d916238d01060bea272d5c29e9aa6b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:590e5d792150d75fa34029d0555b126e65ad50d66818a996303de4af52b65b32", size = 1648967 },
    { url = "https://files.pythonhosted.org/packages/44/f5/f427ef971e00088c7f0f5a4a7e405732e0ce0b87dfc3eec0f1a8c16863d2/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:9c2a4dec596437b02f0c34f92ea799d6e300184a0304c1e54e462af52abeb0a8", size = 1725099 },
    { url = "https://files.pythonhosted.org/packages/d4/0a/34fc018d4e193115b512bc08f6afaf79c23609a6487e47f0d593d1d9df41/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aace119abc495cc4ced8745e3faceb0c22e8202c60b55217405c5f389b569576", size = 1758571 },
    { url = "https://files.pythonhosted.org/packages/b6/69/b466ec346506384a93bcb864ab75a21b6520c64fcc3720ab2056470a657f/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd749731390520a2dc1ce215bcf0ee1018c3e2e3cd834f966a02c0e71ad7d637", size = 1707461 },
    { url = "https://files.pythonhosted.org/packages/f4/fc/3437d3e40581bc7d0816e134fdcae3c7e5c3f21dbdcfbd54402af3973b1c/aiohttp-3.12.11-cp313-cp313-win32.whl", hash = "sha256:65952736356d1fbc9efdd17492dce36e2501f609a14ccb298156e392d3ad8b83", size = 420053 },
    { url = "https://files.pythonhosted.org/packages/6c/cf/cd84df67147c986315c63fef29a6ecadf03bf5528340b8c82eedd988cf57/aiohttp-3.12.11-cp313-cp313-win_amd64.whl", hash = "sha256:854132093e12dd77f5c07975581c42ae51a6a8868dcbbb509c77d1963c3713b7", size = 445988 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.50.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/40/85/4dd9f80da0727c56d7e7f7c627cb724edd9e6df062df6ecc0e90f06e6dbb/anthropic-0.50.0.tar.gz", hash = "sha256:42175ec04ce4ff2fa37cd436710206aadff546ee99d70d974699f59b49adc66f", size = 213021 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/ae/975f97ad5581a9e187a3717e21d79d6c7ad6be926fee9aa8a15b3d9f8f37/anthropic-0.50.0-py3-none-any.whl", hash = "sha256:defbd79327ca2fa61fd7b9eb2f1627dfb1f69c25d49288c52e167ddb84574f80", size = 245291 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "asttokens"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4a/e7/82da0a03e7ba5141f05cce0d302e6eed121ae055e0456ca228bf693984bc/asttokens-3.0.0.tar.gz", hash = "sha256:0dcd8baa8d62b0c1d118b399b2ddba3c4aff271d0d7a9e0d4c1681c79035bbc7", size = 61978 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/8a/c46dcc25341b5bce5472c718902eb3d38600a903b14fa6aeecef3f21a46f/asttokens-3.0.0-py3-none-any.whl", hash = "sha256:e3078351a059199dd5138cb1c706e6430c05eff2ff136af5eb4790f9d28932e2", size = 26918 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285 },
]

[[package]]
name = "black"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "mypy-extensions" },
    { name = "packaging" },
    { name = "pathspec" },
    { name = "platformdirs" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/49/26a7b0f3f35da4b5a65f081943b7bcd22d7002f5f0fb8098ec1ff21cb6ef/black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666", size = 649449 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/87/0edf98916640efa5d0696e1abb0a8357b52e69e82322628f25bf14d263d1/black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f", size = 1650673 },
    { url = "https://files.pythonhosted.org/packages/52/e5/f7bf17207cf87fa6e9b676576749c6b6ed0d70f179a3d812c997870291c3/black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3", size = 1453190 },
    { url = "https://files.pythonhosted.org/packages/e3/ee/adda3d46d4a9120772fae6de454c8495603c37c4c3b9c60f25b1ab6401fe/black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171", size = 1782926 },
    { url = "https://files.pythonhosted.org/packages/cc/64/94eb5f45dcb997d2082f097a3944cfc7fe87e071907f677e80788a2d7b7a/black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18", size = 1442613 },
    { url = "https://files.pythonhosted.org/packages/09/71/54e999902aed72baf26bca0d50781b01838251a462612966e9fc4891eadd/black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717", size = 207646 },
]

[[package]]
name = "cachetools"
version = "5.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/81/3747dad6b14fa2cf53fcf10548cf5aea6913e96fab41a3c198676f8948a5/cachetools-5.5.2.tar.gz", hash = "sha256:1a661caa9175d26759571b2e19580f9d6393969e5dfca11fdb1f947a23e640d4", size = 28380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl", hash = "sha256:d26a22bcc62eb95c3beabd9f1ee5e820d3d2704fe2967cbe350e20c8ffcd3f0a", size = 10080 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "collect"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "anthropic" },
    { name = "beautifulsoup4" },
    { name = "black" },
    { name = "fastapi" },
    { name = "google-ai-generativelanguage" },
    { name = "google-api-python-client" },
    { name = "google-auth-httplib2" },
    { name = "google-cloud-aiplatform", extra = ["tokenization"] },
    { name = "google-cloud-secret-manager" },
    { name = "google-genai" },
    { name = "google-generativeai" },
    { name = "html-to-markdown" },
    { name = "html5lib" },
    { name = "httplib2" },
    { name = "httpx" },
    { name = "ipython" },
    { name = "lxml" },
    { name = "marimo" },
    { name = "markdownify" },
    { name = "mcp", extra = ["cli"] },
    { name = "openai" },
    { name = "pathspec" },
    { name = "pyperclip" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
    { name = "pytest-xdist" },
    { name = "python-json-logger" },
    { name = "readabilipy" },
    { name = "rich" },
    { name = "ruff" },
    { name = "tiktoken" },
    { name = "uvicorn" },
    { name = "yoyo-migrations" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = "&amp;amp;gt;=3.12.11" },
    { name = "anthropic", specifier = "&amp;amp;gt;=0.50.0" },
    { name = "beautifulsoup4", specifier = "&amp;amp;gt;=4.13.4" },
    { name = "black", specifier = "&amp;amp;gt;=25.1.0" },
    { name = "fastapi", specifier = "&amp;amp;gt;=0.116.1" },
    { name = "google-ai-generativelanguage", specifier = "&amp;amp;gt;=0.6.15" },
    { name = "google-api-python-client", specifier = "&amp;amp;gt;=2.169.0" },
    { name = "google-auth-httplib2", specifier = "&amp;amp;gt;=0.2.0" },
    { name = "google-cloud-aiplatform", extras = ["tokenization"], specifier = "&amp;amp;gt;=1.91.0" },
    { name = "google-cloud-secret-manager", specifier = "&amp;amp;gt;=2.23.3" },
    { name = "google-genai", specifier = "&amp;amp;gt;=1.13.0" },
    { name = "google-generativeai", specifier = "&amp;amp;gt;=0.8.5" },
    { name = "html-to-markdown", specifier = "&amp;amp;gt;=1.3.2" },
    { name = "html5lib", specifier = "&amp;amp;gt;=1.1" },
    { name = "httplib2", specifier = "&amp;amp;gt;=0.22.0" },
    { name = "httpx", specifier = "&amp;amp;gt;=0.28.1" },
    { name = "ipython", specifier = "&amp;amp;gt;=9.4.0" },
    { name = "lxml", specifier = "&amp;amp;gt;=5.4.0" },
    { name = "marimo", specifier = "&amp;amp;gt;=0.14.12" },
    { name = "markdownify", specifier = "&amp;amp;gt;=1.1.0" },
    { name = "mcp", extras = ["cli"], specifier = "&amp;amp;gt;=1.7.1" },
    { name = "openai", specifier = "&amp;amp;gt;=1.59.4" },
    { name = "pathspec", specifier = "&amp;amp;gt;=0.12.1" },
    { name = "pyperclip", specifier = "&amp;amp;gt;=1.9.0" },
    { name = "pytest", specifier = "&amp;amp;gt;=8.3.5" },
    { name = "pytest-asyncio", specifier = "&amp;amp;gt;=0.26.0" },
    { name = "pytest-xdist", specifier = "&amp;amp;gt;=3.6.1" },
    { name = "python-json-logger", specifier = "&amp;amp;gt;=3.3.0" },
    { name = "readabilipy", specifier = "&amp;amp;gt;=0.3.0" },
    { name = "rich", specifier = "&amp;amp;gt;=14.0.0" },
    { name = "ruff", specifier = "&amp;amp;gt;=0.11.9" },
    { name = "tiktoken", specifier = "&amp;amp;gt;=0.9.0" },
    { name = "uvicorn", specifier = "&amp;amp;gt;=0.34.2" },
    { name = "yoyo-migrations", specifier = "&amp;amp;gt;=9.0.0" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "decorator"
version = "5.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/fa/6d96a0978d19e17b68d634497769987b16c8f4cd0a7a05048bec693caa6b/decorator-5.2.1.tar.gz", hash = "sha256:65f266143752f734b0a7cc83c46f4618af75b8c5911b00ccb61d0ac9b6da0360", size = 56711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl", hash = "sha256:d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a", size = 9190 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "docstring-parser"
version = "0.16"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/08/12/9c22a58c0b1e29271051222d8906257616da84135af9ed167c9e28f85cb3/docstring_parser-0.16.tar.gz", hash = "sha256:538beabd0af1e2db0146b6bd3caa526c35a34d61af9fd2887f3a8a27a739aa6e", size = 26565 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl", hash = "sha256:bf0a1387354d3691d102edef7ec124f219ef639982d096e26e3b60aeffa90637", size = 36533 },
]

[[package]]
name = "docutils"
version = "0.21.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ae/ed/aefcc8cd0ba62a0560c3c18c33925362d46c6075480bfa4df87b28e169a9/docutils-0.21.2.tar.gz", hash = "sha256:3a6b18732edf182daa3cd12775bbb338cf5691468f91eeeb109deff6ebfa986f", size = 2204444 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/d7/9322c609343d929e75e7e5e6255e614fcc67572cfd083959cdef3b7aad79/docutils-0.21.2-py3-none-any.whl", hash = "sha256:dafca5b9e384f0e419294eb4d2ff9fa826435bf15f15b7bd45723e8ad76811b2", size = 587408 },
]

[[package]]
name = "execnet"
version = "2.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/ff/b4c0dc78fbe20c3e59c0c7334de0c27eb4001a2b2017999af398bf730817/execnet-2.1.1.tar.gz", hash = "sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3", size = 166524 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl", hash = "sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc", size = 40612 },
]

[[package]]
name = "executing"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/50/a9d80c47ff289c611ff12e63f7c5d13942c65d68125160cefd768c73e6e4/executing-2.2.0.tar.gz", hash = "sha256:5d108c028108fe2551d1a7b2e8b713341e2cb4fc0aa7dcf966fa4327a5226755", size = 978693 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7b/8f/c4d9bafc34ad7ad5d8dc16dd1347ee0e507a52c3adb6bfa8887e1c6a26ba/executing-2.2.0-py2.py3-none-any.whl", hash = "sha256:11387150cad388d62750327a53d3339fad4888b39a6fe233c3afbb54ecffd3aa", size = 26702 },
]

[[package]]
name = "fastapi"
version = "0.116.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/78/d7/6c8b3bfe33eeffa208183ec037fee0cce9f7f024089ab1c5d12ef04bd27c/fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143", size = 296485 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565", size = 95631 },
]

[[package]]
name = "frozenlist"
version = "1.6.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5b/bf/a812e2fe6cb3f6c6cfc8d0303bf1742f2286004e5ec41ac8c89cf68cdb54/frozenlist-1.6.2.tar.gz", hash = "sha256:effc641518696471cf4962e8e32050133bc1f7b2851ae8fd0cb8797dd70dc202", size = 43108 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b8/f6/973abfcb8b68f2e8b58071a04ec72f5e1f0acd19dae0d3b7a8abc3d9ab07/frozenlist-1.6.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2ad8851ae1f6695d735f8646bf1e68675871789756f7f7e8dc8224a74eabb9d0", size = 85517 },
    { url = "https://files.pythonhosted.org/packages/c8/d0/ac45f2dcf0afd5f7d57204af8b7516ecbc3599ea681e06f4b25d3845bea8/frozenlist-1.6.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:cd2d5abc0ccd99a2a5b437987f3b1e9c265c1044d2855a09ac68f09bbb8082ca", size = 49916 },
    { url = "https://files.pythonhosted.org/packages/50/cc/99c3f31823630b7411f7c1e83399e91d6b56a5661a5b724935ef5b51f5f5/frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15c33f665faa9b8f8e525b987eeaae6641816e0f6873e8a9c4d224338cebbb55", size = 48107 },
    { url = "https://files.pythonhosted.org/packages/85/4e/38643ce3ee80d222892b694d02c15ea476c4d564493a6fe530347163744e/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3e6c0681783723bb472b6b8304e61ecfcb4c2b11cf7f243d923813c21ae5d2a", size = 255771 },
    { url = "https://files.pythonhosted.org/packages/ca/e6/ceed85a7d5c0f666485384fc393e32353f8088e154a1109e5ef60165d366/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:61bae4d345a26550d0ed9f2c9910ea060f89dbfc642b7b96e9510a95c3a33b3c", size = 252519 },
    { url = "https://files.pythonhosted.org/packages/29/99/9f2e2b90cf918465e3b6ca4eea79e6be53d24fba33937e37d86c3764bbf9/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90e5a84016d0d2fb828f770ede085b5d89155fcb9629b8a3237c960c41c120c3", size = 263348 },
    { url = "https://files.pythonhosted.org/packages/4e/ac/59f3ec4c1b4897186efb4757379915734a48bb16bbc15a9fe0bf0857b679/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55dc289a064c04819d669e6e8a85a1c0416e6c601782093bdc749ae14a2f39da", size = 257858 },
    { url = "https://files.pythonhosted.org/packages/48/4a/19c97510d0c2be1ebaae68383d1b5a256a12a660ca17b0c427b1024d9b92/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b79bcf97ca03c95b044532a4fef6e5ae106a2dd863875b75fde64c553e3f4820", size = 238248 },
    { url = "https://files.pythonhosted.org/packages/ef/64/641aa2b0944fa3d881323948e0d8d6fee746dae03d9023eb510bb80bc46a/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e5e7564d232a782baa3089b25a0d979e2e4d6572d3c7231fcceacc5c22bf0f7", size = 255932 },
    { url = "https://files.pythonhosted.org/packages/6c/f8/5b68d5658fac7332e5d26542a4af0ffc2edca8da8f854f6274882889ee1e/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6fcd8d56880dccdd376afb18f483ab55a0e24036adc9a83c914d4b7bb5729d4e", size = 253329 },
    { url = "https://files.pythonhosted.org/packages/e9/20/379d7a27eb82748b41319bf376bf2c034e7ee11dda94f12b331edcc261ff/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4fbce985c7fe7bafb4d9bf647c835dbe415b465a897b0c79d1bdf0f3fae5fe50", size = 266164 },
    { url = "https://files.pythonhosted.org/packages/13/bd/d7dbf94220020850392cb661bedfdf786398bafae85d1045dd108971d261/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:3bd12d727cd616387d50fe283abebb2db93300c98f8ff1084b68460acd551926", size = 241641 },
    { url = "https://files.pythonhosted.org/packages/a4/70/916fef6284d294077265cd69ad05f228e44f7ed88d9acb690df5a1174049/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:38544cae535ed697960891131731b33bb865b7d197ad62dc380d2dbb1bceff48", size = 261215 },
    { url = "https://files.pythonhosted.org/packages/8f/98/1326a7189fa519692698cddf598f56766b0fea6ac71cddaf64760a055397/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:47396898f98fae5c9b9bb409c3d2cf6106e409730f35a0926aad09dd7acf1ef5", size = 262597 },
    { url = "https://files.pythonhosted.org/packages/f4/d6/0a95ab9289c72e86c37c9b8afe82576556456b6f66a35d242526634130f2/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:d10d835f8ce8571fd555db42d3aef325af903535dad7e6faa7b9c8abe191bffc", size = 258766 },
    { url = "https://files.pythonhosted.org/packages/1b/d0/9e946aabd89ebfcb71ec1371327f0e25d4868cd4439471a6fcb6eaf7b366/frozenlist-1.6.2-cp313-cp313-win32.whl", hash = "sha256:a400fe775a41b6d7a3fef00d88f10cbae4f0074c9804e282013d7797671ba58d", size = 40961 },
    { url = "https://files.pythonhosted.org/packages/43/e9/d714f5eb0fde1413344ded982ae9638307b59651d5c04263af42eb81a315/frozenlist-1.6.2-cp313-cp313-win_amd64.whl", hash = "sha256:cc8b25b321863ed46992558a29bb09b766c41e25f31461666d501be0f893bada", size = 46204 },
    { url = "https://files.pythonhosted.org/packages/f5/7a/8f6dde73862499e60eb390778a1e46b87c1fe3c5722622d731ccda7a173c/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:56de277a0e0ad26a1dcdc99802b4f5becd7fd890807b68e3ecff8ced01d58132", size = 91326 },
    { url = "https://files.pythonhosted.org/packages/79/60/dcdc75edbcf8241e7cb15fced68b3be63f67ff3faaf559c540a7eb63233b/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:9cb386dd69ae91be586aa15cb6f39a19b5f79ffc1511371eca8ff162721c4867", size = 52426 },
    { url = "https://files.pythonhosted.org/packages/64/e6/df2a43ccb2c4f1ea3692aae9a89cfc5dd932a90b7898f98f13ed9e2680a9/frozenlist-1.6.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53835d8a6929c2f16e02616f8b727bd140ce8bf0aeddeafdb290a67c136ca8ad", size = 51460 },
    { url = "https://files.pythonhosted.org/packages/fd/b3/c4f2f7fca9487b25c39bf64535f029316e184072a82f3660ce72defc5421/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cc49f2277e8173abf028d744f8b7d69fe8cc26bffc2de97d47a3b529599fbf50", size = 310270 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/046eb34d8d0fee1a8c9dc91a9ba581283c67a1ace20bcc01c86a53595105/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:65eb9e8a973161bdac5fa06ea6bd261057947adc4f47a7a6ef3d6db30c78c5b4", size = 289062 },
    { url = "https://files.pythonhosted.org/packages/48/7b/80991efaa0aa25e867cf93033c28e9d1310f34f90421eb59eb1f2073d937/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:301eb2f898d863031f8c5a56c88a6c5d976ba11a4a08a1438b96ee3acb5aea80", size = 312202 },
    { url = "https://files.pythonhosted.org/packages/78/6b/6fe30bdababdf82c5b34f0093770c4be6211071e23570721b80b11c9d52a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:207f717fd5e65fddb77d33361ab8fa939f6d89195f11307e073066886b33f2b8", size = 309557 },
    { url = "https://files.pythonhosted.org/packages/9d/ef/b7bf48802fc7d084703ba2173e6a8d0590bea378dcd6a480051c41bddf47/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f83992722642ee0db0333b1dbf205b1a38f97d51a7382eb304ba414d8c3d1e05", size = 282135 },
    { url = "https://files.pythonhosted.org/packages/af/f8/6911a085bce8d0d0df3dfc2560e3e0fb4d6c19ff101014bcf61aa32ba39a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12af99e6023851b36578e5bcc60618b5b30f4650340e29e565cd1936326dbea7", size = 303392 },
    { url = "https://files.pythonhosted.org/packages/9c/5d/b4e0cc6dbd6b9282926a470a919da7c6599ff324ab5268c7ecaff82cb858/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6f01620444a674eaad900a3263574418e99c49e2a5d6e5330753857363b5d59f", size = 309402 },
    { url = "https://files.pythonhosted.org/packages/0f/1b/bf777de3c810e68e8758337fcc97ee8c956376c87aecee9a61ba19a94123/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:82b94c8948341512306ca8ccc702771600b442c6abe5f8ee017e00e452a209e8", size = 312924 },
    { url = "https://files.pythonhosted.org/packages/0e/03/a69b890bc310790fcae61fd3b5be64876811b12db5d50b32e62f65e766bd/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:324a4cf4c220ddb3db1f46ade01e48432c63fa8c26812c710006e7f6cfba4a08", size = 291768 },
    { url = "https://files.pythonhosted.org/packages/70/cc/559386adf987b47c8977c929271d11a72efd92778a0a2f4cc97827a9a25b/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:695284e51458dabb89af7f7dc95c470aa51fd259207aba5378b187909297feef", size = 313305 },
    { url = "https://files.pythonhosted.org/packages/e7/fa/eb0e21730ffccfb2d0d367d863cbaacf8367bdc277b44eabf72f7329ab91/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:9ccbeb1c8dda4f42d0678076aa5cbde941a232be71c67b9d8ca89fbaf395807c", size = 312228 },
    { url = "https://files.pythonhosted.org/packages/d1/c1/8471b67172abc9478ad78c70a3f3a5c4fed6d4bcadc748e1b6dfa06ab2ae/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cbbdf62fcc1864912c592a1ec748fee94f294c6b23215d5e8e9569becb7723ee", size = 309905 },
    { url = "https://files.pythonhosted.org/packages/bb/2c/ee21987c3a175b49d0b827b1e45394a7a5d08c7de5b766ed6d0889d30568/frozenlist-1.6.2-cp313-cp313t-win32.whl", hash = "sha256:76857098ee17258df1a61f934f2bae052b8542c9ea6b187684a737b2e3383a65", size = 44644 },
    { url = "https://files.pythonhosted.org/packages/65/46/fce60f65b1fb17a90c4bf410a5c90cb3b40616cc229e75866f8be97c112c/frozenlist-1.6.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c06a88daba7e891add42f9278cdf7506a49bc04df9b1648be54da1bf1c79b4c6", size = 50607 },
    { url = "https://files.pythonhosted.org/packages/13/be/0ebbb283f2d91b72beaee2d07760b2c47dab875c49c286f5591d3d157198/frozenlist-1.6.2-py3-none-any.whl", hash = "sha256:947abfcc8c42a329bbda6df97a4b9c9cdb4e12c85153b3b57b9d2f02aa5877dc", size = 12582 },
]

[[package]]
name = "google-ai-generativelanguage"
version = "0.6.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/11/d1/48fe5d7a43d278e9f6b5ada810b0a3530bbeac7ed7fcbcd366f932f05316/google_ai_generativelanguage-0.6.15.tar.gz", hash = "sha256:8f6d9dc4c12b065fe2d0289026171acea5183ebf2d0b11cefe12f3821e159ec3", size = 1375443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/a3/67b8a6ff5001a1d8864922f2d6488dc2a14367ceb651bc3f09a947f2f306/google_ai_generativelanguage-0.6.15-py3-none-any.whl", hash = "sha256:5a03ef86377aa184ffef3662ca28f19eeee158733e45d7947982eb953c6ebb6c", size = 1327356 },
]

[[package]]
name = "google-api-core"
version = "2.24.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "googleapis-common-protos" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/09/5c/085bcb872556934bb119e5e09de54daa07873f6866b8f0303c49e72287f7/google_api_core-2.24.2.tar.gz", hash = "sha256:81718493daf06d96d6bc76a91c23874dbf2fac0adbbf542831b805ee6e974696", size = 163516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/95/f472d85adab6e538da2025dfca9e976a0d125cc0af2301f190e77b76e51c/google_api_core-2.24.2-py3-none-any.whl", hash = "sha256:810a63ac95f3c441b7c0e43d344e372887f62ce9071ba972eacf32672e072de9", size = 160061 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
    { name = "grpcio-status" },
]

[[package]]
name = "google-api-python-client"
version = "2.171.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-auth-httplib2" },
    { name = "httplib2" },
    { name = "uritemplate" },
]
sdist = { url = "https://files.pythonhosted.org/packages/35/99/237cd2510aecca9fabb54007e58553274cc43cb3c18512ee1ea574d11b87/google_api_python_client-2.171.0.tar.gz", hash = "sha256:057a5c08d28463c6b9eb89746355de5f14b7ed27a65c11fdbf1d06c66bb66b23", size = 13028937 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/db/c397e3eb3ea18f423855479d0a5852bdc9c3f644e3d4194931fa664a70b4/google_api_python_client-2.171.0-py3-none-any.whl", hash = "sha256:c9c9b76f561e9d9ac14e54a9e2c0842876201d5b96e69e48f967373f0784cbe9", size = 13547393 },
]

[[package]]
name = "google-auth"
version = "2.39.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cachetools" },
    { name = "pyasn1-modules" },
    { name = "rsa" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/8e/8f45c9a32f73e786e954b8f9761c61422955d23c45d1e8c347f9b4b59e8e/google_auth-2.39.0.tar.gz", hash = "sha256:73222d43cdc35a3aeacbfdcaf73142a97839f10de930550d89ebfe1d0a00cde7", size = 274834 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/12/ad37a1ef86006d0a0117fc06a4a00bd461c775356b534b425f00dde208ea/google_auth-2.39.0-py2.py3-none-any.whl", hash = "sha256:0150b6711e97fb9f52fe599f55648950cc4540015565d8fbb31be2ad6e1548a2", size = 212319 },
]

[[package]]
name = "google-auth-httplib2"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "httplib2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/56/be/217a598a818567b28e859ff087f347475c807a5649296fb5a817c58dacef/google-auth-httplib2-0.2.0.tar.gz", hash = "sha256:38aa7badf48f974f1eb9861794e9c0cb2a0511a4ec0679b1f886d108f5640e05", size = 10842 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl", hash = "sha256:b65a0a2123300dd71281a7bf6e64d65a0759287df52729bdd1ae2e47dc311a3d", size = 9253 },
]

[[package]]
name = "google-cloud-aiplatform"
version = "1.91.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "docstring-parser" },
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-bigquery" },
    { name = "google-cloud-resource-manager" },
    { name = "google-cloud-storage" },
    { name = "packaging" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "shapely" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/50/08/5854569782efbbc8efd0aeda3a4486153605104cbab6ac836b2328bae48e/google_cloud_aiplatform-1.91.0.tar.gz", hash = "sha256:b14e5e52b52b6012c7dc253beab34c511fdc53c69b13f436ddb06882c1a92cd7", size = 9102586 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/88/cea8583fadd142e8ef26f8ec14a6ee4d7c69c4e5ab82bea01a077fddddbe/google_cloud_aiplatform-1.91.0-py2.py3-none-any.whl", hash = "sha256:ff8df100c2af692d114a2219d3abbb96110b3e5655f342fdbb6aefad43901b52", size = 7591910 },
]

[package.optional-dependencies]
tokenization = [
    { name = "sentencepiece" },
]

[[package]]
name = "google-cloud-bigquery"
version = "3.31.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-resumable-media" },
    { name = "packaging" },
    { name = "python-dateutil" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/73/91/4c7274f4d5faf13ac000b06353deaf3579575bf0e4bbad07fa68b9f09ba9/google_cloud_bigquery-3.31.0.tar.gz", hash = "sha256:b89dc716dbe4abdb7a4f873f7050100287bc98514e0614c5d54cd6a8e9fb0991", size = 479961 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e8/bc/4cb8c61fc6dd817a4a390b745ec7b305f4578f547a16d09d54c8a790624b/google_cloud_bigquery-3.31.0-py3-none-any.whl", hash = "sha256:97f4a3219854ff01d6a3a57312feecb0b6e13062226b823f867e2d3619c4787b", size = 250099 },
]

[[package]]
name = "google-cloud-core"
version = "2.4.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/b8/2b53838d2acd6ec6168fd284a990c76695e84c65deee79c9f3a4276f6b4f/google_cloud_core-2.4.3.tar.gz", hash = "sha256:1fab62d7102844b278fe6dead3af32408b1df3eb06f5c7e8634cbd40edc4da53", size = 35861 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl", hash = "sha256:5130f9f4c14b4fafdff75c79448f9495cfade0d8775facf1b09c3bf67e027f6e", size = 29348 },
]

[[package]]
name = "google-cloud-resource-manager"
version = "1.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6e/ca/a4648f5038cb94af4b3942815942a03aa9398f9fb0bef55b3f1585b9940d/google_cloud_resource_manager-1.14.2.tar.gz", hash = "sha256:962e2d904c550d7bac48372607904ff7bb3277e3bb4a36d80cc9a37e28e6eb74", size = 446370 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/ea/a92631c358da377af34d3a9682c97af83185c2d66363d5939ab4a1169a7f/google_cloud_resource_manager-1.14.2-py3-none-any.whl", hash = "sha256:d0fa954dedd1d2b8e13feae9099c01b8aac515b648e612834f9942d2795a9900", size = 394344 },
]

[[package]]
name = "google-cloud-secret-manager"
version = "2.24.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/7a/2fa6735ec693d822fe08a76709c4d95d9b5b4c02e83e720497355039d2ee/google_cloud_secret_manager-2.24.0.tar.gz", hash = "sha256:ce573d40ffc2fb7d01719243a94ee17aa243ea642a6ae6c337501e58fbf642b5", size = 269516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/af/db1217cae1809e69a4527ee6293b82a9af2a1fb2313ad110c775e8f3c820/google_cloud_secret_manager-2.24.0-py3-none-any.whl", hash = "sha256:9bea1254827ecc14874bc86c63b899489f8f50bfe1442bfb2517530b30b3a89b", size = 218050 },
]

[[package]]
name = "google-cloud-storage"
version = "2.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-crc32c" },
    { name = "google-resumable-media" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/36/76/4d965702e96bb67976e755bed9828fa50306dca003dbee08b67f41dd265e/google_cloud_storage-2.19.0.tar.gz", hash = "sha256:cd05e9e7191ba6cb68934d8eb76054d9be4562aa89dbc4236feee4d7d51342b2", size = 5535488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/94/6db383d8ee1adf45dc6c73477152b82731fa4c4a46d9c1932cc8757e0fd4/google_cloud_storage-2.19.0-py2.py3-none-any.whl", hash = "sha256:aeb971b5c29cf8ab98445082cbfe7b161a1f48ed275822f59ed3f1524ea54fba", size = 131787 },
]

[[package]]
name = "google-crc32c"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/19/ae/87802e6d9f9d69adfaedfcfd599266bf386a54d0be058b532d04c794f76d/google_crc32c-1.7.1.tar.gz", hash = "sha256:2bff2305f98846f3e825dbeec9ee406f89da7962accdb29356e4eadc251bd472", size = 14495 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/72/b8d785e9184ba6297a8620c8a37cf6e39b81a8ca01bb0796d7cbb28b3386/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_arm64.whl", hash = "sha256:df8b38bdaf1629d62d51be8bdd04888f37c451564c2042d36e5812da9eff3c35", size = 30467 },
    { url = "https://files.pythonhosted.org/packages/34/25/5f18076968212067c4e8ea95bf3b69669f9fc698476e5f5eb97d5b37999f/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_x86_64.whl", hash = "sha256:e42e20a83a29aa2709a0cf271c7f8aefaa23b7ab52e53b322585297bb94d4638", size = 30309 },
    { url = "https://files.pythonhosted.org/packages/92/83/9228fe65bf70e93e419f38bdf6c5ca5083fc6d32886ee79b450ceefd1dbd/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:905a385140bf492ac300026717af339790921f411c0dfd9aa5a9e69a08ed32eb", size = 33133 },
    { url = "https://files.pythonhosted.org/packages/c3/ca/1ea2fd13ff9f8955b85e7956872fdb7050c4ace8a2306a6d177edb9cf7fe/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b211ddaf20f7ebeec5c333448582c224a7c90a9d98826fbab82c0ddc11348e6", size = 32773 },
    { url = "https://files.pythonhosted.org/packages/89/32/a22a281806e3ef21b72db16f948cad22ec68e4bdd384139291e00ff82fe2/google_crc32c-1.7.1-cp313-cp313-win_amd64.whl", hash = "sha256:0f99eaa09a9a7e642a61e06742856eec8b19fc0037832e03f941fe7cf0c8e4db", size = 33475 },
    { url = "https://files.pythonhosted.org/packages/b8/c5/002975aff514e57fc084ba155697a049b3f9b52225ec3bc0f542871dd524/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32d1da0d74ec5634a05f53ef7df18fc646666a25efaaca9fc7dcfd4caf1d98c3", size = 33243 },
    { url = "https://files.pythonhosted.org/packages/61/cb/c585282a03a0cea70fcaa1bf55d5d702d0f2351094d663ec3be1c6c67c52/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e10554d4abc5238823112c2ad7e4560f96c7bf3820b202660373d769d9e6e4c9", size = 32870 },
]

[[package]]
name = "google-genai"
version = "1.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "google-auth" },
    { name = "httpx" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/14/17/8f717f43732ae2b7775f816f0d8f0b39e2a020bbe7ba202f2ddb2f948c3b/google_genai-1.19.0.tar.gz", hash = "sha256:66f5de78075781bfd9e423f1e3592e4240759dfe0ac42ac74a9dcb2c4f662e9d", size = 198000 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c4/ae/64fccdebf5811453ce53b0d5ee23d4f27ef173ef36d3b67dad791a0007aa/google_genai-1.19.0-py3-none-any.whl", hash = "sha256:a2955612e4af8c84f83eb43c1ce4e74e1b714732926d0705e639761938192466", size = 200043 },
]

[[package]]
name = "google-generativeai"
version = "0.8.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-ai-generativelanguage" },
    { name = "google-api-core" },
    { name = "google-api-python-client" },
    { name = "google-auth" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/40/c42ff9ded9f09ec9392879a8e6538a00b2dc185e834a3392917626255419/google_generativeai-0.8.5-py3-none-any.whl", hash = "sha256:22b420817fb263f8ed520b33285f45976d5b21e904da32b80d4fd20c055123a2", size = 155427 },
]

[[package]]
name = "google-resumable-media"
version = "2.7.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-crc32c" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/5a/0efdc02665dca14e0837b62c8a1a93132c264bd02054a15abb2218afe0ae/google_resumable_media-2.7.2.tar.gz", hash = "sha256:5280aed4629f2b60b847b0d42f9857fd4935c11af266744df33d8074cae92fe0", size = 2163099 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl", hash = "sha256:3ce7551e9fe6d99e9a126101d2536612bb73486721951e9562fee0f90c6ababa", size = 81251 },
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/39/24/33db22342cf4a2ea27c9955e6713140fedd51e8b141b5ce5260897020f1a/googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257", size = 145903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/f1/62a193f0227cf15a920390abe675f386dec35f7ae3ffe6da582d3ade42c7/googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8", size = 294530 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
]

[[package]]
name = "grpc-google-iam-v1"
version = "0.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos", extra = ["grpc"] },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/4e/8d0ca3b035e41fe0b3f31ebbb638356af720335e5a11154c330169b40777/grpc_google_iam_v1-0.14.2.tar.gz", hash = "sha256:b3e1fc387a1a329e41672197d0ace9de22c78dd7d215048c4c78712073f7bd20", size = 16259 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/66/6f/dd9b178aee7835b96c2e63715aba6516a9d50f6bebbd1cc1d32c82a2a6c3/grpc_google_iam_v1-0.14.2-py3-none-any.whl", hash = "sha256:a3171468459770907926d56a440b2bb643eec1d7ba215f48f3ecece42b4d8351", size = 19242 },
]

[[package]]
name = "grpcio"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/95/aa11fc09a85d91fbc7dd405dcb2a1e0256989d67bf89fa65ae24b3ba105a/grpcio-1.71.0.tar.gz", hash = "sha256:2b85f7820475ad3edec209d3d89a7909ada16caab05d3f2e08a7e8ae3200a55c", size = 12549828 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/dd/b00cbb45400d06b26126dcfdbdb34bb6c4f28c3ebbd7aea8228679103ef6/grpcio-1.71.0-cp313-cp313-linux_armv7l.whl", hash = "sha256:cebc1b34ba40a312ab480ccdb396ff3c529377a2fce72c45a741f7215bfe8379", size = 5184138 },
    { url = "https://files.pythonhosted.org/packages/ed/0a/4651215983d590ef53aac40ba0e29dda941a02b097892c44fa3357e706e5/grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl", hash = "sha256:85da336e3649a3d2171e82f696b5cad2c6231fdd5bad52616476235681bee5b3", size = 11310747 },
    { url = "https://files.pythonhosted.org/packages/57/a3/149615b247f321e13f60aa512d3509d4215173bdb982c9098d78484de216/grpcio-1.71.0-cp313-cp313-manylinux_2_17_aarch64.whl", hash = "sha256:f9a412f55bb6e8f3bb000e020dbc1e709627dcb3a56f6431fa7076b4c1aab0db", size = 5653991 },
    { url = "https://files.pythonhosted.org/packages/ca/56/29432a3e8d951b5e4e520a40cd93bebaa824a14033ea8e65b0ece1da6167/grpcio-1.71.0-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:47be9584729534660416f6d2a3108aaeac1122f6b5bdbf9fd823e11fe6fbaa29", size = 6312781 },
    { url = "https://files.pythonhosted.org/packages/a3/f8/286e81a62964ceb6ac10b10925261d4871a762d2a763fbf354115f9afc98/grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c9c80ac6091c916db81131d50926a93ab162a7e97e4428ffc186b6e80d6dda4", size = 5910479 },
    { url = "https://files.pythonhosted.org/packages/35/67/d1febb49ec0f599b9e6d4d0d44c2d4afdbed9c3e80deb7587ec788fcf252/grpcio-1.71.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:789d5e2a3a15419374b7b45cd680b1e83bbc1e52b9086e49308e2c0b5bbae6e3", size = 6013262 },
    { url = "https://files.pythonhosted.org/packages/a1/04/f9ceda11755f0104a075ad7163fc0d96e2e3a9fe25ef38adfc74c5790daf/grpcio-1.71.0-cp313-cp313-musllinux_1_1_i686.whl", hash = "sha256:1be857615e26a86d7363e8a163fade914595c81fec962b3d514a4b1e8760467b", size = 6643356 },
    { url = "https://files.pythonhosted.org/packages/fb/ce/236dbc3dc77cf9a9242adcf1f62538734ad64727fabf39e1346ad4bd5c75/grpcio-1.71.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:a76d39b5fafd79ed604c4be0a869ec3581a172a707e2a8d7a4858cb05a5a7637", size = 6186564 },
    { url = "https://files.pythonhosted.org/packages/10/fd/b3348fce9dd4280e221f513dd54024e765b21c348bc475516672da4218e9/grpcio-1.71.0-cp313-cp313-win32.whl", hash = "sha256:74258dce215cb1995083daa17b379a1a5a87d275387b7ffe137f1d5131e2cfbb", size = 3601890 },
    { url = "https://files.pythonhosted.org/packages/be/f8/db5d5f3fc7e296166286c2a397836b8b042f7ad1e11028d82b061701f0f7/grpcio-1.71.0-cp313-cp313-win_amd64.whl", hash = "sha256:22c3bc8d488c039a199f7a003a38cb7635db6656fa96437a8accde8322ce2366", size = 4273308 },
]

[[package]]
name = "grpcio-status"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos" },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d7/53/a911467bece076020456401f55a27415d2d70d3bc2c37af06b44ea41fc5c/grpcio_status-1.71.0.tar.gz", hash = "sha256:11405fed67b68f406b3f3c7c5ae5104a79d2d309666d10d61b152e91d28fb968", size = 13669 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ad/d6/31fbc43ff097d8c4c9fc3df741431b8018f67bf8dfbe6553a555f6e5f675/grpcio_status-1.71.0-py3-none-any.whl", hash = "sha256:843934ef8c09e3e858952887467f8256aac3910c55f077a359a65b2b3cde3e68", size = 14424 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "html-to-markdown"
version = "1.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1d/48/324d3d938e5ff635497965118df510f62725b72e8b378b8710c03b0dd014/html_to_markdown-1.3.3.tar.gz", hash = "sha256:ad4f992d65d96d53e49d0a56a2ae0c52ef606c17592d2d9a87f99e4632a4a9e3", size = 15491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a3/d0/b96f7e3579cada841657e5764bc294bd2abb6c1e1dbcfb88ecf7a63ea5d9/html_to_markdown-1.3.3-py3-none-any.whl", hash = "sha256:09325777400e561d2c5a1569f475f9434e70a6f8ed1b4866bba8d00906136495", size = 14951 },
]

[[package]]
name = "html5lib"
version = "1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
    { name = "webencodings" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ac/b6/b55c3f49042f1df3dcd422b7f224f939892ee94f22abcf503a9b7339eaf2/html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f", size = 272215 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d", size = 112173 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httplib2"
version = "0.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyparsing" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/ad/2371116b22d616c194aa25ec410c9c6c37f23599dcd590502b74db197584/httplib2-0.22.0.tar.gz", hash = "sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81", size = 351116 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl", hash = "sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc", size = 96854 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "zipp" },
]
sdist = { url = "https://files.pythonhosted.org/packages/76/66/650a33bd90f786193e4de4b3ad86ea60b53c89b669a5c7be931fac31cdb0/importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000", size = 56641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd", size = 27656 },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050 },
]

[[package]]
name = "ipython"
version = "9.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "decorator" },
    { name = "ipython-pygments-lexers" },
    { name = "jedi" },
    { name = "matplotlib-inline" },
    { name = "pexpect", marker = "sys_platform != 'emscripten' and sys_platform != 'win32'" },
    { name = "prompt-toolkit" },
    { name = "pygments" },
    { name = "stack-data" },
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/54/80/406f9e3bde1c1fd9bf5a0be9d090f8ae623e401b7670d8f6fdf2ab679891/ipython-9.4.0.tar.gz", hash = "sha256:c033c6d4e7914c3d9768aabe76bbe87ba1dc66a92a05db6bfa1125d81f2ee270", size = 4385338 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/63/f8/0031ee2b906a15a33d6bfc12dd09c3dfa966b3cb5b284ecfb7549e6ac3c4/ipython-9.4.0-py3-none-any.whl", hash = "sha256:25850f025a446d9b359e8d296ba175a36aedd32e83ca9b5060430fe16801f066", size = 611021 },
]

[[package]]
name = "ipython-pygments-lexers"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/4c/5dd1d8af08107f88c7f741ead7a40854b8ac24ddf9ae850afbcf698aa552/ipython_pygments_lexers-1.1.1.tar.gz", hash = "sha256:09c0138009e56b6854f9535736f4171d855c8c08a563a0dcd8022f78355c7e81", size = 8393 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl", hash = "sha256:a9462224a505ade19a605f71f8fa63c2048833ce50abc86768a0d81d876dc81c", size = 8074 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jedi"
version = "0.19.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "parso" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/3a/79a912fbd4d8dd6fbb02bf69afd3bb72cf0c729bb3063c6f4498603db17a/jedi-0.19.2.tar.gz", hash = "sha256:4770dc3de41bde3966b02eb84fbcf557fb33cce26ad23da12c742fb50ecb11f0", size = 1231287 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl", hash = "sha256:a8ef22bde8490f57fe5c7681a3c83cb58874daf72b4784de3cce5b6ef6edb5b9", size = 1572278 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "loro"
version = "1.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a0/32/ce94b1fc342ac90d9ca21bc6e90c727990734a75505cb893b2a71a364faf/loro-1.5.2.tar.gz", hash = "sha256:70e52acb16474f7c1e52aea2a7fe2771516f1e9f73d4edfe40f3193b122402c7", size = 62538 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/09/061e8cecb42f99856580811156d7651d5e8172bb840224c7cd2eb94a8730/loro-1.5.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:dbb94c104e3aba4ea3f1118c72896de978e737bb066a35051bf49895e72540a7", size = 3098320 },
    { url = "https://files.pythonhosted.org/packages/60/6e/96cb1a78869c8ae91e65d73ef4ee9f74bc16fd3baff5a7463f7702687dab/loro-1.5.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:847a10f493399f9b650b588b3d81893dfaa1e45e7091881268094f2b9f7df38b", size = 2882026 },
    { url = "https://files.pythonhosted.org/packages/eb/e7/2a131e3e8072614af1cc2970efc1c30a812eb8b0f5286c7b6b390ae3fc9f/loro-1.5.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:902215b77b35e58286d907e8292f78b014cd9c55a46bc5deb944f555509b7747", size = 3110094 },
    { url = "https://files.pythonhosted.org/packages/8c/63/34efc556a5a7663f045d64b9744c10f7b00386f252fac47c939f1c1795be/loro-1.5.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:19e8c9896348063721ef56631d2275c186faf63f6336079c57f41055c9cc1c30", size = 3202938 },
    { url = "https://files.pythonhosted.org/packages/67/3f/5a37b5f1bec5d633f469754e26bf0ce77a26f7697cd95d0b4a51b9cd90be/loro-1.5.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91e75cd4b26506bb5b564ed24b433147fc8b77e8779b5736bc4f3bfddf270590", size = 3579945 },
    { url = "https://files.pythonhosted.org/packages/78/b3/cd3202d6398524c5e1442688c6825e148eb953aa0de04952fd546c69a398/loro-1.5.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:41e54109599190dede34366476a8f42ae6e9fd7fd439823150e9f70e39d7d54e", size = 3318843 },
    { url = "https://files.pythonhosted.org/packages/a5/65/8ed127c827ed9b540f5660e9c98265702dbfdd71ad59063bd3c799ca0dda/loro-1.5.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd3f330795212f24b9dd710f952f7f7138ba86d6159f524025eb4627641ed4ef", size = 3243417 },
    { url = "https://files.pythonhosted.org/packages/4e/29/6894f6db7a1eb7d5d2936b658b3a26c4ea8ce6b0563dde024b909a63289d/loro-1.5.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ebdd716ce67c182f71a093c552f9a47428f7a3d93b038780bbb0f06779805d0", size = 3511123 },
    { url = "https://files.pythonhosted.org/packages/17/26/230867103d5ec58ef18f8d0bc169a4defb4f865f9969247d4e9c723ae10e/loro-1.5.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a8ac5ff8b697e9a828fe4387da715d78d0f2afcf23bbd76f5089b4122f5e78a3", size = 3256828 },
    { url = "https://files.pythonhosted.org/packages/79/8b/7aed297d9cc236e15674275364e37e938e9335c9dfad49ad35904fa8b1f3/loro-1.5.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3dce7920c45c9c884246898805b270d63550a5dec61d3f33274010c40127a37c", size = 3464838 },
    { url = "https://files.pythonhosted.org/packages/1d/c1/352fd39b61a842dc991bf95aaa75db34b6c353c1a3844da17e01f917deb5/loro-1.5.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:66afec16e22db99f1818906bc7cabda0cb077e0e493882b4c0983a8bc431413d", size = 3502790 },
    { url = "https://files.pythonhosted.org/packages/2c/11/859dfc28b1397d731d2cc710dae0e7cb1cbeb45ab70ec518b4ed4f690a4c/loro-1.5.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9f052715922592f099e9b6553fccb48761c5ad83deefcb0df55effde309eb12d", size = 3414408 },
    { url = "https://files.pythonhosted.org/packages/86/3e/fcd87311399e2eff892fb3a6b6f1d3307a2dfd99811fddf0889bee89d585/loro-1.5.2-cp313-cp313-win32.whl", hash = "sha256:978e9f6b0c9ad8c6b1ab70372eafbe00c41782522b216802cf961a81edd27561", size = 2580638 },
    { url = "https://files.pythonhosted.org/packages/93/06/dd73ca0865630923f18fa4486e66a171a0a26ae8e7541f1c3d93100f1f5b/loro-1.5.2-cp313-cp313-win_amd64.whl", hash = "sha256:3ecebbf9f5f880c6ca9a1628e5f469d3d67b67c1fd50536c52c5f6eae01be549", size = 2743550 },
    { url = "https://files.pythonhosted.org/packages/d2/70/9e5030bb9f1b86520f482605f660e5a192d6f5e56104fee122fe7d3dc72e/loro-1.5.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:354de426d6404cce252fb81be17a1589f1bd47197ba7f730f60fbb52452f49ab", size = 3106619 },
    { url = "https://files.pythonhosted.org/packages/2b/37/43c8e3fa8c6239be1b22c0dfd779a4ab000682dddebc23becd057668c436/loro-1.5.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:18e3b6f07483c5553795fea05c8d318f96c018909dd390c68b81701afb12cac3", size = 3195270 },
    { url = "https://files.pythonhosted.org/packages/b1/d6/8aaa433d08710cb1b95781d56efad366350082798463e35b5a6a4988b160/loro-1.5.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2298b96c5f533807373db27dbf5b10c88f1c5d9e0145feb952e7a813a81af645", size = 3575129 },
    { url = "https://files.pythonhosted.org/packages/51/4e/44425f11da9b5278653c3ca01cdfd4da850f94ead5843d8134043ac825cf/loro-1.5.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0aa8edef791c1b46e19bf86ab17f9dbefc61b8f1fbecc49054d5eb880380d897", size = 3317031 },
    { url = "https://files.pythonhosted.org/packages/3b/ae/af1713c7c3cc91a9d6cc1b812733665875eb30c22e4c9e0e213a9a69b1a2/loro-1.5.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:633c026cbb17c485de40f09aab13362f0c79140913dc67445606e3237092d70f", size = 3251501 },
    { url = "https://files.pythonhosted.org/packages/4b/df/958e8abb78ca47ce06e0088bc5d44b5945ffbd08503936cbc0340b62a5f3/loro-1.5.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:903fed16d40b0373f747ecc398f5b86aaab16c37b4c670f580c2c5301bad4de5", size = 3456858 },
    { url = "https://files.pythonhosted.org/packages/f1/f6/982af3432bde075f1fd3201de0e95f35a868f4e85cee36bb22bb0524b069/loro-1.5.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2f9f77b1f582d86e1a57cdb38a43ea1a5861a6f0d73783335c2efdc3d1dcb793", size = 3494470 },
    { url = "https://files.pythonhosted.org/packages/47/b3/a4725db48fb4c7637076023ccedf7dcb7f24a3d266208f2e2aafb8179861/loro-1.5.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:489230b2716c0a2ad50e205670abed029ba0787c028a62dd31226f7935f5d1fd", size = 3410923 },
]

[[package]]
name = "lxml"
version = "5.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/76/3d/14e82fc7c8fb1b7761f7e748fd47e2ec8276d137b6acfe5a4bb73853e08f/lxml-5.4.0.tar.gz", hash = "sha256:d12832e1dbea4be280b22fd0ea7c9b87f0d8fc51ba06e92dc62d52f804f78ebd", size = 3679479 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/cb/2ba1e9dd953415f58548506fa5549a7f373ae55e80c61c9041b7fd09a38a/lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:773e27b62920199c6197130632c18fb7ead3257fce1ffb7d286912e56ddb79e0", size = 8110086 },
    { url = "https://files.pythonhosted.org/packages/b5/3e/6602a4dca3ae344e8609914d6ab22e52ce42e3e1638c10967568c5c1450d/lxml-5.4.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ce9c671845de9699904b1e9df95acfe8dfc183f2310f163cdaa91a3535af95de", size = 4404613 },
    { url = "https://files.pythonhosted.org/packages/4c/72/bf00988477d3bb452bef9436e45aeea82bb40cdfb4684b83c967c53909c7/lxml-5.4.0-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9454b8d8200ec99a224df8854786262b1bd6461f4280064c807303c642c05e76", size = 5012008 },
    { url = "https://files.pythonhosted.org/packages/92/1f/93e42d93e9e7a44b2d3354c462cd784dbaaf350f7976b5d7c3f85d68d1b1/lxml-5.4.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cccd007d5c95279e529c146d095f1d39ac05139de26c098166c4beb9374b0f4d", size = 4760915 },
    { url = "https://files.pythonhosted.org/packages/45/0b/363009390d0b461cf9976a499e83b68f792e4c32ecef092f3f9ef9c4ba54/lxml-5.4.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0fce1294a0497edb034cb416ad3e77ecc89b313cff7adbee5334e4dc0d11f422", size = 5283890 },
    { url = "https://files.pythonhosted.org/packages/19/dc/6056c332f9378ab476c88e301e6549a0454dbee8f0ae16847414f0eccb74/lxml-5.4.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:24974f774f3a78ac12b95e3a20ef0931795ff04dbb16db81a90c37f589819551", size = 4812644 },
    { url = "https://files.pythonhosted.org/packages/ee/8a/f8c66bbb23ecb9048a46a5ef9b495fd23f7543df642dabeebcb2eeb66592/lxml-5.4.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:497cab4d8254c2a90bf988f162ace2ddbfdd806fce3bda3f581b9d24c852e03c", size = 4921817 },
    { url = "https://files.pythonhosted.org/packages/04/57/2e537083c3f381f83d05d9b176f0d838a9e8961f7ed8ddce3f0217179ce3/lxml-5.4.0-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:e794f698ae4c5084414efea0f5cc9f4ac562ec02d66e1484ff822ef97c2cadff", size = 4753916 },
    { url = "https://files.pythonhosted.org/packages/d8/80/ea8c4072109a350848f1157ce83ccd9439601274035cd045ac31f47f3417/lxml-5.4.0-cp313-cp313-manylinux_2_28_ppc64le.whl", hash = "sha256:2c62891b1ea3094bb12097822b3d44b93fc6c325f2043c4d2736a8ff09e65f60", size = 5289274 },
    { url = "https://files.pythonhosted.org/packages/b3/47/c4be287c48cdc304483457878a3f22999098b9a95f455e3c4bda7ec7fc72/lxml-5.4.0-cp313-cp313-manylinux_2_28_s390x.whl", hash = "sha256:142accb3e4d1edae4b392bd165a9abdee8a3c432a2cca193df995bc3886249c8", size = 4874757 },
    { url = "https://files.pythonhosted.org/packages/2f/04/6ef935dc74e729932e39478e44d8cfe6a83550552eaa072b7c05f6f22488/lxml-5.4.0-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:1a42b3a19346e5601d1b8296ff6ef3d76038058f311902edd574461e9c036982", size = 4947028 },
    { url = "https://files.pythonhosted.org/packages/cb/f9/c33fc8daa373ef8a7daddb53175289024512b6619bc9de36d77dca3df44b/lxml-5.4.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4291d3c409a17febf817259cb37bc62cb7eb398bcc95c1356947e2871911ae61", size = 4834487 },
    { url = "https://files.pythonhosted.org/packages/8d/30/fc92bb595bcb878311e01b418b57d13900f84c2b94f6eca9e5073ea756e6/lxml-5.4.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4f5322cf38fe0e21c2d73901abf68e6329dc02a4994e483adbcf92b568a09a54", size = 5381688 },
    { url = "https://files.pythonhosted.org/packages/43/d1/3ba7bd978ce28bba8e3da2c2e9d5ae3f8f521ad3f0ca6ea4788d086ba00d/lxml-5.4.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0be91891bdb06ebe65122aa6bf3fc94489960cf7e03033c6f83a90863b23c58b", size = 5242043 },
    { url = "https://files.pythonhosted.org/packages/ee/cd/95fa2201041a610c4d08ddaf31d43b98ecc4b1d74b1e7245b1abdab443cb/lxml-5.4.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:15a665ad90054a3d4f397bc40f73948d48e36e4c09f9bcffc7d90c87410e478a", size = 5021569 },
    { url = "https://files.pythonhosted.org/packages/2d/a6/31da006fead660b9512d08d23d31e93ad3477dd47cc42e3285f143443176/lxml-5.4.0-cp313-cp313-win32.whl", hash = "sha256:d5663bc1b471c79f5c833cffbc9b87d7bf13f87e055a5c86c363ccd2348d7e82", size = 3485270 },
    { url = "https://files.pythonhosted.org/packages/fc/14/c115516c62a7d2499781d2d3d7215218c0731b2c940753bf9f9b7b73924d/lxml-5.4.0-cp313-cp313-win_amd64.whl", hash = "sha256:bcb7a1096b4b6b24ce1ac24d4942ad98f983cd3810f9711bcd0293f43a9d8b9f", size = 3814606 },
]

[[package]]
name = "marimo"
version = "0.14.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "docutils" },
    { name = "itsdangerous" },
    { name = "jedi" },
    { name = "loro" },
    { name = "markdown" },
    { name = "narwhals" },
    { name = "packaging" },
    { name = "psutil" },
    { name = "pygments" },
    { name = "pymdown-extensions" },
    { name = "pyyaml" },
    { name = "starlette" },
    { name = "tomlkit" },
    { name = "uvicorn" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0b/6d/8c0bdb68d608561e3039718f171ede292e7da7e7580a51b1f4b2ce6e204f/marimo-0.14.12.tar.gz", hash = "sha256:cf18513e30a5d2e8864930885b674dd89cbc9ad3a5e128b9ecfa48323de6d14f", size = 29622446 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/fa/d802cd61fb4714c17529057dc4b07d48c3e115d0af331907b3d19f5482f6/marimo-0.14.12-py3-none-any.whl", hash = "sha256:154d168ceb8b9f4cc10f8cd9f6299cf0c5d8643b0291370a9e64a88b2f517ed3", size = 30118091 },
]

[[package]]
name = "markdown"
version = "3.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/c2/4ab49206c17f75cb08d6311171f2d65798988db4360c4d1485bd0eedd67c/markdown-3.8.2.tar.gz", hash = "sha256:247b9a70dd12e27f67431ce62523e675b866d254f900c4fe75ce3dda62237c45", size = 362071 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl", hash = "sha256:5c83764dbd4e00bdd94d85a19b8d55ccca20fe35b2e678a1422b380324dd5f24", size = 106827 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "markdownify"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2f/78/c48fed23c7aebc2c16049062e72de1da3220c274de59d28c942acdc9ffb2/markdownify-1.1.0.tar.gz", hash = "sha256:449c0bbbf1401c5112379619524f33b63490a8fa479456d41de9dc9e37560ebd", size = 17127 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/11/b751af7ad41b254a802cf52f7bc1fca7cabe2388132f2ce60a1a6b9b9622/markdownify-1.1.0-py3-none-any.whl", hash = "sha256:32a5a08e9af02c8a6528942224c91b933b4bd2c7d078f9012943776fc313eeef", size = 13901 },
]

[[package]]
name = "matplotlib-inline"
version = "0.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/5b/a36a337438a14116b16480db471ad061c36c3694df7c2084a0da7ba538b7/matplotlib_inline-0.1.7.tar.gz", hash = "sha256:8423b23ec666be3d16e16b60bdd8ac4e86e840ebd1dd11a30b9f117f2fa0ab90", size = 8159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/8e/9ad090d3553c280a8060fbf6e24dc1c0c29704ee7d1c372f0c174aa59285/matplotlib_inline-0.1.7-py3-none-any.whl", hash = "sha256:df192d39a4ff8f21b1895d72e6a13f5fcc5099f00fa84384e0ea28c2cc0653ca", size = 9899 },
]

[[package]]
name = "mcp"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/25/ae/588691c45b38f4fbac07fa3d6d50cea44cc6b35d16ddfdf26e17a0467ab2/mcp-1.7.1.tar.gz", hash = "sha256:eb4f1f53bd717f75dda8a1416e00804b831a8f3c331e23447a03b78f04b43a6e", size = 230903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ae/79/fe0e20c3358997a80911af51bad927b5ea2f343ef95ab092b19c9cc48b59/mcp-1.7.1-py3-none-any.whl", hash = "sha256:f7e6108977db6d03418495426c7ace085ba2341b75197f8727f96f9cfd30057a", size = 100365 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "multidict"
version = "6.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/2f/a3470242707058fe856fe59241eee5635d79087100b7042a867368863a27/multidict-6.4.4.tar.gz", hash = "sha256:69ee9e6ba214b5245031b76233dd95408a0fd57fdb019ddcc1ead4790932a8e8", size = 90183 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/2a/e166d2ffbf4b10131b2d5b0e458f7cee7d986661caceae0de8753042d4b2/multidict-6.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:82ffabefc8d84c2742ad19c37f02cde5ec2a1ee172d19944d380f920a340e4b9", size = 64123 },
    { url = "https://files.pythonhosted.org/packages/8c/96/e200e379ae5b6f95cbae472e0199ea98913f03d8c9a709f42612a432932c/multidict-6.4.4-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6a2f58a66fe2c22615ad26156354005391e26a2f3721c3621504cd87c1ea87bf", size = 38049 },
    { url = "https://files.pythonhosted.org/packages/75/fb/47afd17b83f6a8c7fa863c6d23ac5ba6a0e6145ed8a6bcc8da20b2b2c1d2/multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5883d6ee0fd9d8a48e9174df47540b7545909841ac82354c7ae4cbe9952603bd", size = 37078 },
    { url = "https://files.pythonhosted.org/packages/fa/70/1af3143000eddfb19fd5ca5e78393985ed988ac493bb859800fe0914041f/multidict-6.4.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9abcf56a9511653fa1d052bfc55fbe53dbee8f34e68bd6a5a038731b0ca42d15", size = 224097 },
    { url = "https://files.pythonhosted.org/packages/b1/39/d570c62b53d4fba844e0378ffbcd02ac25ca423d3235047013ba2f6f60f8/multidict-6.4.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:6ed5ae5605d4ad5a049fad2a28bb7193400700ce2f4ae484ab702d1e3749c3f9", size = 230768 },
    { url = "https://files.pythonhosted.org/packages/fd/f8/ed88f2c4d06f752b015933055eb291d9bc184936903752c66f68fb3c95a7/multidict-6.4.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bbfcb60396f9bcfa63e017a180c3105b8c123a63e9d1428a36544e7d37ca9e20", size = 231331 },
    { url = "https://files.pythonhosted.org/packages/9c/6f/8e07cffa32f483ab887b0d56bbd8747ac2c1acd00dc0af6fcf265f4a121e/multidict-6.4.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b0f1987787f5f1e2076b59692352ab29a955b09ccc433c1f6b8e8e18666f608b", size = 230169 },
    { url = "https://files.pythonhosted.org/packages/e6/2b/5dcf173be15e42f330110875a2668ddfc208afc4229097312212dc9c1236/multidict-6.4.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1d0121ccce8c812047d8d43d691a1ad7641f72c4f730474878a5aeae1b8ead8c", size = 222947 },
    { url = "https://files.pythonhosted.org/packages/39/75/4ddcbcebe5ebcd6faa770b629260d15840a5fc07ce8ad295a32e14993726/multidict-6.4.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:83ec4967114295b8afd120a8eec579920c882831a3e4c3331d591a8e5bfbbc0f", size = 215761 },
    { url = "https://files.pythonhosted.org/packages/6a/c9/55e998ae45ff15c5608e384206aa71a11e1b7f48b64d166db400b14a3433/multidict-6.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:995f985e2e268deaf17867801b859a282e0448633f1310e3704b30616d269d69", size = 227605 },
    { url = "https://files.pythonhosted.org/packages/04/49/c2404eac74497503c77071bd2e6f88c7e94092b8a07601536b8dbe99be50/multidict-6.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:d832c608f94b9f92a0ec8b7e949be7792a642b6e535fcf32f3e28fab69eeb046", size = 226144 },
    { url = "https://files.pythonhosted.org/packages/62/c5/0cd0c3c6f18864c40846aa2252cd69d308699cb163e1c0d989ca301684da/multidict-6.4.4-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d21c1212171cf7da703c5b0b7a0e85be23b720818aef502ad187d627316d5645", size = 221100 },
    { url = "https://files.pythonhosted.org/packages/71/7b/f2f3887bea71739a046d601ef10e689528d4f911d84da873b6be9194ffea/multidict-6.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:cbebaa076aaecad3d4bb4c008ecc73b09274c952cf6a1b78ccfd689e51f5a5b0", size = 232731 },
    { url = "https://files.pythonhosted.org/packages/e5/b3/d9de808349df97fa75ec1372758701b5800ebad3c46ae377ad63058fbcc6/multidict-6.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:c93a6fb06cc8e5d3628b2b5fda215a5db01e8f08fc15fadd65662d9b857acbe4", size = 229637 },
    { url = "https://files.pythonhosted.org/packages/5e/57/13207c16b615eb4f1745b44806a96026ef8e1b694008a58226c2d8f5f0a5/multidict-6.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8cd8f81f1310182362fb0c7898145ea9c9b08a71081c5963b40ee3e3cac589b1", size = 225594 },
    { url = "https://files.pythonhosted.org/packages/3a/e4/d23bec2f70221604f5565000632c305fc8f25ba953e8ce2d8a18842b9841/multidict-6.4.4-cp313-cp313-win32.whl", hash = "sha256:3e9f1cd61a0ab857154205fb0b1f3d3ace88d27ebd1409ab7af5096e409614cd", size = 35359 },
    { url = "https://files.pythonhosted.org/packages/a7/7a/cfe1a47632be861b627f46f642c1d031704cc1c0f5c0efbde2ad44aa34bd/multidict-6.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:8ffb40b74400e4455785c2fa37eba434269149ec525fc8329858c862e4b35373", size = 38903 },
    { url = "https://files.pythonhosted.org/packages/68/7b/15c259b0ab49938a0a1c8f3188572802704a779ddb294edc1b2a72252e7c/multidict-6.4.4-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:6a602151dbf177be2450ef38966f4be3467d41a86c6a845070d12e17c858a156", size = 68895 },
    { url = "https://files.pythonhosted.org/packages/f1/7d/168b5b822bccd88142e0a3ce985858fea612404edd228698f5af691020c9/multidict-6.4.4-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d2b9712211b860d123815a80b859075d86a4d54787e247d7fbee9db6832cf1c", size = 40183 },
    { url = "https://files.pythonhosted.org/packages/e0/b7/d4b8d98eb850ef28a4922ba508c31d90715fd9b9da3801a30cea2967130b/multidict-6.4.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:d2fa86af59f8fc1972e121ade052145f6da22758f6996a197d69bb52f8204e7e", size = 39592 },
    { url = "https://files.pythonhosted.org/packages/18/28/a554678898a19583548e742080cf55d169733baf57efc48c2f0273a08583/multidict-6.4.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50855d03e9e4d66eab6947ba688ffb714616f985838077bc4b490e769e48da51", size = 226071 },
    { url = "https://files.pythonhosted.org/packages/ee/dc/7ba6c789d05c310e294f85329efac1bf5b450338d2542498db1491a264df/multidict-6.4.4-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5bce06b83be23225be1905dcdb6b789064fae92499fbc458f59a8c0e68718601", size = 222597 },
    { url = "https://files.pythonhosted.org/packages/24/4f/34eadbbf401b03768dba439be0fb94b0d187facae9142821a3d5599ccb3b/multidict-6.4.4-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:66ed0731f8e5dfd8369a883b6e564aca085fb9289aacabd9decd70568b9a30de", size = 228253 },
    { url = "https://files.pythonhosted.org/packages/c0/e6/493225a3cdb0d8d80d43a94503fc313536a07dae54a3f030d279e629a2bc/multidict-6.4.4-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:329ae97fc2f56f44d91bc47fe0972b1f52d21c4b7a2ac97040da02577e2daca2", size = 226146 },
    { url = "https://files.pythonhosted.org/packages/2f/70/e411a7254dc3bff6f7e6e004303b1b0591358e9f0b7c08639941e0de8bd6/multidict-6.4.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c27e5dcf520923d6474d98b96749e6805f7677e93aaaf62656005b8643f907ab", size = 220585 },
    { url = "https://files.pythonhosted.org/packages/08/8f/beb3ae7406a619100d2b1fb0022c3bb55a8225ab53c5663648ba50dfcd56/multidict-6.4.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:058cc59b9e9b143cc56715e59e22941a5d868c322242278d28123a5d09cdf6b0", size = 212080 },
    { url = "https://files.pythonhosted.org/packages/9c/ec/355124e9d3d01cf8edb072fd14947220f357e1c5bc79c88dff89297e9342/multidict-6.4.4-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:69133376bc9a03f8c47343d33f91f74a99c339e8b58cea90433d8e24bb298031", size = 226558 },
    { url = "https://files.pythonhosted.org/packages/fd/22/d2b95cbebbc2ada3be3812ea9287dcc9712d7f1a012fad041770afddb2ad/multidict-6.4.4-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d6b15c55721b1b115c5ba178c77104123745b1417527ad9641a4c5e2047450f0", size = 212168 },
    { url = "https://files.pythonhosted.org/packages/4d/c5/62bfc0b2f9ce88326dbe7179f9824a939c6c7775b23b95de777267b9725c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:a887b77f51d3d41e6e1a63cf3bc7ddf24de5939d9ff69441387dfefa58ac2e26", size = 217970 },
    { url = "https://files.pythonhosted.org/packages/79/74/977cea1aadc43ff1c75d23bd5bc4768a8fac98c14e5878d6ee8d6bab743c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:632a3bf8f1787f7ef7d3c2f68a7bde5be2f702906f8b5842ad6da9d974d0aab3", size = 226980 },
    { url = "https://files.pythonhosted.org/packages/48/fc/cc4a1a2049df2eb84006607dc428ff237af38e0fcecfdb8a29ca47b1566c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:a145c550900deb7540973c5cdb183b0d24bed6b80bf7bddf33ed8f569082535e", size = 220641 },
    { url = "https://files.pythonhosted.org/packages/3b/6a/a7444d113ab918701988d4abdde373dbdfd2def7bd647207e2bf645c7eac/multidict-6.4.4-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cc5d83c6619ca5c9672cb78b39ed8542f1975a803dee2cda114ff73cbb076edd", size = 221728 },
    { url = "https://files.pythonhosted.org/packages/2b/b0/fdf4c73ad1c55e0f4dbbf2aa59dd37037334091f9a4961646d2b7ac91a86/multidict-6.4.4-cp313-cp313t-win32.whl", hash = "sha256:3312f63261b9df49be9d57aaa6abf53a6ad96d93b24f9cc16cf979956355ce6e", size = 41913 },
    { url = "https://files.pythonhosted.org/packages/8e/92/27989ecca97e542c0d01d05a98a5ae12198a243a9ee12563a0313291511f/multidict-6.4.4-cp313-cp313t-win_amd64.whl", hash = "sha256:ba852168d814b2c73333073e1c7116d9395bea69575a01b0b3c89d2d5a87c8fb", size = 46112 },
    { url = "https://files.pythonhosted.org/packages/84/5d/e17845bb0fa76334477d5de38654d27946d5b5d3695443987a094a71b440/multidict-6.4.4-py3-none-any.whl", hash = "sha256:bd4557071b561a8b3b6075c3ce93cf9bfb6182cb241805c3d66ced3b75eff4ac", size = 10481 },
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/6e/371856a3fb9d31ca8dac321cda606860fa4548858c0cc45d9d1d4ca2628b/mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558", size = 6343 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505", size = 4963 },
]

[[package]]
name = "narwhals"
version = "1.48.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/cd/7395d6c247e821cba6243e9f7ed202fae3fefef643c96581b5ecab927bad/narwhals-1.48.0.tar.gz", hash = "sha256:7243b456cbdb60edb148731a8f9b203f473a373a249ad66c699362508730e63f", size = 515112 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/75/72/5406044d4c251f3d8f78cec05b74839d0332d34c9e94b59120f3697ecf48/narwhals-1.48.0-py3-none-any.whl", hash = "sha256:2bbddc3adeed0c5b15ead8fe61f1d5e459f00c1d2fa60921e52a0f9bdc06077d", size = 376866 },
]

[[package]]
name = "numpy"
version = "2.2.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dc/b2/ce4b867d8cd9c0ee84938ae1e6a6f7926ebf928c9090d036fc3c6a04f946/numpy-2.2.5.tar.gz", hash = "sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291", size = 20273920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e2/a0/0aa7f0f4509a2e07bd7a509042967c2fab635690d4f48c6c7b3afd4f448c/numpy-2.2.5-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4", size = 20935102 },
    { url = "https://files.pythonhosted.org/packages/7e/e4/a6a9f4537542912ec513185396fce52cdd45bdcf3e9d921ab02a93ca5aa9/numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f", size = 14191709 },
    { url = "https://files.pythonhosted.org/packages/be/65/72f3186b6050bbfe9c43cb81f9df59ae63603491d36179cf7a7c8d216758/numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9", size = 5149173 },
    { url = "https://files.pythonhosted.org/packages/e5/e9/83e7a9432378dde5802651307ae5e9ea07bb72b416728202218cd4da2801/numpy-2.2.5-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191", size = 6684502 },
    { url = "https://files.pythonhosted.org/packages/ea/27/b80da6c762394c8ee516b74c1f686fcd16c8f23b14de57ba0cad7349d1d2/numpy-2.2.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372", size = 14084417 },
    { url = "https://files.pythonhosted.org/packages/aa/fc/ebfd32c3e124e6a1043e19c0ab0769818aa69050ce5589b63d05ff185526/numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d", size = 16133807 },
    { url = "https://files.pythonhosted.org/packages/bf/9b/4cc171a0acbe4666f7775cfd21d4eb6bb1d36d3a0431f48a73e9212d2278/numpy-2.2.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7", size = 15575611 },
    { url = "https://files.pythonhosted.org/packages/a3/45/40f4135341850df48f8edcf949cf47b523c404b712774f8855a64c96ef29/numpy-2.2.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73", size = 17895747 },
    { url = "https://files.pythonhosted.org/packages/f8/4c/b32a17a46f0ffbde8cc82df6d3daeaf4f552e346df143e1b188a701a8f09/numpy-2.2.5-cp313-cp313-win32.whl", hash = "sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b", size = 6309594 },
    { url = "https://files.pythonhosted.org/packages/13/ae/72e6276feb9ef06787365b05915bfdb057d01fceb4a43cb80978e518d79b/numpy-2.2.5-cp313-cp313-win_amd64.whl", hash = "sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471", size = 12638356 },
    { url = "https://files.pythonhosted.org/packages/79/56/be8b85a9f2adb688e7ded6324e20149a03541d2b3297c3ffc1a73f46dedb/numpy-2.2.5-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6", size = 20963778 },
    { url = "https://files.pythonhosted.org/packages/ff/77/19c5e62d55bff507a18c3cdff82e94fe174957bad25860a991cac719d3ab/numpy-2.2.5-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba", size = 14207279 },
    { url = "https://files.pythonhosted.org/packages/75/22/aa11f22dc11ff4ffe4e849d9b63bbe8d4ac6d5fae85ddaa67dfe43be3e76/numpy-2.2.5-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133", size = 5199247 },
    { url = "https://files.pythonhosted.org/packages/4f/6c/12d5e760fc62c08eded0394f62039f5a9857f758312bf01632a81d841459/numpy-2.2.5-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376", size = 6711087 },
    { url = "https://files.pythonhosted.org/packages/ef/94/ece8280cf4218b2bee5cec9567629e61e51b4be501e5c6840ceb593db945/numpy-2.2.5-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19", size = 14059964 },
    { url = "https://files.pythonhosted.org/packages/39/41/c5377dac0514aaeec69115830a39d905b1882819c8e65d97fc60e177e19e/numpy-2.2.5-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0", size = 16121214 },
    { url = "https://files.pythonhosted.org/packages/db/54/3b9f89a943257bc8e187145c6bc0eb8e3d615655f7b14e9b490b053e8149/numpy-2.2.5-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a", size = 15575788 },
    { url = "https://files.pythonhosted.org/packages/b1/c4/2e407e85df35b29f79945751b8f8e671057a13a376497d7fb2151ba0d290/numpy-2.2.5-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066", size = 17893672 },
    { url = "https://files.pythonhosted.org/packages/29/7e/d0b44e129d038dba453f00d0e29ebd6eaf2f06055d72b95b9947998aca14/numpy-2.2.5-cp313-cp313t-win32.whl", hash = "sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e", size = 6377102 },
    { url = "https://files.pythonhosted.org/packages/63/be/b85e4aa4bf42c6502851b971f1c326d583fcc68227385f92089cf50a7b45/numpy-2.2.5-cp313-cp313t-win_amd64.whl", hash = "sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8", size = 12750096 },
]

[[package]]
name = "openai"
version = "1.84.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/a3/128caf24e116f48fad3e4d5122cdf84db06c5127911849d51663c66158c8/openai-1.84.0.tar.gz", hash = "sha256:4caa43bdab262cc75680ce1a2322cfc01626204074f7e8d9939ab372acf61698", size = 467066 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/10/f245db006a860dbc1f2e2c8382e0a1762c7753e7971ba43a1dc3f3ec1404/openai-1.84.0-py3-none-any.whl", hash = "sha256:7ec4436c3c933d68dc0f5a0cef0cb3dbc0864a54d62bddaf2ed5f3d521844711", size = 725512 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "parso"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/66/94/68e2e17afaa9169cf6412ab0f28623903be73d1b32e208d9e8e541bb086d/parso-0.8.4.tar.gz", hash = "sha256:eb3a7b58240fb99099a345571deecc0f9540ea5f4dd2fe14c2a99d6b281ab92d", size = 400609 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c6/ac/dac4a63f978e4dcb3c6d3a78c4d8e0192a113d288502a1216950c41b1027/parso-0.8.4-py2.py3-none-any.whl", hash = "sha256:a418670a20291dacd2dddc80c377c5c3791378ee1e8d12bffc35420643d43f18", size = 103650 },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191 },
]

[[package]]
name = "pexpect"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "ptyprocess" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/92/cc564bf6381ff43ce1f4d06852fc19a2f11d180f23dc32d9588bee2f149d/pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f", size = 166450 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523", size = 63772 },
]

[[package]]
name = "platformdirs"
version = "4.3.8"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/8b/3c73abc9c759ecd3f1f7ceff6685840859e8070c4d947c93fae71f6a0bf2/platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc", size = 21362 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/39/979e8e21520d4e47a0bbe349e2713c0aac6f3d853d0e5b34d76206c439aa/platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4", size = 18567 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "proto-plus"
version = "1.26.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f4/ac/87285f15f7cce6d4a008f33f1757fb5a13611ea8914eb58c3d0d26243468/proto_plus-1.26.1.tar.gz", hash = "sha256:21a515a4c4c0088a773899e23c7bbade3d18f9c66c73edd4c7ee3816bc96a012", size = 56142 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl", hash = "sha256:13285478c2dcf2abb829db158e1047e2f1e8d63a077d94263c2b88b043c75a66", size = 50163 },
]

[[package]]
name = "protobuf"
version = "5.29.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/17/7d/b9dca7365f0e2c4fa7c193ff795427cfa6290147e5185ab11ece280a18e7/protobuf-5.29.4.tar.gz", hash = "sha256:4f1dfcd7997b31ef8f53ec82781ff434a28bf71d9102ddde14d076adcfc78c99", size = 424902 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/b2/043a1a1a20edd134563699b0e91862726a0dc9146c090743b6c44d798e75/protobuf-5.29.4-cp310-abi3-win32.whl", hash = "sha256:13eb236f8eb9ec34e63fc8b1d6efd2777d062fa6aaa68268fb67cf77f6839ad7", size = 422709 },
    { url = "https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl", hash = "sha256:bcefcdf3976233f8a502d265eb65ea740c989bacc6c30a58290ed0e519eb4b8d", size = 434506 },
    { url = "https://files.pythonhosted.org/packages/46/de/7c126bbb06aa0f8a7b38aaf8bd746c514d70e6a2a3f6dd460b3b7aad7aae/protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:307ecba1d852ec237e9ba668e087326a67564ef83e45a0189a772ede9e854dd0", size = 417826 },
    { url = "https://files.pythonhosted.org/packages/a2/b5/bade14ae31ba871a139aa45e7a8183d869efe87c34a4850c87b936963261/protobuf-5.29.4-cp38-abi3-manylinux2014_aarch64.whl", hash = "sha256:aec4962f9ea93c431d5714ed1be1c93f13e1a8618e70035ba2b0564d9e633f2e", size = 319574 },
    { url = "https://files.pythonhosted.org/packages/46/88/b01ed2291aae68b708f7d334288ad5fb3e7aa769a9c309c91a0d55cb91b0/protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl", hash = "sha256:d7d3f7d1d5a66ed4942d4fefb12ac4b14a29028b209d4bfb25c68ae172059922", size = 319672 },
    { url = "https://files.pythonhosted.org/packages/12/fb/a586e0c973c95502e054ac5f81f88394f24ccc7982dac19c515acd9e2c93/protobuf-5.29.4-py3-none-any.whl", hash = "sha256:3fde11b505e1597f71b875ef2fc52062b6a9740e5f7c8997ce878b6009145862", size = 172551 },
]

[[package]]
name = "psutil"
version = "7.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2a/80/336820c1ad9286a4ded7e845b2eccfcb27851ab8ac6abece774a6ff4d3de/psutil-7.0.0.tar.gz", hash = "sha256:7be9c3eba38beccb6495ea33afd982a44074b78f28c434a1f51cc07fd315c456", size = 497003 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ed/e6/2d26234410f8b8abdbf891c9da62bee396583f713fb9f3325a4760875d22/psutil-7.0.0-cp36-abi3-macosx_10_9_x86_64.whl", hash = "sha256:101d71dc322e3cffd7cea0650b09b3d08b8e7c4109dd6809fe452dfd00e58b25", size = 238051 },
    { url = "https://files.pythonhosted.org/packages/04/8b/30f930733afe425e3cbfc0e1468a30a18942350c1a8816acfade80c005c4/psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl", hash = "sha256:39db632f6bb862eeccf56660871433e111b6ea58f2caea825571951d4b6aa3da", size = 239535 },
    { url = "https://files.pythonhosted.org/packages/2a/ed/d362e84620dd22876b55389248e522338ed1bf134a5edd3b8231d7207f6d/psutil-7.0.0-cp36-abi3-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1fcee592b4c6f146991ca55919ea3d1f8926497a713ed7faaf8225e174581e91", size = 275004 },
    { url = "https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b1388a4f6875d7e2aff5c4ca1cc16c545ed41dd8bb596cefea80111db353a34", size = 277986 },
    { url = "https://files.pythonhosted.org/packages/eb/a2/709e0fe2f093556c17fbafda93ac032257242cabcc7ff3369e2cb76a97aa/psutil-7.0.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5f098451abc2828f7dc6b58d44b532b22f2088f4999a937557b603ce72b1993", size = 279544 },
    { url = "https://files.pythonhosted.org/packages/50/e6/eecf58810b9d12e6427369784efe814a1eec0f492084ce8eb8f4d89d6d61/psutil-7.0.0-cp37-abi3-win32.whl", hash = "sha256:ba3fcef7523064a6c9da440fc4d6bd07da93ac726b5733c29027d7dc95b39d99", size = 241053 },
    { url = "https://files.pythonhosted.org/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:4cf3d4eb1aa9b348dec30105c55cd9b7d4629285735a102beb4441e38db90553", size = 244885 },
]

[[package]]
name = "ptyprocess"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/20/e5/16ff212c1e452235a90aeb09066144d0c5a6a8c0834397e03f5224495c4e/ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220", size = 70762 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35", size = 13993 },
]

[[package]]
name = "pure-eval"
version = "0.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/05/0a34433a064256a578f1783a10da6df098ceaa4a57bbeaa96a6c0352786b/pure_eval-0.2.3.tar.gz", hash = "sha256:5f4e983f40564c576c7c8635ae88db5956bb2229d7e9237d03b3c0b0190eaf42", size = 19752 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8e/37/efad0257dc6e593a18957422533ff0f87ede7c9c6ea010a2177d738fb82f/pure_eval-0.2.3-py3-none-any.whl", hash = "sha256:1db8e35b67b3d218d818ae653e27f06c3aa420901fa7b081ca98cbedc874e0d0", size = 11842 },
]

[[package]]
name = "pyasn1"
version = "0.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ba/e9/01f1a64245b89f039897cb0130016d79f77d52669aae6ee7b159a6c4c018/pyasn1-0.6.1.tar.gz", hash = "sha256:6f580d2bdd84365380830acf45550f2511469f673cb4a5ae3857a3170128b034", size = 145322 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl", hash = "sha256:0d632f46f2ba09143da3a8afe9e33fb6f92fa2320ab7e886e2d0f7672af84629", size = 83135 },
]

[[package]]
name = "pyasn1-modules"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e9/e6/78ebbb10a8c8e4b61a59249394a4a594c1a7af95593dc933a349c8d00964/pyasn1_modules-0.4.2.tar.gz", hash = "sha256:677091de870a80aae844b1ca6134f54652fa2c8c5a52aa396440ac3106e941e6", size = 307892 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl", hash = "sha256:29253a9207ce32b64c3ac6600edc75368f98473906e8fd1043bd6b5b1de2c14a", size = 181259 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pymdown-extensions"
version = "10.16"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown" },
    { name = "pyyaml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/0a/c06b542ac108bfc73200677309cd9188a3a01b127a63f20cadc18d873d88/pymdown_extensions-10.16.tar.gz", hash = "sha256:71dac4fca63fabeffd3eb9038b756161a33ec6e8d230853d3cecf562155ab3de", size = 853197 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/d4/10bb14004d3c792811e05e21b5e5dcae805aacb739bd12a0540967b99592/pymdown_extensions-10.16-py3-none-any.whl", hash = "sha256:f5dd064a4db588cb2d95229fc4ee63a1b16cc8b4d0e6145c0899ed8723da1df2", size = 266143 },
]

[[package]]
name = "pyparsing"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/22/f1129e69d94ffff626bdb5c835506b3a5b4f3d070f17ea295e12c2c6f60f/pyparsing-3.2.3.tar.gz", hash = "sha256:b9c13f1ab8b3b542f72e28f634bad4de758ab3ce4546e4301970ad6fa77c38be", size = 1088608 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/e7/df2285f3d08fee213f2d041540fa4fc9ca6c2d44cf36d3a035bf2a8d2bcc/pyparsing-3.2.3-py3-none-any.whl", hash = "sha256:a749938e02d6fd0b59b356ca504a24982314bb090c383e3cf201c95ef7e2bfcf", size = 111120 },
]

[[package]]
name = "pyperclip"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz", hash = "sha256:b7de0142ddc81bfc5c7507eea19da920b92252b548b96186caf94a5e2527d310", size = 20961 }

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "pytest-asyncio"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d0/d4/14f53324cb1a6381bef29d698987625d80052bb33932d8e7cbf9b337b17c/pytest_asyncio-1.0.0.tar.gz", hash = "sha256:d15463d13f4456e1ead2594520216b225a16f781e144f8fdf6c5bb4667c48b3f", size = 46960 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/05/ce271016e351fddc8399e546f6e23761967ee09c8c568bbfbecb0c150171/pytest_asyncio-1.0.0-py3-none-any.whl", hash = "sha256:4f024da9f1ef945e680dc68610b52550e36590a67fd31bb3b4943979a1f90ef3", size = 15976 },
]

[[package]]
name = "pytest-xdist"
version = "3.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "execnet" },
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/dc/865845cfe987b21658e871d16e0a24e871e00884c545f246dd8f6f69edda/pytest_xdist-3.7.0.tar.gz", hash = "sha256:f9248c99a7c15b7d2f90715df93610353a485827bc06eefb6566d23f6400f126", size = 87550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/b2/0e802fde6f1c5b2f7ae7e9ad42b83fd4ecebac18a8a8c2f2f14e39dce6e1/pytest_xdist-3.7.0-py3-none-any.whl", hash = "sha256:7d3fbd255998265052435eb9daa4e99b62e6fb9cfb6efd1f858d4d8c0c7f0ca0", size = 46142 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-json-logger"
version = "3.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz", hash = "sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84", size = 16642 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl", hash = "sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7", size = 15163 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "readabilipy"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "html5lib" },
    { name = "lxml" },
    { name = "regex" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b8/e4/260a202516886c2e0cc6e6ae96d1f491792d829098886d9529a2439fbe8e/readabilipy-0.3.0.tar.gz", hash = "sha256:e13313771216953935ac031db4234bdb9725413534bfb3c19dbd6caab0887ae0", size = 35491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dd/46/8a640c6de1a6c6af971f858b2fb178ca5e1db91f223d8ba5f40efe1491e5/readabilipy-0.3.0-py3-none-any.whl", hash = "sha256:d106da0fad11d5fdfcde21f5c5385556bfa8ff0258483037d39ea6b1d6db3943", size = 22158 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "rsa"
version = "4.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/8a/22b7beea3ee0d44b1916c0c1cb0ee3af23b700b6da9f04991899d0c555d4/rsa-4.9.1.tar.gz", hash = "sha256:e7bdbfdb5497da4c07dfd35530e1a902659db6ff241e39d9953cad06ebd0ae75", size = 29034 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl", hash = "sha256:68635866661c6836b8d39430f97a996acbd61bfa49406748ea243539fe239762", size = 34696 },
]

[[package]]
name = "ruff"
version = "0.11.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ed/da/9c6f995903b4d9474b39da91d2d626659af3ff1eeb43e9ae7c119349dba6/ruff-0.11.13.tar.gz", hash = "sha256:26fa247dc68d1d4e72c179e08889a25ac0c7ba4d78aecfc835d49cbfd60bf514", size = 4282054 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7d/ce/a11d381192966e0b4290842cc8d4fac7dc9214ddf627c11c1afff87da29b/ruff-0.11.13-py3-none-linux_armv6l.whl", hash = "sha256:4bdfbf1240533f40042ec00c9e09a3aade6f8c10b6414cf11b519488d2635d46", size = 10292516 },
    { url = "https://files.pythonhosted.org/packages/78/db/87c3b59b0d4e753e40b6a3b4a2642dfd1dcaefbff121ddc64d6c8b47ba00/ruff-0.11.13-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:aef9c9ed1b5ca28bb15c7eac83b8670cf3b20b478195bd49c8d756ba0a36cf48", size = 11106083 },
    { url = "https://files.pythonhosted.org/packages/77/79/d8cec175856ff810a19825d09ce700265f905c643c69f45d2b737e4a470a/ruff-0.11.13-py3-none-macosx_11_0_arm64.whl", hash = "sha256:53b15a9dfdce029c842e9a5aebc3855e9ab7771395979ff85b7c1dedb53ddc2b", size = 10436024 },
    { url = "https://files.pythonhosted.org/packages/8b/5b/f6d94f2980fa1ee854b41568368a2e1252681b9238ab2895e133d303538f/ruff-0.11.13-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ab153241400789138d13f362c43f7edecc0edfffce2afa6a68434000ecd8f69a", size = 10646324 },
    { url = "https://files.pythonhosted.org/packages/6c/9c/b4c2acf24ea4426016d511dfdc787f4ce1ceb835f3c5fbdbcb32b1c63bda/ruff-0.11.13-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:6c51f93029d54a910d3d24f7dd0bb909e31b6cd989a5e4ac513f4eb41629f0dc", size = 10174416 },
    { url = "https://files.pythonhosted.org/packages/f3/10/e2e62f77c65ede8cd032c2ca39c41f48feabedb6e282bfd6073d81bb671d/ruff-0.11.13-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1808b3ed53e1a777c2ef733aca9051dc9bf7c99b26ece15cb59a0320fbdbd629", size = 11724197 },
    { url = "https://files.pythonhosted.org/packages/bb/f0/466fe8469b85c561e081d798c45f8a1d21e0b4a5ef795a1d7f1a9a9ec182/ruff-0.11.13-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:d28ce58b5ecf0f43c1b71edffabe6ed7f245d5336b17805803312ec9bc665933", size = 12511615 },
    { url = "https://files.pythonhosted.org/packages/17/0e/cefe778b46dbd0cbcb03a839946c8f80a06f7968eb298aa4d1a4293f3448/ruff-0.11.13-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:55e4bc3a77842da33c16d55b32c6cac1ec5fb0fbec9c8c513bdce76c4f922165", size = 12117080 },
    { url = "https://files.pythonhosted.org/packages/5d/2c/caaeda564cbe103bed145ea557cb86795b18651b0f6b3ff6a10e84e5a33f/ruff-0.11.13-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:633bf2c6f35678c56ec73189ba6fa19ff1c5e4807a78bf60ef487b9dd272cc71", size = 11326315 },
    { url = "https://files.pythonhosted.org/packages/75/f0/782e7d681d660eda8c536962920c41309e6dd4ebcea9a2714ed5127d44bd/ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ffbc82d70424b275b089166310448051afdc6e914fdab90e08df66c43bb5ca9", size = 11555640 },
    { url = "https://files.pythonhosted.org/packages/5d/d4/3d580c616316c7f07fb3c99dbecfe01fbaea7b6fd9a82b801e72e5de742a/ruff-0.11.13-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:4a9ddd3ec62a9a89578c85842b836e4ac832d4a2e0bfaad3b02243f930ceafcc", size = 10507364 },
    { url = "https://files.pythonhosted.org/packages/5a/dc/195e6f17d7b3ea6b12dc4f3e9de575db7983db187c378d44606e5d503319/ruff-0.11.13-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:d237a496e0778d719efb05058c64d28b757c77824e04ffe8796c7436e26712b7", size = 10141462 },
    { url = "https://files.pythonhosted.org/packages/f4/8e/39a094af6967faa57ecdeacb91bedfb232474ff8c3d20f16a5514e6b3534/ruff-0.11.13-py3-none-musllinux_1_2_i686.whl", hash = "sha256:26816a218ca6ef02142343fd24c70f7cd8c5aa6c203bca284407adf675984432", size = 11121028 },
    { url = "https://files.pythonhosted.org/packages/5a/c0/b0b508193b0e8a1654ec683ebab18d309861f8bd64e3a2f9648b80d392cb/ruff-0.11.13-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:51c3f95abd9331dc5b87c47ac7f376db5616041173826dfd556cfe3d4977f492", size = 11602992 },
    { url = "https://files.pythonhosted.org/packages/7c/91/263e33ab93ab09ca06ce4f8f8547a858cc198072f873ebc9be7466790bae/ruff-0.11.13-py3-none-win32.whl", hash = "sha256:96c27935418e4e8e77a26bb05962817f28b8ef3843a6c6cc49d8783b5507f250", size = 10474944 },
    { url = "https://files.pythonhosted.org/packages/46/f4/7c27734ac2073aae8efb0119cae6931b6fb48017adf048fdf85c19337afc/ruff-0.11.13-py3-none-win_amd64.whl", hash = "sha256:29c3189895a8a6a657b7af4e97d330c8a3afd2c9c8f46c81e2fc5a31866517e3", size = 11548669 },
    { url = "https://files.pythonhosted.org/packages/ec/bf/b273dd11673fed8a6bd46032c0ea2a04b2ac9bfa9c628756a5856ba113b0/ruff-0.11.13-py3-none-win_arm64.whl", hash = "sha256:b4385285e9179d608ff1d2fb9922062663c658605819a6876d8beef0c30b7f3b", size = 10683928 },
]

[[package]]
name = "sentencepiece"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz", hash = "sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843", size = 2632106 }

[[package]]
name = "shapely"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fb/fe/3b0d2f828ffaceadcdcb51b75b9c62d98e62dd95ce575278de35f24a1c20/shapely-2.1.0.tar.gz", hash = "sha256:2cbe90e86fa8fc3ca8af6ffb00a77b246b918c7cf28677b7c21489b678f6b02e", size = 313617 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/77/4e368704b2193e74498473db4461d697cc6083c96f8039367e59009d78bd/shapely-2.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b64423295b563f43a043eb786e7a03200ebe68698e36d2b4b1c39f31dfb50dfb", size = 1830029 },
    { url = "https://files.pythonhosted.org/packages/71/3c/d888597bda680e4de987316b05ca9db07416fa29523beff64f846503302f/shapely-2.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1b5578f45adc25b235b22d1ccb9a0348c8dc36f31983e57ea129a88f96f7b870", size = 1637999 },
    { url = "https://files.pythonhosted.org/packages/03/8d/ee0e23b7ef88fba353c63a81f1f329c77f5703835db7b165e7c0b8b7f839/shapely-2.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1a7e83d383b27f02b684e50ab7f34e511c92e33b6ca164a6a9065705dd64bcb", size = 2929348 },
    { url = "https://files.pythonhosted.org/packages/d1/a7/5c9cb413e4e2ce52c16be717e94abd40ce91b1f8974624d5d56154c5d40b/shapely-2.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:942031eb4d8f7b3b22f43ba42c09c7aa3d843aa10d5cc1619fe816e923b66e55", size = 3048973 },
    { url = "https://files.pythonhosted.org/packages/84/23/45b90c0bd2157b238490ca56ef2eedf959d3514c7d05475f497a2c88b6d9/shapely-2.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d2843c456a2e5627ee6271800f07277c0d2652fb287bf66464571a057dbc00b3", size = 3873148 },
    { url = "https://files.pythonhosted.org/packages/c0/bc/ed7d5d37f5395166042576f0c55a12d7e56102799464ba7ea3a72a38c769/shapely-2.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8c4b17469b7f39a5e6a7cfea79f38ae08a275427f41fe8b48c372e1449147908", size = 4052655 },
    { url = "https://files.pythonhosted.org/packages/c0/8f/a1dafbb10d20d1c569f2db3fb1235488f624dafe8469e8ce65356800ba31/shapely-2.1.0-cp313-cp313-win32.whl", hash = "sha256:30e967abd08fce49513d4187c01b19f139084019f33bec0673e8dbeb557c45e4", size = 1526600 },
    { url = "https://files.pythonhosted.org/packages/e3/f0/9f8cdf2258d7aed742459cea51c70d184de92f5d2d6f5f7f1ded90a18c31/shapely-2.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:1dc8d4364483a14aba4c844b7bd16a6fa3728887e2c33dfa1afa34a3cf4d08a5", size = 1707115 },
    { url = "https://files.pythonhosted.org/packages/75/ed/32952df461753a65b3e5d24c8efb361d3a80aafaef0b70d419063f6f2c11/shapely-2.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:673e073fea099d1c82f666fb7ab0a00a77eff2999130a69357ce11941260d855", size = 1824847 },
    { url = "https://files.pythonhosted.org/packages/ff/b9/2284de512af30b02f93ddcdd2e5c79834a3cf47fa3ca11b0f74396feb046/shapely-2.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:6d1513f915a56de67659fe2047c1ad5ff0f8cbff3519d1e74fced69c9cb0e7da", size = 1631035 },
    { url = "https://files.pythonhosted.org/packages/35/16/a59f252a7e736b73008f10d0950ffeeb0d5953be7c0bdffd39a02a6ba310/shapely-2.1.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0d6a7043178890b9e028d80496ff4c79dc7629bff4d78a2f25323b661756bab8", size = 2968639 },
    { url = "https://files.pythonhosted.org/packages/a5/0a/6a20eca7b0092cfa243117e8e145a58631a4833a0a519ec9b445172e83a0/shapely-2.1.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb638378dc3d76f7e85b67d7e2bb1366811912430ac9247ac00c127c2b444cdc", size = 3055713 },
    { url = "https://files.pythonhosted.org/packages/fb/44/eeb0c7583b1453d1cf7a319a1d738e08f98a5dc993fa1ef3c372983e4cb5/shapely-2.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:737124e87d91d616acf9a911f74ac55e05db02a43a6a7245b3d663817b876055", size = 3890478 },
    { url = "https://files.pythonhosted.org/packages/5d/6e/37ff3c6af1d408cacb0a7d7bfea7b8ab163a5486e35acb08997eae9d8756/shapely-2.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e6c229e7bb87aae5df82fa00b6718987a43ec168cc5affe095cca59d233f314", size = 4036148 },
    { url = "https://files.pythonhosted.org/packages/c8/6a/8c0b7de3aeb5014a23f06c5e9d3c7852ebcf0d6b00fe660b93261e310e24/shapely-2.1.0-cp313-cp313t-win32.whl", hash = "sha256:a9580bda119b1f42f955aa8e52382d5c73f7957e0203bc0c0c60084846f3db94", size = 1535993 },
    { url = "https://files.pythonhosted.org/packages/a8/91/ae80359a58409d52e4d62c7eacc7eb3ddee4b9135f1db884b6a43cf2e174/shapely-2.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e8ff4e5cfd799ba5b6f37b5d5527dbd85b4a47c65b6d459a03d0962d2a9d4d10", size = 1717777 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677 },
]

[[package]]
name = "sqlparse"
version = "0.5.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e5/40/edede8dd6977b0d3da179a342c198ed100dd2aba4be081861ee5911e4da4/sqlparse-0.5.3.tar.gz", hash = "sha256:09f67787f56a0b16ecdbde1bfc7f5d9c3371ca683cfeaa8e6ff60b4807ec9272", size = 84999 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl", hash = "sha256:cf2196ed3418f3ba5de6af7e82c694a9fbdbfecccdfc72e281548517081f16ca", size = 44415 },
]

[[package]]
name = "sse-starlette"
version = "2.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/35/7d8d94eb0474352d55f60f80ebc30f7e59441a29e18886a6425f0bccd0d3/sse_starlette-2.3.3.tar.gz", hash = "sha256:fdd47c254aad42907cfd5c5b83e2282be15be6c51197bf1a9b70b8e990522072", size = 17499 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/20/52fdb5ebb158294b0adb5662235dd396fc7e47aa31c293978d8d8942095a/sse_starlette-2.3.3-py3-none-any.whl", hash = "sha256:8b0a0ced04a329ff7341b01007580dd8cf71331cc21c0ccea677d500618da1e0", size = 10235 },
]

[[package]]
name = "stack-data"
version = "0.6.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "asttokens" },
    { name = "executing" },
    { name = "pure-eval" },
]
sdist = { url = "https://files.pythonhosted.org/packages/28/e3/55dcc2cfbc3ca9c29519eb6884dd1415ecb53b0e934862d3559ddcb7e20b/stack_data-0.6.3.tar.gz", hash = "sha256:836a778de4fec4dcd1dcd89ed8abff8a221f58308462e1c4aa2a3cf30148f0b9", size = 44707 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl", hash = "sha256:d5558e0c25a4cb0853cddad3d77da9891a08cb85dd9f9f91b9f8cd66e511e695", size = 24521 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tabulate"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ec/fe/802052aecb21e3797b8f7902564ab6ea0d60ff8ca23952079064155d1ae1/tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c", size = 81090 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f", size = 35252 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tomlkit"
version = "0.13.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/18/0bbf3884e9eaa38819ebe46a7bd25dcd56b67434402b66a58c4b8e552575/tomlkit-0.13.3.tar.gz", hash = "sha256:430cf247ee57df2b94ee3fbe588e71d362a941ebb545dec29b53961d61add2a1", size = 185207 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl", hash = "sha256:c89c649d79ee40629a9fda55f8ace8c6a1b42deb912b2a8fd8d942ddadb606b0", size = 38901 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "traitlets"
version = "5.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/eb/79/72064e6a701c2183016abbbfedaba506d81e30e232a68c9f0d6f6fcd1574/traitlets-5.14.3.tar.gz", hash = "sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7", size = 161621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/c0/8f5d070730d7836adc9c9b6408dec68c6ced86b304a9b26a14df072a6e8c/traitlets-5.14.3-py3-none-any.whl", hash = "sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f", size = 85359 },
]

[[package]]
name = "typer"
version = "0.15.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/1a/5f36851f439884bcfe8539f6a20ff7516e7b60f319bbaf69a90dc35cc2eb/typer-0.15.3.tar.gz", hash = "sha256:818873625d0569653438316567861899f7e9972f2e6e0c16dab608345ced713c", size = 101641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/48/20/9d953de6f4367163d23ec823200eb3ecb0050a2609691e512c8b95827a9b/typer-0.15.3-py3-none-any.whl", hash = "sha256:c86a65ad77ca531f03de08d1b9cb67cd09ad02ddddf4b34745b5008f43b239bd", size = 45253 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uritemplate"
version = "4.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/60/f174043244c5306c9988380d2cb10009f91563fc4b31293d27e17201af56/uritemplate-4.2.0.tar.gz", hash = "sha256:480c2ed180878955863323eea31b0ede668795de182617fef9c6ca09e6ec9d0e", size = 33267 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/99/3ae339466c9183ea5b8ae87b34c0b897eda475d2aec2307cae60e5cd4f29/uritemplate-4.2.0-py3-none-any.whl", hash = "sha256:962201ba1c4edcab02e60f9a0d3821e82dfc5d2d6662a21abd533879bdb8a686", size = 11488 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166 },
]

[[package]]
name = "webencodings"
version = "0.5.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/02/ae6ceac1baeda530866a85075641cec12989bd8d31af6d5ab4a3e8c92f47/webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923", size = 9721 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78", size = 11774 },
]

[[package]]
name = "websockets"
version = "15.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/e6/26d09fab466b7ca9c7737474c52be4f76a40301b08362eb2dbc19dcc16c1/websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee", size = 177016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/9f/51f0cf64471a9d2b4d0fc6c534f323b664e7095640c34562f5182e5a7195/websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931", size = 175440 },
    { url = "https://files.pythonhosted.org/packages/8a/05/aa116ec9943c718905997412c5989f7ed671bc0188ee2ba89520e8765d7b/websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675", size = 173098 },
    { url = "https://files.pythonhosted.org/packages/ff/0b/33cef55ff24f2d92924923c99926dcce78e7bd922d649467f0eda8368923/websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151", size = 173329 },
    { url = "https://files.pythonhosted.org/packages/31/1d/063b25dcc01faa8fada1469bdf769de3768b7044eac9d41f734fd7b6ad6d/websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22", size = 183111 },
    { url = "https://files.pythonhosted.org/packages/93/53/9a87ee494a51bf63e4ec9241c1ccc4f7c2f45fff85d5bde2ff74fcb68b9e/websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f", size = 182054 },
    { url = "https://files.pythonhosted.org/packages/ff/b2/83a6ddf56cdcbad4e3d841fcc55d6ba7d19aeb89c50f24dd7e859ec0805f/websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8", size = 182496 },
    { url = "https://files.pythonhosted.org/packages/98/41/e7038944ed0abf34c45aa4635ba28136f06052e08fc2168520bb8b25149f/websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375", size = 182829 },
    { url = "https://files.pythonhosted.org/packages/e0/17/de15b6158680c7623c6ef0db361da965ab25d813ae54fcfeae2e5b9ef910/websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d", size = 182217 },
    { url = "https://files.pythonhosted.org/packages/33/2b/1f168cb6041853eef0362fb9554c3824367c5560cbdaad89ac40f8c2edfc/websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4", size = 182195 },
    { url = "https://files.pythonhosted.org/packages/86/eb/20b6cdf273913d0ad05a6a14aed4b9a85591c18a987a3d47f20fa13dcc47/websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa", size = 176393 },
    { url = "https://files.pythonhosted.org/packages/1b/6c/c65773d6cab416a64d191d6ee8a8b1c68a09970ea6909d16965d26bfed1e/websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561", size = 176837 },
    { url = "https://files.pythonhosted.org/packages/fa/a8/5b41e0da817d64113292ab1f8247140aac61cbf6cfd085d6a0fa77f4984f/websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f", size = 169743 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]

[[package]]
name = "yoyo-migrations"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "importlib-metadata" },
    { name = "sqlparse" },
    { name = "tabulate" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/8c/5d/9ef7f808ea955eca9f08043c65bdc81a4694e784c978b24ad72022974a97/yoyo_migrations-9.0.0-py3-none-any.whl", hash = "sha256:fc65d3a6d9449c1c54d64ff2ff98e32a27da356057c60e3471010bfb19ede081", size = 49002 },
]

[[package]]
name = "zipp"
version = "3.23.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e3/02/0f2892c661036d50ede074e376733dca2ae7c6eb617489437771209d4180/zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166", size = 25547 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e", size = 10276 },
]
&amp;lt;/file&amp;gt;
  &amp;lt;file path="prompt_service_notebook.py"&amp;gt;import marimo

__generated_with = "0.14.12"
app = marimo.App(width="medium")


@app.cell
def _():
    from repository.database import SQLite3Database
    from repository.prompt_service import PromptService
    from repository.prompt_models import (
        Prompt,
        PromptType,
        PromptPlanStatus,
        CmdCategory,
    )

    return (
        CmdCategory,
        Prompt,
        PromptPlanStatus,
        PromptService,
        PromptType,
        SQLite3Database,
    )


@app.cell
def _(PromptService, SQLite3Database):
    db = SQLite3Database("data/collect.db")
    with db.get_connection() as conn:
        ps = PromptService(conn)
    return (ps,)


@app.cell
def _(ps):
    cmds = ps.load_cmds_from_disk()
    plans = ps.load_plans_from_disk()
    return cmds, plans


@app.cell
def _(cmds):
    print(f"Num cmds: {len(cmds.loaded_prompts)}\n")
    for cmd in cmds.loaded_prompts:
        print(cmd.name)
    return


@app.cell
def _(plans):
    print(f"Num plans: {len(plans.loaded_prompts)}\n")
    for plan in plans.loaded_prompts:
        print(plan.name)
    return


@app.cell
def _():
    db_name = "collect_completed_add_claude_sdk_processing.md"
    result = db_name.split("_")
    print(result)
    return db_name, result


@app.cell
def _(result):
    print(f"project name: {result[0]}")
    print(f"plan status: {result[1]}")
    return


@app.cell
def _(result):
    namelist = result[2:]
    print(namelist)
    return (namelist,)


@app.cell
def _(namelist):
    newname = ""
    for word in namelist:
        if not word.endswith(".md"):
            newname = newname + word + "_"
        else:
            newname = newname + word
    print(newname)
    return


@app.cell
def _(db_name):
    print(db_name.split("_")[2:])
    return


@app.cell
def _(PromptType):
    def parse_db_name(db_name: str, prompt_type: PromptType) -&amp;amp;gt; str:
        ls = db_name.split("_")
        filename = ""
        if prompt_type == PromptType.PLAN:
            project = ls[0]
            plan_status = ls[1]
            print(f"project: {project}")
            print(f"plan status: {plan_status}")

            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

        if prompt_type == PromptType.CMD:
            cmd_dir = ls[0]
            print(f"cmd/dir: {cmd_dir}")
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

    return (parse_db_name,)


@app.cell
def _(PromptType, db_name, parse_db_name):
    parse_db_name(db_name, PromptType.PLAN)
    return


@app.cell
def _(PromptType, parse_db_name):
    parse_db_name("tools_create_database.md", PromptType.CMD)
    return


@app.cell
def _(CmdCategory, Prompt, PromptPlanStatus, PromptType, ps):
    def new_cmd_prompt(prompt_content: str) -&amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
&amp;lt;/file&amp;gt;
  &amp;lt;file path="Makefile"&amp;gt;PROJECT_NAME := collect

marimo:
	uv run marimo edit

.PHONY: movetools
movetools:
	./movetools

.PHONY: buildsrc
buildsrc: 
	./tools/buildsrc

.PHONY: ensuregithub
ensuregithub:
	./tools/ensure-github-url

lint:
	ruff check .

format:
	black .

test: 
	uv run pytest -v -s -n auto

test-fast:
	uv run pytest -v -n auto -m "not slow"

test-slow:
	uv run pytest -v -s -m slow

test-single:
	uv run pytest -v -s

check: 
	make lint
	make format
	make movetools
	make ensuregithub
	make buildsrc

migrate:
	uv run yoyo apply --config yoyo.ini --batch


&amp;lt;/file&amp;gt;
  &amp;lt;file path=".mcp.json"&amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "/Users/benjaminmetz/.local/bin/uv",
			"args": [
				"--directory",
				"/Users/benjaminmetz/python/collect",
				"run",
				"collect.py"
			]
		}
	}
}
&amp;lt;/file&amp;gt;
  &amp;lt;file path="pyproject.toml"&amp;gt;[project]
name = "collect"
version = "0.1.0"
description = "development toolkit for all the things.."
readme = "README.md"
requires-python = "&amp;amp;gt;=3.13"
dependencies = [
    "aiohttp&amp;amp;gt;=3.12.11",
    "anthropic&amp;amp;gt;=0.50.0",
    "beautifulsoup4&amp;amp;gt;=4.13.4",
    "black&amp;amp;gt;=25.1.0",
    "fastapi&amp;amp;gt;=0.116.1",
    "google-ai-generativelanguage&amp;amp;gt;=0.6.15",
    "google-api-python-client&amp;amp;gt;=2.169.0",
    "google-auth-httplib2&amp;amp;gt;=0.2.0",
    "google-cloud-aiplatform[tokenization]&amp;amp;gt;=1.91.0",
    "google-cloud-secret-manager&amp;amp;gt;=2.23.3",
    "google-genai&amp;amp;gt;=1.13.0",
    "google-generativeai&amp;amp;gt;=0.8.5",
    "html-to-markdown&amp;amp;gt;=1.3.2",
    "html5lib&amp;amp;gt;=1.1",
    "httplib2&amp;amp;gt;=0.22.0",
    "httpx&amp;amp;gt;=0.28.1",
    "ipython&amp;amp;gt;=9.4.0",
    "lxml&amp;amp;gt;=5.4.0",
    "marimo&amp;amp;gt;=0.14.12",
    "markdownify&amp;amp;gt;=1.1.0",
    "mcp[cli]&amp;amp;gt;=1.7.1",
    "openai&amp;amp;gt;=1.59.4",
    "pathspec&amp;amp;gt;=0.12.1",
    "pyperclip&amp;amp;gt;=1.9.0",
    "pytest&amp;amp;gt;=8.3.5",
    "pytest-asyncio&amp;amp;gt;=0.26.0",
    "pytest-xdist&amp;amp;gt;=3.6.1",
    "python-json-logger&amp;amp;gt;=3.3.0",
    "readabilipy&amp;amp;gt;=0.3.0",
    "rich&amp;amp;gt;=14.0.0",
    "ruff&amp;amp;gt;=0.11.9",
    "tiktoken&amp;amp;gt;=0.9.0",
    "uvicorn&amp;amp;gt;=0.34.2",
    "yoyo-migrations&amp;amp;gt;=9.0.0",
]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."]
filterwarnings = [
    "ignore::UserWarning:google.auth._default"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests"
]
# Parallel test execution settings
addopts = [
    "--strict-markers",  # Ensure all marks are registered
    "--tb=short",       # Shorter traceback format
    "--dist=worksteal", # Better work distribution for uneven test times
]
&amp;lt;/file&amp;gt;
  &amp;lt;file path="initial_load.py"&amp;gt;#!/usr/bin/env python3
"""
Initial load script for loading plans and commands from disk into database
Uses PromptService to load from .claude/commands,
.gemini/commands, and _docs/plans
"""
import sys
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import PromptLoadResult, PromptCreateResult


def load_commands_to_db(service: PromptService) -&amp;amp;gt; PromptLoadResult:
    """
    Load commands from .claude and .gemini directories and save to database
    This is primarily for an initial load or to clean and restart from disk

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📁 Loading commands from disk...")

    # Load commands from filesystem
    cmd_result: PromptLoadResult = service.load_cmds_from_disk()

    if cmd_result.errors:
        print(
            f"⚠️  Found {len(cmd_result.errors)
                           } errors while loading commands:"
        )
        for error in cmd_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not cmd_result.loaded_prompts:
        print("ℹ️  No commands found to load")
        return cmd_result

    print(f"Found: {len(cmd_result.loaded_prompts)} to save to database")

    # Save commands to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        cmd_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save command: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (cmd_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=cmd_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def load_plans_to_db(service: PromptService) -&amp;amp;gt; PromptLoadResult:
    """Load plans from _docs/plans directories and save to database

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📋 Loading plans from disk...")

    # Load plans from filesystem
    plan_result: PromptLoadResult = service.load_plans_from_disk()

    if plan_result.errors:
        print(
            f"⚠️  Found {len(plan_result.errors)
                           } errors while loading plans:"
        )
        for error in plan_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not plan_result.loaded_prompts:
        print("ℹ️  No plans found to load")
        return plan_result

    print(
        f"📄 Found {len(plan_result.loaded_prompts)
                     } plans to save to database"
    )

    # Save plans to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        plan_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save plan: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (plan_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=plan_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def main():
    """Main function to orchestrate the complete loading process"""
    print("🚀 Starting initial data load from disk to database...")
    print("=" * 60)

    try:
        # Initialize database connection
        database = SQLite3Database("data/collect.db")

        with database.get_connection() as conn:
            # Create PromptService instance
            service = PromptService(conn)

            # Track overall statistics
            total_loaded = 0
            total_errors = 0

            # Load commands
            cmd_result: PromptLoadResult = load_commands_to_db(service)
            cmd_loaded = len(cmd_result.loaded_prompts)
            cmd_errors = len(cmd_result.errors) if cmd_result.errors else 0

            total_loaded += cmd_loaded
            total_errors += cmd_errors

            print(f"✅ Commands: {cmd_loaded} loaded, {cmd_errors} errors")
            print()

            # Load plans
            plan_result: PromptLoadResult = load_plans_to_db(service)
            plan_loaded = len(plan_result.loaded_prompts)
            plan_errors = len(plan_result.errors) if plan_result.errors else 0

            total_loaded += plan_loaded
            total_errors += plan_errors

            print(f"✅ Plans: {plan_loaded} loaded, {plan_errors} errors")
            print()

            # Print final summary
            print("=" * 60)
            print("📊 FINAL SUMMARY:")
            print(f"   ✅ Total items loaded: {total_loaded}")
            print(f"   ❌ Total errors: {total_errors}")

            if total_errors == 0:
                print("🎉 All data loaded successfully!")
            else:
                print(
                    f"⚠️  Completed with {
                      total_errors} errors - check output above"
                )

    except Exception as e:
        print(f"💥 Fatal error during loading process: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        sys.exit(1)


if __name__ == "__main__":
    main()
&amp;lt;/file&amp;gt;
  &amp;lt;file path=".sync_cache.json"&amp;gt;{
  ".claude/commands/go/go_build_endpoint_test.md": {
    "sha": "bb6aa9bffab635d3455caf7cfa59fc4f43036aac",
    "path": ".claude/commands/go/go_build_endpoint_test.md",
    "last_synced": 1754870996.5877829,
    "converted": true
  },
  ".claude/commands/archive/build_context.md": {
    "sha": "d976db85985179f771925095a73d98142d3ab30b",
    "path": ".claude/commands/archive/build_context.md",
    "last_synced": 1754870996.598152,
    "converted": true
  },
  ".claude/commands/commit.md": {
    "sha": "891c52b5b372e6fffd161dcf9b55930755d9fbf3",
    "path": ".claude/commands/commit.md",
    "last_synced": 1754870996.614573,
    "converted": true
  },
  ".claude/commands/go/create_go_structs.md": {
    "sha": "56a5e5dc2abb511b98b2e1147d428551610aa49e",
    "path": ".claude/commands/go/create_go_structs.md",
    "last_synced": 1754870996.625781,
    "converted": true
  },
  ".claude/commands/diff_code_review.md": {
    "sha": "6ef1818de14d6d1e994d9fd4841bc5486c4d6f20",
    "path": ".claude/commands/diff_code_review.md",
    "last_synced": 1754870996.635034,
    "converted": true
  },
  ".claude/commands/convert_to_toml.md": {
    "sha": "7bdd42b1ffeee07bd06ed5b4b97e99381c8052e9",
    "path": ".claude/commands/convert_to_toml.md",
    "last_synced": 1754870996.642501,
    "converted": true
  },
  ".claude/commands/create_checklist_3.md": {
    "sha": "27f4fa9841078c71d9490d9ef2c967970c053bbf",
    "path": ".claude/commands/create_checklist_3.md",
    "last_synced": 1754870996.688289,
    "converted": true
  },
  ".claude/commands/go/create_go_structsV1.md": {
    "sha": "d7e28d7eb8dee775f9416574d8f06ef11ca42158",
    "path": ".claude/commands/go/create_go_structsV1.md",
    "last_synced": 1754870996.692773,
    "converted": true
  },
  ".claude/commands/go/go_update_config.md": {
    "sha": "3cbaf390b5abd45230dde54705958bd873f16ebc",
    "path": ".claude/commands/go/go_update_config.md",
    "last_synced": 1754871010.753632,
    "converted": true
  },
  ".claude/commands/mcp/copy_to_clipboard.md": {
    "sha": "aad8af9420d4a80933aaf12accfe7a8450679f32",
    "path": ".claude/commands/mcp/copy_to_clipboard.md",
    "last_synced": 1754871011.429687,
    "converted": true
  },
  ".claude/commands/mcp/get_docs.md": {
    "sha": "9e27863768032379a1798cf6da1a881720ed65bf",
    "path": ".claude/commands/mcp/get_docs.md",
    "last_synced": 1754871012.238699,
    "converted": true
  },
  ".claude/commands/model_code_review.md": {
    "sha": "8bc222f49f8587a01362d06ff6475cb8e46ace2e",
    "path": ".claude/commands/model_code_review.md",
    "last_synced": 1754871023.324787,
    "converted": true
  },
  ".claude/commands/pr.md": {
    "sha": "0e8a152f358b515f158d5025428db18847101fc0",
    "path": ".claude/commands/pr.md",
    "last_synced": 1754871024.7506702,
    "converted": true
  },
  ".claude/commands/prime_webapp.md": {
    "sha": "c6f4cfe53f2aa92e1ac5661138989c9f9ff3ec42",
    "path": ".claude/commands/prime_webapp.md",
    "last_synced": 1754871038.662535,
    "converted": true
  },
  ".claude/commands/python/python_update_config.md": {
    "sha": "8562257de3a622c1aefb06be4c78a4ce9c580e2a",
    "path": ".claude/commands/python/python_update_config.md",
    "last_synced": 1754871042.126036,
    "converted": true
  },
  ".claude/commands/read.md": {
    "sha": "524139e35449b4f5c0a88b206699cec5d2f1409b",
    "path": ".claude/commands/read.md",
    "last_synced": 1754871043.723397,
    "converted": true
  },
  ".claude/commands/response_in_markdown.md": {
    "sha": "a540be858d83e4b65df8dd2a23e3656bbdce8ee7",
    "path": ".claude/commands/response_in_markdown.md",
    "last_synced": 1754871049.784717,
    "converted": true
  },
  ".claude/commands/runplan.md": {
    "sha": "08d9b2d8c3bd445c55170adec7e80b9d3733e27a",
    "path": ".claude/commands/runplan.md",
    "last_synced": 1754871056.6670442,
    "converted": true
  },
  ".claude/commands/test_runner.md": {
    "sha": "28e088b2d155b9eca391add1bc75cdf701a3a4d8",
    "path": ".claude/commands/test_runner.md",
    "last_synced": 1754871059.413824,
    "converted": true
  },
  ".claude/commands/tools/create_database.md": {
    "sha": "fcfeeb2b78e39ecf0a1b8b4a85735dd83fa1c8f1",
    "path": ".claude/commands/tools/create_database.md",
    "last_synced": 1754871070.783872,
    "converted": true
  },
  ".claude/commands/tools/extract.md": {
    "sha": "e8946fb23beb24cd8e8689c48cd21b155dfa10a6",
    "path": ".claude/commands/tools/extract.md",
    "last_synced": 1754871074.061621,
    "converted": true
  }
}&amp;lt;/file&amp;gt;
  &amp;lt;file path="llmrunner.py"&amp;gt;import asyncio
from datetime import datetime

from pydantic import BaseModel
from typing import Dict, Union, List, Optional, Any

from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP
from models.gemini_mcp import GeminiMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP


class ModelsToMCP(BaseModel):
    model_config = {"arbitrary_types_allowed": True}
    models_to_mcp: Dict[str, Union[GeminiMCP, AnthropicMCP, OpenAIMCP, XaiMCP]]


class ModelResult(BaseModel):
    model: str
    timestamp: str
    success: bool
    actual_model: Optional[str] = None
    duration_seconds: Optional[float] = None
    response: Optional[Any] = None
    error: Optional[str] = None


class LLMRunnerResults(BaseModel):
    successful_results: List[ModelResult]
    failed_results: List[ModelResult]
    total_models: int
    success_count: int
    failure_count: int


async def llmrunner(prompt: str, models_to_mcp: ModelsToMCP) -&amp;amp;gt; LLMRunnerResults:

    async def call_model(model_name: str) -&amp;amp;gt; dict:
        try:
            start_time = datetime.now()
            iso_time = start_time.isoformat()

            mcp_instance = models_to_mcp.models_to_mcp[model_name]
            print(f"sending to --&amp;amp;gt; {model_name} : at -&amp;amp;gt; {iso_time}")
            response = mcp_instance.send_message(prompt, model=model_name)
            end_time = datetime.now()

            result = ModelResult(
                model=model_name,
                actual_model=model_name,
                timestamp=iso_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                response=response,
                success=True,
            )

            return result

        except Exception as e:
            error_result = ModelResult(
                success=False,
                error=str(e),
                model=model_name,
                timestamp=datetime.now().isoformat(),
            )

            return error_result

    print(
        f"starting runner for: {
          len(models_to_mcp.models_to_mcp.keys())} models -&amp;amp;gt;"
    )
    tasks = [call_model(model) for model in models_to_mcp.models_to_mcp.keys()]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful_results = [
        r for r in results if isinstance(r, ModelResult) and r.success
    ]

    failed_results = [
        r for r in results if isinstance(r, ModelResult) and not r.success
    ]

    return LLMRunnerResults(
        successful_results=successful_results,
        failed_results=failed_results,
        total_models=len(models_to_mcp.models_to_mcp),
        success_count=len(successful_results),
        failure_count=len(failed_results),
    )


def code_review_models_to_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    anthropic_model = config.anthropic_default_code_review_model
    gemini_model = config.gemini_default_code_review_model
    xai_model = config.xai_default_code_review_model
    openai_model = config.openai_default_code_review_model

    gemini_mcp = GeminiMCP(config, secret_mgr, gemini_model)
    openai_mcp = OpenAIMCP(config, secret_mgr, openai_model)
    xai_mcp = XaiMCP(config, secret_mgr, xai_model)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, anthropic_model)

    model_mcps = {
        gemini_model: gemini_mcp,
        openai_model: openai_mcp,
        xai_model: xai_mcp,
        anthropic_model: anthropic_mcp,
    }

    return ModelsToMCP(models_to_mcp=model_mcps)
&amp;lt;/file&amp;gt;
  &amp;lt;file path="README.md"&amp;gt;**Collect** is a command-line toolkit built with Python that functions as an MCP (Model Context Protocol) server. It is designed to assist with AI-driven development by providing tools to fetch web content, process it, and coordinate analysis across multiple AI models.

*   **Multi-Model Integration**: Interact with models from Google (Gemini), Anthropic (Claude), OpenAI (GPT), and XAI (Grok) through a single interface.
*   **Content Processing**: Fetch content from URLs and convert HTML to clean markdown or plain text.
*   **Code &amp;amp;amp; Diff Analysis**: Perform code reviews on files or git diffs using any of the integrated AI models.
*   **Secure Configuration**: Utilizes Google Cloud Secret Manager for API key storage.
*   **Prompt Management**: A version-controlled system for managing and synchronizing prompts between the local filesystem and a SQLite database.
*   **Token Utilities**: Tools to count token usage for various models to manage costs and context windows.

### MCP Server Configuration

#### For Claude Code

To enable Claude Code to use the `collect` MCP server, create a `.mcp.json` file in your project's root directory:

1.  **Create the Configuration File**: In the root of your project where you want to use the collect tools, create a file named `.mcp.json`.
2.  **Add Configuration**: Add the following JSON configuration:

```json
{
  "mcpServers": {
    "collect": {
      "command": "/path/to/.local/bin/uv",
      "args": [
        "--directory",
        "/path/to/collect",
        "run",
        "collect.py"
      ]
    }
  }
}
```

Replace `/path/to/.local/bin/uv` with the full path to your `uv` binary (you can find this with `which uv`), and `/path/to/collect` with the full path to your collect repository.

#### For Gemini CLI

To enable the Gemini CLI to automatically start the `collect` MCP server, you need to configure a `.gemini/settings.json` file in your project's root directory:

1.  **Create the Directory**: If it doesn't already exist, create a `.gemini` directory in the root of the `collect` project.
2.  **Create the Settings File**: Inside the `.gemini` directory, create a file named `settings.json`.
3.  **Add Configuration**: Paste the following JSON configuration into the `settings.json` file.

```json
{
  "mcpServers": {
    "collect": {
      "command": "uv",
      "args": [
        "run",
        "python",
        "collect.py"
      ],
      "workingDirectory": "/Users/benjaminmetz/python/collect",
      "enabled": true
    }
  }
}
```

This configuration tells the Gemini CLI how to launch the `collect` server, specifying the command, arguments, and working directory.

### Command Category System

The command category system dynamically creates categories based on subdirectories configured in the `.env` file. This approach allows for easy extension of command categories without code changes.

#### How Categories Are Created

1. **Configuration**: Command subdirectories are defined in the `.env` file:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
   ```

2. **Dynamic Enum Generation**: The `create_cmd_category_enum()` function in `repository/prompt_models.py` reads the `COMMAND_SUBDIRS` from the `.env` file via the `Config` class and dynamically creates a `CmdCategory` enum at runtime.

3. **Directory Management**: When the `PromptService` initializes, the `cmd_check_dirs()` function in `repository/prompt_service.py`:
   - Reads the subdirectory list from the config
   - Checks for the existence of each configured subdirectory under both `.claude/commands/` and `.gemini/commands/`
   - Automatically creates any missing directories
   - Each subdirectory becomes a valid command category

4. **Category Assignment**: When loading commands from disk:
   - Files directly in `.claude/commands/` or `.gemini/commands/` are assigned the `UNCATEGORIZED` category
   - Files in subdirectories are assigned the category matching the subdirectory name
   - The category is stored as part of the prompt's metadata in the database

#### Adding New Categories

To add new command categories:
1. Update the `COMMAND_SUBDIRS` line in the `.env` file with your new category
2. The system will automatically create the directories and recognize them as valid categories on the next run
3. Commands placed in those directories will be tagged with the new category

#### Example

To add a "rust" category:
1. Edit `.env`:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools,rust
   ```
2. Restart the service or run the prompt service
3. The system will create:
   - `.claude/commands/rust/`
   - `.gemini/commands/rust/`
4. Any `.md` files placed in these directories will be categorized as "rust" commands

#### Current Directory Structure
Based on the `.env` configuration (`COMMAND_SUBDIRS=archive,go,js,mcp,python,tools`), the directory structure is:

```
.claude/
└── commands/
    ├── archive/          # Archived commands
    ├── go/               # Go-specific commands
    ├── js/               # JavaScript commands
    ├── mcp/              # MCP server commands
    ├── python/           # Python-specific commands
    └── tools/            # Tool-related commands

.gemini/
└── commands/
    ├── archive/
    ├── go/
    ├── js/
    ├── mcp/
    ├── python/
    └── tools/
```

Note: Files placed directly in `.claude/commands/` or `.gemini/commands/` (not in subdirectories) are automatically assigned the `UNCATEGORIZED` category.

### Prompt Management System

The project includes a system for managing prompts **that is very much under construction**. Prompts are categorized as either **Commands** (`CMD`) or **Plans** (`PLAN`). This system, located in the `repository/` directory, uses a SQLite database to store and version prompts, while also synchronizing them with the local filesystem.

*   **Core Components**:
    *   `prompt_service.py`: The main service class that orchestrates loading, saving, versioning, and flattening prompts.
    *   `prompt_models.py`: Defines the Pydantic data models for prompts, including `Prompt`, `PromptData`, and various status enums like `PromptType` and `PromptPlanStatus`.
    *   `database.py`: Manages the connection to the `collect.db` SQLite database.
    *   `20250727_01_create-prompt-tables.sql`: The database migration file that defines the schema for the `prompt` and `prompt_history` tables.

*   **Synchronization Workflow**:
    1.  **Loading from Disk**: The `PromptService` can load prompts from predefined directories (`.claude/commands`, `.gemini/commands`, and `_docs/plans`).
    2.  **Database Persistence**: Loaded prompts are saved to the SQLite database. The service checks for existing prompts by name. If a prompt already exists and its content has changed (verified via a SHA256 hash), a new version is created in the `prompt_history` table, and the main `prompt` table is updated.
    3.  **Flattening to Disk**: The service can "flatten" the prompts from the database back to the filesystem, ensuring that the local files are consistent with the database state. This is useful for maintaining a clear and organized prompt library.

*   **Versioning**:
    *   Every time a prompt's content is updated, its `version` number is incremented.
    *   A complete record of all versions is stored in the `prompt_history` table, including a timestamp and a change summary. This allows for a full audit trail of how a prompt has evolved.

This system ensures that prompts are treated as version-controlled assets within the project, providing a structured and auditable way to manage the instructions given to the AI models.
&amp;lt;/file&amp;gt;
  &amp;lt;file path="api.py"&amp;gt;#!/usr/bin/env python

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api import prompt_api_router
from config import Config
import uvicorn

import sys
import logging
from pythonjsonlogger.json import JsonFormatter
from contextlib import asynccontextmanager


# Configure JSON logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(stream=sys.stdout)
handler.setFormatter(JsonFormatter())
handler.setLevel(logging.INFO)

logger.addHandler(handler)

# Load configuration
config = Config()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting prompt API service...")
    app.state.db_path = config.db_path
    app.state.config = config
    logger.info(f"Database path set to: {app.state.db_path}")
    logger.info(f"Service running on port: {config.port}")

    yield

    # Shutdown
    logger.info("Shutting down prompt API service...")


app = FastAPI(
    title="Prompt Service API",
    description="HTTP API for managing prompts and plans",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

app.include_router(prompt_api_router, tags=["prompt_api"])


def main():
    uvicorn.run(app, host="0.0.0.0", port=int(config.port))


if __name__ == "__main__":
    main()
&amp;lt;/file&amp;gt;
  &amp;lt;file path="yoyo.ini"&amp;gt;[DEFAULT]
sources = migrations
database = sqlite:///data/collect.db
batch_mode = on
verbosity = 0

&amp;lt;/file&amp;gt;
  &amp;lt;file path="secret_manager.py"&amp;gt;from google.cloud import secretmanager


class SecretManager:
    def __init__(
        self,
        project_id: str,
    ) -&amp;amp;gt; None:
        self.project_id = project_id
        self.gcp_client = secretmanager.SecretManagerServiceClient()

    def get_secret(self, secret_name: str) -&amp;amp;gt; str:
        response = self.gcp_client.access_secret_version(request={"name": secret_name})
        return response.payload.data.decode("UTF-8").strip()
&amp;lt;/file&amp;gt;
  &amp;lt;file path=".gitignore"&amp;gt;# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
venv/
env/

# Environment variables
.env.local
.env.*.local

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Test coverage
.coverage
htmlcov/
.pytest_cache/
.tox/

# Temporary files
:w
*.tmp
*.temp

# Python version management
.python-version

# UV lock file (optional - some prefer to track this)
# uv.lock
&amp;lt;/file&amp;gt;
  &amp;lt;file path=".env"&amp;gt;PORT=8081
GCP_PROJECT_ID=482777410016
DB_PATH=data/collect.db
ANTHROPIC_API_KEY_PATH=projects/482777410016/secrets/AnthropicMCP/versions/1
ANTHROPIC_MODEL_OPUS=claude-opus-4-20250514
ANTHROPIC_MODEL_SONNET=claude-sonnet-4-20250514
GEMINI_API_KEY_PATH=projects/482777410016/secrets/GeminiTest/versions/1
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/
XAI_API_KEY_PATH=projects/482777410016/secrets/XAI_API_KEY_ELEPHNT/versions/1
GROK_SYSTEM_PROMPT=You are a helpful assistant that can answer questions and help with tasks.
OPENAI_API_KEY_PATH=projects/482777410016/secrets/OpenAIMCP/versions/1

# default code review models for running the code review loop
OPENAI_DEFAULT_CODE_REVIEW_MODEL=o3-mini-2025-01-31
GEMINI_DEFAULT_CODE_REVIEW_MODEL=gemini-2.5-flash-preview-05-20
ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL=claude-opus-4-20250514
XAI_DEFAULT_CODE_REVIEW_MODEL=grok-3-mini-fast-latest

# Command subdirectories
COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
GITHUB_URL=https://github.com/austere-labs/collect
&amp;lt;/file&amp;gt;
  &amp;lt;file path="filterModel.md"&amp;gt;# Gemini Model Filtering Implementation Guide

## Overview

This document describes how to filter Gemini models by version number (2.0, 2.5, etc.) and extract input token limits from the API response.

## Current State

The `GeminiMCP.get_model_list()` method has been updated to return the full API response instead of just model names:

```python
def get_model_list(self) -&amp;amp;gt; Dict:
    # ... API call logic ...
    model_data = response.json()
    return model_data  # Returns full response with all model metadata
```

## Implementation Plan

### 1. Add Filtering Methods to GeminiMCP Class

Add these methods to the `GeminiMCP` class in `models/gemini_mcp.py`:

```python
def filter_models_by_version(self, versions: list[str]) -&amp;amp;gt; list[dict]:
    """
    Filter models by version numbers and include token limits.
    
    Args:
        versions: List of version strings (e.g., ['2.0', '2.5'])
    
    Returns:
        List of dicts with model info including inputTokenLimit
    """
    all_models = self.get_model_list()
    filtered_models = []
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Check if model matches any requested version
        for version in versions:
            if version in model_name:
                model_info = {
                    'name': model_name,
                    'displayName': model.get('displayName', ''),
                    'inputTokenLimit': model.get('inputTokenLimit', 0),
                    'outputTokenLimit': model.get('outputTokenLimit', 0),
                    'description': model.get('description', ''),
                    'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
                }
                filtered_models.append(model_info)
                break
    
    return filtered_models

def get_models_with_token_info(self) -&amp;amp;gt; list[dict]:
    """
    Get all models with their token limit information.
    
    Returns:
        List of models sorted by inputTokenLimit (descending)
    """

    all_models = self.get_model_list()
    models_with_tokens = []

    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Only include models with token limit info
        if input_limit &amp;amp;gt; 0:
            models_with_tokens.append({
                'name': model_name,
                'inputTokenLimit': input_limit,
                'outputTokenLimit': model.get('outputTokenLimit', 0)
            })
    
    # Sort by input token limit (highest first)
    models_with_tokens.sort(key=lambda x: x['inputTokenLimit'], reverse=True)
    return models_with_tokens
```

### 2. Advanced Filtering Function (Standalone)

For more complex filtering needs, you can use this standalone function:

```python
def filter_gemini_models(models_data: dict, 
                        versions: list[str] = None,
                        min_input_tokens: int = None,
                        max_input_tokens: int = None,
                        generation_methods: list[str] = None) -&amp;amp;gt; list[dict]:
    """
    Advanced filtering with multiple criteria.
    
    Args:
        models_data: Response from get_model_list()
        versions: Filter by version numbers (optional)
        min_input_tokens: Minimum inputTokenLimit (optional)
        max_input_tokens: Maximum inputTokenLimit (optional)
        generation_methods: Required generation methods (optional)
    
    Returns:
        Filtered list of model information
    """
    filtered_models = []
    
    for model in models_data.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Apply version filter
        if versions:
            if not any(ver in model_name for ver in versions):
                continue
        
        # Apply token limit filters
        if min_input_tokens and input_limit &amp;amp;lt; min_input_tokens:
            continue
        if max_input_tokens and input_limit &amp;amp;gt; max_input_tokens:
            continue
        
        # Apply generation method filter
        if generation_methods:
            supported_methods = model.get('supportedGenerationMethods', [])
            if not all(method in supported_methods for method in generation_methods):
                continue
        
        # Model passed all filters
        model_info = {
            'name': model_name,
            'displayName': model.get('displayName', ''),
            'inputTokenLimit': input_limit,
            'outputTokenLimit': model.get('outputTokenLimit', 0),
            'description': model.get('description', ''),
            'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
        }
        filtered_models.append(model_info)
    
    return filtered_models
```

## Usage Examples

### Basic Version Filtering

```python
def test_filter_by_version(gemini_mcp):
    # Get models for versions 2.0 and 2.5
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    print(f"Found {len(filtered)} models:")
    for model in filtered:
        print(f"- {model['name']}: {model['inputTokenLimit']:,} input tokens")
```

### Get Models with Token Info

```python
def test_models_with_tokens(gemini_mcp):
    models = gemini_mcp.get_models_with_token_info()
    
    print("Models by input token limit:")
    for model in models[:10]:  # Top 10 models
        print(f"- {model['name']}: {model['inputTokenLimit']:,} tokens")
```

### Advanced Filtering

```python
def test_advanced_filtering(gemini_mcp):
    all_models = gemini_mcp.get_model_list()
    
    # Find 2.5 models with at least 100k input tokens
    filtered = filter_gemini_models(
        all_models,
        versions=['2.5'],
        min_input_tokens=100000,
        generation_methods=['generateContent']
    )
    
    print("High-capacity 2.5 models:")
    for model in filtered:
        print(f"- {model['name']}")
        print(f"  Input limit: {model['inputTokenLimit']:,}")
        print(f"  Output limit: {model['outputTokenLimit']:,}")
```

### Grouping Models by Version

```python
def group_models_by_version(gemini_mcp):
    from collections import defaultdict
    
    all_models = gemini_mcp.get_model_list()
    version_groups = defaultdict(list)
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Extract version pattern
        if '2.5' in model_name:
            version = '2.5'
        elif '2.0' in model_name:
            version = '2.0'
        elif '1.5' in model_name:
            version = '1.5'
        elif '1.0' in model_name:
            version = '1.0'
        else:
            version = 'other'
        
        version_groups[version].append({
            'name': model_name,
            'inputTokenLimit': model.get('inputTokenLimit', 0)
        })
    
    # Display grouped results
    for version, models in sorted(version_groups.items()):
        print(f"\nVersion {version} ({len(models)} models):")
        for model in sorted(models, key=lambda x: x['inputTokenLimit'], reverse=True)[:3]:
            print(f"  - {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Expected Output Format

When filtering models, you'll get results like:

```
Found 15 models:
- gemini-2.5-pro: 2,000,000 input tokens
- gemini-2.5-flash: 1,000,000 input tokens
- gemini-2.5-flash-preview-05-20: 1,000,000 input tokens
- gemini-2.0-flash: 32,768 input tokens
- gemini-2.0-flash-exp: 32,768 input tokens
- gemini-2.0-pro-exp: 32,768 input tokens
```

## API Response Structure

The Gemini API returns model data in this format:

```json
{
  "models": [
    {
      "name": "models/gemini-2.5-flash",
      "displayName": "Gemini 2.5 Flash",
      "description": "Fast and versatile multimodal model",
      "inputTokenLimit": 1000000,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ]
    }
    // ... more models
  ]
}
```

## Testing the Implementation

Add this test to `models/test_gemini_mcp.py`:

```python
def test_filter_models_by_version(gemini_mcp):
    # Test filtering for 2.0 and 2.5 versions
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    assert len(filtered) &amp;amp;gt; 0
    assert all('2.0' in m['name'] or '2.5' in m['name'] for m in filtered)
    assert all('inputTokenLimit' in m for m in filtered)
    
    # Print results for verification
    print(f"\nFound {len(filtered)} models for versions 2.0 and 2.5:")
    for model in sorted(filtered, key=lambda x: x['inputTokenLimit'], reverse=True):
        print(f"  {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Notes

1. **Token Limits**: Not all models return `inputTokenLimit`. Handle missing values gracefully.
2. **Model Names**: The API returns full names like "models/gemini-2.5-flash". We extract just the model part.
3. **Sorting**: Consider sorting results by token limit, name, or version for consistent output.
4. **Caching**: For production use, consider caching the model list as it doesn't change frequently.
&amp;lt;/file&amp;gt;
  &amp;lt;file path="test_llmrunner.py"&amp;gt;import pytest

from llmrunner import (
    llmrunner,
    code_review_models_to_mcp,
    ModelResult,
    LLMRunnerResults,
)


@pytest.fixture
def models_to_mcp():
    return code_review_models_to_mcp()


@pytest.mark.asyncio
async def test_llmrunner(models_to_mcp):
    prompt = "What is 2 + 2?"
    result = await llmrunner(prompt, models_to_mcp)

    assert isinstance(result, LLMRunnerResults)
    assert isinstance(result.successful_results, list)
    assert isinstance(result.failed_results, list)
    assert isinstance(result.total_models, int)
    assert isinstance(result.success_count, int)
    assert isinstance(result.failure_count, int)

    assert result.total_models == len(models_to_mcp.models_to_mcp)
    assert result.success_count + result.failure_count == result.total_models

    for success_result in result.successful_results:
        assert isinstance(success_result, ModelResult)
        assert success_result.success is True
        assert success_result.model is not None
        assert success_result.timestamp is not None
        assert success_result.response is not None
        assert success_result.duration_seconds is not None

    for failed_result in result.failed_results:
        assert isinstance(failed_result, ModelResult)
        assert failed_result.success is False
        assert failed_result.model is not None
        assert failed_result.timestamp is not None
        assert failed_result.error is not None

    print(f"Total models: {result.total_models}")
    print(f"Successful: {result.success_count}")
    print(f"Failed: {result.failure_count}")

    for failed_result in result.failed_results:
        print(
            f"Failed model: {
              failed_result.model} - Error: {failed_result.error}"
        )

    for success_result in result.successful_results:
        print(f"Successful model: {success_result.model}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="GEMINI.md"&amp;gt;# Gemini Code Assistant Context

This document provides context for the Gemini Code Assistant to understand the project structure, conventions, and important files.

## Project Overview

This project is a Python-based MCP (Model Context Protocol) server named "Collect". Its primary purpose is to fetch web content, process it, and facilitate multi-model AI analysis workflows. It provides a unified interface to interact with various AI models (OpenAI, Anthropic, Gemini, XAI) for tasks like code review. The server is built using the `mcp` library and exposes several tools for fetching URLs, converting HTML to markdown, counting tokens, and more. It also includes a database layer using SQLite for data persistence.

**Key Technologies:**

*   **Programming Language:** Python
*   **Framework:** `mcp` (Model Context Protocol)
*   **Key Libraries:** 
    *   `httpx` for asynchronous HTTP requests.
    *   `anthropic`, `openai`, `google-cloud-aiplatform` for interacting with various LLMs.
    *   `readabilipy`, `markdownify`, `beautifulsoup4` for HTML processing.
    *   `pyperclip` for clipboard integration.
    *   `yoyo-migrations` for database schema management.
*   **Package Manager:** `uv`
*   **Testing:** `pytest` with `pytest-asyncio` and `pytest-xdist` for parallel testing.
*   **Linting/Formatting:** `ruff` and `black`.
*   **Database:** SQLite.

## Building and Running

*   **Install Dependencies:** `uv sync`
*   **Run the Server:** `python collect.py`
*   **Run Tests:** `uv run pytest -v -s -n auto`
*   **Run Linter:** `ruff check .`
*   **Run Formatter:** `black .`
*   **Apply Database Migrations:** `uv run yoyo apply --config yoyo.ini --batch`

## Development Conventions

*   **Testing:** Tests are written using `pytest` and are located in files like `test_collect.py`. Asynchronous functions are tested using `@pytest.mark.asyncio`. The project uses `pytest-xdist` for parallel test execution.
*   **Linting and Formatting:** The project uses `ruff` for linting and `black` for formatting. These are run via the `Makefile`.
*   **Configuration:** Project configuration is managed in `config.py`, which loads environment variables from a `.env` file.
*   **Secrets Management:** API keys and other secrets are managed through Google Cloud Secret Manager, as indicated in `secret_manager.py` and `config.py`.
*   **Database:** The project uses SQLite for its database. The database connection logic is in `repository/database.py`. Migrations are handled by `yoyo-migrations`.

## Key Files

*   **`collect.py`:** The main entry point of the MCP server. It defines the available tools, such as `fetch_urls`, `run_code_review`, and `to_markdown`.
*   **`pyproject.toml`:** Defines the project's dependencies and development tool configurations.
*   **`Makefile`:** Provides convenient commands for common development tasks like testing, linting, and formatting.
*   **`config.py`:** Handles the project's configuration by loading environment variables from a `.env` file.
*   **`reviewer/code_review.py`:** Contains the logic for the code review functionality. It takes a diff file, sends it to multiple LLMs, and then formats and saves the results.
*   **`models/`:** This directory contains modules for interacting with different AI models (e.g., `anthropic_mpc.py`, `openai_mpc.py`).
*   **`repository/database.py`:** Contains the logic for connecting to the SQLite database.
*   **`migrations/`:** This directory contains the SQL migration files for the database schema.
&amp;lt;/file&amp;gt;
  &amp;lt;file path="movetools"&amp;gt;#!/bin/bash

# Enhanced setup script to copy tools to user bin directory with colorful output
# This script should be run from the collect project home directory

# Color definitions for enhanced output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly BOLD='\033[1m'
readonly NC='\033[0m' # No Color

# Set default target directory (configurable)
TARGET_DIR=${1:-~/bin}

# Expand tilde to home directory
TARGET_DIR="${TARGET_DIR/#\~/$HOME}"

# Get the directory where this script is located (project home)
PROJECT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;amp;amp;&amp;amp;amp; pwd )"
TOOLS_DIR="$PROJECT_DIR/tools"

# Enhanced printing functions
print_header() {
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${WHITE}  🔧 MOVETOOLS - Tool Installation Script${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
}

print_info() {
    echo -e "${CYAN}ℹ${NC}  $1"
}

print_success() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC}  $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_step() {
    echo -e "\n${BOLD}${PURPLE}▶${NC} ${BOLD}$1${NC}"
}

print_item() {
    echo -e "   ${GREEN}•${NC} $1"
}

# Function to display usage
usage() {
    print_header
    echo -e "${BOLD}USAGE:${NC}"
    echo -e "  $0 [target_directory]"
    echo ""
    echo -e "${BOLD}DESCRIPTION:${NC}"
    echo -e "  ${BOLD}MOVETOOLS${NC} is a comprehensive installation and backup script for the collect project."
    echo -e "  It performs two main functions:"
    echo ""
    echo -e "  ${BOLD}1. Tool Installation:${NC}"
    echo -e "     • Copies all executable tools from the tools/ directory to your bin directory"
    echo -e "     • Makes all copied tools executable with proper permissions"
    echo -e "     • Validates PATH configuration and provides setup guidance"
    echo -e "     • Provides colorful visual feedback throughout the process"
    echo ""
    echo -e "  ${BOLD}2. Dotfiles Backup:${NC}"
    echo -e "     • Backs up your .zshrc configuration to the project's dotfiles/ directory"
    echo -e "     • Copies your Ghostty terminal configuration (~/.config/ghostty)"
    echo -e "     • Backs up your Neovim init.lua configuration (~/.config/nvim/init.lua)"
    echo -e "     • Creates organized dotfiles structure for version control"
    echo ""
    echo -e "${BOLD}ARGUMENTS:${NC}"
    echo -e "  ${CYAN}target_directory${NC}    Optional. Directory to install tools (default: ~/bin)"
    echo ""
    echo -e "${BOLD}OPTIONS:${NC}"
    echo -e "  ${YELLOW}--llm${NC}              Display comprehensive usage information (LLM-friendly)"
    echo -e "  ${YELLOW}--help, -h${NC}         Display this usage information"
    echo ""
    echo -e "${BOLD}FEATURES:${NC}"
    echo -e "  • Enhanced colorful terminal output with status indicators"
    echo -e "  • Automatic directory creation with proper error handling"
    echo -e "  • Tool validation and permission management"
    echo -e "  • PATH verification with configuration suggestions"
    echo -e "  • Comprehensive dotfiles backup across multiple applications"
    echo -e "  • Installation summary with detailed feedback"
    echo ""
    echo -e "${BOLD}EXAMPLES:${NC}"
    echo -e "  ${GREEN}$0${NC}                 # Install tools to ~/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 ~/.local/bin${NC}    # Install tools to ~/.local/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 --llm${NC}           # Show comprehensive help for AI assistants"
    echo -e "  ${GREEN}$0 --help${NC}          # Show this help message"
    echo ""
    echo -e "${BOLD}DIRECTORY STRUCTURE:${NC}"
    echo -e "  ${CYAN}tools/${NC}              # Source directory containing executable tools"
    echo -e "  ${CYAN}dotfiles/zshrc${NC}      # Backed up shell configuration"
    echo -e "  ${CYAN}dotfiles/ghostty/${NC}   # Backed up terminal configuration"
    echo -e "  ${CYAN}dotfiles/nvim/${NC}      # Backed up Neovim configuration"
    echo ""
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    exit 0
}

# Check for help flags
if [ $# -eq 1 ] &amp;amp;amp;&amp;amp;amp; ([ "$1" = "--llm" ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]); then
    usage
fi

# Display header
print_header

# Display configuration
print_step "Configuration"
print_info "Project directory: ${BOLD}$PROJECT_DIR${NC}"
print_info "Tools directory: ${BOLD}$TOOLS_DIR${NC}"
print_info "Target directory: ${BOLD}$TARGET_DIR${NC}"

# Check if tools directory exists
print_step "Validation"
if [ ! -d "$TOOLS_DIR" ]; then
    print_error "Tools directory not found at $TOOLS_DIR"
    exit 1
fi
print_success "Tools directory found"

# Create target directory if it doesn't exist
if [ ! -d "$TARGET_DIR" ]; then
    print_info "Creating target directory..."
    mkdir -p "$TARGET_DIR"
    if [ $? -eq 0 ]; then
        print_success "Target directory created"
    else
        print_error "Failed to create target directory"
        exit 1
    fi
else
    print_success "Target directory exists"
fi

# Copy all files from tools directory
print_step "Copying Tools"
tool_count=0
copied_tools=()

for tool in "$TOOLS_DIR"/*; do
    if [ -f "$tool" ]; then
        tool_name=$(basename "$tool")
        # Skip CLAUDE.md file
        if [ "$tool_name" = "CLAUDE.md" ]; then
            continue
        fi
        if cp "$tool" "$TARGET_DIR/" 2&amp;amp;gt;/dev/null; then
            print_item "Copied ${BOLD}$tool_name${NC}"
            copied_tools+=("$tool_name")
            ((tool_count++))
        else
            print_error "Failed to copy $tool_name"
        fi
    fi
done

if [ $tool_count -eq 0 ]; then
    print_warning "No tools found in $TOOLS_DIR"
    exit 0
fi

# Make all copied tools executable
print_step "Setting Permissions"
executable_count=0
for tool_name in "${copied_tools[@]}"; do
    tool_path="$TARGET_DIR/$tool_name"
    if chmod u+x "$tool_path" 2&amp;amp;gt;/dev/null; then
        print_item "Made ${BOLD}$tool_name${NC} executable"
        ((executable_count++))
    else
        print_error "Failed to make $tool_name executable"
    fi
done

# Display completion summary
print_step "Summary"
print_success "Installation complete!"
print_info "${BOLD}$tool_count${NC} tools copied and ${BOLD}$executable_count${NC} made executable"
print_info "Tools are now available in: ${BOLD}$TARGET_DIR${NC}"

# List installed tools
if [ ${#copied_tools[@]} -gt 0 ]; then
    echo ""
    print_info "${BOLD}Installed tools:${NC}"
    for tool_name in "${copied_tools[@]}"; do
        echo -e "   ${GREEN}▸${NC} ${BOLD}$tool_name${NC}"
    done
fi

# Check if target directory is in PATH
echo ""
if [[ ":$PATH:" != *":$TARGET_DIR:"* ]]; then
    print_warning "Target directory is not in your PATH"
    echo -e "${YELLOW}💡${NC} To use these tools from anywhere, add this line to your ${BOLD}~/.bashrc${NC} or ${BOLD}~/.zshrc${NC}:"
    echo -e "   ${CYAN}export PATH=\"$TARGET_DIR:\$PATH\"${NC}"
else
    print_success "Target directory is already in your PATH"
fi

# Copy dotfiles section
print_step "Copying Dotfiles"
DOTFILES_DIR="$PROJECT_DIR/dotfiles"

# Ensure dotfiles directory exists
if [ ! -d "$DOTFILES_DIR" ]; then
    print_info "Creating dotfiles directory..."
    mkdir -p "$DOTFILES_DIR"
    if [ $? -eq 0 ]; then
        print_success "Dotfiles directory created"
    else
        print_error "Failed to create dotfiles directory"
    fi
else
    print_success "Dotfiles directory exists"
fi

# Copy .zshrc
if [ -f "$HOME/.zshrc" ]; then
    if cp "$HOME/.zshrc" "$DOTFILES_DIR/.zshrc" 2&amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}.zshrc${NC} from home directory"
    else
        print_error "Failed to copy .zshrc"
    fi
else
    print_warning ".zshrc not found in home directory"
fi

# Copy ghostty config
GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"
if [ -d "$GHOSTTY_CONFIG_DIR" ]; then
    # Create ghostty subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/ghostty"
    if cp -r "$GHOSTTY_CONFIG_DIR"/* "$DOTFILES_DIR/ghostty/" 2&amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}ghostty config${NC} from ~/.config/ghostty"
    else
        print_error "Failed to copy ghostty config"
    fi
else
    print_warning "Ghostty config directory not found at ~/.config/ghostty"
fi

# Copy nvim init.lua
NVIM_CONFIG="$HOME/.config/nvim/init.lua"
if [ -f "$NVIM_CONFIG" ]; then
    # Create nvim subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/nvim"
    if cp "$NVIM_CONFIG" "$DOTFILES_DIR/nvim/init.lua" 2&amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}nvim init.lua${NC} from ~/.config/nvim"
    else
        print_error "Failed to copy nvim init.lua"
    fi
else
    print_warning "Nvim init.lua not found at ~/.config/nvim/init.lua"
fi

echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}${GREEN}🎉 Setup completed successfully!${NC}"
echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
&amp;lt;/file&amp;gt;
  &amp;lt;file path="source.xml"&amp;gt;&amp;amp;lt;?xml version='1.0' encoding='utf-8'?&amp;amp;gt;
&amp;amp;lt;source_code project="collect"&amp;amp;gt;
  &amp;amp;lt;file path="test_generate_prompt.py"&amp;amp;gt;import pytest
from config import Config
from collect import generate_prompt


@pytest.fixture
def sample_prompt():
    """Sample prompt content for testing."""
    return """Create a helpful AI assistant that can answer programming questions.
The assistant should be knowledgeable about Python, JavaScript, and web development.
It should provide clear explanations and code examples when appropriate."""


class TestGeneratePrompt:

    @pytest.mark.asyncio
    async def test_generate_prompt_basic(self, sample_prompt):
        """Test basic functionality of generate_prompt."""
        # Check if we have required config
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(sample_prompt)

        # Verify we got a string response
        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;gt; 0

        # The generated prompt should contain relevant content
        # Note: We can't predict exact content, but it should be substantial
        assert len(result) &amp;amp;amp;gt; 50  # Should be more than just a few words

    @pytest.mark.asyncio
    async def test_generate_prompt_with_target_model(self, sample_prompt):
        """Test generate_prompt with target_model parameter."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(
            sample_prompt, target_model="claude-3-7-sonnet-20250219"
        )

        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;gt; 0

    @pytest.mark.asyncio
    async def test_generate_prompt_empty_string(self):
        """Test error handling for empty prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("")

    @pytest.mark.asyncio
    async def test_generate_prompt_whitespace_only(self):
        """Test error handling for whitespace-only prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("   \n\t   ")

    @pytest.mark.asyncio
    async def test_generate_prompt_simple_task(self):
        """Test with a simple task description."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        simple_task = "A coding assistant that helps with Python"
        result = await generate_prompt(simple_task)

        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;gt; len(simple_task)  # Should be expanded


if __name__ == "__main__":
    # Run a simple test

    async def manual_test():
        """Manual test function for quick verification."""
        test_prompt = "Create a Python function that validates email addresses."

        try:
            result = await generate_prompt(test_prompt)
            print(f"Input prompt: {test_prompt}")
            print(f"Generated prompt ({len(result)} chars):")
            print("-" * 50)
            print(result)
            print("-" * 50)
        except Exception as e:
            print(f"Error: {e}")

    # Uncomment to run manual test
    # asyncio.run(manual_test())
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="collect.py"&amp;amp;gt;from typing import List
from mcp.server.fastmcp import FastMCP, Context
import tiktoken
import markdownify
import readabilipy.simple_json
from html_to_markdown import convert_to_markdown
from bs4 import BeautifulSoup
from secret_manager import SecretManager
from config import Config
from models.anthropic_mpc import AnthropicMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP
from models.gemini_mcp import GeminiMCP
from fetcher import Fetcher
import pyperclip
from reviewer.code_review import CodeReviewer
import subprocess
import atexit
import time

mcp = FastMCP("Collect")


@mcp.tool()
async def run_code_review(from_file: str, to_file: str = "codereview"):
    """
    Run code review on a diff file using multiple LLM models.

    Args:
        from_file: Path to the file containing the diff/code to review
        to_file: Directory name to write results to (default: "codereview")

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_code(from_file, to_file)


@mcp.tool()
async def run_git_diff_review(to_file: str = "codereview", staged_only: bool = True):
    """
    Run code review on git diff output.

    Args:
        to_file: Directory name to write results to(default: "codereview")
        staged_only: If True, review only staged changes;
        if False, review all changes

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_diff_from_git(to_file, staged_only)


@mcp.tool()
async def fetch_urls(urls: List[str], ctx: Context = None) -&amp;amp;amp;gt; str:
    """
    Fetch content from multiple URLs concurrently and merge the responses.

    Use this tool when you need to:
    - Retrieve content from multiple web pages at once
    - Compare information across multiple sources
    - Gather data from several API endpoints simultaneously
    - Fetch related pages in parallel for efficiency

    Args:
        urls: List of URLs to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Merged content from all URLs as a single string

    Example:
        fetch_urls(["https://api.example.com/users",
                   "https://api.example.com/posts"])
    """
    fetcher = Fetcher(ctx)
    merged_responses = await fetcher.fetch_urls(urls)
    return merged_responses


@mcp.tool()
async def fetch_url(url: str, ctx: Context = None) -&amp;amp;amp;gt; str:
    """
    Fetch raw content from a single URL.

    Use this tool when you need to:
    - Retrieve raw HTML/JSON from a web page or API
    - Get unprocessed content for custom parsing
    - Access web resources programmatically
    - Fetch data before converting to markdown

    Args:
        url: The URL to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Raw content from the URL(HTML, JSON, or plain text)

    Note: For documentation extraction, consider using get_docs instead.
          For markdown conversion, use to_markdown on the result.
    """
    fetcher = Fetcher(ctx)
    return fetcher.get(url)


@mcp.tool()
async def get_docs(url: str, extract_value: str = None, ctx: Context = None) -&amp;amp;amp;gt; str:
    """
    Fetch and extract specific documentation content from web pages.

    Use this tool when users need to:
    - Extract specific sections from documentation websites
    - Get targeted information from technical docs
    - Retrieve API documentation for specific methods/classes
    - Pull configuration examples from documentation
    - Find specific topics within large documentation sites

    Args:
        url: The URL of the documentation page to fetch
        extract_value: Optional. Specific section/topic to extract(e.g., "authentication",
                      "API endpoints", "installation guide"). If not provided, returns
                      the entire page content.
        ctx: MCP context(automatically provided)

    Returns:
        Extracted documentation content as markdown. If extract_value is specified,
        uses Gemini AI to intelligently extract only the relevant section.

    Examples:
        - get_docs("https://docs.python.org/3/", "datetime module")
        - get_docs("https://fastapi.tiangolo.com/", "dependency injection")
        - get_docs("https://react.dev/", "useEffect hook")
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-flash-preview-05-20"
    gemini = GeminiMCP(config, secret_mgr, model=model)

    if extract_value is None:
        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        return response
    else:
        prompt_prefatory = f"""
        # Documentation Extraction Task

        Extract and format the documentation for: **{extract_value} **

        # Instructions:
        - Focus specifically on the requested section/topic
        - Include code examples, parameters, and usage details if present
        - Maintain original formatting and structure
        - If the exact section isn't found, extract the most relevant related content
        - Return only the extracted documentation content, no meta-commentary

        ## Content to extract: {extract_value}
        """

        prompt = prompt_prefatory + "\n\n"
        response = await gemini.build_prompt_from_url(url, prompt, ctx)
        return response.strip()


@mcp.tool()
async def copy_clipboard(text: str) -&amp;amp;amp;gt; str:
    """
    Copy text to the system clipboard.

    Use this tool when users need to:
    - Copy generated code snippets to clipboard
    - Save formatted text for pasting elsewhere
    - Copy API keys, URLs, or configuration values
    - Transfer content between applications

    Args:
        text: The text content to copy to clipboard

    Note: The text will replace any existing clipboard content.
    """
    pyperclip.copy(text)


@mcp.tool()
def strip_html(html: str) -&amp;amp;amp;gt; str:
    """
    Remove all HTML tags and return plain text content.

    Use this tool when you need to:
    - Extract plain text from HTML pages
    - Remove formatting and tags from web content
    - Clean HTML for text analysis
    - Prepare content for non-HTML processing

    Args:
        html: Raw HTML string to process

    Returns:
        Plain text with all HTML tags removed

    Note: This removes ALL formatting. For readable formatting, use to_markdown instead.
    """
    soup = BeautifulSoup(html, "lxml")
    return soup.get_text()


@mcp.tool()
def to_markdown(html: str) -&amp;amp;amp;gt; str:
    """Extract and convert HTML content to markdown using markdownify
    and readabilipy

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown

    """
    html_to_json = readabilipy.simple_json.simple_json_from_html_string(
        html,
        use_readability=True,
    )
    if not html_to_json["content"]:
        return "&amp;amp;amp;lt;error&amp;amp;amp;gt;Page failed to be simplified from HTML to json&amp;amp;amp;lt;/error&amp;amp;amp;gt;"

    return markdownify.markdownify(
        html_to_json["content"],
        heading_style=markdownify.ATX,
    )


def html_to_markdown(html: str) -&amp;amp;amp;gt; str:
    """This uses html-to-markdown library instead of markdownify

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown as a str
    """

    return convert_to_markdown(
        html,
        heading_style="atx",
    )


@mcp.tool()
async def get_anthropic_model_list() -&amp;amp;amp;gt; List[str]:
    """
    Get the list of available Anthropic Claude models.

    Use this tool when you need to:
    - Check which Claude models are available
    - Verify model names before using them
    - List Anthropic's current model offerings
    - Help users choose between Claude models

    Returns:
        List of available Anthropic model names (e.g., ["claude-3-opus", "claude-3-sonnet"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.get_model_list()


@mcp.tool()
async def get_openai_model_list() -&amp;amp;amp;gt; List[str]:
    """
    Get the list of available OpenAI models.

    Use this tool when you need to:
    - Check which GPT models are available
    - Verify OpenAI model names
    - List current OpenAI offerings
    - Help users choose between GPT models

    Returns:
        List of available OpenAI model names (e.g., ["gpt-4", "gpt-3.5-turbo"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    openai_mcp = OpenAIMCP(config, secret_mgr, model="gpt-4o")
    return openai_mcp.get_model_list()


@mcp.tool()
async def get_xai_model_list() -&amp;amp;amp;gt; List[str]:
    """
    Get the list of available XAI (Grok) models.

    Use this tool when you need to:
    - Check which Grok models are available
    - Verify XAI model names
    - List current Grok offerings
    - Help users choose between Grok models

    Returns:
        List of available XAI model names (e.g., ["grok-3", "grok-3-mini"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-mini-fast-latest")
    return xai_mcp.get_model_list()


@mcp.tool()
async def get_gemini_model_list() -&amp;amp;amp;gt; List[dict]:
    """
    Get the list of available Google Gemini models
    (filtered for 2.0 and 2.5 versions).

    Use this tool when you need to:
    - Check which Gemini models are available with their token limits
    - Verify Google AI model names and capabilities
    - List current Gemini 2.0 and 2.5 offerings
    - Help users choose between Gemini models based on token capacity

    Returns:
        List of model dictionaries sorted by token limit (highest first),
        each containing:
        - model_name: The model identifier (e.g., "gemini-2.5-flash")
        - token_window: Input token limit (e.g., 1048576)

    Example return:
        [
            {"model_name": "gemini-2.5-flash", "token_window": 1048576},
            {"model_name": "gemini-2.0-flash", "token_window": 1048576},
            {"model_name": "gemini-2.5-pro", "token_window": 1048576}
        ]
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.5-flash")
    return gemini_mcp.get_model_list()


@mcp.tool()
async def count_openai_tokens(text: str, model: str = "gpt-4") -&amp;amp;amp;gt; int:
    """
    Count tokens in text using OpenAI's tiktoken tokenizer.

    Use this tool when you need to:
    - Check if content fits within OpenAI model limits
    - Estimate API costs for OpenAI models
    - Split content to fit token windows
    - Optimize prompts for token efficiency

    Args:
        text: The text to count tokens for
        model: OpenAI model name (default: "gpt-4")

    Returns:
        Number of tokens in the text for the specified model
    """
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@mcp.tool()
async def count_anthropic_tokens(text: str) -&amp;amp;amp;gt; int:
    """
    Count tokens in text using Anthropic's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Claude model limits
    - Estimate API costs for Anthropic models
    - Split content for Claude's context window
    - Optimize prompts for Claude

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Anthropic models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.count_tokens(text)


@mcp.tool()
async def count_gemini_tokens(text: str) -&amp;amp;amp;gt; int:
    """
    Count tokens in text using Google Gemini's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Gemini model limits
    - Estimate API costs for Google AI models
    - Split content for Gemini's context window
    - Optimize prompts for Gemini

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Gemini models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.0-flash")
    return gemini_mcp.count_tokens(text)


@mcp.tool()
async def count_grok_tokens(text: str) -&amp;amp;amp;gt; int:
    """
    Count tokens in text using XAI Grok's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Grok model limits
    - Estimate API costs for XAI models
    - Split content for Grok's context window
    - Optimize prompts for Grok

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Grok models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-fast-latest")
    return xai_mcp.count_tokens(text)


@mcp.tool()
async def generate_prompt(prompt: str, target_model: str = None) -&amp;amp;amp;gt; str:
    """
    Generate an optimized AI prompt using Anthropic's experimental prompt engineering API.

    This tool leverages Anthropic's closed research preview API to automatically create
    high-quality, structured prompts from simple task descriptions. The API analyzes
    your input and generates professional-grade prompts optimized for Claude models.

    Use this tool when you need to:
    - Transform simple ideas into comprehensive AI prompts
    - Create structured prompts for specific tasks or roles
    - Optimize prompts for better AI responses
    - Generate consistent prompt templates for repeated use
    - Improve prompt clarity and effectiveness

    Args:
        prompt: A brief description of what you want the AI to do.
                Can be as simple as a role description or task summary.
                Examples:
                - "a helpful programming assistant"
                - "a chef for meal planning"
                - "a technical documentation writer"
                - "analyze code for security vulnerabilities"
        target_model: Optional. The specific model to optimize for (e.g., "claude-3-opus").
                     If not specified, generates a general-purpose prompt.

    Returns:
        A professionally crafted prompt ready for use with Claude or other AI models.
        The generated prompt includes appropriate context, instructions, and structure
        to maximize response quality.

    Raises:
        ValueError: If the prompt is empty or only contains whitespace
        RuntimeError: If the API call fails or returns an unexpected response

    Example:
        &amp;amp;amp;gt;&amp;amp;amp;gt;&amp;amp;amp;gt; result = await generate_prompt("a Python code reviewer")
        &amp;amp;amp;gt;&amp;amp;amp;gt;&amp;amp;amp;gt; print(result)
        "You are an expert Python code reviewer with deep knowledge..."

    Note:
        This uses Anthropic's experimental "prompt-tools" API which requires special
        access. The API is in closed research preview and may change without notice.
    """
    try:
        # Validate input
        task_content = prompt.strip()
        if not task_content:
            raise ValueError("Prompt cannot be empty")

        # Set up Anthropic MCP client
        config = Config()
        secret_mgr = SecretManager(config.project_id)
        anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)

        # Call generate_prompt API with new signature
        response = anthropic_mcp.generate_prompt(task_content, target_model)

        # Extract the generated prompt text from the response
        if response.messages and response.messages[0].content:
            return response.messages[0].content[0].text
        else:
            raise ValueError("No prompt generated in response")

    except ValueError:
        # Re-raise ValueError (like empty prompt) without wrapping
        raise
    except Exception as e:
        raise RuntimeError(f"Error generating prompt: {str(e)}")


def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["uv", "run", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for API to initialize
        time.sleep(2)

        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")

            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()

            atexit.register(cleanup_api)

    except Exception as e:
        print(f"Failed to start API server: {e}")

    # Continue with MCP server startup
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="config.py"&amp;amp;gt;from dotenv import load_dotenv
import os


class Config:
    # Load .env for configuration
    load_dotenv(".env")

    def __init__(self) -&amp;amp;amp;gt; None:
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.port = os.getenv("PORT")
        self.db_path = os.getenv("DB_PATH")
        self.anthropic_key_path = os.getenv("ANTHROPIC_API_KEY_PATH")
        self.anthropic_model_opus = os.getenv("ANTHROPIC_MODEL_OPUS")
        self.anthropic_model_sonnet = os.getenv("ANTHROPIC_MODEL_SONNET")
        self.gemini_api_key_path = os.getenv("GEMINI_API_KEY_PATH")
        self.gemini_base_url = os.getenv("GEMINI_BASE_URL")
        self.xai_api_key_path = os.getenv("XAI_API_KEY_PATH")
        self.grok_system_prompt = os.getenv("GROK_SYSTEM_PROMPT")
        self.openai_api_key_path = os.getenv("OPENAI_API_KEY_PATH")
        self.openai_default_code_review_model = os.getenv(
            "OPENAI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.gemini_default_code_review_model = os.getenv(
            "GEMINI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.anthropic_default_code_review_model = os.getenv(
            "ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.xai_default_code_review_model = os.getenv("XAI_DEFAULT_CODE_REVIEW_MODEL")

        # GitHub configuration
        self.github_url = os.getenv("GITHUB_URL")

        # Command subdirectories - read as comma-separated string
        command_subdirs_str = os.getenv(
            "COMMAND_SUBDIRS", "archive,go,js,mcp,python,tools"
        )
        self.command_subdirs = [
            subdir.strip() for subdir in command_subdirs_str.split(",")
        ]
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="test_collect.py"&amp;amp;gt;import pytest

from collect import (
    count_anthropic_tokens,
    count_gemini_tokens,
    count_openai_tokens,
    count_grok_tokens,
    get_docs,
)

# The @pytest.mark.parametrize decorator runs the test function
#  test_empty_text_returns_zero three separate times, once for each
#  function in the list. Each time, it passes a different token-counting
#  function to the test as the func parameter.

#  This is useful for testing similar functionality across multiple
#  implementations without duplicating test code. In this case, it verifies
#   that all three token-counting functions return zero when given empty
#  text.


@pytest.mark.asyncio
async def test_openai_hello_token_count():
    result = await count_openai_tokens("hello", model="gpt-3.5-turbo")
    assert result == 1


@pytest.mark.parametrize(
    "func,text",
    [
        (count_openai_tokens, "Hello, world!"),
        (count_gemini_tokens, "Hello, Gemini!"),
        (count_anthropic_tokens, "Hello Claude"),
        (count_grok_tokens, "Hello Grok"),
    ],
)
@pytest.mark.asyncio
async def test_nonempty_text_returns_positive_int(func, text):
    n = await func(text)
    assert isinstance(n, int)
    assert n &amp;amp;amp;gt; 0


@pytest.mark.asyncio
async def test_get_docs_with_extract_value():
    url = "https://docs.python.org/3/library/json.html"
    extract_value = "json.dumps"

    result = await get_docs(url, extract_value)

    assert isinstance(result, str)
    assert len(result) &amp;amp;amp;gt; 0
    assert "json.dumps" in result.lower()

    print(f"Extracted docs for {extract_value}:")
    print(result[:500] + "..." if len(result) &amp;amp;amp;gt; 500 else result)


@pytest.mark.asyncio
async def test_get_docs_without_extract_value():
    url = "https://docs.python.org/3/library/json.html"

    result = await get_docs(url)

    assert isinstance(result, str)
    assert len(result) &amp;amp;amp;gt; 0
    # Should contain raw HTML content when no extraction is performed
    assert "html" in result.lower() or "json" in result.lower()

    print(f"Raw content length: {len(result)}")
    print(result[:200] + "..." if len(result) &amp;amp;amp;gt; 200 else result)
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="requirements.txt"&amp;amp;gt;aiohttp==3.11.11
annotated-types==0.7.0
anthropic==0.50.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.8
distro==1.9.0
docstring-parser==0.16
-e file:///Users/benjaminmetz/python/collect
google-api-core==2.24.2
google-auth==2.39.0
google-cloud-aiplatform==1.91.0
google-cloud-bigquery==3.31.0
google-cloud-core==2.4.3
google-cloud-resource-manager==1.14.2
google-cloud-storage==2.19.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
idna==3.10
iniconfig==2.1.0
jiter==0.9.0
markdown-it-py==3.0.0
mcp==1.7.1
mdurl==0.1.2
numpy==2.2.5
packaging==25.0
pluggy==1.5.0
proto-plus==1.26.1
protobuf==5.29.4
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.11.4
pydantic-core==2.33.2
pydantic-settings==2.9.1
pygments==2.19.1
pyperclip==1.9.0
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
regex==2024.11.6
requests==2.32.3
rich==14.0.0
rsa==4.9.1
sentencepiece==0.2.0
shapely==2.1.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
sse-starlette==2.3.3
starlette==0.46.2
tiktoken==0.9.0
typer==0.15.3
typing-extensions==4.13.2
typing-inspection==0.4.0
urllib3==2.4.0
uvicorn==0.34.2
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="uv.lock"&amp;amp;gt;version = 1
revision = 1
requires-python = "&amp;amp;amp;gt;=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.12.11"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/6b/850a842871ab7be0d00686750d0ee9d8fb8e7be981e4e5700bb6c88f1b8f/aiohttp-3.12.11.tar.gz", hash = "sha256:a5149ae1b11ce4cf8b122846bfa3d7c5f29fe3bfe6745ab21b3eea9615bc5564", size = 7814403 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/ac/15e21c6a17b5183d1617505b125c773f554a56e06be577a289151a8e5ce7/aiohttp-3.12.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:5fadc4b67f972a701805aa501cd9d22cdbeda21f9c9ae85e60678f84b1727a16", size = 694170 },
    { url = "https://files.pythonhosted.org/packages/02/5b/347f8aff5793829b3a31a927bd039ec4f22221a32c459b9d19fe880921e3/aiohttp-3.12.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:144d67c29ae36f052584fc45a363e92798441a5af5762d83037aade3e2aa9dc5", size = 471832 },
    { url = "https://files.pythonhosted.org/packages/4b/e5/9ed82f5b6a2dca30940e90820ce2f8203e15111de464bba0980e2c7e169b/aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6b73299e4bf37d14c6e4ca5ce7087b44914a8d9e1f40faedc271f28d64ec277e", size = 464133 },
    { url = "https://files.pythonhosted.org/packages/3c/8d/edcddc41d4f1157a2536143476070ae66de2b839af3724655c2a6358670a/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1226325e98e6d3cdfdaca639efdc3af8e82cd17287ae393626d1bd60626b0e93", size = 1702942 },
    { url = "https://files.pythonhosted.org/packages/b1/2e/efcb6a35d0646ced659edc3172e8e9384246d2cd0b0f3080fc3c441cb511/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0ecae011f2f779271407f2959877230670de3c48f67e5db9fbafa9fddbfa3a", size = 1684207 },
    { url = "https://files.pythonhosted.org/packages/56/f7/0324c499b7c610633d2f5e8af5457fd3a0584f5f4827bc46b673866596ac/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a8a711883eedcd55f2e1ba218d8224b9f20f1dfac90ffca28e78daf891667e3a", size = 1736275 },
    { url = "https://files.pythonhosted.org/packages/98/0f/b7aa0fd1ed777b5d6fb62c0dcf82effb717e8b51c802067fc3bcb703e003/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2601c1fcd9b67e632548cfd3c760741b31490502f6f3e5e21287678c1c6fa1b2", size = 1785648 },
    { url = "https://files.pythonhosted.org/packages/2c/2a/7defcf31010a2964bf17f6c9d9190e3be889f0c5edc3ff2cdac6e60764d7/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d5b11ea794ee54b33d0d817a1aec0ef0dd2026f070b493bc5a67b7e413b95d4", size = 1707981 },
    { url = "https://files.pythonhosted.org/packages/b6/9e/ff3d9a01f533752e81fd92bfe1301ae5a7bd5a306d752ad54f8bc61570fa/aiohttp-3.12.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:109b3544138ce8a5aca598d5e7ff958699e3e19ee3675d27d5ee9c2e30765a4a", size = 1621683 },
    { url = "https://files.pythonhosted.org/packages/2c/98/446c96927f2e7d2eaea95660a60eb6077771d00df834430cec002cadd96b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b795085d063d24c6d09300c85ddd6b9c49816d5c498b40b6899ca24584e936e4", size = 1674706 },
    { url = "https://files.pythonhosted.org/packages/e1/2a/038cb4af5e58994bc9315d0cb6a906d20ddfffb8eb3d0dfcfe8fe95b1939/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ebcbc113f40e4c9c0f8d2b6b31a2dd2a9768f3fa5f623b7e1285684e24f5159f", size = 1706372 },
    { url = "https://files.pythonhosted.org/packages/28/18/dc16cc7cb9b8baf9308f23ecf1e787d916238d01060bea272d5c29e9aa6b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:590e5d792150d75fa34029d0555b126e65ad50d66818a996303de4af52b65b32", size = 1648967 },
    { url = "https://files.pythonhosted.org/packages/44/f5/f427ef971e00088c7f0f5a4a7e405732e0ce0b87dfc3eec0f1a8c16863d2/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:9c2a4dec596437b02f0c34f92ea799d6e300184a0304c1e54e462af52abeb0a8", size = 1725099 },
    { url = "https://files.pythonhosted.org/packages/d4/0a/34fc018d4e193115b512bc08f6afaf79c23609a6487e47f0d593d1d9df41/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aace119abc495cc4ced8745e3faceb0c22e8202c60b55217405c5f389b569576", size = 1758571 },
    { url = "https://files.pythonhosted.org/packages/b6/69/b466ec346506384a93bcb864ab75a21b6520c64fcc3720ab2056470a657f/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd749731390520a2dc1ce215bcf0ee1018c3e2e3cd834f966a02c0e71ad7d637", size = 1707461 },
    { url = "https://files.pythonhosted.org/packages/f4/fc/3437d3e40581bc7d0816e134fdcae3c7e5c3f21dbdcfbd54402af3973b1c/aiohttp-3.12.11-cp313-cp313-win32.whl", hash = "sha256:65952736356d1fbc9efdd17492dce36e2501f609a14ccb298156e392d3ad8b83", size = 420053 },
    { url = "https://files.pythonhosted.org/packages/6c/cf/cd84df67147c986315c63fef29a6ecadf03bf5528340b8c82eedd988cf57/aiohttp-3.12.11-cp313-cp313-win_amd64.whl", hash = "sha256:854132093e12dd77f5c07975581c42ae51a6a8868dcbbb509c77d1963c3713b7", size = 445988 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.50.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/40/85/4dd9f80da0727c56d7e7f7c627cb724edd9e6df062df6ecc0e90f06e6dbb/anthropic-0.50.0.tar.gz", hash = "sha256:42175ec04ce4ff2fa37cd436710206aadff546ee99d70d974699f59b49adc66f", size = 213021 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/ae/975f97ad5581a9e187a3717e21d79d6c7ad6be926fee9aa8a15b3d9f8f37/anthropic-0.50.0-py3-none-any.whl", hash = "sha256:defbd79327ca2fa61fd7b9eb2f1627dfb1f69c25d49288c52e167ddb84574f80", size = 245291 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "asttokens"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4a/e7/82da0a03e7ba5141f05cce0d302e6eed121ae055e0456ca228bf693984bc/asttokens-3.0.0.tar.gz", hash = "sha256:0dcd8baa8d62b0c1d118b399b2ddba3c4aff271d0d7a9e0d4c1681c79035bbc7", size = 61978 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/8a/c46dcc25341b5bce5472c718902eb3d38600a903b14fa6aeecef3f21a46f/asttokens-3.0.0-py3-none-any.whl", hash = "sha256:e3078351a059199dd5138cb1c706e6430c05eff2ff136af5eb4790f9d28932e2", size = 26918 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285 },
]

[[package]]
name = "black"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "mypy-extensions" },
    { name = "packaging" },
    { name = "pathspec" },
    { name = "platformdirs" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/49/26a7b0f3f35da4b5a65f081943b7bcd22d7002f5f0fb8098ec1ff21cb6ef/black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666", size = 649449 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/87/0edf98916640efa5d0696e1abb0a8357b52e69e82322628f25bf14d263d1/black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f", size = 1650673 },
    { url = "https://files.pythonhosted.org/packages/52/e5/f7bf17207cf87fa6e9b676576749c6b6ed0d70f179a3d812c997870291c3/black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3", size = 1453190 },
    { url = "https://files.pythonhosted.org/packages/e3/ee/adda3d46d4a9120772fae6de454c8495603c37c4c3b9c60f25b1ab6401fe/black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171", size = 1782926 },
    { url = "https://files.pythonhosted.org/packages/cc/64/94eb5f45dcb997d2082f097a3944cfc7fe87e071907f677e80788a2d7b7a/black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18", size = 1442613 },
    { url = "https://files.pythonhosted.org/packages/09/71/54e999902aed72baf26bca0d50781b01838251a462612966e9fc4891eadd/black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717", size = 207646 },
]

[[package]]
name = "cachetools"
version = "5.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/81/3747dad6b14fa2cf53fcf10548cf5aea6913e96fab41a3c198676f8948a5/cachetools-5.5.2.tar.gz", hash = "sha256:1a661caa9175d26759571b2e19580f9d6393969e5dfca11fdb1f947a23e640d4", size = 28380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl", hash = "sha256:d26a22bcc62eb95c3beabd9f1ee5e820d3d2704fe2967cbe350e20c8ffcd3f0a", size = 10080 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "collect"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "anthropic" },
    { name = "beautifulsoup4" },
    { name = "black" },
    { name = "fastapi" },
    { name = "google-ai-generativelanguage" },
    { name = "google-api-python-client" },
    { name = "google-auth-httplib2" },
    { name = "google-cloud-aiplatform", extra = ["tokenization"] },
    { name = "google-cloud-secret-manager" },
    { name = "google-genai" },
    { name = "google-generativeai" },
    { name = "html-to-markdown" },
    { name = "html5lib" },
    { name = "httplib2" },
    { name = "httpx" },
    { name = "ipython" },
    { name = "lxml" },
    { name = "marimo" },
    { name = "markdownify" },
    { name = "mcp", extra = ["cli"] },
    { name = "openai" },
    { name = "pathspec" },
    { name = "pyperclip" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
    { name = "pytest-xdist" },
    { name = "python-json-logger" },
    { name = "readabilipy" },
    { name = "rich" },
    { name = "ruff" },
    { name = "tiktoken" },
    { name = "uvicorn" },
    { name = "yoyo-migrations" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = "&amp;amp;amp;gt;=3.12.11" },
    { name = "anthropic", specifier = "&amp;amp;amp;gt;=0.50.0" },
    { name = "beautifulsoup4", specifier = "&amp;amp;amp;gt;=4.13.4" },
    { name = "black", specifier = "&amp;amp;amp;gt;=25.1.0" },
    { name = "fastapi", specifier = "&amp;amp;amp;gt;=0.116.1" },
    { name = "google-ai-generativelanguage", specifier = "&amp;amp;amp;gt;=0.6.15" },
    { name = "google-api-python-client", specifier = "&amp;amp;amp;gt;=2.169.0" },
    { name = "google-auth-httplib2", specifier = "&amp;amp;amp;gt;=0.2.0" },
    { name = "google-cloud-aiplatform", extras = ["tokenization"], specifier = "&amp;amp;amp;gt;=1.91.0" },
    { name = "google-cloud-secret-manager", specifier = "&amp;amp;amp;gt;=2.23.3" },
    { name = "google-genai", specifier = "&amp;amp;amp;gt;=1.13.0" },
    { name = "google-generativeai", specifier = "&amp;amp;amp;gt;=0.8.5" },
    { name = "html-to-markdown", specifier = "&amp;amp;amp;gt;=1.3.2" },
    { name = "html5lib", specifier = "&amp;amp;amp;gt;=1.1" },
    { name = "httplib2", specifier = "&amp;amp;amp;gt;=0.22.0" },
    { name = "httpx", specifier = "&amp;amp;amp;gt;=0.28.1" },
    { name = "ipython", specifier = "&amp;amp;amp;gt;=9.4.0" },
    { name = "lxml", specifier = "&amp;amp;amp;gt;=5.4.0" },
    { name = "marimo", specifier = "&amp;amp;amp;gt;=0.14.12" },
    { name = "markdownify", specifier = "&amp;amp;amp;gt;=1.1.0" },
    { name = "mcp", extras = ["cli"], specifier = "&amp;amp;amp;gt;=1.7.1" },
    { name = "openai", specifier = "&amp;amp;amp;gt;=1.59.4" },
    { name = "pathspec", specifier = "&amp;amp;amp;gt;=0.12.1" },
    { name = "pyperclip", specifier = "&amp;amp;amp;gt;=1.9.0" },
    { name = "pytest", specifier = "&amp;amp;amp;gt;=8.3.5" },
    { name = "pytest-asyncio", specifier = "&amp;amp;amp;gt;=0.26.0" },
    { name = "pytest-xdist", specifier = "&amp;amp;amp;gt;=3.6.1" },
    { name = "python-json-logger", specifier = "&amp;amp;amp;gt;=3.3.0" },
    { name = "readabilipy", specifier = "&amp;amp;amp;gt;=0.3.0" },
    { name = "rich", specifier = "&amp;amp;amp;gt;=14.0.0" },
    { name = "ruff", specifier = "&amp;amp;amp;gt;=0.11.9" },
    { name = "tiktoken", specifier = "&amp;amp;amp;gt;=0.9.0" },
    { name = "uvicorn", specifier = "&amp;amp;amp;gt;=0.34.2" },
    { name = "yoyo-migrations", specifier = "&amp;amp;amp;gt;=9.0.0" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "decorator"
version = "5.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/fa/6d96a0978d19e17b68d634497769987b16c8f4cd0a7a05048bec693caa6b/decorator-5.2.1.tar.gz", hash = "sha256:65f266143752f734b0a7cc83c46f4618af75b8c5911b00ccb61d0ac9b6da0360", size = 56711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl", hash = "sha256:d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a", size = 9190 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "docstring-parser"
version = "0.16"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/08/12/9c22a58c0b1e29271051222d8906257616da84135af9ed167c9e28f85cb3/docstring_parser-0.16.tar.gz", hash = "sha256:538beabd0af1e2db0146b6bd3caa526c35a34d61af9fd2887f3a8a27a739aa6e", size = 26565 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl", hash = "sha256:bf0a1387354d3691d102edef7ec124f219ef639982d096e26e3b60aeffa90637", size = 36533 },
]

[[package]]
name = "docutils"
version = "0.21.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ae/ed/aefcc8cd0ba62a0560c3c18c33925362d46c6075480bfa4df87b28e169a9/docutils-0.21.2.tar.gz", hash = "sha256:3a6b18732edf182daa3cd12775bbb338cf5691468f91eeeb109deff6ebfa986f", size = 2204444 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/d7/9322c609343d929e75e7e5e6255e614fcc67572cfd083959cdef3b7aad79/docutils-0.21.2-py3-none-any.whl", hash = "sha256:dafca5b9e384f0e419294eb4d2ff9fa826435bf15f15b7bd45723e8ad76811b2", size = 587408 },
]

[[package]]
name = "execnet"
version = "2.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/ff/b4c0dc78fbe20c3e59c0c7334de0c27eb4001a2b2017999af398bf730817/execnet-2.1.1.tar.gz", hash = "sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3", size = 166524 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl", hash = "sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc", size = 40612 },
]

[[package]]
name = "executing"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/50/a9d80c47ff289c611ff12e63f7c5d13942c65d68125160cefd768c73e6e4/executing-2.2.0.tar.gz", hash = "sha256:5d108c028108fe2551d1a7b2e8b713341e2cb4fc0aa7dcf966fa4327a5226755", size = 978693 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7b/8f/c4d9bafc34ad7ad5d8dc16dd1347ee0e507a52c3adb6bfa8887e1c6a26ba/executing-2.2.0-py2.py3-none-any.whl", hash = "sha256:11387150cad388d62750327a53d3339fad4888b39a6fe233c3afbb54ecffd3aa", size = 26702 },
]

[[package]]
name = "fastapi"
version = "0.116.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/78/d7/6c8b3bfe33eeffa208183ec037fee0cce9f7f024089ab1c5d12ef04bd27c/fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143", size = 296485 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565", size = 95631 },
]

[[package]]
name = "frozenlist"
version = "1.6.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5b/bf/a812e2fe6cb3f6c6cfc8d0303bf1742f2286004e5ec41ac8c89cf68cdb54/frozenlist-1.6.2.tar.gz", hash = "sha256:effc641518696471cf4962e8e32050133bc1f7b2851ae8fd0cb8797dd70dc202", size = 43108 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b8/f6/973abfcb8b68f2e8b58071a04ec72f5e1f0acd19dae0d3b7a8abc3d9ab07/frozenlist-1.6.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2ad8851ae1f6695d735f8646bf1e68675871789756f7f7e8dc8224a74eabb9d0", size = 85517 },
    { url = "https://files.pythonhosted.org/packages/c8/d0/ac45f2dcf0afd5f7d57204af8b7516ecbc3599ea681e06f4b25d3845bea8/frozenlist-1.6.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:cd2d5abc0ccd99a2a5b437987f3b1e9c265c1044d2855a09ac68f09bbb8082ca", size = 49916 },
    { url = "https://files.pythonhosted.org/packages/50/cc/99c3f31823630b7411f7c1e83399e91d6b56a5661a5b724935ef5b51f5f5/frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15c33f665faa9b8f8e525b987eeaae6641816e0f6873e8a9c4d224338cebbb55", size = 48107 },
    { url = "https://files.pythonhosted.org/packages/85/4e/38643ce3ee80d222892b694d02c15ea476c4d564493a6fe530347163744e/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3e6c0681783723bb472b6b8304e61ecfcb4c2b11cf7f243d923813c21ae5d2a", size = 255771 },
    { url = "https://files.pythonhosted.org/packages/ca/e6/ceed85a7d5c0f666485384fc393e32353f8088e154a1109e5ef60165d366/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:61bae4d345a26550d0ed9f2c9910ea060f89dbfc642b7b96e9510a95c3a33b3c", size = 252519 },
    { url = "https://files.pythonhosted.org/packages/29/99/9f2e2b90cf918465e3b6ca4eea79e6be53d24fba33937e37d86c3764bbf9/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90e5a84016d0d2fb828f770ede085b5d89155fcb9629b8a3237c960c41c120c3", size = 263348 },
    { url = "https://files.pythonhosted.org/packages/4e/ac/59f3ec4c1b4897186efb4757379915734a48bb16bbc15a9fe0bf0857b679/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55dc289a064c04819d669e6e8a85a1c0416e6c601782093bdc749ae14a2f39da", size = 257858 },
    { url = "https://files.pythonhosted.org/packages/48/4a/19c97510d0c2be1ebaae68383d1b5a256a12a660ca17b0c427b1024d9b92/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b79bcf97ca03c95b044532a4fef6e5ae106a2dd863875b75fde64c553e3f4820", size = 238248 },
    { url = "https://files.pythonhosted.org/packages/ef/64/641aa2b0944fa3d881323948e0d8d6fee746dae03d9023eb510bb80bc46a/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e5e7564d232a782baa3089b25a0d979e2e4d6572d3c7231fcceacc5c22bf0f7", size = 255932 },
    { url = "https://files.pythonhosted.org/packages/6c/f8/5b68d5658fac7332e5d26542a4af0ffc2edca8da8f854f6274882889ee1e/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6fcd8d56880dccdd376afb18f483ab55a0e24036adc9a83c914d4b7bb5729d4e", size = 253329 },
    { url = "https://files.pythonhosted.org/packages/e9/20/379d7a27eb82748b41319bf376bf2c034e7ee11dda94f12b331edcc261ff/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4fbce985c7fe7bafb4d9bf647c835dbe415b465a897b0c79d1bdf0f3fae5fe50", size = 266164 },
    { url = "https://files.pythonhosted.org/packages/13/bd/d7dbf94220020850392cb661bedfdf786398bafae85d1045dd108971d261/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:3bd12d727cd616387d50fe283abebb2db93300c98f8ff1084b68460acd551926", size = 241641 },
    { url = "https://files.pythonhosted.org/packages/a4/70/916fef6284d294077265cd69ad05f228e44f7ed88d9acb690df5a1174049/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:38544cae535ed697960891131731b33bb865b7d197ad62dc380d2dbb1bceff48", size = 261215 },
    { url = "https://files.pythonhosted.org/packages/8f/98/1326a7189fa519692698cddf598f56766b0fea6ac71cddaf64760a055397/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:47396898f98fae5c9b9bb409c3d2cf6106e409730f35a0926aad09dd7acf1ef5", size = 262597 },
    { url = "https://files.pythonhosted.org/packages/f4/d6/0a95ab9289c72e86c37c9b8afe82576556456b6f66a35d242526634130f2/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:d10d835f8ce8571fd555db42d3aef325af903535dad7e6faa7b9c8abe191bffc", size = 258766 },
    { url = "https://files.pythonhosted.org/packages/1b/d0/9e946aabd89ebfcb71ec1371327f0e25d4868cd4439471a6fcb6eaf7b366/frozenlist-1.6.2-cp313-cp313-win32.whl", hash = "sha256:a400fe775a41b6d7a3fef00d88f10cbae4f0074c9804e282013d7797671ba58d", size = 40961 },
    { url = "https://files.pythonhosted.org/packages/43/e9/d714f5eb0fde1413344ded982ae9638307b59651d5c04263af42eb81a315/frozenlist-1.6.2-cp313-cp313-win_amd64.whl", hash = "sha256:cc8b25b321863ed46992558a29bb09b766c41e25f31461666d501be0f893bada", size = 46204 },
    { url = "https://files.pythonhosted.org/packages/f5/7a/8f6dde73862499e60eb390778a1e46b87c1fe3c5722622d731ccda7a173c/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:56de277a0e0ad26a1dcdc99802b4f5becd7fd890807b68e3ecff8ced01d58132", size = 91326 },
    { url = "https://files.pythonhosted.org/packages/79/60/dcdc75edbcf8241e7cb15fced68b3be63f67ff3faaf559c540a7eb63233b/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:9cb386dd69ae91be586aa15cb6f39a19b5f79ffc1511371eca8ff162721c4867", size = 52426 },
    { url = "https://files.pythonhosted.org/packages/64/e6/df2a43ccb2c4f1ea3692aae9a89cfc5dd932a90b7898f98f13ed9e2680a9/frozenlist-1.6.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53835d8a6929c2f16e02616f8b727bd140ce8bf0aeddeafdb290a67c136ca8ad", size = 51460 },
    { url = "https://files.pythonhosted.org/packages/fd/b3/c4f2f7fca9487b25c39bf64535f029316e184072a82f3660ce72defc5421/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cc49f2277e8173abf028d744f8b7d69fe8cc26bffc2de97d47a3b529599fbf50", size = 310270 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/046eb34d8d0fee1a8c9dc91a9ba581283c67a1ace20bcc01c86a53595105/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:65eb9e8a973161bdac5fa06ea6bd261057947adc4f47a7a6ef3d6db30c78c5b4", size = 289062 },
    { url = "https://files.pythonhosted.org/packages/48/7b/80991efaa0aa25e867cf93033c28e9d1310f34f90421eb59eb1f2073d937/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:301eb2f898d863031f8c5a56c88a6c5d976ba11a4a08a1438b96ee3acb5aea80", size = 312202 },
    { url = "https://files.pythonhosted.org/packages/78/6b/6fe30bdababdf82c5b34f0093770c4be6211071e23570721b80b11c9d52a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:207f717fd5e65fddb77d33361ab8fa939f6d89195f11307e073066886b33f2b8", size = 309557 },
    { url = "https://files.pythonhosted.org/packages/9d/ef/b7bf48802fc7d084703ba2173e6a8d0590bea378dcd6a480051c41bddf47/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f83992722642ee0db0333b1dbf205b1a38f97d51a7382eb304ba414d8c3d1e05", size = 282135 },
    { url = "https://files.pythonhosted.org/packages/af/f8/6911a085bce8d0d0df3dfc2560e3e0fb4d6c19ff101014bcf61aa32ba39a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12af99e6023851b36578e5bcc60618b5b30f4650340e29e565cd1936326dbea7", size = 303392 },
    { url = "https://files.pythonhosted.org/packages/9c/5d/b4e0cc6dbd6b9282926a470a919da7c6599ff324ab5268c7ecaff82cb858/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6f01620444a674eaad900a3263574418e99c49e2a5d6e5330753857363b5d59f", size = 309402 },
    { url = "https://files.pythonhosted.org/packages/0f/1b/bf777de3c810e68e8758337fcc97ee8c956376c87aecee9a61ba19a94123/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:82b94c8948341512306ca8ccc702771600b442c6abe5f8ee017e00e452a209e8", size = 312924 },
    { url = "https://files.pythonhosted.org/packages/0e/03/a69b890bc310790fcae61fd3b5be64876811b12db5d50b32e62f65e766bd/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:324a4cf4c220ddb3db1f46ade01e48432c63fa8c26812c710006e7f6cfba4a08", size = 291768 },
    { url = "https://files.pythonhosted.org/packages/70/cc/559386adf987b47c8977c929271d11a72efd92778a0a2f4cc97827a9a25b/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:695284e51458dabb89af7f7dc95c470aa51fd259207aba5378b187909297feef", size = 313305 },
    { url = "https://files.pythonhosted.org/packages/e7/fa/eb0e21730ffccfb2d0d367d863cbaacf8367bdc277b44eabf72f7329ab91/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:9ccbeb1c8dda4f42d0678076aa5cbde941a232be71c67b9d8ca89fbaf395807c", size = 312228 },
    { url = "https://files.pythonhosted.org/packages/d1/c1/8471b67172abc9478ad78c70a3f3a5c4fed6d4bcadc748e1b6dfa06ab2ae/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cbbdf62fcc1864912c592a1ec748fee94f294c6b23215d5e8e9569becb7723ee", size = 309905 },
    { url = "https://files.pythonhosted.org/packages/bb/2c/ee21987c3a175b49d0b827b1e45394a7a5d08c7de5b766ed6d0889d30568/frozenlist-1.6.2-cp313-cp313t-win32.whl", hash = "sha256:76857098ee17258df1a61f934f2bae052b8542c9ea6b187684a737b2e3383a65", size = 44644 },
    { url = "https://files.pythonhosted.org/packages/65/46/fce60f65b1fb17a90c4bf410a5c90cb3b40616cc229e75866f8be97c112c/frozenlist-1.6.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c06a88daba7e891add42f9278cdf7506a49bc04df9b1648be54da1bf1c79b4c6", size = 50607 },
    { url = "https://files.pythonhosted.org/packages/13/be/0ebbb283f2d91b72beaee2d07760b2c47dab875c49c286f5591d3d157198/frozenlist-1.6.2-py3-none-any.whl", hash = "sha256:947abfcc8c42a329bbda6df97a4b9c9cdb4e12c85153b3b57b9d2f02aa5877dc", size = 12582 },
]

[[package]]
name = "google-ai-generativelanguage"
version = "0.6.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/11/d1/48fe5d7a43d278e9f6b5ada810b0a3530bbeac7ed7fcbcd366f932f05316/google_ai_generativelanguage-0.6.15.tar.gz", hash = "sha256:8f6d9dc4c12b065fe2d0289026171acea5183ebf2d0b11cefe12f3821e159ec3", size = 1375443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/a3/67b8a6ff5001a1d8864922f2d6488dc2a14367ceb651bc3f09a947f2f306/google_ai_generativelanguage-0.6.15-py3-none-any.whl", hash = "sha256:5a03ef86377aa184ffef3662ca28f19eeee158733e45d7947982eb953c6ebb6c", size = 1327356 },
]

[[package]]
name = "google-api-core"
version = "2.24.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "googleapis-common-protos" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/09/5c/085bcb872556934bb119e5e09de54daa07873f6866b8f0303c49e72287f7/google_api_core-2.24.2.tar.gz", hash = "sha256:81718493daf06d96d6bc76a91c23874dbf2fac0adbbf542831b805ee6e974696", size = 163516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/95/f472d85adab6e538da2025dfca9e976a0d125cc0af2301f190e77b76e51c/google_api_core-2.24.2-py3-none-any.whl", hash = "sha256:810a63ac95f3c441b7c0e43d344e372887f62ce9071ba972eacf32672e072de9", size = 160061 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
    { name = "grpcio-status" },
]

[[package]]
name = "google-api-python-client"
version = "2.171.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-auth-httplib2" },
    { name = "httplib2" },
    { name = "uritemplate" },
]
sdist = { url = "https://files.pythonhosted.org/packages/35/99/237cd2510aecca9fabb54007e58553274cc43cb3c18512ee1ea574d11b87/google_api_python_client-2.171.0.tar.gz", hash = "sha256:057a5c08d28463c6b9eb89746355de5f14b7ed27a65c11fdbf1d06c66bb66b23", size = 13028937 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/db/c397e3eb3ea18f423855479d0a5852bdc9c3f644e3d4194931fa664a70b4/google_api_python_client-2.171.0-py3-none-any.whl", hash = "sha256:c9c9b76f561e9d9ac14e54a9e2c0842876201d5b96e69e48f967373f0784cbe9", size = 13547393 },
]

[[package]]
name = "google-auth"
version = "2.39.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cachetools" },
    { name = "pyasn1-modules" },
    { name = "rsa" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/8e/8f45c9a32f73e786e954b8f9761c61422955d23c45d1e8c347f9b4b59e8e/google_auth-2.39.0.tar.gz", hash = "sha256:73222d43cdc35a3aeacbfdcaf73142a97839f10de930550d89ebfe1d0a00cde7", size = 274834 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/12/ad37a1ef86006d0a0117fc06a4a00bd461c775356b534b425f00dde208ea/google_auth-2.39.0-py2.py3-none-any.whl", hash = "sha256:0150b6711e97fb9f52fe599f55648950cc4540015565d8fbb31be2ad6e1548a2", size = 212319 },
]

[[package]]
name = "google-auth-httplib2"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "httplib2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/56/be/217a598a818567b28e859ff087f347475c807a5649296fb5a817c58dacef/google-auth-httplib2-0.2.0.tar.gz", hash = "sha256:38aa7badf48f974f1eb9861794e9c0cb2a0511a4ec0679b1f886d108f5640e05", size = 10842 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl", hash = "sha256:b65a0a2123300dd71281a7bf6e64d65a0759287df52729bdd1ae2e47dc311a3d", size = 9253 },
]

[[package]]
name = "google-cloud-aiplatform"
version = "1.91.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "docstring-parser" },
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-bigquery" },
    { name = "google-cloud-resource-manager" },
    { name = "google-cloud-storage" },
    { name = "packaging" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "shapely" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/50/08/5854569782efbbc8efd0aeda3a4486153605104cbab6ac836b2328bae48e/google_cloud_aiplatform-1.91.0.tar.gz", hash = "sha256:b14e5e52b52b6012c7dc253beab34c511fdc53c69b13f436ddb06882c1a92cd7", size = 9102586 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/88/cea8583fadd142e8ef26f8ec14a6ee4d7c69c4e5ab82bea01a077fddddbe/google_cloud_aiplatform-1.91.0-py2.py3-none-any.whl", hash = "sha256:ff8df100c2af692d114a2219d3abbb96110b3e5655f342fdbb6aefad43901b52", size = 7591910 },
]

[package.optional-dependencies]
tokenization = [
    { name = "sentencepiece" },
]

[[package]]
name = "google-cloud-bigquery"
version = "3.31.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-resumable-media" },
    { name = "packaging" },
    { name = "python-dateutil" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/73/91/4c7274f4d5faf13ac000b06353deaf3579575bf0e4bbad07fa68b9f09ba9/google_cloud_bigquery-3.31.0.tar.gz", hash = "sha256:b89dc716dbe4abdb7a4f873f7050100287bc98514e0614c5d54cd6a8e9fb0991", size = 479961 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e8/bc/4cb8c61fc6dd817a4a390b745ec7b305f4578f547a16d09d54c8a790624b/google_cloud_bigquery-3.31.0-py3-none-any.whl", hash = "sha256:97f4a3219854ff01d6a3a57312feecb0b6e13062226b823f867e2d3619c4787b", size = 250099 },
]

[[package]]
name = "google-cloud-core"
version = "2.4.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/b8/2b53838d2acd6ec6168fd284a990c76695e84c65deee79c9f3a4276f6b4f/google_cloud_core-2.4.3.tar.gz", hash = "sha256:1fab62d7102844b278fe6dead3af32408b1df3eb06f5c7e8634cbd40edc4da53", size = 35861 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl", hash = "sha256:5130f9f4c14b4fafdff75c79448f9495cfade0d8775facf1b09c3bf67e027f6e", size = 29348 },
]

[[package]]
name = "google-cloud-resource-manager"
version = "1.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6e/ca/a4648f5038cb94af4b3942815942a03aa9398f9fb0bef55b3f1585b9940d/google_cloud_resource_manager-1.14.2.tar.gz", hash = "sha256:962e2d904c550d7bac48372607904ff7bb3277e3bb4a36d80cc9a37e28e6eb74", size = 446370 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/ea/a92631c358da377af34d3a9682c97af83185c2d66363d5939ab4a1169a7f/google_cloud_resource_manager-1.14.2-py3-none-any.whl", hash = "sha256:d0fa954dedd1d2b8e13feae9099c01b8aac515b648e612834f9942d2795a9900", size = 394344 },
]

[[package]]
name = "google-cloud-secret-manager"
version = "2.24.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/7a/2fa6735ec693d822fe08a76709c4d95d9b5b4c02e83e720497355039d2ee/google_cloud_secret_manager-2.24.0.tar.gz", hash = "sha256:ce573d40ffc2fb7d01719243a94ee17aa243ea642a6ae6c337501e58fbf642b5", size = 269516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/af/db1217cae1809e69a4527ee6293b82a9af2a1fb2313ad110c775e8f3c820/google_cloud_secret_manager-2.24.0-py3-none-any.whl", hash = "sha256:9bea1254827ecc14874bc86c63b899489f8f50bfe1442bfb2517530b30b3a89b", size = 218050 },
]

[[package]]
name = "google-cloud-storage"
version = "2.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-crc32c" },
    { name = "google-resumable-media" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/36/76/4d965702e96bb67976e755bed9828fa50306dca003dbee08b67f41dd265e/google_cloud_storage-2.19.0.tar.gz", hash = "sha256:cd05e9e7191ba6cb68934d8eb76054d9be4562aa89dbc4236feee4d7d51342b2", size = 5535488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/94/6db383d8ee1adf45dc6c73477152b82731fa4c4a46d9c1932cc8757e0fd4/google_cloud_storage-2.19.0-py2.py3-none-any.whl", hash = "sha256:aeb971b5c29cf8ab98445082cbfe7b161a1f48ed275822f59ed3f1524ea54fba", size = 131787 },
]

[[package]]
name = "google-crc32c"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/19/ae/87802e6d9f9d69adfaedfcfd599266bf386a54d0be058b532d04c794f76d/google_crc32c-1.7.1.tar.gz", hash = "sha256:2bff2305f98846f3e825dbeec9ee406f89da7962accdb29356e4eadc251bd472", size = 14495 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/72/b8d785e9184ba6297a8620c8a37cf6e39b81a8ca01bb0796d7cbb28b3386/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_arm64.whl", hash = "sha256:df8b38bdaf1629d62d51be8bdd04888f37c451564c2042d36e5812da9eff3c35", size = 30467 },
    { url = "https://files.pythonhosted.org/packages/34/25/5f18076968212067c4e8ea95bf3b69669f9fc698476e5f5eb97d5b37999f/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_x86_64.whl", hash = "sha256:e42e20a83a29aa2709a0cf271c7f8aefaa23b7ab52e53b322585297bb94d4638", size = 30309 },
    { url = "https://files.pythonhosted.org/packages/92/83/9228fe65bf70e93e419f38bdf6c5ca5083fc6d32886ee79b450ceefd1dbd/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:905a385140bf492ac300026717af339790921f411c0dfd9aa5a9e69a08ed32eb", size = 33133 },
    { url = "https://files.pythonhosted.org/packages/c3/ca/1ea2fd13ff9f8955b85e7956872fdb7050c4ace8a2306a6d177edb9cf7fe/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b211ddaf20f7ebeec5c333448582c224a7c90a9d98826fbab82c0ddc11348e6", size = 32773 },
    { url = "https://files.pythonhosted.org/packages/89/32/a22a281806e3ef21b72db16f948cad22ec68e4bdd384139291e00ff82fe2/google_crc32c-1.7.1-cp313-cp313-win_amd64.whl", hash = "sha256:0f99eaa09a9a7e642a61e06742856eec8b19fc0037832e03f941fe7cf0c8e4db", size = 33475 },
    { url = "https://files.pythonhosted.org/packages/b8/c5/002975aff514e57fc084ba155697a049b3f9b52225ec3bc0f542871dd524/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32d1da0d74ec5634a05f53ef7df18fc646666a25efaaca9fc7dcfd4caf1d98c3", size = 33243 },
    { url = "https://files.pythonhosted.org/packages/61/cb/c585282a03a0cea70fcaa1bf55d5d702d0f2351094d663ec3be1c6c67c52/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e10554d4abc5238823112c2ad7e4560f96c7bf3820b202660373d769d9e6e4c9", size = 32870 },
]

[[package]]
name = "google-genai"
version = "1.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "google-auth" },
    { name = "httpx" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/14/17/8f717f43732ae2b7775f816f0d8f0b39e2a020bbe7ba202f2ddb2f948c3b/google_genai-1.19.0.tar.gz", hash = "sha256:66f5de78075781bfd9e423f1e3592e4240759dfe0ac42ac74a9dcb2c4f662e9d", size = 198000 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c4/ae/64fccdebf5811453ce53b0d5ee23d4f27ef173ef36d3b67dad791a0007aa/google_genai-1.19.0-py3-none-any.whl", hash = "sha256:a2955612e4af8c84f83eb43c1ce4e74e1b714732926d0705e639761938192466", size = 200043 },
]

[[package]]
name = "google-generativeai"
version = "0.8.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-ai-generativelanguage" },
    { name = "google-api-core" },
    { name = "google-api-python-client" },
    { name = "google-auth" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/40/c42ff9ded9f09ec9392879a8e6538a00b2dc185e834a3392917626255419/google_generativeai-0.8.5-py3-none-any.whl", hash = "sha256:22b420817fb263f8ed520b33285f45976d5b21e904da32b80d4fd20c055123a2", size = 155427 },
]

[[package]]
name = "google-resumable-media"
version = "2.7.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-crc32c" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/5a/0efdc02665dca14e0837b62c8a1a93132c264bd02054a15abb2218afe0ae/google_resumable_media-2.7.2.tar.gz", hash = "sha256:5280aed4629f2b60b847b0d42f9857fd4935c11af266744df33d8074cae92fe0", size = 2163099 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl", hash = "sha256:3ce7551e9fe6d99e9a126101d2536612bb73486721951e9562fee0f90c6ababa", size = 81251 },
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/39/24/33db22342cf4a2ea27c9955e6713140fedd51e8b141b5ce5260897020f1a/googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257", size = 145903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/f1/62a193f0227cf15a920390abe675f386dec35f7ae3ffe6da582d3ade42c7/googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8", size = 294530 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
]

[[package]]
name = "grpc-google-iam-v1"
version = "0.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos", extra = ["grpc"] },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/4e/8d0ca3b035e41fe0b3f31ebbb638356af720335e5a11154c330169b40777/grpc_google_iam_v1-0.14.2.tar.gz", hash = "sha256:b3e1fc387a1a329e41672197d0ace9de22c78dd7d215048c4c78712073f7bd20", size = 16259 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/66/6f/dd9b178aee7835b96c2e63715aba6516a9d50f6bebbd1cc1d32c82a2a6c3/grpc_google_iam_v1-0.14.2-py3-none-any.whl", hash = "sha256:a3171468459770907926d56a440b2bb643eec1d7ba215f48f3ecece42b4d8351", size = 19242 },
]

[[package]]
name = "grpcio"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/95/aa11fc09a85d91fbc7dd405dcb2a1e0256989d67bf89fa65ae24b3ba105a/grpcio-1.71.0.tar.gz", hash = "sha256:2b85f7820475ad3edec209d3d89a7909ada16caab05d3f2e08a7e8ae3200a55c", size = 12549828 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/dd/b00cbb45400d06b26126dcfdbdb34bb6c4f28c3ebbd7aea8228679103ef6/grpcio-1.71.0-cp313-cp313-linux_armv7l.whl", hash = "sha256:cebc1b34ba40a312ab480ccdb396ff3c529377a2fce72c45a741f7215bfe8379", size = 5184138 },
    { url = "https://files.pythonhosted.org/packages/ed/0a/4651215983d590ef53aac40ba0e29dda941a02b097892c44fa3357e706e5/grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl", hash = "sha256:85da336e3649a3d2171e82f696b5cad2c6231fdd5bad52616476235681bee5b3", size = 11310747 },
    { url = "https://files.pythonhosted.org/packages/57/a3/149615b247f321e13f60aa512d3509d4215173bdb982c9098d78484de216/grpcio-1.71.0-cp313-cp313-manylinux_2_17_aarch64.whl", hash = "sha256:f9a412f55bb6e8f3bb000e020dbc1e709627dcb3a56f6431fa7076b4c1aab0db", size = 5653991 },
    { url = "https://files.pythonhosted.org/packages/ca/56/29432a3e8d951b5e4e520a40cd93bebaa824a14033ea8e65b0ece1da6167/grpcio-1.71.0-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:47be9584729534660416f6d2a3108aaeac1122f6b5bdbf9fd823e11fe6fbaa29", size = 6312781 },
    { url = "https://files.pythonhosted.org/packages/a3/f8/286e81a62964ceb6ac10b10925261d4871a762d2a763fbf354115f9afc98/grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c9c80ac6091c916db81131d50926a93ab162a7e97e4428ffc186b6e80d6dda4", size = 5910479 },
    { url = "https://files.pythonhosted.org/packages/35/67/d1febb49ec0f599b9e6d4d0d44c2d4afdbed9c3e80deb7587ec788fcf252/grpcio-1.71.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:789d5e2a3a15419374b7b45cd680b1e83bbc1e52b9086e49308e2c0b5bbae6e3", size = 6013262 },
    { url = "https://files.pythonhosted.org/packages/a1/04/f9ceda11755f0104a075ad7163fc0d96e2e3a9fe25ef38adfc74c5790daf/grpcio-1.71.0-cp313-cp313-musllinux_1_1_i686.whl", hash = "sha256:1be857615e26a86d7363e8a163fade914595c81fec962b3d514a4b1e8760467b", size = 6643356 },
    { url = "https://files.pythonhosted.org/packages/fb/ce/236dbc3dc77cf9a9242adcf1f62538734ad64727fabf39e1346ad4bd5c75/grpcio-1.71.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:a76d39b5fafd79ed604c4be0a869ec3581a172a707e2a8d7a4858cb05a5a7637", size = 6186564 },
    { url = "https://files.pythonhosted.org/packages/10/fd/b3348fce9dd4280e221f513dd54024e765b21c348bc475516672da4218e9/grpcio-1.71.0-cp313-cp313-win32.whl", hash = "sha256:74258dce215cb1995083daa17b379a1a5a87d275387b7ffe137f1d5131e2cfbb", size = 3601890 },
    { url = "https://files.pythonhosted.org/packages/be/f8/db5d5f3fc7e296166286c2a397836b8b042f7ad1e11028d82b061701f0f7/grpcio-1.71.0-cp313-cp313-win_amd64.whl", hash = "sha256:22c3bc8d488c039a199f7a003a38cb7635db6656fa96437a8accde8322ce2366", size = 4273308 },
]

[[package]]
name = "grpcio-status"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos" },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d7/53/a911467bece076020456401f55a27415d2d70d3bc2c37af06b44ea41fc5c/grpcio_status-1.71.0.tar.gz", hash = "sha256:11405fed67b68f406b3f3c7c5ae5104a79d2d309666d10d61b152e91d28fb968", size = 13669 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ad/d6/31fbc43ff097d8c4c9fc3df741431b8018f67bf8dfbe6553a555f6e5f675/grpcio_status-1.71.0-py3-none-any.whl", hash = "sha256:843934ef8c09e3e858952887467f8256aac3910c55f077a359a65b2b3cde3e68", size = 14424 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "html-to-markdown"
version = "1.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1d/48/324d3d938e5ff635497965118df510f62725b72e8b378b8710c03b0dd014/html_to_markdown-1.3.3.tar.gz", hash = "sha256:ad4f992d65d96d53e49d0a56a2ae0c52ef606c17592d2d9a87f99e4632a4a9e3", size = 15491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a3/d0/b96f7e3579cada841657e5764bc294bd2abb6c1e1dbcfb88ecf7a63ea5d9/html_to_markdown-1.3.3-py3-none-any.whl", hash = "sha256:09325777400e561d2c5a1569f475f9434e70a6f8ed1b4866bba8d00906136495", size = 14951 },
]

[[package]]
name = "html5lib"
version = "1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
    { name = "webencodings" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ac/b6/b55c3f49042f1df3dcd422b7f224f939892ee94f22abcf503a9b7339eaf2/html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f", size = 272215 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d", size = 112173 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httplib2"
version = "0.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyparsing" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/ad/2371116b22d616c194aa25ec410c9c6c37f23599dcd590502b74db197584/httplib2-0.22.0.tar.gz", hash = "sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81", size = 351116 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl", hash = "sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc", size = 96854 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "zipp" },
]
sdist = { url = "https://files.pythonhosted.org/packages/76/66/650a33bd90f786193e4de4b3ad86ea60b53c89b669a5c7be931fac31cdb0/importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000", size = 56641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd", size = 27656 },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050 },
]

[[package]]
name = "ipython"
version = "9.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "decorator" },
    { name = "ipython-pygments-lexers" },
    { name = "jedi" },
    { name = "matplotlib-inline" },
    { name = "pexpect", marker = "sys_platform != 'emscripten' and sys_platform != 'win32'" },
    { name = "prompt-toolkit" },
    { name = "pygments" },
    { name = "stack-data" },
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/54/80/406f9e3bde1c1fd9bf5a0be9d090f8ae623e401b7670d8f6fdf2ab679891/ipython-9.4.0.tar.gz", hash = "sha256:c033c6d4e7914c3d9768aabe76bbe87ba1dc66a92a05db6bfa1125d81f2ee270", size = 4385338 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/63/f8/0031ee2b906a15a33d6bfc12dd09c3dfa966b3cb5b284ecfb7549e6ac3c4/ipython-9.4.0-py3-none-any.whl", hash = "sha256:25850f025a446d9b359e8d296ba175a36aedd32e83ca9b5060430fe16801f066", size = 611021 },
]

[[package]]
name = "ipython-pygments-lexers"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/4c/5dd1d8af08107f88c7f741ead7a40854b8ac24ddf9ae850afbcf698aa552/ipython_pygments_lexers-1.1.1.tar.gz", hash = "sha256:09c0138009e56b6854f9535736f4171d855c8c08a563a0dcd8022f78355c7e81", size = 8393 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl", hash = "sha256:a9462224a505ade19a605f71f8fa63c2048833ce50abc86768a0d81d876dc81c", size = 8074 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jedi"
version = "0.19.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "parso" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/3a/79a912fbd4d8dd6fbb02bf69afd3bb72cf0c729bb3063c6f4498603db17a/jedi-0.19.2.tar.gz", hash = "sha256:4770dc3de41bde3966b02eb84fbcf557fb33cce26ad23da12c742fb50ecb11f0", size = 1231287 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl", hash = "sha256:a8ef22bde8490f57fe5c7681a3c83cb58874daf72b4784de3cce5b6ef6edb5b9", size = 1572278 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "loro"
version = "1.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a0/32/ce94b1fc342ac90d9ca21bc6e90c727990734a75505cb893b2a71a364faf/loro-1.5.2.tar.gz", hash = "sha256:70e52acb16474f7c1e52aea2a7fe2771516f1e9f73d4edfe40f3193b122402c7", size = 62538 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/09/061e8cecb42f99856580811156d7651d5e8172bb840224c7cd2eb94a8730/loro-1.5.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:dbb94c104e3aba4ea3f1118c72896de978e737bb066a35051bf49895e72540a7", size = 3098320 },
    { url = "https://files.pythonhosted.org/packages/60/6e/96cb1a78869c8ae91e65d73ef4ee9f74bc16fd3baff5a7463f7702687dab/loro-1.5.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:847a10f493399f9b650b588b3d81893dfaa1e45e7091881268094f2b9f7df38b", size = 2882026 },
    { url = "https://files.pythonhosted.org/packages/eb/e7/2a131e3e8072614af1cc2970efc1c30a812eb8b0f5286c7b6b390ae3fc9f/loro-1.5.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:902215b77b35e58286d907e8292f78b014cd9c55a46bc5deb944f555509b7747", size = 3110094 },
    { url = "https://files.pythonhosted.org/packages/8c/63/34efc556a5a7663f045d64b9744c10f7b00386f252fac47c939f1c1795be/loro-1.5.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:19e8c9896348063721ef56631d2275c186faf63f6336079c57f41055c9cc1c30", size = 3202938 },
    { url = "https://files.pythonhosted.org/packages/67/3f/5a37b5f1bec5d633f469754e26bf0ce77a26f7697cd95d0b4a51b9cd90be/loro-1.5.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91e75cd4b26506bb5b564ed24b433147fc8b77e8779b5736bc4f3bfddf270590", size = 3579945 },
    { url = "https://files.pythonhosted.org/packages/78/b3/cd3202d6398524c5e1442688c6825e148eb953aa0de04952fd546c69a398/loro-1.5.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:41e54109599190dede34366476a8f42ae6e9fd7fd439823150e9f70e39d7d54e", size = 3318843 },
    { url = "https://files.pythonhosted.org/packages/a5/65/8ed127c827ed9b540f5660e9c98265702dbfdd71ad59063bd3c799ca0dda/loro-1.5.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd3f330795212f24b9dd710f952f7f7138ba86d6159f524025eb4627641ed4ef", size = 3243417 },
    { url = "https://files.pythonhosted.org/packages/4e/29/6894f6db7a1eb7d5d2936b658b3a26c4ea8ce6b0563dde024b909a63289d/loro-1.5.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ebdd716ce67c182f71a093c552f9a47428f7a3d93b038780bbb0f06779805d0", size = 3511123 },
    { url = "https://files.pythonhosted.org/packages/17/26/230867103d5ec58ef18f8d0bc169a4defb4f865f9969247d4e9c723ae10e/loro-1.5.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a8ac5ff8b697e9a828fe4387da715d78d0f2afcf23bbd76f5089b4122f5e78a3", size = 3256828 },
    { url = "https://files.pythonhosted.org/packages/79/8b/7aed297d9cc236e15674275364e37e938e9335c9dfad49ad35904fa8b1f3/loro-1.5.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3dce7920c45c9c884246898805b270d63550a5dec61d3f33274010c40127a37c", size = 3464838 },
    { url = "https://files.pythonhosted.org/packages/1d/c1/352fd39b61a842dc991bf95aaa75db34b6c353c1a3844da17e01f917deb5/loro-1.5.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:66afec16e22db99f1818906bc7cabda0cb077e0e493882b4c0983a8bc431413d", size = 3502790 },
    { url = "https://files.pythonhosted.org/packages/2c/11/859dfc28b1397d731d2cc710dae0e7cb1cbeb45ab70ec518b4ed4f690a4c/loro-1.5.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9f052715922592f099e9b6553fccb48761c5ad83deefcb0df55effde309eb12d", size = 3414408 },
    { url = "https://files.pythonhosted.org/packages/86/3e/fcd87311399e2eff892fb3a6b6f1d3307a2dfd99811fddf0889bee89d585/loro-1.5.2-cp313-cp313-win32.whl", hash = "sha256:978e9f6b0c9ad8c6b1ab70372eafbe00c41782522b216802cf961a81edd27561", size = 2580638 },
    { url = "https://files.pythonhosted.org/packages/93/06/dd73ca0865630923f18fa4486e66a171a0a26ae8e7541f1c3d93100f1f5b/loro-1.5.2-cp313-cp313-win_amd64.whl", hash = "sha256:3ecebbf9f5f880c6ca9a1628e5f469d3d67b67c1fd50536c52c5f6eae01be549", size = 2743550 },
    { url = "https://files.pythonhosted.org/packages/d2/70/9e5030bb9f1b86520f482605f660e5a192d6f5e56104fee122fe7d3dc72e/loro-1.5.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:354de426d6404cce252fb81be17a1589f1bd47197ba7f730f60fbb52452f49ab", size = 3106619 },
    { url = "https://files.pythonhosted.org/packages/2b/37/43c8e3fa8c6239be1b22c0dfd779a4ab000682dddebc23becd057668c436/loro-1.5.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:18e3b6f07483c5553795fea05c8d318f96c018909dd390c68b81701afb12cac3", size = 3195270 },
    { url = "https://files.pythonhosted.org/packages/b1/d6/8aaa433d08710cb1b95781d56efad366350082798463e35b5a6a4988b160/loro-1.5.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2298b96c5f533807373db27dbf5b10c88f1c5d9e0145feb952e7a813a81af645", size = 3575129 },
    { url = "https://files.pythonhosted.org/packages/51/4e/44425f11da9b5278653c3ca01cdfd4da850f94ead5843d8134043ac825cf/loro-1.5.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0aa8edef791c1b46e19bf86ab17f9dbefc61b8f1fbecc49054d5eb880380d897", size = 3317031 },
    { url = "https://files.pythonhosted.org/packages/3b/ae/af1713c7c3cc91a9d6cc1b812733665875eb30c22e4c9e0e213a9a69b1a2/loro-1.5.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:633c026cbb17c485de40f09aab13362f0c79140913dc67445606e3237092d70f", size = 3251501 },
    { url = "https://files.pythonhosted.org/packages/4b/df/958e8abb78ca47ce06e0088bc5d44b5945ffbd08503936cbc0340b62a5f3/loro-1.5.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:903fed16d40b0373f747ecc398f5b86aaab16c37b4c670f580c2c5301bad4de5", size = 3456858 },
    { url = "https://files.pythonhosted.org/packages/f1/f6/982af3432bde075f1fd3201de0e95f35a868f4e85cee36bb22bb0524b069/loro-1.5.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2f9f77b1f582d86e1a57cdb38a43ea1a5861a6f0d73783335c2efdc3d1dcb793", size = 3494470 },
    { url = "https://files.pythonhosted.org/packages/47/b3/a4725db48fb4c7637076023ccedf7dcb7f24a3d266208f2e2aafb8179861/loro-1.5.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:489230b2716c0a2ad50e205670abed029ba0787c028a62dd31226f7935f5d1fd", size = 3410923 },
]

[[package]]
name = "lxml"
version = "5.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/76/3d/14e82fc7c8fb1b7761f7e748fd47e2ec8276d137b6acfe5a4bb73853e08f/lxml-5.4.0.tar.gz", hash = "sha256:d12832e1dbea4be280b22fd0ea7c9b87f0d8fc51ba06e92dc62d52f804f78ebd", size = 3679479 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/cb/2ba1e9dd953415f58548506fa5549a7f373ae55e80c61c9041b7fd09a38a/lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:773e27b62920199c6197130632c18fb7ead3257fce1ffb7d286912e56ddb79e0", size = 8110086 },
    { url = "https://files.pythonhosted.org/packages/b5/3e/6602a4dca3ae344e8609914d6ab22e52ce42e3e1638c10967568c5c1450d/lxml-5.4.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ce9c671845de9699904b1e9df95acfe8dfc183f2310f163cdaa91a3535af95de", size = 4404613 },
    { url = "https://files.pythonhosted.org/packages/4c/72/bf00988477d3bb452bef9436e45aeea82bb40cdfb4684b83c967c53909c7/lxml-5.4.0-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9454b8d8200ec99a224df8854786262b1bd6461f4280064c807303c642c05e76", size = 5012008 },
    { url = "https://files.pythonhosted.org/packages/92/1f/93e42d93e9e7a44b2d3354c462cd784dbaaf350f7976b5d7c3f85d68d1b1/lxml-5.4.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cccd007d5c95279e529c146d095f1d39ac05139de26c098166c4beb9374b0f4d", size = 4760915 },
    { url = "https://files.pythonhosted.org/packages/45/0b/363009390d0b461cf9976a499e83b68f792e4c32ecef092f3f9ef9c4ba54/lxml-5.4.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0fce1294a0497edb034cb416ad3e77ecc89b313cff7adbee5334e4dc0d11f422", size = 5283890 },
    { url = "https://files.pythonhosted.org/packages/19/dc/6056c332f9378ab476c88e301e6549a0454dbee8f0ae16847414f0eccb74/lxml-5.4.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:24974f774f3a78ac12b95e3a20ef0931795ff04dbb16db81a90c37f589819551", size = 4812644 },
    { url = "https://files.pythonhosted.org/packages/ee/8a/f8c66bbb23ecb9048a46a5ef9b495fd23f7543df642dabeebcb2eeb66592/lxml-5.4.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:497cab4d8254c2a90bf988f162ace2ddbfdd806fce3bda3f581b9d24c852e03c", size = 4921817 },
    { url = "https://files.pythonhosted.org/packages/04/57/2e537083c3f381f83d05d9b176f0d838a9e8961f7ed8ddce3f0217179ce3/lxml-5.4.0-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:e794f698ae4c5084414efea0f5cc9f4ac562ec02d66e1484ff822ef97c2cadff", size = 4753916 },
    { url = "https://files.pythonhosted.org/packages/d8/80/ea8c4072109a350848f1157ce83ccd9439601274035cd045ac31f47f3417/lxml-5.4.0-cp313-cp313-manylinux_2_28_ppc64le.whl", hash = "sha256:2c62891b1ea3094bb12097822b3d44b93fc6c325f2043c4d2736a8ff09e65f60", size = 5289274 },
    { url = "https://files.pythonhosted.org/packages/b3/47/c4be287c48cdc304483457878a3f22999098b9a95f455e3c4bda7ec7fc72/lxml-5.4.0-cp313-cp313-manylinux_2_28_s390x.whl", hash = "sha256:142accb3e4d1edae4b392bd165a9abdee8a3c432a2cca193df995bc3886249c8", size = 4874757 },
    { url = "https://files.pythonhosted.org/packages/2f/04/6ef935dc74e729932e39478e44d8cfe6a83550552eaa072b7c05f6f22488/lxml-5.4.0-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:1a42b3a19346e5601d1b8296ff6ef3d76038058f311902edd574461e9c036982", size = 4947028 },
    { url = "https://files.pythonhosted.org/packages/cb/f9/c33fc8daa373ef8a7daddb53175289024512b6619bc9de36d77dca3df44b/lxml-5.4.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4291d3c409a17febf817259cb37bc62cb7eb398bcc95c1356947e2871911ae61", size = 4834487 },
    { url = "https://files.pythonhosted.org/packages/8d/30/fc92bb595bcb878311e01b418b57d13900f84c2b94f6eca9e5073ea756e6/lxml-5.4.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4f5322cf38fe0e21c2d73901abf68e6329dc02a4994e483adbcf92b568a09a54", size = 5381688 },
    { url = "https://files.pythonhosted.org/packages/43/d1/3ba7bd978ce28bba8e3da2c2e9d5ae3f8f521ad3f0ca6ea4788d086ba00d/lxml-5.4.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0be91891bdb06ebe65122aa6bf3fc94489960cf7e03033c6f83a90863b23c58b", size = 5242043 },
    { url = "https://files.pythonhosted.org/packages/ee/cd/95fa2201041a610c4d08ddaf31d43b98ecc4b1d74b1e7245b1abdab443cb/lxml-5.4.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:15a665ad90054a3d4f397bc40f73948d48e36e4c09f9bcffc7d90c87410e478a", size = 5021569 },
    { url = "https://files.pythonhosted.org/packages/2d/a6/31da006fead660b9512d08d23d31e93ad3477dd47cc42e3285f143443176/lxml-5.4.0-cp313-cp313-win32.whl", hash = "sha256:d5663bc1b471c79f5c833cffbc9b87d7bf13f87e055a5c86c363ccd2348d7e82", size = 3485270 },
    { url = "https://files.pythonhosted.org/packages/fc/14/c115516c62a7d2499781d2d3d7215218c0731b2c940753bf9f9b7b73924d/lxml-5.4.0-cp313-cp313-win_amd64.whl", hash = "sha256:bcb7a1096b4b6b24ce1ac24d4942ad98f983cd3810f9711bcd0293f43a9d8b9f", size = 3814606 },
]

[[package]]
name = "marimo"
version = "0.14.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "docutils" },
    { name = "itsdangerous" },
    { name = "jedi" },
    { name = "loro" },
    { name = "markdown" },
    { name = "narwhals" },
    { name = "packaging" },
    { name = "psutil" },
    { name = "pygments" },
    { name = "pymdown-extensions" },
    { name = "pyyaml" },
    { name = "starlette" },
    { name = "tomlkit" },
    { name = "uvicorn" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0b/6d/8c0bdb68d608561e3039718f171ede292e7da7e7580a51b1f4b2ce6e204f/marimo-0.14.12.tar.gz", hash = "sha256:cf18513e30a5d2e8864930885b674dd89cbc9ad3a5e128b9ecfa48323de6d14f", size = 29622446 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/fa/d802cd61fb4714c17529057dc4b07d48c3e115d0af331907b3d19f5482f6/marimo-0.14.12-py3-none-any.whl", hash = "sha256:154d168ceb8b9f4cc10f8cd9f6299cf0c5d8643b0291370a9e64a88b2f517ed3", size = 30118091 },
]

[[package]]
name = "markdown"
version = "3.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/c2/4ab49206c17f75cb08d6311171f2d65798988db4360c4d1485bd0eedd67c/markdown-3.8.2.tar.gz", hash = "sha256:247b9a70dd12e27f67431ce62523e675b866d254f900c4fe75ce3dda62237c45", size = 362071 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl", hash = "sha256:5c83764dbd4e00bdd94d85a19b8d55ccca20fe35b2e678a1422b380324dd5f24", size = 106827 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "markdownify"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2f/78/c48fed23c7aebc2c16049062e72de1da3220c274de59d28c942acdc9ffb2/markdownify-1.1.0.tar.gz", hash = "sha256:449c0bbbf1401c5112379619524f33b63490a8fa479456d41de9dc9e37560ebd", size = 17127 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/11/b751af7ad41b254a802cf52f7bc1fca7cabe2388132f2ce60a1a6b9b9622/markdownify-1.1.0-py3-none-any.whl", hash = "sha256:32a5a08e9af02c8a6528942224c91b933b4bd2c7d078f9012943776fc313eeef", size = 13901 },
]

[[package]]
name = "matplotlib-inline"
version = "0.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/5b/a36a337438a14116b16480db471ad061c36c3694df7c2084a0da7ba538b7/matplotlib_inline-0.1.7.tar.gz", hash = "sha256:8423b23ec666be3d16e16b60bdd8ac4e86e840ebd1dd11a30b9f117f2fa0ab90", size = 8159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/8e/9ad090d3553c280a8060fbf6e24dc1c0c29704ee7d1c372f0c174aa59285/matplotlib_inline-0.1.7-py3-none-any.whl", hash = "sha256:df192d39a4ff8f21b1895d72e6a13f5fcc5099f00fa84384e0ea28c2cc0653ca", size = 9899 },
]

[[package]]
name = "mcp"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/25/ae/588691c45b38f4fbac07fa3d6d50cea44cc6b35d16ddfdf26e17a0467ab2/mcp-1.7.1.tar.gz", hash = "sha256:eb4f1f53bd717f75dda8a1416e00804b831a8f3c331e23447a03b78f04b43a6e", size = 230903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ae/79/fe0e20c3358997a80911af51bad927b5ea2f343ef95ab092b19c9cc48b59/mcp-1.7.1-py3-none-any.whl", hash = "sha256:f7e6108977db6d03418495426c7ace085ba2341b75197f8727f96f9cfd30057a", size = 100365 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "multidict"
version = "6.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/2f/a3470242707058fe856fe59241eee5635d79087100b7042a867368863a27/multidict-6.4.4.tar.gz", hash = "sha256:69ee9e6ba214b5245031b76233dd95408a0fd57fdb019ddcc1ead4790932a8e8", size = 90183 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/2a/e166d2ffbf4b10131b2d5b0e458f7cee7d986661caceae0de8753042d4b2/multidict-6.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:82ffabefc8d84c2742ad19c37f02cde5ec2a1ee172d19944d380f920a340e4b9", size = 64123 },
    { url = "https://files.pythonhosted.org/packages/8c/96/e200e379ae5b6f95cbae472e0199ea98913f03d8c9a709f42612a432932c/multidict-6.4.4-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6a2f58a66fe2c22615ad26156354005391e26a2f3721c3621504cd87c1ea87bf", size = 38049 },
    { url = "https://files.pythonhosted.org/packages/75/fb/47afd17b83f6a8c7fa863c6d23ac5ba6a0e6145ed8a6bcc8da20b2b2c1d2/multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5883d6ee0fd9d8a48e9174df47540b7545909841ac82354c7ae4cbe9952603bd", size = 37078 },
    { url = "https://files.pythonhosted.org/packages/fa/70/1af3143000eddfb19fd5ca5e78393985ed988ac493bb859800fe0914041f/multidict-6.4.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9abcf56a9511653fa1d052bfc55fbe53dbee8f34e68bd6a5a038731b0ca42d15", size = 224097 },
    { url = "https://files.pythonhosted.org/packages/b1/39/d570c62b53d4fba844e0378ffbcd02ac25ca423d3235047013ba2f6f60f8/multidict-6.4.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:6ed5ae5605d4ad5a049fad2a28bb7193400700ce2f4ae484ab702d1e3749c3f9", size = 230768 },
    { url = "https://files.pythonhosted.org/packages/fd/f8/ed88f2c4d06f752b015933055eb291d9bc184936903752c66f68fb3c95a7/multidict-6.4.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bbfcb60396f9bcfa63e017a180c3105b8c123a63e9d1428a36544e7d37ca9e20", size = 231331 },
    { url = "https://files.pythonhosted.org/packages/9c/6f/8e07cffa32f483ab887b0d56bbd8747ac2c1acd00dc0af6fcf265f4a121e/multidict-6.4.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b0f1987787f5f1e2076b59692352ab29a955b09ccc433c1f6b8e8e18666f608b", size = 230169 },
    { url = "https://files.pythonhosted.org/packages/e6/2b/5dcf173be15e42f330110875a2668ddfc208afc4229097312212dc9c1236/multidict-6.4.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1d0121ccce8c812047d8d43d691a1ad7641f72c4f730474878a5aeae1b8ead8c", size = 222947 },
    { url = "https://files.pythonhosted.org/packages/39/75/4ddcbcebe5ebcd6faa770b629260d15840a5fc07ce8ad295a32e14993726/multidict-6.4.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:83ec4967114295b8afd120a8eec579920c882831a3e4c3331d591a8e5bfbbc0f", size = 215761 },
    { url = "https://files.pythonhosted.org/packages/6a/c9/55e998ae45ff15c5608e384206aa71a11e1b7f48b64d166db400b14a3433/multidict-6.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:995f985e2e268deaf17867801b859a282e0448633f1310e3704b30616d269d69", size = 227605 },
    { url = "https://files.pythonhosted.org/packages/04/49/c2404eac74497503c77071bd2e6f88c7e94092b8a07601536b8dbe99be50/multidict-6.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:d832c608f94b9f92a0ec8b7e949be7792a642b6e535fcf32f3e28fab69eeb046", size = 226144 },
    { url = "https://files.pythonhosted.org/packages/62/c5/0cd0c3c6f18864c40846aa2252cd69d308699cb163e1c0d989ca301684da/multidict-6.4.4-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d21c1212171cf7da703c5b0b7a0e85be23b720818aef502ad187d627316d5645", size = 221100 },
    { url = "https://files.pythonhosted.org/packages/71/7b/f2f3887bea71739a046d601ef10e689528d4f911d84da873b6be9194ffea/multidict-6.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:cbebaa076aaecad3d4bb4c008ecc73b09274c952cf6a1b78ccfd689e51f5a5b0", size = 232731 },
    { url = "https://files.pythonhosted.org/packages/e5/b3/d9de808349df97fa75ec1372758701b5800ebad3c46ae377ad63058fbcc6/multidict-6.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:c93a6fb06cc8e5d3628b2b5fda215a5db01e8f08fc15fadd65662d9b857acbe4", size = 229637 },
    { url = "https://files.pythonhosted.org/packages/5e/57/13207c16b615eb4f1745b44806a96026ef8e1b694008a58226c2d8f5f0a5/multidict-6.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8cd8f81f1310182362fb0c7898145ea9c9b08a71081c5963b40ee3e3cac589b1", size = 225594 },
    { url = "https://files.pythonhosted.org/packages/3a/e4/d23bec2f70221604f5565000632c305fc8f25ba953e8ce2d8a18842b9841/multidict-6.4.4-cp313-cp313-win32.whl", hash = "sha256:3e9f1cd61a0ab857154205fb0b1f3d3ace88d27ebd1409ab7af5096e409614cd", size = 35359 },
    { url = "https://files.pythonhosted.org/packages/a7/7a/cfe1a47632be861b627f46f642c1d031704cc1c0f5c0efbde2ad44aa34bd/multidict-6.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:8ffb40b74400e4455785c2fa37eba434269149ec525fc8329858c862e4b35373", size = 38903 },
    { url = "https://files.pythonhosted.org/packages/68/7b/15c259b0ab49938a0a1c8f3188572802704a779ddb294edc1b2a72252e7c/multidict-6.4.4-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:6a602151dbf177be2450ef38966f4be3467d41a86c6a845070d12e17c858a156", size = 68895 },
    { url = "https://files.pythonhosted.org/packages/f1/7d/168b5b822bccd88142e0a3ce985858fea612404edd228698f5af691020c9/multidict-6.4.4-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d2b9712211b860d123815a80b859075d86a4d54787e247d7fbee9db6832cf1c", size = 40183 },
    { url = "https://files.pythonhosted.org/packages/e0/b7/d4b8d98eb850ef28a4922ba508c31d90715fd9b9da3801a30cea2967130b/multidict-6.4.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:d2fa86af59f8fc1972e121ade052145f6da22758f6996a197d69bb52f8204e7e", size = 39592 },
    { url = "https://files.pythonhosted.org/packages/18/28/a554678898a19583548e742080cf55d169733baf57efc48c2f0273a08583/multidict-6.4.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50855d03e9e4d66eab6947ba688ffb714616f985838077bc4b490e769e48da51", size = 226071 },
    { url = "https://files.pythonhosted.org/packages/ee/dc/7ba6c789d05c310e294f85329efac1bf5b450338d2542498db1491a264df/multidict-6.4.4-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5bce06b83be23225be1905dcdb6b789064fae92499fbc458f59a8c0e68718601", size = 222597 },
    { url = "https://files.pythonhosted.org/packages/24/4f/34eadbbf401b03768dba439be0fb94b0d187facae9142821a3d5599ccb3b/multidict-6.4.4-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:66ed0731f8e5dfd8369a883b6e564aca085fb9289aacabd9decd70568b9a30de", size = 228253 },
    { url = "https://files.pythonhosted.org/packages/c0/e6/493225a3cdb0d8d80d43a94503fc313536a07dae54a3f030d279e629a2bc/multidict-6.4.4-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:329ae97fc2f56f44d91bc47fe0972b1f52d21c4b7a2ac97040da02577e2daca2", size = 226146 },
    { url = "https://files.pythonhosted.org/packages/2f/70/e411a7254dc3bff6f7e6e004303b1b0591358e9f0b7c08639941e0de8bd6/multidict-6.4.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c27e5dcf520923d6474d98b96749e6805f7677e93aaaf62656005b8643f907ab", size = 220585 },
    { url = "https://files.pythonhosted.org/packages/08/8f/beb3ae7406a619100d2b1fb0022c3bb55a8225ab53c5663648ba50dfcd56/multidict-6.4.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:058cc59b9e9b143cc56715e59e22941a5d868c322242278d28123a5d09cdf6b0", size = 212080 },
    { url = "https://files.pythonhosted.org/packages/9c/ec/355124e9d3d01cf8edb072fd14947220f357e1c5bc79c88dff89297e9342/multidict-6.4.4-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:69133376bc9a03f8c47343d33f91f74a99c339e8b58cea90433d8e24bb298031", size = 226558 },
    { url = "https://files.pythonhosted.org/packages/fd/22/d2b95cbebbc2ada3be3812ea9287dcc9712d7f1a012fad041770afddb2ad/multidict-6.4.4-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d6b15c55721b1b115c5ba178c77104123745b1417527ad9641a4c5e2047450f0", size = 212168 },
    { url = "https://files.pythonhosted.org/packages/4d/c5/62bfc0b2f9ce88326dbe7179f9824a939c6c7775b23b95de777267b9725c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:a887b77f51d3d41e6e1a63cf3bc7ddf24de5939d9ff69441387dfefa58ac2e26", size = 217970 },
    { url = "https://files.pythonhosted.org/packages/79/74/977cea1aadc43ff1c75d23bd5bc4768a8fac98c14e5878d6ee8d6bab743c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:632a3bf8f1787f7ef7d3c2f68a7bde5be2f702906f8b5842ad6da9d974d0aab3", size = 226980 },
    { url = "https://files.pythonhosted.org/packages/48/fc/cc4a1a2049df2eb84006607dc428ff237af38e0fcecfdb8a29ca47b1566c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:a145c550900deb7540973c5cdb183b0d24bed6b80bf7bddf33ed8f569082535e", size = 220641 },
    { url = "https://files.pythonhosted.org/packages/3b/6a/a7444d113ab918701988d4abdde373dbdfd2def7bd647207e2bf645c7eac/multidict-6.4.4-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cc5d83c6619ca5c9672cb78b39ed8542f1975a803dee2cda114ff73cbb076edd", size = 221728 },
    { url = "https://files.pythonhosted.org/packages/2b/b0/fdf4c73ad1c55e0f4dbbf2aa59dd37037334091f9a4961646d2b7ac91a86/multidict-6.4.4-cp313-cp313t-win32.whl", hash = "sha256:3312f63261b9df49be9d57aaa6abf53a6ad96d93b24f9cc16cf979956355ce6e", size = 41913 },
    { url = "https://files.pythonhosted.org/packages/8e/92/27989ecca97e542c0d01d05a98a5ae12198a243a9ee12563a0313291511f/multidict-6.4.4-cp313-cp313t-win_amd64.whl", hash = "sha256:ba852168d814b2c73333073e1c7116d9395bea69575a01b0b3c89d2d5a87c8fb", size = 46112 },
    { url = "https://files.pythonhosted.org/packages/84/5d/e17845bb0fa76334477d5de38654d27946d5b5d3695443987a094a71b440/multidict-6.4.4-py3-none-any.whl", hash = "sha256:bd4557071b561a8b3b6075c3ce93cf9bfb6182cb241805c3d66ced3b75eff4ac", size = 10481 },
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/6e/371856a3fb9d31ca8dac321cda606860fa4548858c0cc45d9d1d4ca2628b/mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558", size = 6343 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505", size = 4963 },
]

[[package]]
name = "narwhals"
version = "1.48.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/cd/7395d6c247e821cba6243e9f7ed202fae3fefef643c96581b5ecab927bad/narwhals-1.48.0.tar.gz", hash = "sha256:7243b456cbdb60edb148731a8f9b203f473a373a249ad66c699362508730e63f", size = 515112 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/75/72/5406044d4c251f3d8f78cec05b74839d0332d34c9e94b59120f3697ecf48/narwhals-1.48.0-py3-none-any.whl", hash = "sha256:2bbddc3adeed0c5b15ead8fe61f1d5e459f00c1d2fa60921e52a0f9bdc06077d", size = 376866 },
]

[[package]]
name = "numpy"
version = "2.2.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dc/b2/ce4b867d8cd9c0ee84938ae1e6a6f7926ebf928c9090d036fc3c6a04f946/numpy-2.2.5.tar.gz", hash = "sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291", size = 20273920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e2/a0/0aa7f0f4509a2e07bd7a509042967c2fab635690d4f48c6c7b3afd4f448c/numpy-2.2.5-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4", size = 20935102 },
    { url = "https://files.pythonhosted.org/packages/7e/e4/a6a9f4537542912ec513185396fce52cdd45bdcf3e9d921ab02a93ca5aa9/numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f", size = 14191709 },
    { url = "https://files.pythonhosted.org/packages/be/65/72f3186b6050bbfe9c43cb81f9df59ae63603491d36179cf7a7c8d216758/numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9", size = 5149173 },
    { url = "https://files.pythonhosted.org/packages/e5/e9/83e7a9432378dde5802651307ae5e9ea07bb72b416728202218cd4da2801/numpy-2.2.5-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191", size = 6684502 },
    { url = "https://files.pythonhosted.org/packages/ea/27/b80da6c762394c8ee516b74c1f686fcd16c8f23b14de57ba0cad7349d1d2/numpy-2.2.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372", size = 14084417 },
    { url = "https://files.pythonhosted.org/packages/aa/fc/ebfd32c3e124e6a1043e19c0ab0769818aa69050ce5589b63d05ff185526/numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d", size = 16133807 },
    { url = "https://files.pythonhosted.org/packages/bf/9b/4cc171a0acbe4666f7775cfd21d4eb6bb1d36d3a0431f48a73e9212d2278/numpy-2.2.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7", size = 15575611 },
    { url = "https://files.pythonhosted.org/packages/a3/45/40f4135341850df48f8edcf949cf47b523c404b712774f8855a64c96ef29/numpy-2.2.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73", size = 17895747 },
    { url = "https://files.pythonhosted.org/packages/f8/4c/b32a17a46f0ffbde8cc82df6d3daeaf4f552e346df143e1b188a701a8f09/numpy-2.2.5-cp313-cp313-win32.whl", hash = "sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b", size = 6309594 },
    { url = "https://files.pythonhosted.org/packages/13/ae/72e6276feb9ef06787365b05915bfdb057d01fceb4a43cb80978e518d79b/numpy-2.2.5-cp313-cp313-win_amd64.whl", hash = "sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471", size = 12638356 },
    { url = "https://files.pythonhosted.org/packages/79/56/be8b85a9f2adb688e7ded6324e20149a03541d2b3297c3ffc1a73f46dedb/numpy-2.2.5-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6", size = 20963778 },
    { url = "https://files.pythonhosted.org/packages/ff/77/19c5e62d55bff507a18c3cdff82e94fe174957bad25860a991cac719d3ab/numpy-2.2.5-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba", size = 14207279 },
    { url = "https://files.pythonhosted.org/packages/75/22/aa11f22dc11ff4ffe4e849d9b63bbe8d4ac6d5fae85ddaa67dfe43be3e76/numpy-2.2.5-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133", size = 5199247 },
    { url = "https://files.pythonhosted.org/packages/4f/6c/12d5e760fc62c08eded0394f62039f5a9857f758312bf01632a81d841459/numpy-2.2.5-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376", size = 6711087 },
    { url = "https://files.pythonhosted.org/packages/ef/94/ece8280cf4218b2bee5cec9567629e61e51b4be501e5c6840ceb593db945/numpy-2.2.5-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19", size = 14059964 },
    { url = "https://files.pythonhosted.org/packages/39/41/c5377dac0514aaeec69115830a39d905b1882819c8e65d97fc60e177e19e/numpy-2.2.5-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0", size = 16121214 },
    { url = "https://files.pythonhosted.org/packages/db/54/3b9f89a943257bc8e187145c6bc0eb8e3d615655f7b14e9b490b053e8149/numpy-2.2.5-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a", size = 15575788 },
    { url = "https://files.pythonhosted.org/packages/b1/c4/2e407e85df35b29f79945751b8f8e671057a13a376497d7fb2151ba0d290/numpy-2.2.5-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066", size = 17893672 },
    { url = "https://files.pythonhosted.org/packages/29/7e/d0b44e129d038dba453f00d0e29ebd6eaf2f06055d72b95b9947998aca14/numpy-2.2.5-cp313-cp313t-win32.whl", hash = "sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e", size = 6377102 },
    { url = "https://files.pythonhosted.org/packages/63/be/b85e4aa4bf42c6502851b971f1c326d583fcc68227385f92089cf50a7b45/numpy-2.2.5-cp313-cp313t-win_amd64.whl", hash = "sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8", size = 12750096 },
]

[[package]]
name = "openai"
version = "1.84.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/a3/128caf24e116f48fad3e4d5122cdf84db06c5127911849d51663c66158c8/openai-1.84.0.tar.gz", hash = "sha256:4caa43bdab262cc75680ce1a2322cfc01626204074f7e8d9939ab372acf61698", size = 467066 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/10/f245db006a860dbc1f2e2c8382e0a1762c7753e7971ba43a1dc3f3ec1404/openai-1.84.0-py3-none-any.whl", hash = "sha256:7ec4436c3c933d68dc0f5a0cef0cb3dbc0864a54d62bddaf2ed5f3d521844711", size = 725512 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "parso"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/66/94/68e2e17afaa9169cf6412ab0f28623903be73d1b32e208d9e8e541bb086d/parso-0.8.4.tar.gz", hash = "sha256:eb3a7b58240fb99099a345571deecc0f9540ea5f4dd2fe14c2a99d6b281ab92d", size = 400609 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c6/ac/dac4a63f978e4dcb3c6d3a78c4d8e0192a113d288502a1216950c41b1027/parso-0.8.4-py2.py3-none-any.whl", hash = "sha256:a418670a20291dacd2dddc80c377c5c3791378ee1e8d12bffc35420643d43f18", size = 103650 },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191 },
]

[[package]]
name = "pexpect"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "ptyprocess" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/92/cc564bf6381ff43ce1f4d06852fc19a2f11d180f23dc32d9588bee2f149d/pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f", size = 166450 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523", size = 63772 },
]

[[package]]
name = "platformdirs"
version = "4.3.8"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/8b/3c73abc9c759ecd3f1f7ceff6685840859e8070c4d947c93fae71f6a0bf2/platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc", size = 21362 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/39/979e8e21520d4e47a0bbe349e2713c0aac6f3d853d0e5b34d76206c439aa/platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4", size = 18567 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "proto-plus"
version = "1.26.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f4/ac/87285f15f7cce6d4a008f33f1757fb5a13611ea8914eb58c3d0d26243468/proto_plus-1.26.1.tar.gz", hash = "sha256:21a515a4c4c0088a773899e23c7bbade3d18f9c66c73edd4c7ee3816bc96a012", size = 56142 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl", hash = "sha256:13285478c2dcf2abb829db158e1047e2f1e8d63a077d94263c2b88b043c75a66", size = 50163 },
]

[[package]]
name = "protobuf"
version = "5.29.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/17/7d/b9dca7365f0e2c4fa7c193ff795427cfa6290147e5185ab11ece280a18e7/protobuf-5.29.4.tar.gz", hash = "sha256:4f1dfcd7997b31ef8f53ec82781ff434a28bf71d9102ddde14d076adcfc78c99", size = 424902 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/b2/043a1a1a20edd134563699b0e91862726a0dc9146c090743b6c44d798e75/protobuf-5.29.4-cp310-abi3-win32.whl", hash = "sha256:13eb236f8eb9ec34e63fc8b1d6efd2777d062fa6aaa68268fb67cf77f6839ad7", size = 422709 },
    { url = "https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl", hash = "sha256:bcefcdf3976233f8a502d265eb65ea740c989bacc6c30a58290ed0e519eb4b8d", size = 434506 },
    { url = "https://files.pythonhosted.org/packages/46/de/7c126bbb06aa0f8a7b38aaf8bd746c514d70e6a2a3f6dd460b3b7aad7aae/protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:307ecba1d852ec237e9ba668e087326a67564ef83e45a0189a772ede9e854dd0", size = 417826 },
    { url = "https://files.pythonhosted.org/packages/a2/b5/bade14ae31ba871a139aa45e7a8183d869efe87c34a4850c87b936963261/protobuf-5.29.4-cp38-abi3-manylinux2014_aarch64.whl", hash = "sha256:aec4962f9ea93c431d5714ed1be1c93f13e1a8618e70035ba2b0564d9e633f2e", size = 319574 },
    { url = "https://files.pythonhosted.org/packages/46/88/b01ed2291aae68b708f7d334288ad5fb3e7aa769a9c309c91a0d55cb91b0/protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl", hash = "sha256:d7d3f7d1d5a66ed4942d4fefb12ac4b14a29028b209d4bfb25c68ae172059922", size = 319672 },
    { url = "https://files.pythonhosted.org/packages/12/fb/a586e0c973c95502e054ac5f81f88394f24ccc7982dac19c515acd9e2c93/protobuf-5.29.4-py3-none-any.whl", hash = "sha256:3fde11b505e1597f71b875ef2fc52062b6a9740e5f7c8997ce878b6009145862", size = 172551 },
]

[[package]]
name = "psutil"
version = "7.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2a/80/336820c1ad9286a4ded7e845b2eccfcb27851ab8ac6abece774a6ff4d3de/psutil-7.0.0.tar.gz", hash = "sha256:7be9c3eba38beccb6495ea33afd982a44074b78f28c434a1f51cc07fd315c456", size = 497003 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ed/e6/2d26234410f8b8abdbf891c9da62bee396583f713fb9f3325a4760875d22/psutil-7.0.0-cp36-abi3-macosx_10_9_x86_64.whl", hash = "sha256:101d71dc322e3cffd7cea0650b09b3d08b8e7c4109dd6809fe452dfd00e58b25", size = 238051 },
    { url = "https://files.pythonhosted.org/packages/04/8b/30f930733afe425e3cbfc0e1468a30a18942350c1a8816acfade80c005c4/psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl", hash = "sha256:39db632f6bb862eeccf56660871433e111b6ea58f2caea825571951d4b6aa3da", size = 239535 },
    { url = "https://files.pythonhosted.org/packages/2a/ed/d362e84620dd22876b55389248e522338ed1bf134a5edd3b8231d7207f6d/psutil-7.0.0-cp36-abi3-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1fcee592b4c6f146991ca55919ea3d1f8926497a713ed7faaf8225e174581e91", size = 275004 },
    { url = "https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b1388a4f6875d7e2aff5c4ca1cc16c545ed41dd8bb596cefea80111db353a34", size = 277986 },
    { url = "https://files.pythonhosted.org/packages/eb/a2/709e0fe2f093556c17fbafda93ac032257242cabcc7ff3369e2cb76a97aa/psutil-7.0.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5f098451abc2828f7dc6b58d44b532b22f2088f4999a937557b603ce72b1993", size = 279544 },
    { url = "https://files.pythonhosted.org/packages/50/e6/eecf58810b9d12e6427369784efe814a1eec0f492084ce8eb8f4d89d6d61/psutil-7.0.0-cp37-abi3-win32.whl", hash = "sha256:ba3fcef7523064a6c9da440fc4d6bd07da93ac726b5733c29027d7dc95b39d99", size = 241053 },
    { url = "https://files.pythonhosted.org/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:4cf3d4eb1aa9b348dec30105c55cd9b7d4629285735a102beb4441e38db90553", size = 244885 },
]

[[package]]
name = "ptyprocess"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/20/e5/16ff212c1e452235a90aeb09066144d0c5a6a8c0834397e03f5224495c4e/ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220", size = 70762 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35", size = 13993 },
]

[[package]]
name = "pure-eval"
version = "0.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/05/0a34433a064256a578f1783a10da6df098ceaa4a57bbeaa96a6c0352786b/pure_eval-0.2.3.tar.gz", hash = "sha256:5f4e983f40564c576c7c8635ae88db5956bb2229d7e9237d03b3c0b0190eaf42", size = 19752 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8e/37/efad0257dc6e593a18957422533ff0f87ede7c9c6ea010a2177d738fb82f/pure_eval-0.2.3-py3-none-any.whl", hash = "sha256:1db8e35b67b3d218d818ae653e27f06c3aa420901fa7b081ca98cbedc874e0d0", size = 11842 },
]

[[package]]
name = "pyasn1"
version = "0.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ba/e9/01f1a64245b89f039897cb0130016d79f77d52669aae6ee7b159a6c4c018/pyasn1-0.6.1.tar.gz", hash = "sha256:6f580d2bdd84365380830acf45550f2511469f673cb4a5ae3857a3170128b034", size = 145322 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl", hash = "sha256:0d632f46f2ba09143da3a8afe9e33fb6f92fa2320ab7e886e2d0f7672af84629", size = 83135 },
]

[[package]]
name = "pyasn1-modules"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e9/e6/78ebbb10a8c8e4b61a59249394a4a594c1a7af95593dc933a349c8d00964/pyasn1_modules-0.4.2.tar.gz", hash = "sha256:677091de870a80aae844b1ca6134f54652fa2c8c5a52aa396440ac3106e941e6", size = 307892 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl", hash = "sha256:29253a9207ce32b64c3ac6600edc75368f98473906e8fd1043bd6b5b1de2c14a", size = 181259 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pymdown-extensions"
version = "10.16"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown" },
    { name = "pyyaml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/0a/c06b542ac108bfc73200677309cd9188a3a01b127a63f20cadc18d873d88/pymdown_extensions-10.16.tar.gz", hash = "sha256:71dac4fca63fabeffd3eb9038b756161a33ec6e8d230853d3cecf562155ab3de", size = 853197 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/d4/10bb14004d3c792811e05e21b5e5dcae805aacb739bd12a0540967b99592/pymdown_extensions-10.16-py3-none-any.whl", hash = "sha256:f5dd064a4db588cb2d95229fc4ee63a1b16cc8b4d0e6145c0899ed8723da1df2", size = 266143 },
]

[[package]]
name = "pyparsing"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/22/f1129e69d94ffff626bdb5c835506b3a5b4f3d070f17ea295e12c2c6f60f/pyparsing-3.2.3.tar.gz", hash = "sha256:b9c13f1ab8b3b542f72e28f634bad4de758ab3ce4546e4301970ad6fa77c38be", size = 1088608 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/e7/df2285f3d08fee213f2d041540fa4fc9ca6c2d44cf36d3a035bf2a8d2bcc/pyparsing-3.2.3-py3-none-any.whl", hash = "sha256:a749938e02d6fd0b59b356ca504a24982314bb090c383e3cf201c95ef7e2bfcf", size = 111120 },
]

[[package]]
name = "pyperclip"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz", hash = "sha256:b7de0142ddc81bfc5c7507eea19da920b92252b548b96186caf94a5e2527d310", size = 20961 }

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "pytest-asyncio"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d0/d4/14f53324cb1a6381bef29d698987625d80052bb33932d8e7cbf9b337b17c/pytest_asyncio-1.0.0.tar.gz", hash = "sha256:d15463d13f4456e1ead2594520216b225a16f781e144f8fdf6c5bb4667c48b3f", size = 46960 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/05/ce271016e351fddc8399e546f6e23761967ee09c8c568bbfbecb0c150171/pytest_asyncio-1.0.0-py3-none-any.whl", hash = "sha256:4f024da9f1ef945e680dc68610b52550e36590a67fd31bb3b4943979a1f90ef3", size = 15976 },
]

[[package]]
name = "pytest-xdist"
version = "3.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "execnet" },
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/dc/865845cfe987b21658e871d16e0a24e871e00884c545f246dd8f6f69edda/pytest_xdist-3.7.0.tar.gz", hash = "sha256:f9248c99a7c15b7d2f90715df93610353a485827bc06eefb6566d23f6400f126", size = 87550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/b2/0e802fde6f1c5b2f7ae7e9ad42b83fd4ecebac18a8a8c2f2f14e39dce6e1/pytest_xdist-3.7.0-py3-none-any.whl", hash = "sha256:7d3fbd255998265052435eb9daa4e99b62e6fb9cfb6efd1f858d4d8c0c7f0ca0", size = 46142 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-json-logger"
version = "3.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz", hash = "sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84", size = 16642 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl", hash = "sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7", size = 15163 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "readabilipy"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "html5lib" },
    { name = "lxml" },
    { name = "regex" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b8/e4/260a202516886c2e0cc6e6ae96d1f491792d829098886d9529a2439fbe8e/readabilipy-0.3.0.tar.gz", hash = "sha256:e13313771216953935ac031db4234bdb9725413534bfb3c19dbd6caab0887ae0", size = 35491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dd/46/8a640c6de1a6c6af971f858b2fb178ca5e1db91f223d8ba5f40efe1491e5/readabilipy-0.3.0-py3-none-any.whl", hash = "sha256:d106da0fad11d5fdfcde21f5c5385556bfa8ff0258483037d39ea6b1d6db3943", size = 22158 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "rsa"
version = "4.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/8a/22b7beea3ee0d44b1916c0c1cb0ee3af23b700b6da9f04991899d0c555d4/rsa-4.9.1.tar.gz", hash = "sha256:e7bdbfdb5497da4c07dfd35530e1a902659db6ff241e39d9953cad06ebd0ae75", size = 29034 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl", hash = "sha256:68635866661c6836b8d39430f97a996acbd61bfa49406748ea243539fe239762", size = 34696 },
]

[[package]]
name = "ruff"
version = "0.11.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ed/da/9c6f995903b4d9474b39da91d2d626659af3ff1eeb43e9ae7c119349dba6/ruff-0.11.13.tar.gz", hash = "sha256:26fa247dc68d1d4e72c179e08889a25ac0c7ba4d78aecfc835d49cbfd60bf514", size = 4282054 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7d/ce/a11d381192966e0b4290842cc8d4fac7dc9214ddf627c11c1afff87da29b/ruff-0.11.13-py3-none-linux_armv6l.whl", hash = "sha256:4bdfbf1240533f40042ec00c9e09a3aade6f8c10b6414cf11b519488d2635d46", size = 10292516 },
    { url = "https://files.pythonhosted.org/packages/78/db/87c3b59b0d4e753e40b6a3b4a2642dfd1dcaefbff121ddc64d6c8b47ba00/ruff-0.11.13-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:aef9c9ed1b5ca28bb15c7eac83b8670cf3b20b478195bd49c8d756ba0a36cf48", size = 11106083 },
    { url = "https://files.pythonhosted.org/packages/77/79/d8cec175856ff810a19825d09ce700265f905c643c69f45d2b737e4a470a/ruff-0.11.13-py3-none-macosx_11_0_arm64.whl", hash = "sha256:53b15a9dfdce029c842e9a5aebc3855e9ab7771395979ff85b7c1dedb53ddc2b", size = 10436024 },
    { url = "https://files.pythonhosted.org/packages/8b/5b/f6d94f2980fa1ee854b41568368a2e1252681b9238ab2895e133d303538f/ruff-0.11.13-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ab153241400789138d13f362c43f7edecc0edfffce2afa6a68434000ecd8f69a", size = 10646324 },
    { url = "https://files.pythonhosted.org/packages/6c/9c/b4c2acf24ea4426016d511dfdc787f4ce1ceb835f3c5fbdbcb32b1c63bda/ruff-0.11.13-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:6c51f93029d54a910d3d24f7dd0bb909e31b6cd989a5e4ac513f4eb41629f0dc", size = 10174416 },
    { url = "https://files.pythonhosted.org/packages/f3/10/e2e62f77c65ede8cd032c2ca39c41f48feabedb6e282bfd6073d81bb671d/ruff-0.11.13-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1808b3ed53e1a777c2ef733aca9051dc9bf7c99b26ece15cb59a0320fbdbd629", size = 11724197 },
    { url = "https://files.pythonhosted.org/packages/bb/f0/466fe8469b85c561e081d798c45f8a1d21e0b4a5ef795a1d7f1a9a9ec182/ruff-0.11.13-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:d28ce58b5ecf0f43c1b71edffabe6ed7f245d5336b17805803312ec9bc665933", size = 12511615 },
    { url = "https://files.pythonhosted.org/packages/17/0e/cefe778b46dbd0cbcb03a839946c8f80a06f7968eb298aa4d1a4293f3448/ruff-0.11.13-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:55e4bc3a77842da33c16d55b32c6cac1ec5fb0fbec9c8c513bdce76c4f922165", size = 12117080 },
    { url = "https://files.pythonhosted.org/packages/5d/2c/caaeda564cbe103bed145ea557cb86795b18651b0f6b3ff6a10e84e5a33f/ruff-0.11.13-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:633bf2c6f35678c56ec73189ba6fa19ff1c5e4807a78bf60ef487b9dd272cc71", size = 11326315 },
    { url = "https://files.pythonhosted.org/packages/75/f0/782e7d681d660eda8c536962920c41309e6dd4ebcea9a2714ed5127d44bd/ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ffbc82d70424b275b089166310448051afdc6e914fdab90e08df66c43bb5ca9", size = 11555640 },
    { url = "https://files.pythonhosted.org/packages/5d/d4/3d580c616316c7f07fb3c99dbecfe01fbaea7b6fd9a82b801e72e5de742a/ruff-0.11.13-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:4a9ddd3ec62a9a89578c85842b836e4ac832d4a2e0bfaad3b02243f930ceafcc", size = 10507364 },
    { url = "https://files.pythonhosted.org/packages/5a/dc/195e6f17d7b3ea6b12dc4f3e9de575db7983db187c378d44606e5d503319/ruff-0.11.13-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:d237a496e0778d719efb05058c64d28b757c77824e04ffe8796c7436e26712b7", size = 10141462 },
    { url = "https://files.pythonhosted.org/packages/f4/8e/39a094af6967faa57ecdeacb91bedfb232474ff8c3d20f16a5514e6b3534/ruff-0.11.13-py3-none-musllinux_1_2_i686.whl", hash = "sha256:26816a218ca6ef02142343fd24c70f7cd8c5aa6c203bca284407adf675984432", size = 11121028 },
    { url = "https://files.pythonhosted.org/packages/5a/c0/b0b508193b0e8a1654ec683ebab18d309861f8bd64e3a2f9648b80d392cb/ruff-0.11.13-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:51c3f95abd9331dc5b87c47ac7f376db5616041173826dfd556cfe3d4977f492", size = 11602992 },
    { url = "https://files.pythonhosted.org/packages/7c/91/263e33ab93ab09ca06ce4f8f8547a858cc198072f873ebc9be7466790bae/ruff-0.11.13-py3-none-win32.whl", hash = "sha256:96c27935418e4e8e77a26bb05962817f28b8ef3843a6c6cc49d8783b5507f250", size = 10474944 },
    { url = "https://files.pythonhosted.org/packages/46/f4/7c27734ac2073aae8efb0119cae6931b6fb48017adf048fdf85c19337afc/ruff-0.11.13-py3-none-win_amd64.whl", hash = "sha256:29c3189895a8a6a657b7af4e97d330c8a3afd2c9c8f46c81e2fc5a31866517e3", size = 11548669 },
    { url = "https://files.pythonhosted.org/packages/ec/bf/b273dd11673fed8a6bd46032c0ea2a04b2ac9bfa9c628756a5856ba113b0/ruff-0.11.13-py3-none-win_arm64.whl", hash = "sha256:b4385285e9179d608ff1d2fb9922062663c658605819a6876d8beef0c30b7f3b", size = 10683928 },
]

[[package]]
name = "sentencepiece"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz", hash = "sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843", size = 2632106 }

[[package]]
name = "shapely"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fb/fe/3b0d2f828ffaceadcdcb51b75b9c62d98e62dd95ce575278de35f24a1c20/shapely-2.1.0.tar.gz", hash = "sha256:2cbe90e86fa8fc3ca8af6ffb00a77b246b918c7cf28677b7c21489b678f6b02e", size = 313617 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/77/4e368704b2193e74498473db4461d697cc6083c96f8039367e59009d78bd/shapely-2.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b64423295b563f43a043eb786e7a03200ebe68698e36d2b4b1c39f31dfb50dfb", size = 1830029 },
    { url = "https://files.pythonhosted.org/packages/71/3c/d888597bda680e4de987316b05ca9db07416fa29523beff64f846503302f/shapely-2.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1b5578f45adc25b235b22d1ccb9a0348c8dc36f31983e57ea129a88f96f7b870", size = 1637999 },
    { url = "https://files.pythonhosted.org/packages/03/8d/ee0e23b7ef88fba353c63a81f1f329c77f5703835db7b165e7c0b8b7f839/shapely-2.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1a7e83d383b27f02b684e50ab7f34e511c92e33b6ca164a6a9065705dd64bcb", size = 2929348 },
    { url = "https://files.pythonhosted.org/packages/d1/a7/5c9cb413e4e2ce52c16be717e94abd40ce91b1f8974624d5d56154c5d40b/shapely-2.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:942031eb4d8f7b3b22f43ba42c09c7aa3d843aa10d5cc1619fe816e923b66e55", size = 3048973 },
    { url = "https://files.pythonhosted.org/packages/84/23/45b90c0bd2157b238490ca56ef2eedf959d3514c7d05475f497a2c88b6d9/shapely-2.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d2843c456a2e5627ee6271800f07277c0d2652fb287bf66464571a057dbc00b3", size = 3873148 },
    { url = "https://files.pythonhosted.org/packages/c0/bc/ed7d5d37f5395166042576f0c55a12d7e56102799464ba7ea3a72a38c769/shapely-2.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8c4b17469b7f39a5e6a7cfea79f38ae08a275427f41fe8b48c372e1449147908", size = 4052655 },
    { url = "https://files.pythonhosted.org/packages/c0/8f/a1dafbb10d20d1c569f2db3fb1235488f624dafe8469e8ce65356800ba31/shapely-2.1.0-cp313-cp313-win32.whl", hash = "sha256:30e967abd08fce49513d4187c01b19f139084019f33bec0673e8dbeb557c45e4", size = 1526600 },
    { url = "https://files.pythonhosted.org/packages/e3/f0/9f8cdf2258d7aed742459cea51c70d184de92f5d2d6f5f7f1ded90a18c31/shapely-2.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:1dc8d4364483a14aba4c844b7bd16a6fa3728887e2c33dfa1afa34a3cf4d08a5", size = 1707115 },
    { url = "https://files.pythonhosted.org/packages/75/ed/32952df461753a65b3e5d24c8efb361d3a80aafaef0b70d419063f6f2c11/shapely-2.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:673e073fea099d1c82f666fb7ab0a00a77eff2999130a69357ce11941260d855", size = 1824847 },
    { url = "https://files.pythonhosted.org/packages/ff/b9/2284de512af30b02f93ddcdd2e5c79834a3cf47fa3ca11b0f74396feb046/shapely-2.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:6d1513f915a56de67659fe2047c1ad5ff0f8cbff3519d1e74fced69c9cb0e7da", size = 1631035 },
    { url = "https://files.pythonhosted.org/packages/35/16/a59f252a7e736b73008f10d0950ffeeb0d5953be7c0bdffd39a02a6ba310/shapely-2.1.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0d6a7043178890b9e028d80496ff4c79dc7629bff4d78a2f25323b661756bab8", size = 2968639 },
    { url = "https://files.pythonhosted.org/packages/a5/0a/6a20eca7b0092cfa243117e8e145a58631a4833a0a519ec9b445172e83a0/shapely-2.1.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb638378dc3d76f7e85b67d7e2bb1366811912430ac9247ac00c127c2b444cdc", size = 3055713 },
    { url = "https://files.pythonhosted.org/packages/fb/44/eeb0c7583b1453d1cf7a319a1d738e08f98a5dc993fa1ef3c372983e4cb5/shapely-2.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:737124e87d91d616acf9a911f74ac55e05db02a43a6a7245b3d663817b876055", size = 3890478 },
    { url = "https://files.pythonhosted.org/packages/5d/6e/37ff3c6af1d408cacb0a7d7bfea7b8ab163a5486e35acb08997eae9d8756/shapely-2.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e6c229e7bb87aae5df82fa00b6718987a43ec168cc5affe095cca59d233f314", size = 4036148 },
    { url = "https://files.pythonhosted.org/packages/c8/6a/8c0b7de3aeb5014a23f06c5e9d3c7852ebcf0d6b00fe660b93261e310e24/shapely-2.1.0-cp313-cp313t-win32.whl", hash = "sha256:a9580bda119b1f42f955aa8e52382d5c73f7957e0203bc0c0c60084846f3db94", size = 1535993 },
    { url = "https://files.pythonhosted.org/packages/a8/91/ae80359a58409d52e4d62c7eacc7eb3ddee4b9135f1db884b6a43cf2e174/shapely-2.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e8ff4e5cfd799ba5b6f37b5d5527dbd85b4a47c65b6d459a03d0962d2a9d4d10", size = 1717777 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677 },
]

[[package]]
name = "sqlparse"
version = "0.5.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e5/40/edede8dd6977b0d3da179a342c198ed100dd2aba4be081861ee5911e4da4/sqlparse-0.5.3.tar.gz", hash = "sha256:09f67787f56a0b16ecdbde1bfc7f5d9c3371ca683cfeaa8e6ff60b4807ec9272", size = 84999 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl", hash = "sha256:cf2196ed3418f3ba5de6af7e82c694a9fbdbfecccdfc72e281548517081f16ca", size = 44415 },
]

[[package]]
name = "sse-starlette"
version = "2.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/35/7d8d94eb0474352d55f60f80ebc30f7e59441a29e18886a6425f0bccd0d3/sse_starlette-2.3.3.tar.gz", hash = "sha256:fdd47c254aad42907cfd5c5b83e2282be15be6c51197bf1a9b70b8e990522072", size = 17499 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/20/52fdb5ebb158294b0adb5662235dd396fc7e47aa31c293978d8d8942095a/sse_starlette-2.3.3-py3-none-any.whl", hash = "sha256:8b0a0ced04a329ff7341b01007580dd8cf71331cc21c0ccea677d500618da1e0", size = 10235 },
]

[[package]]
name = "stack-data"
version = "0.6.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "asttokens" },
    { name = "executing" },
    { name = "pure-eval" },
]
sdist = { url = "https://files.pythonhosted.org/packages/28/e3/55dcc2cfbc3ca9c29519eb6884dd1415ecb53b0e934862d3559ddcb7e20b/stack_data-0.6.3.tar.gz", hash = "sha256:836a778de4fec4dcd1dcd89ed8abff8a221f58308462e1c4aa2a3cf30148f0b9", size = 44707 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl", hash = "sha256:d5558e0c25a4cb0853cddad3d77da9891a08cb85dd9f9f91b9f8cd66e511e695", size = 24521 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tabulate"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ec/fe/802052aecb21e3797b8f7902564ab6ea0d60ff8ca23952079064155d1ae1/tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c", size = 81090 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f", size = 35252 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tomlkit"
version = "0.13.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/18/0bbf3884e9eaa38819ebe46a7bd25dcd56b67434402b66a58c4b8e552575/tomlkit-0.13.3.tar.gz", hash = "sha256:430cf247ee57df2b94ee3fbe588e71d362a941ebb545dec29b53961d61add2a1", size = 185207 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl", hash = "sha256:c89c649d79ee40629a9fda55f8ace8c6a1b42deb912b2a8fd8d942ddadb606b0", size = 38901 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "traitlets"
version = "5.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/eb/79/72064e6a701c2183016abbbfedaba506d81e30e232a68c9f0d6f6fcd1574/traitlets-5.14.3.tar.gz", hash = "sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7", size = 161621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/c0/8f5d070730d7836adc9c9b6408dec68c6ced86b304a9b26a14df072a6e8c/traitlets-5.14.3-py3-none-any.whl", hash = "sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f", size = 85359 },
]

[[package]]
name = "typer"
version = "0.15.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/1a/5f36851f439884bcfe8539f6a20ff7516e7b60f319bbaf69a90dc35cc2eb/typer-0.15.3.tar.gz", hash = "sha256:818873625d0569653438316567861899f7e9972f2e6e0c16dab608345ced713c", size = 101641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/48/20/9d953de6f4367163d23ec823200eb3ecb0050a2609691e512c8b95827a9b/typer-0.15.3-py3-none-any.whl", hash = "sha256:c86a65ad77ca531f03de08d1b9cb67cd09ad02ddddf4b34745b5008f43b239bd", size = 45253 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uritemplate"
version = "4.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/60/f174043244c5306c9988380d2cb10009f91563fc4b31293d27e17201af56/uritemplate-4.2.0.tar.gz", hash = "sha256:480c2ed180878955863323eea31b0ede668795de182617fef9c6ca09e6ec9d0e", size = 33267 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/99/3ae339466c9183ea5b8ae87b34c0b897eda475d2aec2307cae60e5cd4f29/uritemplate-4.2.0-py3-none-any.whl", hash = "sha256:962201ba1c4edcab02e60f9a0d3821e82dfc5d2d6662a21abd533879bdb8a686", size = 11488 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166 },
]

[[package]]
name = "webencodings"
version = "0.5.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/02/ae6ceac1baeda530866a85075641cec12989bd8d31af6d5ab4a3e8c92f47/webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923", size = 9721 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78", size = 11774 },
]

[[package]]
name = "websockets"
version = "15.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/e6/26d09fab466b7ca9c7737474c52be4f76a40301b08362eb2dbc19dcc16c1/websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee", size = 177016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/9f/51f0cf64471a9d2b4d0fc6c534f323b664e7095640c34562f5182e5a7195/websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931", size = 175440 },
    { url = "https://files.pythonhosted.org/packages/8a/05/aa116ec9943c718905997412c5989f7ed671bc0188ee2ba89520e8765d7b/websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675", size = 173098 },
    { url = "https://files.pythonhosted.org/packages/ff/0b/33cef55ff24f2d92924923c99926dcce78e7bd922d649467f0eda8368923/websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151", size = 173329 },
    { url = "https://files.pythonhosted.org/packages/31/1d/063b25dcc01faa8fada1469bdf769de3768b7044eac9d41f734fd7b6ad6d/websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22", size = 183111 },
    { url = "https://files.pythonhosted.org/packages/93/53/9a87ee494a51bf63e4ec9241c1ccc4f7c2f45fff85d5bde2ff74fcb68b9e/websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f", size = 182054 },
    { url = "https://files.pythonhosted.org/packages/ff/b2/83a6ddf56cdcbad4e3d841fcc55d6ba7d19aeb89c50f24dd7e859ec0805f/websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8", size = 182496 },
    { url = "https://files.pythonhosted.org/packages/98/41/e7038944ed0abf34c45aa4635ba28136f06052e08fc2168520bb8b25149f/websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375", size = 182829 },
    { url = "https://files.pythonhosted.org/packages/e0/17/de15b6158680c7623c6ef0db361da965ab25d813ae54fcfeae2e5b9ef910/websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d", size = 182217 },
    { url = "https://files.pythonhosted.org/packages/33/2b/1f168cb6041853eef0362fb9554c3824367c5560cbdaad89ac40f8c2edfc/websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4", size = 182195 },
    { url = "https://files.pythonhosted.org/packages/86/eb/20b6cdf273913d0ad05a6a14aed4b9a85591c18a987a3d47f20fa13dcc47/websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa", size = 176393 },
    { url = "https://files.pythonhosted.org/packages/1b/6c/c65773d6cab416a64d191d6ee8a8b1c68a09970ea6909d16965d26bfed1e/websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561", size = 176837 },
    { url = "https://files.pythonhosted.org/packages/fa/a8/5b41e0da817d64113292ab1f8247140aac61cbf6cfd085d6a0fa77f4984f/websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f", size = 169743 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]

[[package]]
name = "yoyo-migrations"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "importlib-metadata" },
    { name = "sqlparse" },
    { name = "tabulate" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/8c/5d/9ef7f808ea955eca9f08043c65bdc81a4694e784c978b24ad72022974a97/yoyo_migrations-9.0.0-py3-none-any.whl", hash = "sha256:fc65d3a6d9449c1c54d64ff2ff98e32a27da356057c60e3471010bfb19ede081", size = 49002 },
]

[[package]]
name = "zipp"
version = "3.23.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e3/02/0f2892c661036d50ede074e376733dca2ae7c6eb617489437771209d4180/zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166", size = 25547 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e", size = 10276 },
]
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="prompt_service_notebook.py"&amp;amp;gt;import marimo

__generated_with = "0.14.12"
app = marimo.App(width="medium")


@app.cell
def _():
    from repository.database import SQLite3Database
    from repository.prompt_service import PromptService
    from repository.prompt_models import (
        Prompt,
        PromptType,
        PromptPlanStatus,
        CmdCategory,
    )

    return (
        CmdCategory,
        Prompt,
        PromptPlanStatus,
        PromptService,
        PromptType,
        SQLite3Database,
    )


@app.cell
def _(PromptService, SQLite3Database):
    db = SQLite3Database("data/collect.db")
    with db.get_connection() as conn:
        ps = PromptService(conn)
    return (ps,)


@app.cell
def _(ps):
    cmds = ps.load_cmds_from_disk()
    plans = ps.load_plans_from_disk()
    return cmds, plans


@app.cell
def _(cmds):
    print(f"Num cmds: {len(cmds.loaded_prompts)}\n")
    for cmd in cmds.loaded_prompts:
        print(cmd.name)
    return


@app.cell
def _(plans):
    print(f"Num plans: {len(plans.loaded_prompts)}\n")
    for plan in plans.loaded_prompts:
        print(plan.name)
    return


@app.cell
def _():
    db_name = "collect_completed_add_claude_sdk_processing.md"
    result = db_name.split("_")
    print(result)
    return db_name, result


@app.cell
def _(result):
    print(f"project name: {result[0]}")
    print(f"plan status: {result[1]}")
    return


@app.cell
def _(result):
    namelist = result[2:]
    print(namelist)
    return (namelist,)


@app.cell
def _(namelist):
    newname = ""
    for word in namelist:
        if not word.endswith(".md"):
            newname = newname + word + "_"
        else:
            newname = newname + word
    print(newname)
    return


@app.cell
def _(db_name):
    print(db_name.split("_")[2:])
    return


@app.cell
def _(PromptType):
    def parse_db_name(db_name: str, prompt_type: PromptType) -&amp;amp;amp;gt; str:
        ls = db_name.split("_")
        filename = ""
        if prompt_type == PromptType.PLAN:
            project = ls[0]
            plan_status = ls[1]
            print(f"project: {project}")
            print(f"plan status: {plan_status}")

            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

        if prompt_type == PromptType.CMD:
            cmd_dir = ls[0]
            print(f"cmd/dir: {cmd_dir}")
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

    return (parse_db_name,)


@app.cell
def _(PromptType, db_name, parse_db_name):
    parse_db_name(db_name, PromptType.PLAN)
    return


@app.cell
def _(PromptType, parse_db_name):
    parse_db_name("tools_create_database.md", PromptType.CMD)
    return


@app.cell
def _(CmdCategory, Prompt, PromptPlanStatus, PromptType, ps):
    def new_cmd_prompt(prompt_content: str) -&amp;amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="Makefile"&amp;amp;gt;PROJECT_NAME := collect

marimo:
	uv run marimo edit

.PHONY: movetools
movetools:
	./movetools

.PHONY: buildsrc
buildsrc: 
	./tools/buildsrc

.PHONY: ensuregithub
ensuregithub:
	./tools/ensure-github-url

lint:
	ruff check .

format:
	black .

test: 
	uv run pytest -v -s -n auto

test-fast:
	uv run pytest -v -n auto -m "not slow"

test-slow:
	uv run pytest -v -s -m slow

test-single:
	uv run pytest -v -s

check: 
	make lint
	make format
	make movetools
	make ensuregithub
	make buildsrc

migrate:
	uv run yoyo apply --config yoyo.ini --batch


&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".mcp.json"&amp;amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "/Users/benjaminmetz/.local/bin/uv",
			"args": [
				"--directory",
				"/Users/benjaminmetz/python/collect",
				"run",
				"collect.py"
			]
		}
	}
}
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="pyproject.toml"&amp;amp;gt;[project]
name = "collect"
version = "0.1.0"
description = "development toolkit for all the things.."
readme = "README.md"
requires-python = "&amp;amp;amp;gt;=3.13"
dependencies = [
    "aiohttp&amp;amp;amp;gt;=3.12.11",
    "anthropic&amp;amp;amp;gt;=0.50.0",
    "beautifulsoup4&amp;amp;amp;gt;=4.13.4",
    "black&amp;amp;amp;gt;=25.1.0",
    "fastapi&amp;amp;amp;gt;=0.116.1",
    "google-ai-generativelanguage&amp;amp;amp;gt;=0.6.15",
    "google-api-python-client&amp;amp;amp;gt;=2.169.0",
    "google-auth-httplib2&amp;amp;amp;gt;=0.2.0",
    "google-cloud-aiplatform[tokenization]&amp;amp;amp;gt;=1.91.0",
    "google-cloud-secret-manager&amp;amp;amp;gt;=2.23.3",
    "google-genai&amp;amp;amp;gt;=1.13.0",
    "google-generativeai&amp;amp;amp;gt;=0.8.5",
    "html-to-markdown&amp;amp;amp;gt;=1.3.2",
    "html5lib&amp;amp;amp;gt;=1.1",
    "httplib2&amp;amp;amp;gt;=0.22.0",
    "httpx&amp;amp;amp;gt;=0.28.1",
    "ipython&amp;amp;amp;gt;=9.4.0",
    "lxml&amp;amp;amp;gt;=5.4.0",
    "marimo&amp;amp;amp;gt;=0.14.12",
    "markdownify&amp;amp;amp;gt;=1.1.0",
    "mcp[cli]&amp;amp;amp;gt;=1.7.1",
    "openai&amp;amp;amp;gt;=1.59.4",
    "pathspec&amp;amp;amp;gt;=0.12.1",
    "pyperclip&amp;amp;amp;gt;=1.9.0",
    "pytest&amp;amp;amp;gt;=8.3.5",
    "pytest-asyncio&amp;amp;amp;gt;=0.26.0",
    "pytest-xdist&amp;amp;amp;gt;=3.6.1",
    "python-json-logger&amp;amp;amp;gt;=3.3.0",
    "readabilipy&amp;amp;amp;gt;=0.3.0",
    "rich&amp;amp;amp;gt;=14.0.0",
    "ruff&amp;amp;amp;gt;=0.11.9",
    "tiktoken&amp;amp;amp;gt;=0.9.0",
    "uvicorn&amp;amp;amp;gt;=0.34.2",
    "yoyo-migrations&amp;amp;amp;gt;=9.0.0",
]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."]
filterwarnings = [
    "ignore::UserWarning:google.auth._default"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests"
]
# Parallel test execution settings
addopts = [
    "--strict-markers",  # Ensure all marks are registered
    "--tb=short",       # Shorter traceback format
    "--dist=worksteal", # Better work distribution for uneven test times
]
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="initial_load.py"&amp;amp;gt;#!/usr/bin/env python3
"""
Initial load script for loading plans and commands from disk into database
Uses PromptService to load from .claude/commands,
.gemini/commands, and _docs/plans
"""
import sys
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import PromptLoadResult, PromptCreateResult


def load_commands_to_db(service: PromptService) -&amp;amp;amp;gt; PromptLoadResult:
    """
    Load commands from .claude and .gemini directories and save to database
    This is primarily for an initial load or to clean and restart from disk

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📁 Loading commands from disk...")

    # Load commands from filesystem
    cmd_result: PromptLoadResult = service.load_cmds_from_disk()

    if cmd_result.errors:
        print(
            f"⚠️  Found {len(cmd_result.errors)
                           } errors while loading commands:"
        )
        for error in cmd_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not cmd_result.loaded_prompts:
        print("ℹ️  No commands found to load")
        return cmd_result

    print(f"Found: {len(cmd_result.loaded_prompts)} to save to database")

    # Save commands to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        cmd_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save command: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (cmd_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=cmd_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def load_plans_to_db(service: PromptService) -&amp;amp;amp;gt; PromptLoadResult:
    """Load plans from _docs/plans directories and save to database

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📋 Loading plans from disk...")

    # Load plans from filesystem
    plan_result: PromptLoadResult = service.load_plans_from_disk()

    if plan_result.errors:
        print(
            f"⚠️  Found {len(plan_result.errors)
                           } errors while loading plans:"
        )
        for error in plan_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not plan_result.loaded_prompts:
        print("ℹ️  No plans found to load")
        return plan_result

    print(
        f"📄 Found {len(plan_result.loaded_prompts)
                     } plans to save to database"
    )

    # Save plans to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        plan_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save plan: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (plan_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=plan_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def main():
    """Main function to orchestrate the complete loading process"""
    print("🚀 Starting initial data load from disk to database...")
    print("=" * 60)

    try:
        # Initialize database connection
        database = SQLite3Database("data/collect.db")

        with database.get_connection() as conn:
            # Create PromptService instance
            service = PromptService(conn)

            # Track overall statistics
            total_loaded = 0
            total_errors = 0

            # Load commands
            cmd_result: PromptLoadResult = load_commands_to_db(service)
            cmd_loaded = len(cmd_result.loaded_prompts)
            cmd_errors = len(cmd_result.errors) if cmd_result.errors else 0

            total_loaded += cmd_loaded
            total_errors += cmd_errors

            print(f"✅ Commands: {cmd_loaded} loaded, {cmd_errors} errors")
            print()

            # Load plans
            plan_result: PromptLoadResult = load_plans_to_db(service)
            plan_loaded = len(plan_result.loaded_prompts)
            plan_errors = len(plan_result.errors) if plan_result.errors else 0

            total_loaded += plan_loaded
            total_errors += plan_errors

            print(f"✅ Plans: {plan_loaded} loaded, {plan_errors} errors")
            print()

            # Print final summary
            print("=" * 60)
            print("📊 FINAL SUMMARY:")
            print(f"   ✅ Total items loaded: {total_loaded}")
            print(f"   ❌ Total errors: {total_errors}")

            if total_errors == 0:
                print("🎉 All data loaded successfully!")
            else:
                print(
                    f"⚠️  Completed with {
                      total_errors} errors - check output above"
                )

    except Exception as e:
        print(f"💥 Fatal error during loading process: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        sys.exit(1)


if __name__ == "__main__":
    main()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".sync_cache.json"&amp;amp;gt;{
  ".claude/commands/go/go_build_endpoint_test.md": {
    "sha": "bb6aa9bffab635d3455caf7cfa59fc4f43036aac",
    "path": ".claude/commands/go/go_build_endpoint_test.md",
    "last_synced": 1754870996.5877829,
    "converted": true
  },
  ".claude/commands/archive/build_context.md": {
    "sha": "d976db85985179f771925095a73d98142d3ab30b",
    "path": ".claude/commands/archive/build_context.md",
    "last_synced": 1754870996.598152,
    "converted": true
  },
  ".claude/commands/commit.md": {
    "sha": "891c52b5b372e6fffd161dcf9b55930755d9fbf3",
    "path": ".claude/commands/commit.md",
    "last_synced": 1754870996.614573,
    "converted": true
  },
  ".claude/commands/go/create_go_structs.md": {
    "sha": "56a5e5dc2abb511b98b2e1147d428551610aa49e",
    "path": ".claude/commands/go/create_go_structs.md",
    "last_synced": 1754870996.625781,
    "converted": true
  },
  ".claude/commands/diff_code_review.md": {
    "sha": "6ef1818de14d6d1e994d9fd4841bc5486c4d6f20",
    "path": ".claude/commands/diff_code_review.md",
    "last_synced": 1754870996.635034,
    "converted": true
  },
  ".claude/commands/convert_to_toml.md": {
    "sha": "7bdd42b1ffeee07bd06ed5b4b97e99381c8052e9",
    "path": ".claude/commands/convert_to_toml.md",
    "last_synced": 1754870996.642501,
    "converted": true
  },
  ".claude/commands/create_checklist_3.md": {
    "sha": "27f4fa9841078c71d9490d9ef2c967970c053bbf",
    "path": ".claude/commands/create_checklist_3.md",
    "last_synced": 1754870996.688289,
    "converted": true
  },
  ".claude/commands/go/create_go_structsV1.md": {
    "sha": "d7e28d7eb8dee775f9416574d8f06ef11ca42158",
    "path": ".claude/commands/go/create_go_structsV1.md",
    "last_synced": 1754870996.692773,
    "converted": true
  },
  ".claude/commands/go/go_update_config.md": {
    "sha": "3cbaf390b5abd45230dde54705958bd873f16ebc",
    "path": ".claude/commands/go/go_update_config.md",
    "last_synced": 1754871010.753632,
    "converted": true
  },
  ".claude/commands/mcp/copy_to_clipboard.md": {
    "sha": "aad8af9420d4a80933aaf12accfe7a8450679f32",
    "path": ".claude/commands/mcp/copy_to_clipboard.md",
    "last_synced": 1754871011.429687,
    "converted": true
  },
  ".claude/commands/mcp/get_docs.md": {
    "sha": "9e27863768032379a1798cf6da1a881720ed65bf",
    "path": ".claude/commands/mcp/get_docs.md",
    "last_synced": 1754871012.238699,
    "converted": true
  },
  ".claude/commands/model_code_review.md": {
    "sha": "8bc222f49f8587a01362d06ff6475cb8e46ace2e",
    "path": ".claude/commands/model_code_review.md",
    "last_synced": 1754871023.324787,
    "converted": true
  },
  ".claude/commands/pr.md": {
    "sha": "0e8a152f358b515f158d5025428db18847101fc0",
    "path": ".claude/commands/pr.md",
    "last_synced": 1754871024.7506702,
    "converted": true
  },
  ".claude/commands/prime_webapp.md": {
    "sha": "c6f4cfe53f2aa92e1ac5661138989c9f9ff3ec42",
    "path": ".claude/commands/prime_webapp.md",
    "last_synced": 1754871038.662535,
    "converted": true
  },
  ".claude/commands/python/python_update_config.md": {
    "sha": "8562257de3a622c1aefb06be4c78a4ce9c580e2a",
    "path": ".claude/commands/python/python_update_config.md",
    "last_synced": 1754871042.126036,
    "converted": true
  },
  ".claude/commands/read.md": {
    "sha": "524139e35449b4f5c0a88b206699cec5d2f1409b",
    "path": ".claude/commands/read.md",
    "last_synced": 1754871043.723397,
    "converted": true
  },
  ".claude/commands/response_in_markdown.md": {
    "sha": "a540be858d83e4b65df8dd2a23e3656bbdce8ee7",
    "path": ".claude/commands/response_in_markdown.md",
    "last_synced": 1754871049.784717,
    "converted": true
  },
  ".claude/commands/runplan.md": {
    "sha": "08d9b2d8c3bd445c55170adec7e80b9d3733e27a",
    "path": ".claude/commands/runplan.md",
    "last_synced": 1754871056.6670442,
    "converted": true
  },
  ".claude/commands/test_runner.md": {
    "sha": "28e088b2d155b9eca391add1bc75cdf701a3a4d8",
    "path": ".claude/commands/test_runner.md",
    "last_synced": 1754871059.413824,
    "converted": true
  },
  ".claude/commands/tools/create_database.md": {
    "sha": "fcfeeb2b78e39ecf0a1b8b4a85735dd83fa1c8f1",
    "path": ".claude/commands/tools/create_database.md",
    "last_synced": 1754871070.783872,
    "converted": true
  },
  ".claude/commands/tools/extract.md": {
    "sha": "e8946fb23beb24cd8e8689c48cd21b155dfa10a6",
    "path": ".claude/commands/tools/extract.md",
    "last_synced": 1754871074.061621,
    "converted": true
  }
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="llmrunner.py"&amp;amp;gt;import asyncio
from datetime import datetime

from pydantic import BaseModel
from typing import Dict, Union, List, Optional, Any

from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP
from models.gemini_mcp import GeminiMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP


class ModelsToMCP(BaseModel):
    model_config = {"arbitrary_types_allowed": True}
    models_to_mcp: Dict[str, Union[GeminiMCP, AnthropicMCP, OpenAIMCP, XaiMCP]]


class ModelResult(BaseModel):
    model: str
    timestamp: str
    success: bool
    actual_model: Optional[str] = None
    duration_seconds: Optional[float] = None
    response: Optional[Any] = None
    error: Optional[str] = None


class LLMRunnerResults(BaseModel):
    successful_results: List[ModelResult]
    failed_results: List[ModelResult]
    total_models: int
    success_count: int
    failure_count: int


async def llmrunner(prompt: str, models_to_mcp: ModelsToMCP) -&amp;amp;amp;gt; LLMRunnerResults:

    async def call_model(model_name: str) -&amp;amp;amp;gt; dict:
        try:
            start_time = datetime.now()
            iso_time = start_time.isoformat()

            mcp_instance = models_to_mcp.models_to_mcp[model_name]
            print(f"sending to --&amp;amp;amp;gt; {model_name} : at -&amp;amp;amp;gt; {iso_time}")
            response = mcp_instance.send_message(prompt, model=model_name)
            end_time = datetime.now()

            result = ModelResult(
                model=model_name,
                actual_model=model_name,
                timestamp=iso_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                response=response,
                success=True,
            )

            return result

        except Exception as e:
            error_result = ModelResult(
                success=False,
                error=str(e),
                model=model_name,
                timestamp=datetime.now().isoformat(),
            )

            return error_result

    print(
        f"starting runner for: {
          len(models_to_mcp.models_to_mcp.keys())} models -&amp;amp;amp;gt;"
    )
    tasks = [call_model(model) for model in models_to_mcp.models_to_mcp.keys()]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful_results = [
        r for r in results if isinstance(r, ModelResult) and r.success
    ]

    failed_results = [
        r for r in results if isinstance(r, ModelResult) and not r.success
    ]

    return LLMRunnerResults(
        successful_results=successful_results,
        failed_results=failed_results,
        total_models=len(models_to_mcp.models_to_mcp),
        success_count=len(successful_results),
        failure_count=len(failed_results),
    )


def code_review_models_to_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    anthropic_model = config.anthropic_default_code_review_model
    gemini_model = config.gemini_default_code_review_model
    xai_model = config.xai_default_code_review_model
    openai_model = config.openai_default_code_review_model

    gemini_mcp = GeminiMCP(config, secret_mgr, gemini_model)
    openai_mcp = OpenAIMCP(config, secret_mgr, openai_model)
    xai_mcp = XaiMCP(config, secret_mgr, xai_model)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, anthropic_model)

    model_mcps = {
        gemini_model: gemini_mcp,
        openai_model: openai_mcp,
        xai_model: xai_mcp,
        anthropic_model: anthropic_mcp,
    }

    return ModelsToMCP(models_to_mcp=model_mcps)
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="README.md"&amp;amp;gt;**Collect** is a command-line toolkit built with Python that functions as an MCP (Model Context Protocol) server. It is designed to assist with AI-driven development by providing tools to fetch web content, process it, and coordinate analysis across multiple AI models.

*   **Multi-Model Integration**: Interact with models from Google (Gemini), Anthropic (Claude), OpenAI (GPT), and XAI (Grok) through a single interface.
*   **Content Processing**: Fetch content from URLs and convert HTML to clean markdown or plain text.
*   **Code &amp;amp;amp;amp; Diff Analysis**: Perform code reviews on files or git diffs using any of the integrated AI models.
*   **Secure Configuration**: Utilizes Google Cloud Secret Manager for API key storage.
*   **Prompt Management**: A version-controlled system for managing and synchronizing prompts between the local filesystem and a SQLite database.
*   **Token Utilities**: Tools to count token usage for various models to manage costs and context windows.

### MCP Server Configuration

#### For Claude Code

To enable Claude Code to use the `collect` MCP server, create a `.mcp.json` file in your project's root directory:

1.  **Create the Configuration File**: In the root of your project where you want to use the collect tools, create a file named `.mcp.json`.
2.  **Add Configuration**: Add the following JSON configuration:

```json
{
  "mcpServers": {
    "collect": {
      "command": "/path/to/.local/bin/uv",
      "args": [
        "--directory",
        "/path/to/collect",
        "run",
        "collect.py"
      ]
    }
  }
}
```

Replace `/path/to/.local/bin/uv` with the full path to your `uv` binary (you can find this with `which uv`), and `/path/to/collect` with the full path to your collect repository.

#### For Gemini CLI

To enable the Gemini CLI to automatically start the `collect` MCP server, you need to configure a `.gemini/settings.json` file in your project's root directory:

1.  **Create the Directory**: If it doesn't already exist, create a `.gemini` directory in the root of the `collect` project.
2.  **Create the Settings File**: Inside the `.gemini` directory, create a file named `settings.json`.
3.  **Add Configuration**: Paste the following JSON configuration into the `settings.json` file.

```json
{
  "mcpServers": {
    "collect": {
      "command": "uv",
      "args": [
        "run",
        "python",
        "collect.py"
      ],
      "workingDirectory": "/Users/benjaminmetz/python/collect",
      "enabled": true
    }
  }
}
```

This configuration tells the Gemini CLI how to launch the `collect` server, specifying the command, arguments, and working directory.

### Command Category System

The command category system dynamically creates categories based on subdirectories configured in the `.env` file. This approach allows for easy extension of command categories without code changes.

#### How Categories Are Created

1. **Configuration**: Command subdirectories are defined in the `.env` file:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
   ```

2. **Dynamic Enum Generation**: The `create_cmd_category_enum()` function in `repository/prompt_models.py` reads the `COMMAND_SUBDIRS` from the `.env` file via the `Config` class and dynamically creates a `CmdCategory` enum at runtime.

3. **Directory Management**: When the `PromptService` initializes, the `cmd_check_dirs()` function in `repository/prompt_service.py`:
   - Reads the subdirectory list from the config
   - Checks for the existence of each configured subdirectory under both `.claude/commands/` and `.gemini/commands/`
   - Automatically creates any missing directories
   - Each subdirectory becomes a valid command category

4. **Category Assignment**: When loading commands from disk:
   - Files directly in `.claude/commands/` or `.gemini/commands/` are assigned the `UNCATEGORIZED` category
   - Files in subdirectories are assigned the category matching the subdirectory name
   - The category is stored as part of the prompt's metadata in the database

#### Adding New Categories

To add new command categories:
1. Update the `COMMAND_SUBDIRS` line in the `.env` file with your new category
2. The system will automatically create the directories and recognize them as valid categories on the next run
3. Commands placed in those directories will be tagged with the new category

#### Example

To add a "rust" category:
1. Edit `.env`:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools,rust
   ```
2. Restart the service or run the prompt service
3. The system will create:
   - `.claude/commands/rust/`
   - `.gemini/commands/rust/`
4. Any `.md` files placed in these directories will be categorized as "rust" commands

#### Current Directory Structure
Based on the `.env` configuration (`COMMAND_SUBDIRS=archive,go,js,mcp,python,tools`), the directory structure is:

```
.claude/
└── commands/
    ├── archive/          # Archived commands
    ├── go/               # Go-specific commands
    ├── js/               # JavaScript commands
    ├── mcp/              # MCP server commands
    ├── python/           # Python-specific commands
    └── tools/            # Tool-related commands

.gemini/
└── commands/
    ├── archive/
    ├── go/
    ├── js/
    ├── mcp/
    ├── python/
    └── tools/
```

Note: Files placed directly in `.claude/commands/` or `.gemini/commands/` (not in subdirectories) are automatically assigned the `UNCATEGORIZED` category.

### Prompt Management System

The project includes a system for managing prompts **that is very much under construction**. Prompts are categorized as either **Commands** (`CMD`) or **Plans** (`PLAN`). This system, located in the `repository/` directory, uses a SQLite database to store and version prompts, while also synchronizing them with the local filesystem.

*   **Core Components**:
    *   `prompt_service.py`: The main service class that orchestrates loading, saving, versioning, and flattening prompts.
    *   `prompt_models.py`: Defines the Pydantic data models for prompts, including `Prompt`, `PromptData`, and various status enums like `PromptType` and `PromptPlanStatus`.
    *   `database.py`: Manages the connection to the `collect.db` SQLite database.
    *   `20250727_01_create-prompt-tables.sql`: The database migration file that defines the schema for the `prompt` and `prompt_history` tables.

*   **Synchronization Workflow**:
    1.  **Loading from Disk**: The `PromptService` can load prompts from predefined directories (`.claude/commands`, `.gemini/commands`, and `_docs/plans`).
    2.  **Database Persistence**: Loaded prompts are saved to the SQLite database. The service checks for existing prompts by name. If a prompt already exists and its content has changed (verified via a SHA256 hash), a new version is created in the `prompt_history` table, and the main `prompt` table is updated.
    3.  **Flattening to Disk**: The service can "flatten" the prompts from the database back to the filesystem, ensuring that the local files are consistent with the database state. This is useful for maintaining a clear and organized prompt library.

*   **Versioning**:
    *   Every time a prompt's content is updated, its `version` number is incremented.
    *   A complete record of all versions is stored in the `prompt_history` table, including a timestamp and a change summary. This allows for a full audit trail of how a prompt has evolved.

This system ensures that prompts are treated as version-controlled assets within the project, providing a structured and auditable way to manage the instructions given to the AI models.
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="api.py"&amp;amp;gt;#!/usr/bin/env python

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api import prompt_api_router
from config import Config
import uvicorn

import sys
import logging
from pythonjsonlogger.json import JsonFormatter
from contextlib import asynccontextmanager


# Configure JSON logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(stream=sys.stdout)
handler.setFormatter(JsonFormatter())
handler.setLevel(logging.INFO)

logger.addHandler(handler)

# Load configuration
config = Config()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting prompt API service...")
    app.state.db_path = config.db_path
    app.state.config = config
    logger.info(f"Database path set to: {app.state.db_path}")
    logger.info(f"Service running on port: {config.port}")

    yield

    # Shutdown
    logger.info("Shutting down prompt API service...")


app = FastAPI(
    title="Prompt Service API",
    description="HTTP API for managing prompts and plans",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

app.include_router(prompt_api_router, tags=["prompt_api"])


def main():
    uvicorn.run(app, host="0.0.0.0", port=int(config.port))


if __name__ == "__main__":
    main()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="yoyo.ini"&amp;amp;gt;[DEFAULT]
sources = migrations
database = sqlite:///data/collect.db
batch_mode = on
verbosity = 0

&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="secret_manager.py"&amp;amp;gt;from google.cloud import secretmanager


class SecretManager:
    def __init__(
        self,
        project_id: str,
    ) -&amp;amp;amp;gt; None:
        self.project_id = project_id
        self.gcp_client = secretmanager.SecretManagerServiceClient()

    def get_secret(self, secret_name: str) -&amp;amp;amp;gt; str:
        response = self.gcp_client.access_secret_version(request={"name": secret_name})
        return response.payload.data.decode("UTF-8").strip()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".gitignore"&amp;amp;gt;# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
venv/
env/

# Environment variables
.env.local
.env.*.local

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Test coverage
.coverage
htmlcov/
.pytest_cache/
.tox/

# Temporary files
:w
*.tmp
*.temp

# Python version management
.python-version

# UV lock file (optional - some prefer to track this)
# uv.lock
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".env"&amp;amp;gt;PORT=8081
GCP_PROJECT_ID=482777410016
DB_PATH=data/collect.db
ANTHROPIC_API_KEY_PATH=projects/482777410016/secrets/AnthropicMCP/versions/1
ANTHROPIC_MODEL_OPUS=claude-opus-4-20250514
ANTHROPIC_MODEL_SONNET=claude-sonnet-4-20250514
GEMINI_API_KEY_PATH=projects/482777410016/secrets/GeminiTest/versions/1
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/
XAI_API_KEY_PATH=projects/482777410016/secrets/XAI_API_KEY_ELEPHNT/versions/1
GROK_SYSTEM_PROMPT=You are a helpful assistant that can answer questions and help with tasks.
OPENAI_API_KEY_PATH=projects/482777410016/secrets/OpenAIMCP/versions/1

# default code review models for running the code review loop
OPENAI_DEFAULT_CODE_REVIEW_MODEL=o3-mini-2025-01-31
GEMINI_DEFAULT_CODE_REVIEW_MODEL=gemini-2.5-flash-preview-05-20
ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL=claude-opus-4-20250514
XAI_DEFAULT_CODE_REVIEW_MODEL=grok-3-mini-fast-latest

# Command subdirectories
COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
GITHUB_URL=https://github.com/austere-labs/collect
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="filterModel.md"&amp;amp;gt;# Gemini Model Filtering Implementation Guide

## Overview

This document describes how to filter Gemini models by version number (2.0, 2.5, etc.) and extract input token limits from the API response.

## Current State

The `GeminiMCP.get_model_list()` method has been updated to return the full API response instead of just model names:

```python
def get_model_list(self) -&amp;amp;amp;gt; Dict:
    # ... API call logic ...
    model_data = response.json()
    return model_data  # Returns full response with all model metadata
```

## Implementation Plan

### 1. Add Filtering Methods to GeminiMCP Class

Add these methods to the `GeminiMCP` class in `models/gemini_mcp.py`:

```python
def filter_models_by_version(self, versions: list[str]) -&amp;amp;amp;gt; list[dict]:
    """
    Filter models by version numbers and include token limits.
    
    Args:
        versions: List of version strings (e.g., ['2.0', '2.5'])
    
    Returns:
        List of dicts with model info including inputTokenLimit
    """
    all_models = self.get_model_list()
    filtered_models = []
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Check if model matches any requested version
        for version in versions:
            if version in model_name:
                model_info = {
                    'name': model_name,
                    'displayName': model.get('displayName', ''),
                    'inputTokenLimit': model.get('inputTokenLimit', 0),
                    'outputTokenLimit': model.get('outputTokenLimit', 0),
                    'description': model.get('description', ''),
                    'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
                }
                filtered_models.append(model_info)
                break
    
    return filtered_models

def get_models_with_token_info(self) -&amp;amp;amp;gt; list[dict]:
    """
    Get all models with their token limit information.
    
    Returns:
        List of models sorted by inputTokenLimit (descending)
    """

    all_models = self.get_model_list()
    models_with_tokens = []

    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Only include models with token limit info
        if input_limit &amp;amp;amp;gt; 0:
            models_with_tokens.append({
                'name': model_name,
                'inputTokenLimit': input_limit,
                'outputTokenLimit': model.get('outputTokenLimit', 0)
            })
    
    # Sort by input token limit (highest first)
    models_with_tokens.sort(key=lambda x: x['inputTokenLimit'], reverse=True)
    return models_with_tokens
```

### 2. Advanced Filtering Function (Standalone)

For more complex filtering needs, you can use this standalone function:

```python
def filter_gemini_models(models_data: dict, 
                        versions: list[str] = None,
                        min_input_tokens: int = None,
                        max_input_tokens: int = None,
                        generation_methods: list[str] = None) -&amp;amp;amp;gt; list[dict]:
    """
    Advanced filtering with multiple criteria.
    
    Args:
        models_data: Response from get_model_list()
        versions: Filter by version numbers (optional)
        min_input_tokens: Minimum inputTokenLimit (optional)
        max_input_tokens: Maximum inputTokenLimit (optional)
        generation_methods: Required generation methods (optional)
    
    Returns:
        Filtered list of model information
    """
    filtered_models = []
    
    for model in models_data.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Apply version filter
        if versions:
            if not any(ver in model_name for ver in versions):
                continue
        
        # Apply token limit filters
        if min_input_tokens and input_limit &amp;amp;amp;lt; min_input_tokens:
            continue
        if max_input_tokens and input_limit &amp;amp;amp;gt; max_input_tokens:
            continue
        
        # Apply generation method filter
        if generation_methods:
            supported_methods = model.get('supportedGenerationMethods', [])
            if not all(method in supported_methods for method in generation_methods):
                continue
        
        # Model passed all filters
        model_info = {
            'name': model_name,
            'displayName': model.get('displayName', ''),
            'inputTokenLimit': input_limit,
            'outputTokenLimit': model.get('outputTokenLimit', 0),
            'description': model.get('description', ''),
            'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
        }
        filtered_models.append(model_info)
    
    return filtered_models
```

## Usage Examples

### Basic Version Filtering

```python
def test_filter_by_version(gemini_mcp):
    # Get models for versions 2.0 and 2.5
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    print(f"Found {len(filtered)} models:")
    for model in filtered:
        print(f"- {model['name']}: {model['inputTokenLimit']:,} input tokens")
```

### Get Models with Token Info

```python
def test_models_with_tokens(gemini_mcp):
    models = gemini_mcp.get_models_with_token_info()
    
    print("Models by input token limit:")
    for model in models[:10]:  # Top 10 models
        print(f"- {model['name']}: {model['inputTokenLimit']:,} tokens")
```

### Advanced Filtering

```python
def test_advanced_filtering(gemini_mcp):
    all_models = gemini_mcp.get_model_list()
    
    # Find 2.5 models with at least 100k input tokens
    filtered = filter_gemini_models(
        all_models,
        versions=['2.5'],
        min_input_tokens=100000,
        generation_methods=['generateContent']
    )
    
    print("High-capacity 2.5 models:")
    for model in filtered:
        print(f"- {model['name']}")
        print(f"  Input limit: {model['inputTokenLimit']:,}")
        print(f"  Output limit: {model['outputTokenLimit']:,}")
```

### Grouping Models by Version

```python
def group_models_by_version(gemini_mcp):
    from collections import defaultdict
    
    all_models = gemini_mcp.get_model_list()
    version_groups = defaultdict(list)
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Extract version pattern
        if '2.5' in model_name:
            version = '2.5'
        elif '2.0' in model_name:
            version = '2.0'
        elif '1.5' in model_name:
            version = '1.5'
        elif '1.0' in model_name:
            version = '1.0'
        else:
            version = 'other'
        
        version_groups[version].append({
            'name': model_name,
            'inputTokenLimit': model.get('inputTokenLimit', 0)
        })
    
    # Display grouped results
    for version, models in sorted(version_groups.items()):
        print(f"\nVersion {version} ({len(models)} models):")
        for model in sorted(models, key=lambda x: x['inputTokenLimit'], reverse=True)[:3]:
            print(f"  - {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Expected Output Format

When filtering models, you'll get results like:

```
Found 15 models:
- gemini-2.5-pro: 2,000,000 input tokens
- gemini-2.5-flash: 1,000,000 input tokens
- gemini-2.5-flash-preview-05-20: 1,000,000 input tokens
- gemini-2.0-flash: 32,768 input tokens
- gemini-2.0-flash-exp: 32,768 input tokens
- gemini-2.0-pro-exp: 32,768 input tokens
```

## API Response Structure

The Gemini API returns model data in this format:

```json
{
  "models": [
    {
      "name": "models/gemini-2.5-flash",
      "displayName": "Gemini 2.5 Flash",
      "description": "Fast and versatile multimodal model",
      "inputTokenLimit": 1000000,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ]
    }
    // ... more models
  ]
}
```

## Testing the Implementation

Add this test to `models/test_gemini_mcp.py`:

```python
def test_filter_models_by_version(gemini_mcp):
    # Test filtering for 2.0 and 2.5 versions
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    assert len(filtered) &amp;amp;amp;gt; 0
    assert all('2.0' in m['name'] or '2.5' in m['name'] for m in filtered)
    assert all('inputTokenLimit' in m for m in filtered)
    
    # Print results for verification
    print(f"\nFound {len(filtered)} models for versions 2.0 and 2.5:")
    for model in sorted(filtered, key=lambda x: x['inputTokenLimit'], reverse=True):
        print(f"  {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Notes

1. **Token Limits**: Not all models return `inputTokenLimit`. Handle missing values gracefully.
2. **Model Names**: The API returns full names like "models/gemini-2.5-flash". We extract just the model part.
3. **Sorting**: Consider sorting results by token limit, name, or version for consistent output.
4. **Caching**: For production use, consider caching the model list as it doesn't change frequently.
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="test_llmrunner.py"&amp;amp;gt;import pytest

from llmrunner import (
    llmrunner,
    code_review_models_to_mcp,
    ModelResult,
    LLMRunnerResults,
)


@pytest.fixture
def models_to_mcp():
    return code_review_models_to_mcp()


@pytest.mark.asyncio
async def test_llmrunner(models_to_mcp):
    prompt = "What is 2 + 2?"
    result = await llmrunner(prompt, models_to_mcp)

    assert isinstance(result, LLMRunnerResults)
    assert isinstance(result.successful_results, list)
    assert isinstance(result.failed_results, list)
    assert isinstance(result.total_models, int)
    assert isinstance(result.success_count, int)
    assert isinstance(result.failure_count, int)

    assert result.total_models == len(models_to_mcp.models_to_mcp)
    assert result.success_count + result.failure_count == result.total_models

    for success_result in result.successful_results:
        assert isinstance(success_result, ModelResult)
        assert success_result.success is True
        assert success_result.model is not None
        assert success_result.timestamp is not None
        assert success_result.response is not None
        assert success_result.duration_seconds is not None

    for failed_result in result.failed_results:
        assert isinstance(failed_result, ModelResult)
        assert failed_result.success is False
        assert failed_result.model is not None
        assert failed_result.timestamp is not None
        assert failed_result.error is not None

    print(f"Total models: {result.total_models}")
    print(f"Successful: {result.success_count}")
    print(f"Failed: {result.failure_count}")

    for failed_result in result.failed_results:
        print(
            f"Failed model: {
              failed_result.model} - Error: {failed_result.error}"
        )

    for success_result in result.successful_results:
        print(f"Successful model: {success_result.model}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="GEMINI.md"&amp;amp;gt;# Gemini Code Assistant Context

This document provides context for the Gemini Code Assistant to understand the project structure, conventions, and important files.

## Project Overview

This project is a Python-based MCP (Model Context Protocol) server named "Collect". Its primary purpose is to fetch web content, process it, and facilitate multi-model AI analysis workflows. It provides a unified interface to interact with various AI models (OpenAI, Anthropic, Gemini, XAI) for tasks like code review. The server is built using the `mcp` library and exposes several tools for fetching URLs, converting HTML to markdown, counting tokens, and more. It also includes a database layer using SQLite for data persistence.

**Key Technologies:**

*   **Programming Language:** Python
*   **Framework:** `mcp` (Model Context Protocol)
*   **Key Libraries:** 
    *   `httpx` for asynchronous HTTP requests.
    *   `anthropic`, `openai`, `google-cloud-aiplatform` for interacting with various LLMs.
    *   `readabilipy`, `markdownify`, `beautifulsoup4` for HTML processing.
    *   `pyperclip` for clipboard integration.
    *   `yoyo-migrations` for database schema management.
*   **Package Manager:** `uv`
*   **Testing:** `pytest` with `pytest-asyncio` and `pytest-xdist` for parallel testing.
*   **Linting/Formatting:** `ruff` and `black`.
*   **Database:** SQLite.

## Building and Running

*   **Install Dependencies:** `uv sync`
*   **Run the Server:** `python collect.py`
*   **Run Tests:** `uv run pytest -v -s -n auto`
*   **Run Linter:** `ruff check .`
*   **Run Formatter:** `black .`
*   **Apply Database Migrations:** `uv run yoyo apply --config yoyo.ini --batch`

## Development Conventions

*   **Testing:** Tests are written using `pytest` and are located in files like `test_collect.py`. Asynchronous functions are tested using `@pytest.mark.asyncio`. The project uses `pytest-xdist` for parallel test execution.
*   **Linting and Formatting:** The project uses `ruff` for linting and `black` for formatting. These are run via the `Makefile`.
*   **Configuration:** Project configuration is managed in `config.py`, which loads environment variables from a `.env` file.
*   **Secrets Management:** API keys and other secrets are managed through Google Cloud Secret Manager, as indicated in `secret_manager.py` and `config.py`.
*   **Database:** The project uses SQLite for its database. The database connection logic is in `repository/database.py`. Migrations are handled by `yoyo-migrations`.

## Key Files

*   **`collect.py`:** The main entry point of the MCP server. It defines the available tools, such as `fetch_urls`, `run_code_review`, and `to_markdown`.
*   **`pyproject.toml`:** Defines the project's dependencies and development tool configurations.
*   **`Makefile`:** Provides convenient commands for common development tasks like testing, linting, and formatting.
*   **`config.py`:** Handles the project's configuration by loading environment variables from a `.env` file.
*   **`reviewer/code_review.py`:** Contains the logic for the code review functionality. It takes a diff file, sends it to multiple LLMs, and then formats and saves the results.
*   **`models/`:** This directory contains modules for interacting with different AI models (e.g., `anthropic_mpc.py`, `openai_mpc.py`).
*   **`repository/database.py`:** Contains the logic for connecting to the SQLite database.
*   **`migrations/`:** This directory contains the SQL migration files for the database schema.
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="movetools"&amp;amp;gt;#!/bin/bash

# Enhanced setup script to copy tools to user bin directory with colorful output
# This script should be run from the collect project home directory

# Color definitions for enhanced output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly BOLD='\033[1m'
readonly NC='\033[0m' # No Color

# Set default target directory (configurable)
TARGET_DIR=${1:-~/bin}

# Expand tilde to home directory
TARGET_DIR="${TARGET_DIR/#\~/$HOME}"

# Get the directory where this script is located (project home)
PROJECT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;amp;amp;amp;&amp;amp;amp;amp; pwd )"
TOOLS_DIR="$PROJECT_DIR/tools"

# Enhanced printing functions
print_header() {
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${WHITE}  🔧 MOVETOOLS - Tool Installation Script${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
}

print_info() {
    echo -e "${CYAN}ℹ${NC}  $1"
}

print_success() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC}  $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_step() {
    echo -e "\n${BOLD}${PURPLE}▶${NC} ${BOLD}$1${NC}"
}

print_item() {
    echo -e "   ${GREEN}•${NC} $1"
}

# Function to display usage
usage() {
    print_header
    echo -e "${BOLD}USAGE:${NC}"
    echo -e "  $0 [target_directory]"
    echo ""
    echo -e "${BOLD}DESCRIPTION:${NC}"
    echo -e "  ${BOLD}MOVETOOLS${NC} is a comprehensive installation and backup script for the collect project."
    echo -e "  It performs two main functions:"
    echo ""
    echo -e "  ${BOLD}1. Tool Installation:${NC}"
    echo -e "     • Copies all executable tools from the tools/ directory to your bin directory"
    echo -e "     • Makes all copied tools executable with proper permissions"
    echo -e "     • Validates PATH configuration and provides setup guidance"
    echo -e "     • Provides colorful visual feedback throughout the process"
    echo ""
    echo -e "  ${BOLD}2. Dotfiles Backup:${NC}"
    echo -e "     • Backs up your .zshrc configuration to the project's dotfiles/ directory"
    echo -e "     • Copies your Ghostty terminal configuration (~/.config/ghostty)"
    echo -e "     • Backs up your Neovim init.lua configuration (~/.config/nvim/init.lua)"
    echo -e "     • Creates organized dotfiles structure for version control"
    echo ""
    echo -e "${BOLD}ARGUMENTS:${NC}"
    echo -e "  ${CYAN}target_directory${NC}    Optional. Directory to install tools (default: ~/bin)"
    echo ""
    echo -e "${BOLD}OPTIONS:${NC}"
    echo -e "  ${YELLOW}--llm${NC}              Display comprehensive usage information (LLM-friendly)"
    echo -e "  ${YELLOW}--help, -h${NC}         Display this usage information"
    echo ""
    echo -e "${BOLD}FEATURES:${NC}"
    echo -e "  • Enhanced colorful terminal output with status indicators"
    echo -e "  • Automatic directory creation with proper error handling"
    echo -e "  • Tool validation and permission management"
    echo -e "  • PATH verification with configuration suggestions"
    echo -e "  • Comprehensive dotfiles backup across multiple applications"
    echo -e "  • Installation summary with detailed feedback"
    echo ""
    echo -e "${BOLD}EXAMPLES:${NC}"
    echo -e "  ${GREEN}$0${NC}                 # Install tools to ~/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 ~/.local/bin${NC}    # Install tools to ~/.local/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 --llm${NC}           # Show comprehensive help for AI assistants"
    echo -e "  ${GREEN}$0 --help${NC}          # Show this help message"
    echo ""
    echo -e "${BOLD}DIRECTORY STRUCTURE:${NC}"
    echo -e "  ${CYAN}tools/${NC}              # Source directory containing executable tools"
    echo -e "  ${CYAN}dotfiles/zshrc${NC}      # Backed up shell configuration"
    echo -e "  ${CYAN}dotfiles/ghostty/${NC}   # Backed up terminal configuration"
    echo -e "  ${CYAN}dotfiles/nvim/${NC}      # Backed up Neovim configuration"
    echo ""
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    exit 0
}

# Check for help flags
if [ $# -eq 1 ] &amp;amp;amp;amp;&amp;amp;amp;amp; ([ "$1" = "--llm" ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]); then
    usage
fi

# Display header
print_header

# Display configuration
print_step "Configuration"
print_info "Project directory: ${BOLD}$PROJECT_DIR${NC}"
print_info "Tools directory: ${BOLD}$TOOLS_DIR${NC}"
print_info "Target directory: ${BOLD}$TARGET_DIR${NC}"

# Check if tools directory exists
print_step "Validation"
if [ ! -d "$TOOLS_DIR" ]; then
    print_error "Tools directory not found at $TOOLS_DIR"
    exit 1
fi
print_success "Tools directory found"

# Create target directory if it doesn't exist
if [ ! -d "$TARGET_DIR" ]; then
    print_info "Creating target directory..."
    mkdir -p "$TARGET_DIR"
    if [ $? -eq 0 ]; then
        print_success "Target directory created"
    else
        print_error "Failed to create target directory"
        exit 1
    fi
else
    print_success "Target directory exists"
fi

# Copy all files from tools directory
print_step "Copying Tools"
tool_count=0
copied_tools=()

for tool in "$TOOLS_DIR"/*; do
    if [ -f "$tool" ]; then
        tool_name=$(basename "$tool")
        # Skip CLAUDE.md file
        if [ "$tool_name" = "CLAUDE.md" ]; then
            continue
        fi
        if cp "$tool" "$TARGET_DIR/" 2&amp;amp;amp;gt;/dev/null; then
            print_item "Copied ${BOLD}$tool_name${NC}"
            copied_tools+=("$tool_name")
            ((tool_count++))
        else
            print_error "Failed to copy $tool_name"
        fi
    fi
done

if [ $tool_count -eq 0 ]; then
    print_warning "No tools found in $TOOLS_DIR"
    exit 0
fi

# Make all copied tools executable
print_step "Setting Permissions"
executable_count=0
for tool_name in "${copied_tools[@]}"; do
    tool_path="$TARGET_DIR/$tool_name"
    if chmod u+x "$tool_path" 2&amp;amp;amp;gt;/dev/null; then
        print_item "Made ${BOLD}$tool_name${NC} executable"
        ((executable_count++))
    else
        print_error "Failed to make $tool_name executable"
    fi
done

# Display completion summary
print_step "Summary"
print_success "Installation complete!"
print_info "${BOLD}$tool_count${NC} tools copied and ${BOLD}$executable_count${NC} made executable"
print_info "Tools are now available in: ${BOLD}$TARGET_DIR${NC}"

# List installed tools
if [ ${#copied_tools[@]} -gt 0 ]; then
    echo ""
    print_info "${BOLD}Installed tools:${NC}"
    for tool_name in "${copied_tools[@]}"; do
        echo -e "   ${GREEN}▸${NC} ${BOLD}$tool_name${NC}"
    done
fi

# Check if target directory is in PATH
echo ""
if [[ ":$PATH:" != *":$TARGET_DIR:"* ]]; then
    print_warning "Target directory is not in your PATH"
    echo -e "${YELLOW}💡${NC} To use these tools from anywhere, add this line to your ${BOLD}~/.bashrc${NC} or ${BOLD}~/.zshrc${NC}:"
    echo -e "   ${CYAN}export PATH=\"$TARGET_DIR:\$PATH\"${NC}"
else
    print_success "Target directory is already in your PATH"
fi

# Copy dotfiles section
print_step "Copying Dotfiles"
DOTFILES_DIR="$PROJECT_DIR/dotfiles"

# Ensure dotfiles directory exists
if [ ! -d "$DOTFILES_DIR" ]; then
    print_info "Creating dotfiles directory..."
    mkdir -p "$DOTFILES_DIR"
    if [ $? -eq 0 ]; then
        print_success "Dotfiles directory created"
    else
        print_error "Failed to create dotfiles directory"
    fi
else
    print_success "Dotfiles directory exists"
fi

# Copy .zshrc
if [ -f "$HOME/.zshrc" ]; then
    if cp "$HOME/.zshrc" "$DOTFILES_DIR/.zshrc" 2&amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}.zshrc${NC} from home directory"
    else
        print_error "Failed to copy .zshrc"
    fi
else
    print_warning ".zshrc not found in home directory"
fi

# Copy ghostty config
GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"
if [ -d "$GHOSTTY_CONFIG_DIR" ]; then
    # Create ghostty subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/ghostty"
    if cp -r "$GHOSTTY_CONFIG_DIR"/* "$DOTFILES_DIR/ghostty/" 2&amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}ghostty config${NC} from ~/.config/ghostty"
    else
        print_error "Failed to copy ghostty config"
    fi
else
    print_warning "Ghostty config directory not found at ~/.config/ghostty"
fi

# Copy nvim init.lua
NVIM_CONFIG="$HOME/.config/nvim/init.lua"
if [ -f "$NVIM_CONFIG" ]; then
    # Create nvim subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/nvim"
    if cp "$NVIM_CONFIG" "$DOTFILES_DIR/nvim/init.lua" 2&amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}nvim init.lua${NC} from ~/.config/nvim"
    else
        print_error "Failed to copy nvim init.lua"
    fi
else
    print_warning "Nvim init.lua not found at ~/.config/nvim/init.lua"
fi

echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}${GREEN}🎉 Setup completed successfully!${NC}"
echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="source.xml"&amp;amp;gt;&amp;amp;amp;lt;?xml version='1.0' encoding='utf-8'?&amp;amp;amp;gt;
&amp;amp;amp;lt;source_code project="collect"&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="test_generate_prompt.py"&amp;amp;amp;gt;import pytest
from config import Config
from collect import generate_prompt


@pytest.fixture
def sample_prompt():
    """Sample prompt content for testing."""
    return """Create a helpful AI assistant that can answer programming questions.
The assistant should be knowledgeable about Python, JavaScript, and web development.
It should provide clear explanations and code examples when appropriate."""


class TestGeneratePrompt:

    @pytest.mark.asyncio
    async def test_generate_prompt_basic(self, sample_prompt):
        """Test basic functionality of generate_prompt."""
        # Check if we have required config
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(sample_prompt)

        # Verify we got a string response
        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;amp;gt; 0

        # The generated prompt should contain relevant content
        # Note: We can't predict exact content, but it should be substantial
        assert len(result) &amp;amp;amp;amp;gt; 50  # Should be more than just a few words

    @pytest.mark.asyncio
    async def test_generate_prompt_with_target_model(self, sample_prompt):
        """Test generate_prompt with target_model parameter."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        result = await generate_prompt(
            sample_prompt, target_model="claude-3-7-sonnet-20250219"
        )

        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;amp;gt; 0

    @pytest.mark.asyncio
    async def test_generate_prompt_empty_string(self):
        """Test error handling for empty prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("")

    @pytest.mark.asyncio
    async def test_generate_prompt_whitespace_only(self):
        """Test error handling for whitespace-only prompt."""
        with pytest.raises(ValueError, match="Prompt cannot be empty"):
            await generate_prompt("   \n\t   ")

    @pytest.mark.asyncio
    async def test_generate_prompt_simple_task(self):
        """Test with a simple task description."""
        config = Config()
        if not config.project_id or not config.anthropic_key_path:
            pytest.skip("Missing GCP_PROJECT_ID or ANTHROPIC_KEY_PATH in .env")

        simple_task = "A coding assistant that helps with Python"
        result = await generate_prompt(simple_task)

        assert isinstance(result, str)
        assert len(result) &amp;amp;amp;amp;gt; len(simple_task)  # Should be expanded


if __name__ == "__main__":
    # Run a simple test

    async def manual_test():
        """Manual test function for quick verification."""
        test_prompt = "Create a Python function that validates email addresses."

        try:
            result = await generate_prompt(test_prompt)
            print(f"Input prompt: {test_prompt}")
            print(f"Generated prompt ({len(result)} chars):")
            print("-" * 50)
            print(result)
            print("-" * 50)
        except Exception as e:
            print(f"Error: {e}")

    # Uncomment to run manual test
    # asyncio.run(manual_test())
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="collect.py"&amp;amp;amp;gt;from typing import List
from mcp.server.fastmcp import FastMCP, Context
import tiktoken
import markdownify
import readabilipy.simple_json
from html_to_markdown import convert_to_markdown
from bs4 import BeautifulSoup
from secret_manager import SecretManager
from config import Config
from models.anthropic_mpc import AnthropicMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP
from models.gemini_mcp import GeminiMCP
from fetcher import Fetcher
import pyperclip
from reviewer.code_review import CodeReviewer
import subprocess
import atexit
import time

mcp = FastMCP("Collect")


@mcp.tool()
async def run_code_review(from_file: str, to_file: str = "codereview"):
    """
    Run code review on a diff file using multiple LLM models.

    Args:
        from_file: Path to the file containing the diff/code to review
        to_file: Directory name to write results to (default: "codereview")

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_code(from_file, to_file)


@mcp.tool()
async def run_git_diff_review(to_file: str = "codereview", staged_only: bool = True):
    """
    Run code review on git diff output.

    Args:
        to_file: Directory name to write results to(default: "codereview")
        staged_only: If True, review only staged changes;
        if False, review all changes

    Returns:
        Summary of the code review results
    """
    reviewer = CodeReviewer(to_file)
    return await reviewer.review_diff_from_git(to_file, staged_only)


@mcp.tool()
async def fetch_urls(urls: List[str], ctx: Context = None) -&amp;amp;amp;amp;gt; str:
    """
    Fetch content from multiple URLs concurrently and merge the responses.

    Use this tool when you need to:
    - Retrieve content from multiple web pages at once
    - Compare information across multiple sources
    - Gather data from several API endpoints simultaneously
    - Fetch related pages in parallel for efficiency

    Args:
        urls: List of URLs to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Merged content from all URLs as a single string

    Example:
        fetch_urls(["https://api.example.com/users",
                   "https://api.example.com/posts"])
    """
    fetcher = Fetcher(ctx)
    merged_responses = await fetcher.fetch_urls(urls)
    return merged_responses


@mcp.tool()
async def fetch_url(url: str, ctx: Context = None) -&amp;amp;amp;amp;gt; str:
    """
    Fetch raw content from a single URL.

    Use this tool when you need to:
    - Retrieve raw HTML/JSON from a web page or API
    - Get unprocessed content for custom parsing
    - Access web resources programmatically
    - Fetch data before converting to markdown

    Args:
        url: The URL to fetch content from
        ctx: MCP context(automatically provided)

    Returns:
        Raw content from the URL(HTML, JSON, or plain text)

    Note: For documentation extraction, consider using get_docs instead.
          For markdown conversion, use to_markdown on the result.
    """
    fetcher = Fetcher(ctx)
    return fetcher.get(url)


@mcp.tool()
async def get_docs(url: str, extract_value: str = None, ctx: Context = None) -&amp;amp;amp;amp;gt; str:
    """
    Fetch and extract specific documentation content from web pages.

    Use this tool when users need to:
    - Extract specific sections from documentation websites
    - Get targeted information from technical docs
    - Retrieve API documentation for specific methods/classes
    - Pull configuration examples from documentation
    - Find specific topics within large documentation sites

    Args:
        url: The URL of the documentation page to fetch
        extract_value: Optional. Specific section/topic to extract(e.g., "authentication",
                      "API endpoints", "installation guide"). If not provided, returns
                      the entire page content.
        ctx: MCP context(automatically provided)

    Returns:
        Extracted documentation content as markdown. If extract_value is specified,
        uses Gemini AI to intelligently extract only the relevant section.

    Examples:
        - get_docs("https://docs.python.org/3/", "datetime module")
        - get_docs("https://fastapi.tiangolo.com/", "dependency injection")
        - get_docs("https://react.dev/", "useEffect hook")
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-flash-preview-05-20"
    gemini = GeminiMCP(config, secret_mgr, model=model)

    if extract_value is None:
        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        return response
    else:
        prompt_prefatory = f"""
        # Documentation Extraction Task

        Extract and format the documentation for: **{extract_value} **

        # Instructions:
        - Focus specifically on the requested section/topic
        - Include code examples, parameters, and usage details if present
        - Maintain original formatting and structure
        - If the exact section isn't found, extract the most relevant related content
        - Return only the extracted documentation content, no meta-commentary

        ## Content to extract: {extract_value}
        """

        prompt = prompt_prefatory + "\n\n"
        response = await gemini.build_prompt_from_url(url, prompt, ctx)
        return response.strip()


@mcp.tool()
async def copy_clipboard(text: str) -&amp;amp;amp;amp;gt; str:
    """
    Copy text to the system clipboard.

    Use this tool when users need to:
    - Copy generated code snippets to clipboard
    - Save formatted text for pasting elsewhere
    - Copy API keys, URLs, or configuration values
    - Transfer content between applications

    Args:
        text: The text content to copy to clipboard

    Note: The text will replace any existing clipboard content.
    """
    pyperclip.copy(text)


@mcp.tool()
def strip_html(html: str) -&amp;amp;amp;amp;gt; str:
    """
    Remove all HTML tags and return plain text content.

    Use this tool when you need to:
    - Extract plain text from HTML pages
    - Remove formatting and tags from web content
    - Clean HTML for text analysis
    - Prepare content for non-HTML processing

    Args:
        html: Raw HTML string to process

    Returns:
        Plain text with all HTML tags removed

    Note: This removes ALL formatting. For readable formatting, use to_markdown instead.
    """
    soup = BeautifulSoup(html, "lxml")
    return soup.get_text()


@mcp.tool()
def to_markdown(html: str) -&amp;amp;amp;amp;gt; str:
    """Extract and convert HTML content to markdown using markdownify
    and readabilipy

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown

    """
    html_to_json = readabilipy.simple_json.simple_json_from_html_string(
        html,
        use_readability=True,
    )
    if not html_to_json["content"]:
        return "&amp;amp;amp;amp;lt;error&amp;amp;amp;amp;gt;Page failed to be simplified from HTML to json&amp;amp;amp;amp;lt;/error&amp;amp;amp;amp;gt;"

    return markdownify.markdownify(
        html_to_json["content"],
        heading_style=markdownify.ATX,
    )


def html_to_markdown(html: str) -&amp;amp;amp;amp;gt; str:
    """This uses html-to-markdown library instead of markdownify

    Args:
        html: Raw HTML retrieved from fetch_url or fetch_urls

    Returns:
        Simplified markdown as a str
    """

    return convert_to_markdown(
        html,
        heading_style="atx",
    )


@mcp.tool()
async def get_anthropic_model_list() -&amp;amp;amp;amp;gt; List[str]:
    """
    Get the list of available Anthropic Claude models.

    Use this tool when you need to:
    - Check which Claude models are available
    - Verify model names before using them
    - List Anthropic's current model offerings
    - Help users choose between Claude models

    Returns:
        List of available Anthropic model names (e.g., ["claude-3-opus", "claude-3-sonnet"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.get_model_list()


@mcp.tool()
async def get_openai_model_list() -&amp;amp;amp;amp;gt; List[str]:
    """
    Get the list of available OpenAI models.

    Use this tool when you need to:
    - Check which GPT models are available
    - Verify OpenAI model names
    - List current OpenAI offerings
    - Help users choose between GPT models

    Returns:
        List of available OpenAI model names (e.g., ["gpt-4", "gpt-3.5-turbo"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    openai_mcp = OpenAIMCP(config, secret_mgr, model="gpt-4o")
    return openai_mcp.get_model_list()


@mcp.tool()
async def get_xai_model_list() -&amp;amp;amp;amp;gt; List[str]:
    """
    Get the list of available XAI (Grok) models.

    Use this tool when you need to:
    - Check which Grok models are available
    - Verify XAI model names
    - List current Grok offerings
    - Help users choose between Grok models

    Returns:
        List of available XAI model names (e.g., ["grok-3", "grok-3-mini"])
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-mini-fast-latest")
    return xai_mcp.get_model_list()


@mcp.tool()
async def get_gemini_model_list() -&amp;amp;amp;amp;gt; List[dict]:
    """
    Get the list of available Google Gemini models
    (filtered for 2.0 and 2.5 versions).

    Use this tool when you need to:
    - Check which Gemini models are available with their token limits
    - Verify Google AI model names and capabilities
    - List current Gemini 2.0 and 2.5 offerings
    - Help users choose between Gemini models based on token capacity

    Returns:
        List of model dictionaries sorted by token limit (highest first),
        each containing:
        - model_name: The model identifier (e.g., "gemini-2.5-flash")
        - token_window: Input token limit (e.g., 1048576)

    Example return:
        [
            {"model_name": "gemini-2.5-flash", "token_window": 1048576},
            {"model_name": "gemini-2.0-flash", "token_window": 1048576},
            {"model_name": "gemini-2.5-pro", "token_window": 1048576}
        ]
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.5-flash")
    return gemini_mcp.get_model_list()


@mcp.tool()
async def count_openai_tokens(text: str, model: str = "gpt-4") -&amp;amp;amp;amp;gt; int:
    """
    Count tokens in text using OpenAI's tiktoken tokenizer.

    Use this tool when you need to:
    - Check if content fits within OpenAI model limits
    - Estimate API costs for OpenAI models
    - Split content to fit token windows
    - Optimize prompts for token efficiency

    Args:
        text: The text to count tokens for
        model: OpenAI model name (default: "gpt-4")

    Returns:
        Number of tokens in the text for the specified model
    """
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@mcp.tool()
async def count_anthropic_tokens(text: str) -&amp;amp;amp;amp;gt; int:
    """
    Count tokens in text using Anthropic's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Claude model limits
    - Estimate API costs for Anthropic models
    - Split content for Claude's context window
    - Optimize prompts for Claude

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Anthropic models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    anthropic_mcp = AnthropicMCP(config, secret_mgr, model)
    return anthropic_mcp.count_tokens(text)


@mcp.tool()
async def count_gemini_tokens(text: str) -&amp;amp;amp;amp;gt; int:
    """
    Count tokens in text using Google Gemini's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Gemini model limits
    - Estimate API costs for Google AI models
    - Split content for Gemini's context window
    - Optimize prompts for Gemini

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Gemini models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    gemini_mcp = GeminiMCP(config, secret_mgr, model="gemini-2.0-flash")
    return gemini_mcp.count_tokens(text)


@mcp.tool()
async def count_grok_tokens(text: str) -&amp;amp;amp;amp;gt; int:
    """
    Count tokens in text using XAI Grok's tokenizer.

    Use this tool when you need to:
    - Check if content fits within Grok model limits
    - Estimate API costs for XAI models
    - Split content for Grok's context window
    - Optimize prompts for Grok

    Args:
        text: The text to count tokens for

    Returns:
        Number of tokens in the text for Grok models
    """
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    xai_mcp = XaiMCP(config, secret_mgr, model="grok-3-fast-latest")
    return xai_mcp.count_tokens(text)


@mcp.tool()
async def generate_prompt(prompt: str, target_model: str = None) -&amp;amp;amp;amp;gt; str:
    """
    Generate an optimized AI prompt using Anthropic's experimental prompt engineering API.

    This tool leverages Anthropic's closed research preview API to automatically create
    high-quality, structured prompts from simple task descriptions. The API analyzes
    your input and generates professional-grade prompts optimized for Claude models.

    Use this tool when you need to:
    - Transform simple ideas into comprehensive AI prompts
    - Create structured prompts for specific tasks or roles
    - Optimize prompts for better AI responses
    - Generate consistent prompt templates for repeated use
    - Improve prompt clarity and effectiveness

    Args:
        prompt: A brief description of what you want the AI to do.
                Can be as simple as a role description or task summary.
                Examples:
                - "a helpful programming assistant"
                - "a chef for meal planning"
                - "a technical documentation writer"
                - "analyze code for security vulnerabilities"
        target_model: Optional. The specific model to optimize for (e.g., "claude-3-opus").
                     If not specified, generates a general-purpose prompt.

    Returns:
        A professionally crafted prompt ready for use with Claude or other AI models.
        The generated prompt includes appropriate context, instructions, and structure
        to maximize response quality.

    Raises:
        ValueError: If the prompt is empty or only contains whitespace
        RuntimeError: If the API call fails or returns an unexpected response

    Example:
        &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; result = await generate_prompt("a Python code reviewer")
        &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; print(result)
        "You are an expert Python code reviewer with deep knowledge..."

    Note:
        This uses Anthropic's experimental "prompt-tools" API which requires special
        access. The API is in closed research preview and may change without notice.
    """
    try:
        # Validate input
        task_content = prompt.strip()
        if not task_content:
            raise ValueError("Prompt cannot be empty")

        # Set up Anthropic MCP client
        config = Config()
        secret_mgr = SecretManager(config.project_id)
        anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)

        # Call generate_prompt API with new signature
        response = anthropic_mcp.generate_prompt(task_content, target_model)

        # Extract the generated prompt text from the response
        if response.messages and response.messages[0].content:
            return response.messages[0].content[0].text
        else:
            raise ValueError("No prompt generated in response")

    except ValueError:
        # Re-raise ValueError (like empty prompt) without wrapping
        raise
    except Exception as e:
        raise RuntimeError(f"Error generating prompt: {str(e)}")


def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["uv", "run", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for API to initialize
        time.sleep(2)

        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")

            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()

            atexit.register(cleanup_api)

    except Exception as e:
        print(f"Failed to start API server: {e}")

    # Continue with MCP server startup
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="config.py"&amp;amp;amp;gt;from dotenv import load_dotenv
import os


class Config:
    # Load .env for configuration
    load_dotenv(".env")

    def __init__(self) -&amp;amp;amp;amp;gt; None:
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.port = os.getenv("PORT")
        self.db_path = os.getenv("DB_PATH")
        self.anthropic_key_path = os.getenv("ANTHROPIC_API_KEY_PATH")
        self.anthropic_model_opus = os.getenv("ANTHROPIC_MODEL_OPUS")
        self.anthropic_model_sonnet = os.getenv("ANTHROPIC_MODEL_SONNET")
        self.gemini_api_key_path = os.getenv("GEMINI_API_KEY_PATH")
        self.gemini_base_url = os.getenv("GEMINI_BASE_URL")
        self.xai_api_key_path = os.getenv("XAI_API_KEY_PATH")
        self.grok_system_prompt = os.getenv("GROK_SYSTEM_PROMPT")
        self.openai_api_key_path = os.getenv("OPENAI_API_KEY_PATH")
        self.openai_default_code_review_model = os.getenv(
            "OPENAI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.gemini_default_code_review_model = os.getenv(
            "GEMINI_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.anthropic_default_code_review_model = os.getenv(
            "ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL"
        )
        self.xai_default_code_review_model = os.getenv("XAI_DEFAULT_CODE_REVIEW_MODEL")

        # GitHub configuration
        self.github_url = os.getenv("GITHUB_URL")

        # Command subdirectories - read as comma-separated string
        command_subdirs_str = os.getenv(
            "COMMAND_SUBDIRS", "archive,go,js,mcp,python,tools"
        )
        self.command_subdirs = [
            subdir.strip() for subdir in command_subdirs_str.split(",")
        ]
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="test_collect.py"&amp;amp;amp;gt;import pytest

from collect import (
    count_anthropic_tokens,
    count_gemini_tokens,
    count_openai_tokens,
    count_grok_tokens,
    get_docs,
)

# The @pytest.mark.parametrize decorator runs the test function
#  test_empty_text_returns_zero three separate times, once for each
#  function in the list. Each time, it passes a different token-counting
#  function to the test as the func parameter.

#  This is useful for testing similar functionality across multiple
#  implementations without duplicating test code. In this case, it verifies
#   that all three token-counting functions return zero when given empty
#  text.


@pytest.mark.asyncio
async def test_openai_hello_token_count():
    result = await count_openai_tokens("hello", model="gpt-3.5-turbo")
    assert result == 1


@pytest.mark.parametrize(
    "func,text",
    [
        (count_openai_tokens, "Hello, world!"),
        (count_gemini_tokens, "Hello, Gemini!"),
        (count_anthropic_tokens, "Hello Claude"),
        (count_grok_tokens, "Hello Grok"),
    ],
)
@pytest.mark.asyncio
async def test_nonempty_text_returns_positive_int(func, text):
    n = await func(text)
    assert isinstance(n, int)
    assert n &amp;amp;amp;amp;gt; 0


@pytest.mark.asyncio
async def test_get_docs_with_extract_value():
    url = "https://docs.python.org/3/library/json.html"
    extract_value = "json.dumps"

    result = await get_docs(url, extract_value)

    assert isinstance(result, str)
    assert len(result) &amp;amp;amp;amp;gt; 0
    assert "json.dumps" in result.lower()

    print(f"Extracted docs for {extract_value}:")
    print(result[:500] + "..." if len(result) &amp;amp;amp;amp;gt; 500 else result)


@pytest.mark.asyncio
async def test_get_docs_without_extract_value():
    url = "https://docs.python.org/3/library/json.html"

    result = await get_docs(url)

    assert isinstance(result, str)
    assert len(result) &amp;amp;amp;amp;gt; 0
    # Should contain raw HTML content when no extraction is performed
    assert "html" in result.lower() or "json" in result.lower()

    print(f"Raw content length: {len(result)}")
    print(result[:200] + "..." if len(result) &amp;amp;amp;amp;gt; 200 else result)
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="requirements.txt"&amp;amp;amp;gt;aiohttp==3.11.11
annotated-types==0.7.0
anthropic==0.50.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.8
distro==1.9.0
docstring-parser==0.16
-e file:///Users/benjaminmetz/python/collect
google-api-core==2.24.2
google-auth==2.39.0
google-cloud-aiplatform==1.91.0
google-cloud-bigquery==3.31.0
google-cloud-core==2.4.3
google-cloud-resource-manager==1.14.2
google-cloud-storage==2.19.0
google-crc32c==1.7.1
google-resumable-media==2.7.2
googleapis-common-protos==1.70.0
grpc-google-iam-v1==0.14.2
grpcio==1.71.0
grpcio-status==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
idna==3.10
iniconfig==2.1.0
jiter==0.9.0
markdown-it-py==3.0.0
mcp==1.7.1
mdurl==0.1.2
numpy==2.2.5
packaging==25.0
pluggy==1.5.0
proto-plus==1.26.1
protobuf==5.29.4
pyasn1==0.6.1
pyasn1-modules==0.4.2
pydantic==2.11.4
pydantic-core==2.33.2
pydantic-settings==2.9.1
pygments==2.19.1
pyperclip==1.9.0
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
regex==2024.11.6
requests==2.32.3
rich==14.0.0
rsa==4.9.1
sentencepiece==0.2.0
shapely==2.1.0
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
sse-starlette==2.3.3
starlette==0.46.2
tiktoken==0.9.0
typer==0.15.3
typing-extensions==4.13.2
typing-inspection==0.4.0
urllib3==2.4.0
uvicorn==0.34.2
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="uv.lock"&amp;amp;amp;gt;version = 1
revision = 1
requires-python = "&amp;amp;amp;amp;gt;=3.13"

[[package]]
name = "aiohappyeyeballs"
version = "2.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/26/30/f84a107a9c4331c14b2b586036f40965c128aa4fee4dda5d3d51cb14ad54/aiohappyeyeballs-2.6.1.tar.gz", hash = "sha256:c3f9d0113123803ccadfdf3f0faa505bc78e6a72d1cc4806cbd719826e943558", size = 22760 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl", hash = "sha256:f349ba8f4b75cb25c99c5c2d84e997e485204d2902a9597802b0371f09331fb8", size = 15265 },
]

[[package]]
name = "aiohttp"
version = "3.12.11"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/6b/850a842871ab7be0d00686750d0ee9d8fb8e7be981e4e5700bb6c88f1b8f/aiohttp-3.12.11.tar.gz", hash = "sha256:a5149ae1b11ce4cf8b122846bfa3d7c5f29fe3bfe6745ab21b3eea9615bc5564", size = 7814403 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/ac/15e21c6a17b5183d1617505b125c773f554a56e06be577a289151a8e5ce7/aiohttp-3.12.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:5fadc4b67f972a701805aa501cd9d22cdbeda21f9c9ae85e60678f84b1727a16", size = 694170 },
    { url = "https://files.pythonhosted.org/packages/02/5b/347f8aff5793829b3a31a927bd039ec4f22221a32c459b9d19fe880921e3/aiohttp-3.12.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:144d67c29ae36f052584fc45a363e92798441a5af5762d83037aade3e2aa9dc5", size = 471832 },
    { url = "https://files.pythonhosted.org/packages/4b/e5/9ed82f5b6a2dca30940e90820ce2f8203e15111de464bba0980e2c7e169b/aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6b73299e4bf37d14c6e4ca5ce7087b44914a8d9e1f40faedc271f28d64ec277e", size = 464133 },
    { url = "https://files.pythonhosted.org/packages/3c/8d/edcddc41d4f1157a2536143476070ae66de2b839af3724655c2a6358670a/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1226325e98e6d3cdfdaca639efdc3af8e82cd17287ae393626d1bd60626b0e93", size = 1702942 },
    { url = "https://files.pythonhosted.org/packages/b1/2e/efcb6a35d0646ced659edc3172e8e9384246d2cd0b0f3080fc3c441cb511/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:7a0ecae011f2f779271407f2959877230670de3c48f67e5db9fbafa9fddbfa3a", size = 1684207 },
    { url = "https://files.pythonhosted.org/packages/56/f7/0324c499b7c610633d2f5e8af5457fd3a0584f5f4827bc46b673866596ac/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a8a711883eedcd55f2e1ba218d8224b9f20f1dfac90ffca28e78daf891667e3a", size = 1736275 },
    { url = "https://files.pythonhosted.org/packages/98/0f/b7aa0fd1ed777b5d6fb62c0dcf82effb717e8b51c802067fc3bcb703e003/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2601c1fcd9b67e632548cfd3c760741b31490502f6f3e5e21287678c1c6fa1b2", size = 1785648 },
    { url = "https://files.pythonhosted.org/packages/2c/2a/7defcf31010a2964bf17f6c9d9190e3be889f0c5edc3ff2cdac6e60764d7/aiohttp-3.12.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d5b11ea794ee54b33d0d817a1aec0ef0dd2026f070b493bc5a67b7e413b95d4", size = 1707981 },
    { url = "https://files.pythonhosted.org/packages/b6/9e/ff3d9a01f533752e81fd92bfe1301ae5a7bd5a306d752ad54f8bc61570fa/aiohttp-3.12.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:109b3544138ce8a5aca598d5e7ff958699e3e19ee3675d27d5ee9c2e30765a4a", size = 1621683 },
    { url = "https://files.pythonhosted.org/packages/2c/98/446c96927f2e7d2eaea95660a60eb6077771d00df834430cec002cadd96b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b795085d063d24c6d09300c85ddd6b9c49816d5c498b40b6899ca24584e936e4", size = 1674706 },
    { url = "https://files.pythonhosted.org/packages/e1/2a/038cb4af5e58994bc9315d0cb6a906d20ddfffb8eb3d0dfcfe8fe95b1939/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ebcbc113f40e4c9c0f8d2b6b31a2dd2a9768f3fa5f623b7e1285684e24f5159f", size = 1706372 },
    { url = "https://files.pythonhosted.org/packages/28/18/dc16cc7cb9b8baf9308f23ecf1e787d916238d01060bea272d5c29e9aa6b/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:590e5d792150d75fa34029d0555b126e65ad50d66818a996303de4af52b65b32", size = 1648967 },
    { url = "https://files.pythonhosted.org/packages/44/f5/f427ef971e00088c7f0f5a4a7e405732e0ce0b87dfc3eec0f1a8c16863d2/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:9c2a4dec596437b02f0c34f92ea799d6e300184a0304c1e54e462af52abeb0a8", size = 1725099 },
    { url = "https://files.pythonhosted.org/packages/d4/0a/34fc018d4e193115b512bc08f6afaf79c23609a6487e47f0d593d1d9df41/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aace119abc495cc4ced8745e3faceb0c22e8202c60b55217405c5f389b569576", size = 1758571 },
    { url = "https://files.pythonhosted.org/packages/b6/69/b466ec346506384a93bcb864ab75a21b6520c64fcc3720ab2056470a657f/aiohttp-3.12.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd749731390520a2dc1ce215bcf0ee1018c3e2e3cd834f966a02c0e71ad7d637", size = 1707461 },
    { url = "https://files.pythonhosted.org/packages/f4/fc/3437d3e40581bc7d0816e134fdcae3c7e5c3f21dbdcfbd54402af3973b1c/aiohttp-3.12.11-cp313-cp313-win32.whl", hash = "sha256:65952736356d1fbc9efdd17492dce36e2501f609a14ccb298156e392d3ad8b83", size = 420053 },
    { url = "https://files.pythonhosted.org/packages/6c/cf/cd84df67147c986315c63fef29a6ecadf03bf5528340b8c82eedd988cf57/aiohttp-3.12.11-cp313-cp313-win_amd64.whl", hash = "sha256:854132093e12dd77f5c07975581c42ae51a6a8868dcbbb509c77d1963c3713b7", size = 445988 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.50.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/40/85/4dd9f80da0727c56d7e7f7c627cb724edd9e6df062df6ecc0e90f06e6dbb/anthropic-0.50.0.tar.gz", hash = "sha256:42175ec04ce4ff2fa37cd436710206aadff546ee99d70d974699f59b49adc66f", size = 213021 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/ae/975f97ad5581a9e187a3717e21d79d6c7ad6be926fee9aa8a15b3d9f8f37/anthropic-0.50.0-py3-none-any.whl", hash = "sha256:defbd79327ca2fa61fd7b9eb2f1627dfb1f69c25d49288c52e167ddb84574f80", size = 245291 },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916 },
]

[[package]]
name = "asttokens"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4a/e7/82da0a03e7ba5141f05cce0d302e6eed121ae055e0456ca228bf693984bc/asttokens-3.0.0.tar.gz", hash = "sha256:0dcd8baa8d62b0c1d118b399b2ddba3c4aff271d0d7a9e0d4c1681c79035bbc7", size = 61978 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/8a/c46dcc25341b5bce5472c718902eb3d38600a903b14fa6aeecef3f21a46f/asttokens-3.0.0-py3-none-any.whl", hash = "sha256:e3078351a059199dd5138cb1c706e6430c05eff2ff136af5eb4790f9d28932e2", size = 26918 },
]

[[package]]
name = "attrs"
version = "25.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5a/b0/1367933a8532ee6ff8d63537de4f1177af4bff9f3e829baf7331f595bb24/attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b", size = 812032 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3", size = 63815 },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285 },
]

[[package]]
name = "black"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "mypy-extensions" },
    { name = "packaging" },
    { name = "pathspec" },
    { name = "platformdirs" },
]
sdist = { url = "https://files.pythonhosted.org/packages/94/49/26a7b0f3f35da4b5a65f081943b7bcd22d7002f5f0fb8098ec1ff21cb6ef/black-25.1.0.tar.gz", hash = "sha256:33496d5cd1222ad73391352b4ae8da15253c5de89b93a80b3e2c8d9a19ec2666", size = 649449 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/87/0edf98916640efa5d0696e1abb0a8357b52e69e82322628f25bf14d263d1/black-25.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8f0b18a02996a836cc9c9c78e5babec10930862827b1b724ddfe98ccf2f2fe4f", size = 1650673 },
    { url = "https://files.pythonhosted.org/packages/52/e5/f7bf17207cf87fa6e9b676576749c6b6ed0d70f179a3d812c997870291c3/black-25.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:afebb7098bfbc70037a053b91ae8437c3857482d3a690fefc03e9ff7aa9a5fd3", size = 1453190 },
    { url = "https://files.pythonhosted.org/packages/e3/ee/adda3d46d4a9120772fae6de454c8495603c37c4c3b9c60f25b1ab6401fe/black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:030b9759066a4ee5e5aca28c3c77f9c64789cdd4de8ac1df642c40b708be6171", size = 1782926 },
    { url = "https://files.pythonhosted.org/packages/cc/64/94eb5f45dcb997d2082f097a3944cfc7fe87e071907f677e80788a2d7b7a/black-25.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:a22f402b410566e2d1c950708c77ebf5ebd5d0d88a6a2e87c86d9fb48afa0d18", size = 1442613 },
    { url = "https://files.pythonhosted.org/packages/09/71/54e999902aed72baf26bca0d50781b01838251a462612966e9fc4891eadd/black-25.1.0-py3-none-any.whl", hash = "sha256:95e8176dae143ba9097f351d174fdaf0ccd29efb414b362ae3fd72bf0f710717", size = 207646 },
]

[[package]]
name = "cachetools"
version = "5.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/81/3747dad6b14fa2cf53fcf10548cf5aea6913e96fab41a3c198676f8948a5/cachetools-5.5.2.tar.gz", hash = "sha256:1a661caa9175d26759571b2e19580f9d6393969e5dfca11fdb1f947a23e640d4", size = 28380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl", hash = "sha256:d26a22bcc62eb95c3beabd9f1ee5e820d3d2704fe2967cbe350e20c8ffcd3f0a", size = 10080 },
]

[[package]]
name = "certifi"
version = "2025.4.26"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e8/9e/c05b3920a3b7d20d3d3310465f50348e5b3694f4f88c6daf736eef3024c4/certifi-2025.4.26.tar.gz", hash = "sha256:0a816057ea3cdefcef70270d2c515e4506bbc954f417fa5ade2021213bb8f0c6", size = 160705 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4a/7e/3db2bd1b1f9e95f7cddca6d6e75e2f2bd9f51b1246e546d88addca0106bd/certifi-2025.4.26-py3-none-any.whl", hash = "sha256:30350364dfe371162649852c63336a15c70c6510c2ad5015b21c2345311805f3", size = 159618 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622 },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435 },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653 },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231 },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243 },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442 },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147 },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057 },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454 },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174 },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166 },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064 },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641 },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "collect"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "anthropic" },
    { name = "beautifulsoup4" },
    { name = "black" },
    { name = "fastapi" },
    { name = "google-ai-generativelanguage" },
    { name = "google-api-python-client" },
    { name = "google-auth-httplib2" },
    { name = "google-cloud-aiplatform", extra = ["tokenization"] },
    { name = "google-cloud-secret-manager" },
    { name = "google-genai" },
    { name = "google-generativeai" },
    { name = "html-to-markdown" },
    { name = "html5lib" },
    { name = "httplib2" },
    { name = "httpx" },
    { name = "ipython" },
    { name = "lxml" },
    { name = "marimo" },
    { name = "markdownify" },
    { name = "mcp", extra = ["cli"] },
    { name = "openai" },
    { name = "pathspec" },
    { name = "pyperclip" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
    { name = "pytest-xdist" },
    { name = "python-json-logger" },
    { name = "readabilipy" },
    { name = "ruff" },
    { name = "tiktoken" },
    { name = "uvicorn" },
    { name = "yoyo-migrations" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = "&amp;amp;amp;amp;gt;=3.12.11" },
    { name = "anthropic", specifier = "&amp;amp;amp;amp;gt;=0.50.0" },
    { name = "beautifulsoup4", specifier = "&amp;amp;amp;amp;gt;=4.13.4" },
    { name = "black", specifier = "&amp;amp;amp;amp;gt;=25.1.0" },
    { name = "fastapi", specifier = "&amp;amp;amp;amp;gt;=0.116.1" },
    { name = "google-ai-generativelanguage", specifier = "&amp;amp;amp;amp;gt;=0.6.15" },
    { name = "google-api-python-client", specifier = "&amp;amp;amp;amp;gt;=2.169.0" },
    { name = "google-auth-httplib2", specifier = "&amp;amp;amp;amp;gt;=0.2.0" },
    { name = "google-cloud-aiplatform", extras = ["tokenization"], specifier = "&amp;amp;amp;amp;gt;=1.91.0" },
    { name = "google-cloud-secret-manager", specifier = "&amp;amp;amp;amp;gt;=2.23.3" },
    { name = "google-genai", specifier = "&amp;amp;amp;amp;gt;=1.13.0" },
    { name = "google-generativeai", specifier = "&amp;amp;amp;amp;gt;=0.8.5" },
    { name = "html-to-markdown", specifier = "&amp;amp;amp;amp;gt;=1.3.2" },
    { name = "html5lib", specifier = "&amp;amp;amp;amp;gt;=1.1" },
    { name = "httplib2", specifier = "&amp;amp;amp;amp;gt;=0.22.0" },
    { name = "httpx", specifier = "&amp;amp;amp;amp;gt;=0.28.1" },
    { name = "ipython", specifier = "&amp;amp;amp;amp;gt;=9.4.0" },
    { name = "lxml", specifier = "&amp;amp;amp;amp;gt;=5.4.0" },
    { name = "marimo", specifier = "&amp;amp;amp;amp;gt;=0.14.12" },
    { name = "markdownify", specifier = "&amp;amp;amp;amp;gt;=1.1.0" },
    { name = "mcp", extras = ["cli"], specifier = "&amp;amp;amp;amp;gt;=1.7.1" },
    { name = "openai", specifier = "&amp;amp;amp;amp;gt;=1.59.4" },
    { name = "pathspec", specifier = "&amp;amp;amp;amp;gt;=0.12.1" },
    { name = "pyperclip", specifier = "&amp;amp;amp;amp;gt;=1.9.0" },
    { name = "pytest", specifier = "&amp;amp;amp;amp;gt;=8.3.5" },
    { name = "pytest-asyncio", specifier = "&amp;amp;amp;amp;gt;=0.26.0" },
    { name = "pytest-xdist", specifier = "&amp;amp;amp;amp;gt;=3.6.1" },
    { name = "python-json-logger", specifier = "&amp;amp;amp;amp;gt;=3.3.0" },
    { name = "readabilipy", specifier = "&amp;amp;amp;amp;gt;=0.3.0" },
    { name = "ruff", specifier = "&amp;amp;amp;amp;gt;=0.11.9" },
    { name = "tiktoken", specifier = "&amp;amp;amp;amp;gt;=0.9.0" },
    { name = "uvicorn", specifier = "&amp;amp;amp;amp;gt;=0.34.2" },
    { name = "yoyo-migrations", specifier = "&amp;amp;amp;amp;gt;=9.0.0" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "decorator"
version = "5.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/fa/6d96a0978d19e17b68d634497769987b16c8f4cd0a7a05048bec693caa6b/decorator-5.2.1.tar.gz", hash = "sha256:65f266143752f734b0a7cc83c46f4618af75b8c5911b00ccb61d0ac9b6da0360", size = 56711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl", hash = "sha256:d316bb415a2d9e2d2b3abcc4084c6502fc09240e292cd76a76afc106a1c8e04a", size = 9190 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "docstring-parser"
version = "0.16"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/08/12/9c22a58c0b1e29271051222d8906257616da84135af9ed167c9e28f85cb3/docstring_parser-0.16.tar.gz", hash = "sha256:538beabd0af1e2db0146b6bd3caa526c35a34d61af9fd2887f3a8a27a739aa6e", size = 26565 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl", hash = "sha256:bf0a1387354d3691d102edef7ec124f219ef639982d096e26e3b60aeffa90637", size = 36533 },
]

[[package]]
name = "docutils"
version = "0.21.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ae/ed/aefcc8cd0ba62a0560c3c18c33925362d46c6075480bfa4df87b28e169a9/docutils-0.21.2.tar.gz", hash = "sha256:3a6b18732edf182daa3cd12775bbb338cf5691468f91eeeb109deff6ebfa986f", size = 2204444 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/d7/9322c609343d929e75e7e5e6255e614fcc67572cfd083959cdef3b7aad79/docutils-0.21.2-py3-none-any.whl", hash = "sha256:dafca5b9e384f0e419294eb4d2ff9fa826435bf15f15b7bd45723e8ad76811b2", size = 587408 },
]

[[package]]
name = "execnet"
version = "2.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/ff/b4c0dc78fbe20c3e59c0c7334de0c27eb4001a2b2017999af398bf730817/execnet-2.1.1.tar.gz", hash = "sha256:5189b52c6121c24feae288166ab41b32549c7e2348652736540b9e6e7d4e72e3", size = 166524 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/43/09/2aea36ff60d16dd8879bdb2f5b3ee0ba8d08cbbdcdfe870e695ce3784385/execnet-2.1.1-py3-none-any.whl", hash = "sha256:26dee51f1b80cebd6d0ca8e74dd8745419761d3bef34163928cbebbdc4749fdc", size = 40612 },
]

[[package]]
name = "executing"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/50/a9d80c47ff289c611ff12e63f7c5d13942c65d68125160cefd768c73e6e4/executing-2.2.0.tar.gz", hash = "sha256:5d108c028108fe2551d1a7b2e8b713341e2cb4fc0aa7dcf966fa4327a5226755", size = 978693 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7b/8f/c4d9bafc34ad7ad5d8dc16dd1347ee0e507a52c3adb6bfa8887e1c6a26ba/executing-2.2.0-py2.py3-none-any.whl", hash = "sha256:11387150cad388d62750327a53d3339fad4888b39a6fe233c3afbb54ecffd3aa", size = 26702 },
]

[[package]]
name = "fastapi"
version = "0.116.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/78/d7/6c8b3bfe33eeffa208183ec037fee0cce9f7f024089ab1c5d12ef04bd27c/fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143", size = 296485 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565", size = 95631 },
]

[[package]]
name = "frozenlist"
version = "1.6.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/5b/bf/a812e2fe6cb3f6c6cfc8d0303bf1742f2286004e5ec41ac8c89cf68cdb54/frozenlist-1.6.2.tar.gz", hash = "sha256:effc641518696471cf4962e8e32050133bc1f7b2851ae8fd0cb8797dd70dc202", size = 43108 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b8/f6/973abfcb8b68f2e8b58071a04ec72f5e1f0acd19dae0d3b7a8abc3d9ab07/frozenlist-1.6.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2ad8851ae1f6695d735f8646bf1e68675871789756f7f7e8dc8224a74eabb9d0", size = 85517 },
    { url = "https://files.pythonhosted.org/packages/c8/d0/ac45f2dcf0afd5f7d57204af8b7516ecbc3599ea681e06f4b25d3845bea8/frozenlist-1.6.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:cd2d5abc0ccd99a2a5b437987f3b1e9c265c1044d2855a09ac68f09bbb8082ca", size = 49916 },
    { url = "https://files.pythonhosted.org/packages/50/cc/99c3f31823630b7411f7c1e83399e91d6b56a5661a5b724935ef5b51f5f5/frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15c33f665faa9b8f8e525b987eeaae6641816e0f6873e8a9c4d224338cebbb55", size = 48107 },
    { url = "https://files.pythonhosted.org/packages/85/4e/38643ce3ee80d222892b694d02c15ea476c4d564493a6fe530347163744e/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3e6c0681783723bb472b6b8304e61ecfcb4c2b11cf7f243d923813c21ae5d2a", size = 255771 },
    { url = "https://files.pythonhosted.org/packages/ca/e6/ceed85a7d5c0f666485384fc393e32353f8088e154a1109e5ef60165d366/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:61bae4d345a26550d0ed9f2c9910ea060f89dbfc642b7b96e9510a95c3a33b3c", size = 252519 },
    { url = "https://files.pythonhosted.org/packages/29/99/9f2e2b90cf918465e3b6ca4eea79e6be53d24fba33937e37d86c3764bbf9/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:90e5a84016d0d2fb828f770ede085b5d89155fcb9629b8a3237c960c41c120c3", size = 263348 },
    { url = "https://files.pythonhosted.org/packages/4e/ac/59f3ec4c1b4897186efb4757379915734a48bb16bbc15a9fe0bf0857b679/frozenlist-1.6.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55dc289a064c04819d669e6e8a85a1c0416e6c601782093bdc749ae14a2f39da", size = 257858 },
    { url = "https://files.pythonhosted.org/packages/48/4a/19c97510d0c2be1ebaae68383d1b5a256a12a660ca17b0c427b1024d9b92/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b79bcf97ca03c95b044532a4fef6e5ae106a2dd863875b75fde64c553e3f4820", size = 238248 },
    { url = "https://files.pythonhosted.org/packages/ef/64/641aa2b0944fa3d881323948e0d8d6fee746dae03d9023eb510bb80bc46a/frozenlist-1.6.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e5e7564d232a782baa3089b25a0d979e2e4d6572d3c7231fcceacc5c22bf0f7", size = 255932 },
    { url = "https://files.pythonhosted.org/packages/6c/f8/5b68d5658fac7332e5d26542a4af0ffc2edca8da8f854f6274882889ee1e/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6fcd8d56880dccdd376afb18f483ab55a0e24036adc9a83c914d4b7bb5729d4e", size = 253329 },
    { url = "https://files.pythonhosted.org/packages/e9/20/379d7a27eb82748b41319bf376bf2c034e7ee11dda94f12b331edcc261ff/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4fbce985c7fe7bafb4d9bf647c835dbe415b465a897b0c79d1bdf0f3fae5fe50", size = 266164 },
    { url = "https://files.pythonhosted.org/packages/13/bd/d7dbf94220020850392cb661bedfdf786398bafae85d1045dd108971d261/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:3bd12d727cd616387d50fe283abebb2db93300c98f8ff1084b68460acd551926", size = 241641 },
    { url = "https://files.pythonhosted.org/packages/a4/70/916fef6284d294077265cd69ad05f228e44f7ed88d9acb690df5a1174049/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:38544cae535ed697960891131731b33bb865b7d197ad62dc380d2dbb1bceff48", size = 261215 },
    { url = "https://files.pythonhosted.org/packages/8f/98/1326a7189fa519692698cddf598f56766b0fea6ac71cddaf64760a055397/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:47396898f98fae5c9b9bb409c3d2cf6106e409730f35a0926aad09dd7acf1ef5", size = 262597 },
    { url = "https://files.pythonhosted.org/packages/f4/d6/0a95ab9289c72e86c37c9b8afe82576556456b6f66a35d242526634130f2/frozenlist-1.6.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:d10d835f8ce8571fd555db42d3aef325af903535dad7e6faa7b9c8abe191bffc", size = 258766 },
    { url = "https://files.pythonhosted.org/packages/1b/d0/9e946aabd89ebfcb71ec1371327f0e25d4868cd4439471a6fcb6eaf7b366/frozenlist-1.6.2-cp313-cp313-win32.whl", hash = "sha256:a400fe775a41b6d7a3fef00d88f10cbae4f0074c9804e282013d7797671ba58d", size = 40961 },
    { url = "https://files.pythonhosted.org/packages/43/e9/d714f5eb0fde1413344ded982ae9638307b59651d5c04263af42eb81a315/frozenlist-1.6.2-cp313-cp313-win_amd64.whl", hash = "sha256:cc8b25b321863ed46992558a29bb09b766c41e25f31461666d501be0f893bada", size = 46204 },
    { url = "https://files.pythonhosted.org/packages/f5/7a/8f6dde73862499e60eb390778a1e46b87c1fe3c5722622d731ccda7a173c/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:56de277a0e0ad26a1dcdc99802b4f5becd7fd890807b68e3ecff8ced01d58132", size = 91326 },
    { url = "https://files.pythonhosted.org/packages/79/60/dcdc75edbcf8241e7cb15fced68b3be63f67ff3faaf559c540a7eb63233b/frozenlist-1.6.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:9cb386dd69ae91be586aa15cb6f39a19b5f79ffc1511371eca8ff162721c4867", size = 52426 },
    { url = "https://files.pythonhosted.org/packages/64/e6/df2a43ccb2c4f1ea3692aae9a89cfc5dd932a90b7898f98f13ed9e2680a9/frozenlist-1.6.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:53835d8a6929c2f16e02616f8b727bd140ce8bf0aeddeafdb290a67c136ca8ad", size = 51460 },
    { url = "https://files.pythonhosted.org/packages/fd/b3/c4f2f7fca9487b25c39bf64535f029316e184072a82f3660ce72defc5421/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cc49f2277e8173abf028d744f8b7d69fe8cc26bffc2de97d47a3b529599fbf50", size = 310270 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/046eb34d8d0fee1a8c9dc91a9ba581283c67a1ace20bcc01c86a53595105/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:65eb9e8a973161bdac5fa06ea6bd261057947adc4f47a7a6ef3d6db30c78c5b4", size = 289062 },
    { url = "https://files.pythonhosted.org/packages/48/7b/80991efaa0aa25e867cf93033c28e9d1310f34f90421eb59eb1f2073d937/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:301eb2f898d863031f8c5a56c88a6c5d976ba11a4a08a1438b96ee3acb5aea80", size = 312202 },
    { url = "https://files.pythonhosted.org/packages/78/6b/6fe30bdababdf82c5b34f0093770c4be6211071e23570721b80b11c9d52a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:207f717fd5e65fddb77d33361ab8fa939f6d89195f11307e073066886b33f2b8", size = 309557 },
    { url = "https://files.pythonhosted.org/packages/9d/ef/b7bf48802fc7d084703ba2173e6a8d0590bea378dcd6a480051c41bddf47/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f83992722642ee0db0333b1dbf205b1a38f97d51a7382eb304ba414d8c3d1e05", size = 282135 },
    { url = "https://files.pythonhosted.org/packages/af/f8/6911a085bce8d0d0df3dfc2560e3e0fb4d6c19ff101014bcf61aa32ba39a/frozenlist-1.6.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12af99e6023851b36578e5bcc60618b5b30f4650340e29e565cd1936326dbea7", size = 303392 },
    { url = "https://files.pythonhosted.org/packages/9c/5d/b4e0cc6dbd6b9282926a470a919da7c6599ff324ab5268c7ecaff82cb858/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:6f01620444a674eaad900a3263574418e99c49e2a5d6e5330753857363b5d59f", size = 309402 },
    { url = "https://files.pythonhosted.org/packages/0f/1b/bf777de3c810e68e8758337fcc97ee8c956376c87aecee9a61ba19a94123/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:82b94c8948341512306ca8ccc702771600b442c6abe5f8ee017e00e452a209e8", size = 312924 },
    { url = "https://files.pythonhosted.org/packages/0e/03/a69b890bc310790fcae61fd3b5be64876811b12db5d50b32e62f65e766bd/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:324a4cf4c220ddb3db1f46ade01e48432c63fa8c26812c710006e7f6cfba4a08", size = 291768 },
    { url = "https://files.pythonhosted.org/packages/70/cc/559386adf987b47c8977c929271d11a72efd92778a0a2f4cc97827a9a25b/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:695284e51458dabb89af7f7dc95c470aa51fd259207aba5378b187909297feef", size = 313305 },
    { url = "https://files.pythonhosted.org/packages/e7/fa/eb0e21730ffccfb2d0d367d863cbaacf8367bdc277b44eabf72f7329ab91/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:9ccbeb1c8dda4f42d0678076aa5cbde941a232be71c67b9d8ca89fbaf395807c", size = 312228 },
    { url = "https://files.pythonhosted.org/packages/d1/c1/8471b67172abc9478ad78c70a3f3a5c4fed6d4bcadc748e1b6dfa06ab2ae/frozenlist-1.6.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cbbdf62fcc1864912c592a1ec748fee94f294c6b23215d5e8e9569becb7723ee", size = 309905 },
    { url = "https://files.pythonhosted.org/packages/bb/2c/ee21987c3a175b49d0b827b1e45394a7a5d08c7de5b766ed6d0889d30568/frozenlist-1.6.2-cp313-cp313t-win32.whl", hash = "sha256:76857098ee17258df1a61f934f2bae052b8542c9ea6b187684a737b2e3383a65", size = 44644 },
    { url = "https://files.pythonhosted.org/packages/65/46/fce60f65b1fb17a90c4bf410a5c90cb3b40616cc229e75866f8be97c112c/frozenlist-1.6.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c06a88daba7e891add42f9278cdf7506a49bc04df9b1648be54da1bf1c79b4c6", size = 50607 },
    { url = "https://files.pythonhosted.org/packages/13/be/0ebbb283f2d91b72beaee2d07760b2c47dab875c49c286f5591d3d157198/frozenlist-1.6.2-py3-none-any.whl", hash = "sha256:947abfcc8c42a329bbda6df97a4b9c9cdb4e12c85153b3b57b9d2f02aa5877dc", size = 12582 },
]

[[package]]
name = "google-ai-generativelanguage"
version = "0.6.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/11/d1/48fe5d7a43d278e9f6b5ada810b0a3530bbeac7ed7fcbcd366f932f05316/google_ai_generativelanguage-0.6.15.tar.gz", hash = "sha256:8f6d9dc4c12b065fe2d0289026171acea5183ebf2d0b11cefe12f3821e159ec3", size = 1375443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/a3/67b8a6ff5001a1d8864922f2d6488dc2a14367ceb651bc3f09a947f2f306/google_ai_generativelanguage-0.6.15-py3-none-any.whl", hash = "sha256:5a03ef86377aa184ffef3662ca28f19eeee158733e45d7947982eb953c6ebb6c", size = 1327356 },
]

[[package]]
name = "google-api-core"
version = "2.24.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "googleapis-common-protos" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/09/5c/085bcb872556934bb119e5e09de54daa07873f6866b8f0303c49e72287f7/google_api_core-2.24.2.tar.gz", hash = "sha256:81718493daf06d96d6bc76a91c23874dbf2fac0adbbf542831b805ee6e974696", size = 163516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/95/f472d85adab6e538da2025dfca9e976a0d125cc0af2301f190e77b76e51c/google_api_core-2.24.2-py3-none-any.whl", hash = "sha256:810a63ac95f3c441b7c0e43d344e372887f62ce9071ba972eacf32672e072de9", size = 160061 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
    { name = "grpcio-status" },
]

[[package]]
name = "google-api-python-client"
version = "2.171.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-auth-httplib2" },
    { name = "httplib2" },
    { name = "uritemplate" },
]
sdist = { url = "https://files.pythonhosted.org/packages/35/99/237cd2510aecca9fabb54007e58553274cc43cb3c18512ee1ea574d11b87/google_api_python_client-2.171.0.tar.gz", hash = "sha256:057a5c08d28463c6b9eb89746355de5f14b7ed27a65c11fdbf1d06c66bb66b23", size = 13028937 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/db/c397e3eb3ea18f423855479d0a5852bdc9c3f644e3d4194931fa664a70b4/google_api_python_client-2.171.0-py3-none-any.whl", hash = "sha256:c9c9b76f561e9d9ac14e54a9e2c0842876201d5b96e69e48f967373f0784cbe9", size = 13547393 },
]

[[package]]
name = "google-auth"
version = "2.39.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cachetools" },
    { name = "pyasn1-modules" },
    { name = "rsa" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/8e/8f45c9a32f73e786e954b8f9761c61422955d23c45d1e8c347f9b4b59e8e/google_auth-2.39.0.tar.gz", hash = "sha256:73222d43cdc35a3aeacbfdcaf73142a97839f10de930550d89ebfe1d0a00cde7", size = 274834 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/12/ad37a1ef86006d0a0117fc06a4a00bd461c775356b534b425f00dde208ea/google_auth-2.39.0-py2.py3-none-any.whl", hash = "sha256:0150b6711e97fb9f52fe599f55648950cc4540015565d8fbb31be2ad6e1548a2", size = 212319 },
]

[[package]]
name = "google-auth-httplib2"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-auth" },
    { name = "httplib2" },
]
sdist = { url = "https://files.pythonhosted.org/packages/56/be/217a598a818567b28e859ff087f347475c807a5649296fb5a817c58dacef/google-auth-httplib2-0.2.0.tar.gz", hash = "sha256:38aa7badf48f974f1eb9861794e9c0cb2a0511a4ec0679b1f886d108f5640e05", size = 10842 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl", hash = "sha256:b65a0a2123300dd71281a7bf6e64d65a0759287df52729bdd1ae2e47dc311a3d", size = 9253 },
]

[[package]]
name = "google-cloud-aiplatform"
version = "1.91.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "docstring-parser" },
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-bigquery" },
    { name = "google-cloud-resource-manager" },
    { name = "google-cloud-storage" },
    { name = "packaging" },
    { name = "proto-plus" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "shapely" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/50/08/5854569782efbbc8efd0aeda3a4486153605104cbab6ac836b2328bae48e/google_cloud_aiplatform-1.91.0.tar.gz", hash = "sha256:b14e5e52b52b6012c7dc253beab34c511fdc53c69b13f436ddb06882c1a92cd7", size = 9102586 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/88/cea8583fadd142e8ef26f8ec14a6ee4d7c69c4e5ab82bea01a077fddddbe/google_cloud_aiplatform-1.91.0-py2.py3-none-any.whl", hash = "sha256:ff8df100c2af692d114a2219d3abbb96110b3e5655f342fdbb6aefad43901b52", size = 7591910 },
]

[package.optional-dependencies]
tokenization = [
    { name = "sentencepiece" },
]

[[package]]
name = "google-cloud-bigquery"
version = "3.31.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-resumable-media" },
    { name = "packaging" },
    { name = "python-dateutil" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/73/91/4c7274f4d5faf13ac000b06353deaf3579575bf0e4bbad07fa68b9f09ba9/google_cloud_bigquery-3.31.0.tar.gz", hash = "sha256:b89dc716dbe4abdb7a4f873f7050100287bc98514e0614c5d54cd6a8e9fb0991", size = 479961 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e8/bc/4cb8c61fc6dd817a4a390b745ec7b305f4578f547a16d09d54c8a790624b/google_cloud_bigquery-3.31.0-py3-none-any.whl", hash = "sha256:97f4a3219854ff01d6a3a57312feecb0b6e13062226b823f867e2d3619c4787b", size = 250099 },
]

[[package]]
name = "google-cloud-core"
version = "2.4.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/b8/2b53838d2acd6ec6168fd284a990c76695e84c65deee79c9f3a4276f6b4f/google_cloud_core-2.4.3.tar.gz", hash = "sha256:1fab62d7102844b278fe6dead3af32408b1df3eb06f5c7e8634cbd40edc4da53", size = 35861 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/86/bda7241a8da2d28a754aad2ba0f6776e35b67e37c36ae0c45d49370f1014/google_cloud_core-2.4.3-py2.py3-none-any.whl", hash = "sha256:5130f9f4c14b4fafdff75c79448f9495cfade0d8775facf1b09c3bf67e027f6e", size = 29348 },
]

[[package]]
name = "google-cloud-resource-manager"
version = "1.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6e/ca/a4648f5038cb94af4b3942815942a03aa9398f9fb0bef55b3f1585b9940d/google_cloud_resource_manager-1.14.2.tar.gz", hash = "sha256:962e2d904c550d7bac48372607904ff7bb3277e3bb4a36d80cc9a37e28e6eb74", size = 446370 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/ea/a92631c358da377af34d3a9682c97af83185c2d66363d5939ab4a1169a7f/google_cloud_resource_manager-1.14.2-py3-none-any.whl", hash = "sha256:d0fa954dedd1d2b8e13feae9099c01b8aac515b648e612834f9942d2795a9900", size = 394344 },
]

[[package]]
name = "google-cloud-secret-manager"
version = "2.24.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core", extra = ["grpc"] },
    { name = "google-auth" },
    { name = "grpc-google-iam-v1" },
    { name = "proto-plus" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/7a/2fa6735ec693d822fe08a76709c4d95d9b5b4c02e83e720497355039d2ee/google_cloud_secret_manager-2.24.0.tar.gz", hash = "sha256:ce573d40ffc2fb7d01719243a94ee17aa243ea642a6ae6c337501e58fbf642b5", size = 269516 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/be/af/db1217cae1809e69a4527ee6293b82a9af2a1fb2313ad110c775e8f3c820/google_cloud_secret_manager-2.24.0-py3-none-any.whl", hash = "sha256:9bea1254827ecc14874bc86c63b899489f8f50bfe1442bfb2517530b30b3a89b", size = 218050 },
]

[[package]]
name = "google-cloud-storage"
version = "2.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-api-core" },
    { name = "google-auth" },
    { name = "google-cloud-core" },
    { name = "google-crc32c" },
    { name = "google-resumable-media" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/36/76/4d965702e96bb67976e755bed9828fa50306dca003dbee08b67f41dd265e/google_cloud_storage-2.19.0.tar.gz", hash = "sha256:cd05e9e7191ba6cb68934d8eb76054d9be4562aa89dbc4236feee4d7d51342b2", size = 5535488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d5/94/6db383d8ee1adf45dc6c73477152b82731fa4c4a46d9c1932cc8757e0fd4/google_cloud_storage-2.19.0-py2.py3-none-any.whl", hash = "sha256:aeb971b5c29cf8ab98445082cbfe7b161a1f48ed275822f59ed3f1524ea54fba", size = 131787 },
]

[[package]]
name = "google-crc32c"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/19/ae/87802e6d9f9d69adfaedfcfd599266bf386a54d0be058b532d04c794f76d/google_crc32c-1.7.1.tar.gz", hash = "sha256:2bff2305f98846f3e825dbeec9ee406f89da7962accdb29356e4eadc251bd472", size = 14495 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/72/b8d785e9184ba6297a8620c8a37cf6e39b81a8ca01bb0796d7cbb28b3386/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_arm64.whl", hash = "sha256:df8b38bdaf1629d62d51be8bdd04888f37c451564c2042d36e5812da9eff3c35", size = 30467 },
    { url = "https://files.pythonhosted.org/packages/34/25/5f18076968212067c4e8ea95bf3b69669f9fc698476e5f5eb97d5b37999f/google_crc32c-1.7.1-cp313-cp313-macosx_12_0_x86_64.whl", hash = "sha256:e42e20a83a29aa2709a0cf271c7f8aefaa23b7ab52e53b322585297bb94d4638", size = 30309 },
    { url = "https://files.pythonhosted.org/packages/92/83/9228fe65bf70e93e419f38bdf6c5ca5083fc6d32886ee79b450ceefd1dbd/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:905a385140bf492ac300026717af339790921f411c0dfd9aa5a9e69a08ed32eb", size = 33133 },
    { url = "https://files.pythonhosted.org/packages/c3/ca/1ea2fd13ff9f8955b85e7956872fdb7050c4ace8a2306a6d177edb9cf7fe/google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b211ddaf20f7ebeec5c333448582c224a7c90a9d98826fbab82c0ddc11348e6", size = 32773 },
    { url = "https://files.pythonhosted.org/packages/89/32/a22a281806e3ef21b72db16f948cad22ec68e4bdd384139291e00ff82fe2/google_crc32c-1.7.1-cp313-cp313-win_amd64.whl", hash = "sha256:0f99eaa09a9a7e642a61e06742856eec8b19fc0037832e03f941fe7cf0c8e4db", size = 33475 },
    { url = "https://files.pythonhosted.org/packages/b8/c5/002975aff514e57fc084ba155697a049b3f9b52225ec3bc0f542871dd524/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32d1da0d74ec5634a05f53ef7df18fc646666a25efaaca9fc7dcfd4caf1d98c3", size = 33243 },
    { url = "https://files.pythonhosted.org/packages/61/cb/c585282a03a0cea70fcaa1bf55d5d702d0f2351094d663ec3be1c6c67c52/google_crc32c-1.7.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e10554d4abc5238823112c2ad7e4560f96c7bf3820b202660373d769d9e6e4c9", size = 32870 },
]

[[package]]
name = "google-genai"
version = "1.19.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "google-auth" },
    { name = "httpx" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "typing-extensions" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/14/17/8f717f43732ae2b7775f816f0d8f0b39e2a020bbe7ba202f2ddb2f948c3b/google_genai-1.19.0.tar.gz", hash = "sha256:66f5de78075781bfd9e423f1e3592e4240759dfe0ac42ac74a9dcb2c4f662e9d", size = 198000 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c4/ae/64fccdebf5811453ce53b0d5ee23d4f27ef173ef36d3b67dad791a0007aa/google_genai-1.19.0-py3-none-any.whl", hash = "sha256:a2955612e4af8c84f83eb43c1ce4e74e1b714732926d0705e639761938192466", size = 200043 },
]

[[package]]
name = "google-generativeai"
version = "0.8.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-ai-generativelanguage" },
    { name = "google-api-core" },
    { name = "google-api-python-client" },
    { name = "google-auth" },
    { name = "protobuf" },
    { name = "pydantic" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/40/c42ff9ded9f09ec9392879a8e6538a00b2dc185e834a3392917626255419/google_generativeai-0.8.5-py3-none-any.whl", hash = "sha256:22b420817fb263f8ed520b33285f45976d5b21e904da32b80d4fd20c055123a2", size = 155427 },
]

[[package]]
name = "google-resumable-media"
version = "2.7.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "google-crc32c" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/5a/0efdc02665dca14e0837b62c8a1a93132c264bd02054a15abb2218afe0ae/google_resumable_media-2.7.2.tar.gz", hash = "sha256:5280aed4629f2b60b847b0d42f9857fd4935c11af266744df33d8074cae92fe0", size = 2163099 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/82/35/b8d3baf8c46695858cb9d8835a53baa1eeb9906ddaf2f728a5f5b640fd1e/google_resumable_media-2.7.2-py2.py3-none-any.whl", hash = "sha256:3ce7551e9fe6d99e9a126101d2536612bb73486721951e9562fee0f90c6ababa", size = 81251 },
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/39/24/33db22342cf4a2ea27c9955e6713140fedd51e8b141b5ce5260897020f1a/googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257", size = 145903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/f1/62a193f0227cf15a920390abe675f386dec35f7ae3ffe6da582d3ade42c7/googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8", size = 294530 },
]

[package.optional-dependencies]
grpc = [
    { name = "grpcio" },
]

[[package]]
name = "grpc-google-iam-v1"
version = "0.14.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos", extra = ["grpc"] },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/4e/8d0ca3b035e41fe0b3f31ebbb638356af720335e5a11154c330169b40777/grpc_google_iam_v1-0.14.2.tar.gz", hash = "sha256:b3e1fc387a1a329e41672197d0ace9de22c78dd7d215048c4c78712073f7bd20", size = 16259 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/66/6f/dd9b178aee7835b96c2e63715aba6516a9d50f6bebbd1cc1d32c82a2a6c3/grpc_google_iam_v1-0.14.2-py3-none-any.whl", hash = "sha256:a3171468459770907926d56a440b2bb643eec1d7ba215f48f3ecece42b4d8351", size = 19242 },
]

[[package]]
name = "grpcio"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/95/aa11fc09a85d91fbc7dd405dcb2a1e0256989d67bf89fa65ae24b3ba105a/grpcio-1.71.0.tar.gz", hash = "sha256:2b85f7820475ad3edec209d3d89a7909ada16caab05d3f2e08a7e8ae3200a55c", size = 12549828 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/dd/b00cbb45400d06b26126dcfdbdb34bb6c4f28c3ebbd7aea8228679103ef6/grpcio-1.71.0-cp313-cp313-linux_armv7l.whl", hash = "sha256:cebc1b34ba40a312ab480ccdb396ff3c529377a2fce72c45a741f7215bfe8379", size = 5184138 },
    { url = "https://files.pythonhosted.org/packages/ed/0a/4651215983d590ef53aac40ba0e29dda941a02b097892c44fa3357e706e5/grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl", hash = "sha256:85da336e3649a3d2171e82f696b5cad2c6231fdd5bad52616476235681bee5b3", size = 11310747 },
    { url = "https://files.pythonhosted.org/packages/57/a3/149615b247f321e13f60aa512d3509d4215173bdb982c9098d78484de216/grpcio-1.71.0-cp313-cp313-manylinux_2_17_aarch64.whl", hash = "sha256:f9a412f55bb6e8f3bb000e020dbc1e709627dcb3a56f6431fa7076b4c1aab0db", size = 5653991 },
    { url = "https://files.pythonhosted.org/packages/ca/56/29432a3e8d951b5e4e520a40cd93bebaa824a14033ea8e65b0ece1da6167/grpcio-1.71.0-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:47be9584729534660416f6d2a3108aaeac1122f6b5bdbf9fd823e11fe6fbaa29", size = 6312781 },
    { url = "https://files.pythonhosted.org/packages/a3/f8/286e81a62964ceb6ac10b10925261d4871a762d2a763fbf354115f9afc98/grpcio-1.71.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c9c80ac6091c916db81131d50926a93ab162a7e97e4428ffc186b6e80d6dda4", size = 5910479 },
    { url = "https://files.pythonhosted.org/packages/35/67/d1febb49ec0f599b9e6d4d0d44c2d4afdbed9c3e80deb7587ec788fcf252/grpcio-1.71.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:789d5e2a3a15419374b7b45cd680b1e83bbc1e52b9086e49308e2c0b5bbae6e3", size = 6013262 },
    { url = "https://files.pythonhosted.org/packages/a1/04/f9ceda11755f0104a075ad7163fc0d96e2e3a9fe25ef38adfc74c5790daf/grpcio-1.71.0-cp313-cp313-musllinux_1_1_i686.whl", hash = "sha256:1be857615e26a86d7363e8a163fade914595c81fec962b3d514a4b1e8760467b", size = 6643356 },
    { url = "https://files.pythonhosted.org/packages/fb/ce/236dbc3dc77cf9a9242adcf1f62538734ad64727fabf39e1346ad4bd5c75/grpcio-1.71.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:a76d39b5fafd79ed604c4be0a869ec3581a172a707e2a8d7a4858cb05a5a7637", size = 6186564 },
    { url = "https://files.pythonhosted.org/packages/10/fd/b3348fce9dd4280e221f513dd54024e765b21c348bc475516672da4218e9/grpcio-1.71.0-cp313-cp313-win32.whl", hash = "sha256:74258dce215cb1995083daa17b379a1a5a87d275387b7ffe137f1d5131e2cfbb", size = 3601890 },
    { url = "https://files.pythonhosted.org/packages/be/f8/db5d5f3fc7e296166286c2a397836b8b042f7ad1e11028d82b061701f0f7/grpcio-1.71.0-cp313-cp313-win_amd64.whl", hash = "sha256:22c3bc8d488c039a199f7a003a38cb7635db6656fa96437a8accde8322ce2366", size = 4273308 },
]

[[package]]
name = "grpcio-status"
version = "1.71.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "googleapis-common-protos" },
    { name = "grpcio" },
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d7/53/a911467bece076020456401f55a27415d2d70d3bc2c37af06b44ea41fc5c/grpcio_status-1.71.0.tar.gz", hash = "sha256:11405fed67b68f406b3f3c7c5ae5104a79d2d309666d10d61b152e91d28fb968", size = 13669 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ad/d6/31fbc43ff097d8c4c9fc3df741431b8018f67bf8dfbe6553a555f6e5f675/grpcio_status-1.71.0-py3-none-any.whl", hash = "sha256:843934ef8c09e3e858952887467f8256aac3910c55f077a359a65b2b3cde3e68", size = 14424 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "html-to-markdown"
version = "1.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1d/48/324d3d938e5ff635497965118df510f62725b72e8b378b8710c03b0dd014/html_to_markdown-1.3.3.tar.gz", hash = "sha256:ad4f992d65d96d53e49d0a56a2ae0c52ef606c17592d2d9a87f99e4632a4a9e3", size = 15491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a3/d0/b96f7e3579cada841657e5764bc294bd2abb6c1e1dbcfb88ecf7a63ea5d9/html_to_markdown-1.3.3-py3-none-any.whl", hash = "sha256:09325777400e561d2c5a1569f475f9434e70a6f8ed1b4866bba8d00906136495", size = 14951 },
]

[[package]]
name = "html5lib"
version = "1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
    { name = "webencodings" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ac/b6/b55c3f49042f1df3dcd422b7f224f939892ee94f22abcf503a9b7339eaf2/html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f", size = 272215 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d", size = 112173 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httplib2"
version = "0.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyparsing" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/ad/2371116b22d616c194aa25ec410c9c6c37f23599dcd590502b74db197584/httplib2-0.22.0.tar.gz", hash = "sha256:d7a10bc5ef5ab08322488bde8c726eeee5c8618723fdb399597ec58f3d82df81", size = 351116 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl", hash = "sha256:14ae0a53c1ba8f3d37e9e27cf37eabb0fb9980f435ba405d546948b009dd64dc", size = 96854 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "zipp" },
]
sdist = { url = "https://files.pythonhosted.org/packages/76/66/650a33bd90f786193e4de4b3ad86ea60b53c89b669a5c7be931fac31cdb0/importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000", size = 56641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd", size = 27656 },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050 },
]

[[package]]
name = "ipython"
version = "9.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "decorator" },
    { name = "ipython-pygments-lexers" },
    { name = "jedi" },
    { name = "matplotlib-inline" },
    { name = "pexpect", marker = "sys_platform != 'emscripten' and sys_platform != 'win32'" },
    { name = "prompt-toolkit" },
    { name = "pygments" },
    { name = "stack-data" },
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/54/80/406f9e3bde1c1fd9bf5a0be9d090f8ae623e401b7670d8f6fdf2ab679891/ipython-9.4.0.tar.gz", hash = "sha256:c033c6d4e7914c3d9768aabe76bbe87ba1dc66a92a05db6bfa1125d81f2ee270", size = 4385338 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/63/f8/0031ee2b906a15a33d6bfc12dd09c3dfa966b3cb5b284ecfb7549e6ac3c4/ipython-9.4.0-py3-none-any.whl", hash = "sha256:25850f025a446d9b359e8d296ba175a36aedd32e83ca9b5060430fe16801f066", size = 611021 },
]

[[package]]
name = "ipython-pygments-lexers"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/4c/5dd1d8af08107f88c7f741ead7a40854b8ac24ddf9ae850afbcf698aa552/ipython_pygments_lexers-1.1.1.tar.gz", hash = "sha256:09c0138009e56b6854f9535736f4171d855c8c08a563a0dcd8022f78355c7e81", size = 8393 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/33/1f075bf72b0b747cb3288d011319aaf64083cf2efef8354174e3ed4540e2/ipython_pygments_lexers-1.1.1-py3-none-any.whl", hash = "sha256:a9462224a505ade19a605f71f8fa63c2048833ce50abc86768a0d81d876dc81c", size = 8074 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jedi"
version = "0.19.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "parso" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/3a/79a912fbd4d8dd6fbb02bf69afd3bb72cf0c729bb3063c6f4498603db17a/jedi-0.19.2.tar.gz", hash = "sha256:4770dc3de41bde3966b02eb84fbcf557fb33cce26ad23da12c742fb50ecb11f0", size = 1231287 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c0/5a/9cac0c82afec3d09ccd97c8b6502d48f165f9124db81b4bcb90b4af974ee/jedi-0.19.2-py2.py3-none-any.whl", hash = "sha256:a8ef22bde8490f57fe5c7681a3c83cb58874daf72b4784de3cce5b6ef6edb5b9", size = 1572278 },
]

[[package]]
name = "jiter"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1e/c2/e4562507f52f0af7036da125bb699602ead37a2332af0788f8e0a3417f36/jiter-0.9.0.tar.gz", hash = "sha256:aadba0964deb424daa24492abc3d229c60c4a31bfee205aedbf1acc7639d7893", size = 162604 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/1b/4cd165c362e8f2f520fdb43245e2b414f42a255921248b4f8b9c8d871ff1/jiter-0.9.0-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:2764891d3f3e8b18dce2cff24949153ee30c9239da7c00f032511091ba688ff7", size = 308197 },
    { url = "https://files.pythonhosted.org/packages/13/aa/7a890dfe29c84c9a82064a9fe36079c7c0309c91b70c380dc138f9bea44a/jiter-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:387b22fbfd7a62418d5212b4638026d01723761c75c1c8232a8b8c37c2f1003b", size = 318160 },
    { url = "https://files.pythonhosted.org/packages/6a/38/5888b43fc01102f733f085673c4f0be5a298f69808ec63de55051754e390/jiter-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d8da8629ccae3606c61d9184970423655fb4e33d03330bcdfe52d234d32f69", size = 341259 },
    { url = "https://files.pythonhosted.org/packages/3d/5e/bbdbb63305bcc01006de683b6228cd061458b9b7bb9b8d9bc348a58e5dc2/jiter-0.9.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a1be73d8982bdc278b7b9377426a4b44ceb5c7952073dd7488e4ae96b88e1103", size = 363730 },
    { url = "https://files.pythonhosted.org/packages/75/85/53a3edc616992fe4af6814c25f91ee3b1e22f7678e979b6ea82d3bc0667e/jiter-0.9.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2228eaaaa111ec54b9e89f7481bffb3972e9059301a878d085b2b449fbbde635", size = 405126 },
    { url = "https://files.pythonhosted.org/packages/ae/b3/1ee26b12b2693bd3f0b71d3188e4e5d817b12e3c630a09e099e0a89e28fa/jiter-0.9.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:11509bfecbc319459647d4ac3fd391d26fdf530dad00c13c4dadabf5b81f01a4", size = 393668 },
    { url = "https://files.pythonhosted.org/packages/11/87/e084ce261950c1861773ab534d49127d1517b629478304d328493f980791/jiter-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3f22238da568be8bbd8e0650e12feeb2cfea15eda4f9fc271d3b362a4fa0604d", size = 352350 },
    { url = "https://files.pythonhosted.org/packages/f0/06/7dca84b04987e9df563610aa0bc154ea176e50358af532ab40ffb87434df/jiter-0.9.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:17f5d55eb856597607562257c8e36c42bc87f16bef52ef7129b7da11afc779f3", size = 384204 },
    { url = "https://files.pythonhosted.org/packages/16/2f/82e1c6020db72f397dd070eec0c85ebc4df7c88967bc86d3ce9864148f28/jiter-0.9.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:6a99bed9fbb02f5bed416d137944419a69aa4c423e44189bc49718859ea83bc5", size = 520322 },
    { url = "https://files.pythonhosted.org/packages/36/fd/4f0cd3abe83ce208991ca61e7e5df915aa35b67f1c0633eb7cf2f2e88ec7/jiter-0.9.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e057adb0cd1bd39606100be0eafe742de2de88c79df632955b9ab53a086b3c8d", size = 512184 },
    { url = "https://files.pythonhosted.org/packages/a0/3c/8a56f6d547731a0b4410a2d9d16bf39c861046f91f57c98f7cab3d2aa9ce/jiter-0.9.0-cp313-cp313-win32.whl", hash = "sha256:f7e6850991f3940f62d387ccfa54d1a92bd4bb9f89690b53aea36b4364bcab53", size = 206504 },
    { url = "https://files.pythonhosted.org/packages/f4/1c/0c996fd90639acda75ed7fa698ee5fd7d80243057185dc2f63d4c1c9f6b9/jiter-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:c8ae3bf27cd1ac5e6e8b7a27487bf3ab5f82318211ec2e1346a5b058756361f7", size = 204943 },
    { url = "https://files.pythonhosted.org/packages/78/0f/77a63ca7aa5fed9a1b9135af57e190d905bcd3702b36aca46a01090d39ad/jiter-0.9.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f0b2827fb88dda2cbecbbc3e596ef08d69bda06c6f57930aec8e79505dc17001", size = 317281 },
    { url = "https://files.pythonhosted.org/packages/f9/39/a3a1571712c2bf6ec4c657f0d66da114a63a2e32b7e4eb8e0b83295ee034/jiter-0.9.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:062b756ceb1d40b0b28f326cba26cfd575a4918415b036464a52f08632731e5a", size = 350273 },
    { url = "https://files.pythonhosted.org/packages/ee/47/3729f00f35a696e68da15d64eb9283c330e776f3b5789bac7f2c0c4df209/jiter-0.9.0-cp313-cp313t-win_amd64.whl", hash = "sha256:6f7838bc467ab7e8ef9f387bd6de195c43bad82a569c1699cb822f6609dd4cdf", size = 206867 },
]

[[package]]
name = "loro"
version = "1.5.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a0/32/ce94b1fc342ac90d9ca21bc6e90c727990734a75505cb893b2a71a364faf/loro-1.5.2.tar.gz", hash = "sha256:70e52acb16474f7c1e52aea2a7fe2771516f1e9f73d4edfe40f3193b122402c7", size = 62538 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/09/061e8cecb42f99856580811156d7651d5e8172bb840224c7cd2eb94a8730/loro-1.5.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:dbb94c104e3aba4ea3f1118c72896de978e737bb066a35051bf49895e72540a7", size = 3098320 },
    { url = "https://files.pythonhosted.org/packages/60/6e/96cb1a78869c8ae91e65d73ef4ee9f74bc16fd3baff5a7463f7702687dab/loro-1.5.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:847a10f493399f9b650b588b3d81893dfaa1e45e7091881268094f2b9f7df38b", size = 2882026 },
    { url = "https://files.pythonhosted.org/packages/eb/e7/2a131e3e8072614af1cc2970efc1c30a812eb8b0f5286c7b6b390ae3fc9f/loro-1.5.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:902215b77b35e58286d907e8292f78b014cd9c55a46bc5deb944f555509b7747", size = 3110094 },
    { url = "https://files.pythonhosted.org/packages/8c/63/34efc556a5a7663f045d64b9744c10f7b00386f252fac47c939f1c1795be/loro-1.5.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:19e8c9896348063721ef56631d2275c186faf63f6336079c57f41055c9cc1c30", size = 3202938 },
    { url = "https://files.pythonhosted.org/packages/67/3f/5a37b5f1bec5d633f469754e26bf0ce77a26f7697cd95d0b4a51b9cd90be/loro-1.5.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91e75cd4b26506bb5b564ed24b433147fc8b77e8779b5736bc4f3bfddf270590", size = 3579945 },
    { url = "https://files.pythonhosted.org/packages/78/b3/cd3202d6398524c5e1442688c6825e148eb953aa0de04952fd546c69a398/loro-1.5.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:41e54109599190dede34366476a8f42ae6e9fd7fd439823150e9f70e39d7d54e", size = 3318843 },
    { url = "https://files.pythonhosted.org/packages/a5/65/8ed127c827ed9b540f5660e9c98265702dbfdd71ad59063bd3c799ca0dda/loro-1.5.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd3f330795212f24b9dd710f952f7f7138ba86d6159f524025eb4627641ed4ef", size = 3243417 },
    { url = "https://files.pythonhosted.org/packages/4e/29/6894f6db7a1eb7d5d2936b658b3a26c4ea8ce6b0563dde024b909a63289d/loro-1.5.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:5ebdd716ce67c182f71a093c552f9a47428f7a3d93b038780bbb0f06779805d0", size = 3511123 },
    { url = "https://files.pythonhosted.org/packages/17/26/230867103d5ec58ef18f8d0bc169a4defb4f865f9969247d4e9c723ae10e/loro-1.5.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a8ac5ff8b697e9a828fe4387da715d78d0f2afcf23bbd76f5089b4122f5e78a3", size = 3256828 },
    { url = "https://files.pythonhosted.org/packages/79/8b/7aed297d9cc236e15674275364e37e938e9335c9dfad49ad35904fa8b1f3/loro-1.5.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3dce7920c45c9c884246898805b270d63550a5dec61d3f33274010c40127a37c", size = 3464838 },
    { url = "https://files.pythonhosted.org/packages/1d/c1/352fd39b61a842dc991bf95aaa75db34b6c353c1a3844da17e01f917deb5/loro-1.5.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:66afec16e22db99f1818906bc7cabda0cb077e0e493882b4c0983a8bc431413d", size = 3502790 },
    { url = "https://files.pythonhosted.org/packages/2c/11/859dfc28b1397d731d2cc710dae0e7cb1cbeb45ab70ec518b4ed4f690a4c/loro-1.5.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9f052715922592f099e9b6553fccb48761c5ad83deefcb0df55effde309eb12d", size = 3414408 },
    { url = "https://files.pythonhosted.org/packages/86/3e/fcd87311399e2eff892fb3a6b6f1d3307a2dfd99811fddf0889bee89d585/loro-1.5.2-cp313-cp313-win32.whl", hash = "sha256:978e9f6b0c9ad8c6b1ab70372eafbe00c41782522b216802cf961a81edd27561", size = 2580638 },
    { url = "https://files.pythonhosted.org/packages/93/06/dd73ca0865630923f18fa4486e66a171a0a26ae8e7541f1c3d93100f1f5b/loro-1.5.2-cp313-cp313-win_amd64.whl", hash = "sha256:3ecebbf9f5f880c6ca9a1628e5f469d3d67b67c1fd50536c52c5f6eae01be549", size = 2743550 },
    { url = "https://files.pythonhosted.org/packages/d2/70/9e5030bb9f1b86520f482605f660e5a192d6f5e56104fee122fe7d3dc72e/loro-1.5.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:354de426d6404cce252fb81be17a1589f1bd47197ba7f730f60fbb52452f49ab", size = 3106619 },
    { url = "https://files.pythonhosted.org/packages/2b/37/43c8e3fa8c6239be1b22c0dfd779a4ab000682dddebc23becd057668c436/loro-1.5.2-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:18e3b6f07483c5553795fea05c8d318f96c018909dd390c68b81701afb12cac3", size = 3195270 },
    { url = "https://files.pythonhosted.org/packages/b1/d6/8aaa433d08710cb1b95781d56efad366350082798463e35b5a6a4988b160/loro-1.5.2-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2298b96c5f533807373db27dbf5b10c88f1c5d9e0145feb952e7a813a81af645", size = 3575129 },
    { url = "https://files.pythonhosted.org/packages/51/4e/44425f11da9b5278653c3ca01cdfd4da850f94ead5843d8134043ac825cf/loro-1.5.2-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0aa8edef791c1b46e19bf86ab17f9dbefc61b8f1fbecc49054d5eb880380d897", size = 3317031 },
    { url = "https://files.pythonhosted.org/packages/3b/ae/af1713c7c3cc91a9d6cc1b812733665875eb30c22e4c9e0e213a9a69b1a2/loro-1.5.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:633c026cbb17c485de40f09aab13362f0c79140913dc67445606e3237092d70f", size = 3251501 },
    { url = "https://files.pythonhosted.org/packages/4b/df/958e8abb78ca47ce06e0088bc5d44b5945ffbd08503936cbc0340b62a5f3/loro-1.5.2-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:903fed16d40b0373f747ecc398f5b86aaab16c37b4c670f580c2c5301bad4de5", size = 3456858 },
    { url = "https://files.pythonhosted.org/packages/f1/f6/982af3432bde075f1fd3201de0e95f35a868f4e85cee36bb22bb0524b069/loro-1.5.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2f9f77b1f582d86e1a57cdb38a43ea1a5861a6f0d73783335c2efdc3d1dcb793", size = 3494470 },
    { url = "https://files.pythonhosted.org/packages/47/b3/a4725db48fb4c7637076023ccedf7dcb7f24a3d266208f2e2aafb8179861/loro-1.5.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:489230b2716c0a2ad50e205670abed029ba0787c028a62dd31226f7935f5d1fd", size = 3410923 },
]

[[package]]
name = "lxml"
version = "5.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/76/3d/14e82fc7c8fb1b7761f7e748fd47e2ec8276d137b6acfe5a4bb73853e08f/lxml-5.4.0.tar.gz", hash = "sha256:d12832e1dbea4be280b22fd0ea7c9b87f0d8fc51ba06e92dc62d52f804f78ebd", size = 3679479 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/cb/2ba1e9dd953415f58548506fa5549a7f373ae55e80c61c9041b7fd09a38a/lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:773e27b62920199c6197130632c18fb7ead3257fce1ffb7d286912e56ddb79e0", size = 8110086 },
    { url = "https://files.pythonhosted.org/packages/b5/3e/6602a4dca3ae344e8609914d6ab22e52ce42e3e1638c10967568c5c1450d/lxml-5.4.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ce9c671845de9699904b1e9df95acfe8dfc183f2310f163cdaa91a3535af95de", size = 4404613 },
    { url = "https://files.pythonhosted.org/packages/4c/72/bf00988477d3bb452bef9436e45aeea82bb40cdfb4684b83c967c53909c7/lxml-5.4.0-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9454b8d8200ec99a224df8854786262b1bd6461f4280064c807303c642c05e76", size = 5012008 },
    { url = "https://files.pythonhosted.org/packages/92/1f/93e42d93e9e7a44b2d3354c462cd784dbaaf350f7976b5d7c3f85d68d1b1/lxml-5.4.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cccd007d5c95279e529c146d095f1d39ac05139de26c098166c4beb9374b0f4d", size = 4760915 },
    { url = "https://files.pythonhosted.org/packages/45/0b/363009390d0b461cf9976a499e83b68f792e4c32ecef092f3f9ef9c4ba54/lxml-5.4.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0fce1294a0497edb034cb416ad3e77ecc89b313cff7adbee5334e4dc0d11f422", size = 5283890 },
    { url = "https://files.pythonhosted.org/packages/19/dc/6056c332f9378ab476c88e301e6549a0454dbee8f0ae16847414f0eccb74/lxml-5.4.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:24974f774f3a78ac12b95e3a20ef0931795ff04dbb16db81a90c37f589819551", size = 4812644 },
    { url = "https://files.pythonhosted.org/packages/ee/8a/f8c66bbb23ecb9048a46a5ef9b495fd23f7543df642dabeebcb2eeb66592/lxml-5.4.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:497cab4d8254c2a90bf988f162ace2ddbfdd806fce3bda3f581b9d24c852e03c", size = 4921817 },
    { url = "https://files.pythonhosted.org/packages/04/57/2e537083c3f381f83d05d9b176f0d838a9e8961f7ed8ddce3f0217179ce3/lxml-5.4.0-cp313-cp313-manylinux_2_28_aarch64.whl", hash = "sha256:e794f698ae4c5084414efea0f5cc9f4ac562ec02d66e1484ff822ef97c2cadff", size = 4753916 },
    { url = "https://files.pythonhosted.org/packages/d8/80/ea8c4072109a350848f1157ce83ccd9439601274035cd045ac31f47f3417/lxml-5.4.0-cp313-cp313-manylinux_2_28_ppc64le.whl", hash = "sha256:2c62891b1ea3094bb12097822b3d44b93fc6c325f2043c4d2736a8ff09e65f60", size = 5289274 },
    { url = "https://files.pythonhosted.org/packages/b3/47/c4be287c48cdc304483457878a3f22999098b9a95f455e3c4bda7ec7fc72/lxml-5.4.0-cp313-cp313-manylinux_2_28_s390x.whl", hash = "sha256:142accb3e4d1edae4b392bd165a9abdee8a3c432a2cca193df995bc3886249c8", size = 4874757 },
    { url = "https://files.pythonhosted.org/packages/2f/04/6ef935dc74e729932e39478e44d8cfe6a83550552eaa072b7c05f6f22488/lxml-5.4.0-cp313-cp313-manylinux_2_28_x86_64.whl", hash = "sha256:1a42b3a19346e5601d1b8296ff6ef3d76038058f311902edd574461e9c036982", size = 4947028 },
    { url = "https://files.pythonhosted.org/packages/cb/f9/c33fc8daa373ef8a7daddb53175289024512b6619bc9de36d77dca3df44b/lxml-5.4.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4291d3c409a17febf817259cb37bc62cb7eb398bcc95c1356947e2871911ae61", size = 4834487 },
    { url = "https://files.pythonhosted.org/packages/8d/30/fc92bb595bcb878311e01b418b57d13900f84c2b94f6eca9e5073ea756e6/lxml-5.4.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4f5322cf38fe0e21c2d73901abf68e6329dc02a4994e483adbcf92b568a09a54", size = 5381688 },
    { url = "https://files.pythonhosted.org/packages/43/d1/3ba7bd978ce28bba8e3da2c2e9d5ae3f8f521ad3f0ca6ea4788d086ba00d/lxml-5.4.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0be91891bdb06ebe65122aa6bf3fc94489960cf7e03033c6f83a90863b23c58b", size = 5242043 },
    { url = "https://files.pythonhosted.org/packages/ee/cd/95fa2201041a610c4d08ddaf31d43b98ecc4b1d74b1e7245b1abdab443cb/lxml-5.4.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:15a665ad90054a3d4f397bc40f73948d48e36e4c09f9bcffc7d90c87410e478a", size = 5021569 },
    { url = "https://files.pythonhosted.org/packages/2d/a6/31da006fead660b9512d08d23d31e93ad3477dd47cc42e3285f143443176/lxml-5.4.0-cp313-cp313-win32.whl", hash = "sha256:d5663bc1b471c79f5c833cffbc9b87d7bf13f87e055a5c86c363ccd2348d7e82", size = 3485270 },
    { url = "https://files.pythonhosted.org/packages/fc/14/c115516c62a7d2499781d2d3d7215218c0731b2c940753bf9f9b7b73924d/lxml-5.4.0-cp313-cp313-win_amd64.whl", hash = "sha256:bcb7a1096b4b6b24ce1ac24d4942ad98f983cd3810f9711bcd0293f43a9d8b9f", size = 3814606 },
]

[[package]]
name = "marimo"
version = "0.14.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "docutils" },
    { name = "itsdangerous" },
    { name = "jedi" },
    { name = "loro" },
    { name = "markdown" },
    { name = "narwhals" },
    { name = "packaging" },
    { name = "psutil" },
    { name = "pygments" },
    { name = "pymdown-extensions" },
    { name = "pyyaml" },
    { name = "starlette" },
    { name = "tomlkit" },
    { name = "uvicorn" },
    { name = "websockets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0b/6d/8c0bdb68d608561e3039718f171ede292e7da7e7580a51b1f4b2ce6e204f/marimo-0.14.12.tar.gz", hash = "sha256:cf18513e30a5d2e8864930885b674dd89cbc9ad3a5e128b9ecfa48323de6d14f", size = 29622446 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/fa/d802cd61fb4714c17529057dc4b07d48c3e115d0af331907b3d19f5482f6/marimo-0.14.12-py3-none-any.whl", hash = "sha256:154d168ceb8b9f4cc10f8cd9f6299cf0c5d8643b0291370a9e64a88b2f517ed3", size = 30118091 },
]

[[package]]
name = "markdown"
version = "3.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/c2/4ab49206c17f75cb08d6311171f2d65798988db4360c4d1485bd0eedd67c/markdown-3.8.2.tar.gz", hash = "sha256:247b9a70dd12e27f67431ce62523e675b866d254f900c4fe75ce3dda62237c45", size = 362071 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl", hash = "sha256:5c83764dbd4e00bdd94d85a19b8d55ccca20fe35b2e678a1422b380324dd5f24", size = 106827 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "markdownify"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2f/78/c48fed23c7aebc2c16049062e72de1da3220c274de59d28c942acdc9ffb2/markdownify-1.1.0.tar.gz", hash = "sha256:449c0bbbf1401c5112379619524f33b63490a8fa479456d41de9dc9e37560ebd", size = 17127 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/11/b751af7ad41b254a802cf52f7bc1fca7cabe2388132f2ce60a1a6b9b9622/markdownify-1.1.0-py3-none-any.whl", hash = "sha256:32a5a08e9af02c8a6528942224c91b933b4bd2c7d078f9012943776fc313eeef", size = 13901 },
]

[[package]]
name = "matplotlib-inline"
version = "0.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "traitlets" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/5b/a36a337438a14116b16480db471ad061c36c3694df7c2084a0da7ba538b7/matplotlib_inline-0.1.7.tar.gz", hash = "sha256:8423b23ec666be3d16e16b60bdd8ac4e86e840ebd1dd11a30b9f117f2fa0ab90", size = 8159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8f/8e/9ad090d3553c280a8060fbf6e24dc1c0c29704ee7d1c372f0c174aa59285/matplotlib_inline-0.1.7-py3-none-any.whl", hash = "sha256:df192d39a4ff8f21b1895d72e6a13f5fcc5099f00fa84384e0ea28c2cc0653ca", size = 9899 },
]

[[package]]
name = "mcp"
version = "1.7.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "python-multipart" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn", marker = "sys_platform != 'emscripten'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/25/ae/588691c45b38f4fbac07fa3d6d50cea44cc6b35d16ddfdf26e17a0467ab2/mcp-1.7.1.tar.gz", hash = "sha256:eb4f1f53bd717f75dda8a1416e00804b831a8f3c331e23447a03b78f04b43a6e", size = 230903 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ae/79/fe0e20c3358997a80911af51bad927b5ea2f343ef95ab092b19c9cc48b59/mcp-1.7.1-py3-none-any.whl", hash = "sha256:f7e6108977db6d03418495426c7ace085ba2341b75197f8727f96f9cfd30057a", size = 100365 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "multidict"
version = "6.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/91/2f/a3470242707058fe856fe59241eee5635d79087100b7042a867368863a27/multidict-6.4.4.tar.gz", hash = "sha256:69ee9e6ba214b5245031b76233dd95408a0fd57fdb019ddcc1ead4790932a8e8", size = 90183 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/2a/e166d2ffbf4b10131b2d5b0e458f7cee7d986661caceae0de8753042d4b2/multidict-6.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:82ffabefc8d84c2742ad19c37f02cde5ec2a1ee172d19944d380f920a340e4b9", size = 64123 },
    { url = "https://files.pythonhosted.org/packages/8c/96/e200e379ae5b6f95cbae472e0199ea98913f03d8c9a709f42612a432932c/multidict-6.4.4-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6a2f58a66fe2c22615ad26156354005391e26a2f3721c3621504cd87c1ea87bf", size = 38049 },
    { url = "https://files.pythonhosted.org/packages/75/fb/47afd17b83f6a8c7fa863c6d23ac5ba6a0e6145ed8a6bcc8da20b2b2c1d2/multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5883d6ee0fd9d8a48e9174df47540b7545909841ac82354c7ae4cbe9952603bd", size = 37078 },
    { url = "https://files.pythonhosted.org/packages/fa/70/1af3143000eddfb19fd5ca5e78393985ed988ac493bb859800fe0914041f/multidict-6.4.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9abcf56a9511653fa1d052bfc55fbe53dbee8f34e68bd6a5a038731b0ca42d15", size = 224097 },
    { url = "https://files.pythonhosted.org/packages/b1/39/d570c62b53d4fba844e0378ffbcd02ac25ca423d3235047013ba2f6f60f8/multidict-6.4.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:6ed5ae5605d4ad5a049fad2a28bb7193400700ce2f4ae484ab702d1e3749c3f9", size = 230768 },
    { url = "https://files.pythonhosted.org/packages/fd/f8/ed88f2c4d06f752b015933055eb291d9bc184936903752c66f68fb3c95a7/multidict-6.4.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bbfcb60396f9bcfa63e017a180c3105b8c123a63e9d1428a36544e7d37ca9e20", size = 231331 },
    { url = "https://files.pythonhosted.org/packages/9c/6f/8e07cffa32f483ab887b0d56bbd8747ac2c1acd00dc0af6fcf265f4a121e/multidict-6.4.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b0f1987787f5f1e2076b59692352ab29a955b09ccc433c1f6b8e8e18666f608b", size = 230169 },
    { url = "https://files.pythonhosted.org/packages/e6/2b/5dcf173be15e42f330110875a2668ddfc208afc4229097312212dc9c1236/multidict-6.4.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1d0121ccce8c812047d8d43d691a1ad7641f72c4f730474878a5aeae1b8ead8c", size = 222947 },
    { url = "https://files.pythonhosted.org/packages/39/75/4ddcbcebe5ebcd6faa770b629260d15840a5fc07ce8ad295a32e14993726/multidict-6.4.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:83ec4967114295b8afd120a8eec579920c882831a3e4c3331d591a8e5bfbbc0f", size = 215761 },
    { url = "https://files.pythonhosted.org/packages/6a/c9/55e998ae45ff15c5608e384206aa71a11e1b7f48b64d166db400b14a3433/multidict-6.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:995f985e2e268deaf17867801b859a282e0448633f1310e3704b30616d269d69", size = 227605 },
    { url = "https://files.pythonhosted.org/packages/04/49/c2404eac74497503c77071bd2e6f88c7e94092b8a07601536b8dbe99be50/multidict-6.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:d832c608f94b9f92a0ec8b7e949be7792a642b6e535fcf32f3e28fab69eeb046", size = 226144 },
    { url = "https://files.pythonhosted.org/packages/62/c5/0cd0c3c6f18864c40846aa2252cd69d308699cb163e1c0d989ca301684da/multidict-6.4.4-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:d21c1212171cf7da703c5b0b7a0e85be23b720818aef502ad187d627316d5645", size = 221100 },
    { url = "https://files.pythonhosted.org/packages/71/7b/f2f3887bea71739a046d601ef10e689528d4f911d84da873b6be9194ffea/multidict-6.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:cbebaa076aaecad3d4bb4c008ecc73b09274c952cf6a1b78ccfd689e51f5a5b0", size = 232731 },
    { url = "https://files.pythonhosted.org/packages/e5/b3/d9de808349df97fa75ec1372758701b5800ebad3c46ae377ad63058fbcc6/multidict-6.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:c93a6fb06cc8e5d3628b2b5fda215a5db01e8f08fc15fadd65662d9b857acbe4", size = 229637 },
    { url = "https://files.pythonhosted.org/packages/5e/57/13207c16b615eb4f1745b44806a96026ef8e1b694008a58226c2d8f5f0a5/multidict-6.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8cd8f81f1310182362fb0c7898145ea9c9b08a71081c5963b40ee3e3cac589b1", size = 225594 },
    { url = "https://files.pythonhosted.org/packages/3a/e4/d23bec2f70221604f5565000632c305fc8f25ba953e8ce2d8a18842b9841/multidict-6.4.4-cp313-cp313-win32.whl", hash = "sha256:3e9f1cd61a0ab857154205fb0b1f3d3ace88d27ebd1409ab7af5096e409614cd", size = 35359 },
    { url = "https://files.pythonhosted.org/packages/a7/7a/cfe1a47632be861b627f46f642c1d031704cc1c0f5c0efbde2ad44aa34bd/multidict-6.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:8ffb40b74400e4455785c2fa37eba434269149ec525fc8329858c862e4b35373", size = 38903 },
    { url = "https://files.pythonhosted.org/packages/68/7b/15c259b0ab49938a0a1c8f3188572802704a779ddb294edc1b2a72252e7c/multidict-6.4.4-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:6a602151dbf177be2450ef38966f4be3467d41a86c6a845070d12e17c858a156", size = 68895 },
    { url = "https://files.pythonhosted.org/packages/f1/7d/168b5b822bccd88142e0a3ce985858fea612404edd228698f5af691020c9/multidict-6.4.4-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0d2b9712211b860d123815a80b859075d86a4d54787e247d7fbee9db6832cf1c", size = 40183 },
    { url = "https://files.pythonhosted.org/packages/e0/b7/d4b8d98eb850ef28a4922ba508c31d90715fd9b9da3801a30cea2967130b/multidict-6.4.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:d2fa86af59f8fc1972e121ade052145f6da22758f6996a197d69bb52f8204e7e", size = 39592 },
    { url = "https://files.pythonhosted.org/packages/18/28/a554678898a19583548e742080cf55d169733baf57efc48c2f0273a08583/multidict-6.4.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50855d03e9e4d66eab6947ba688ffb714616f985838077bc4b490e769e48da51", size = 226071 },
    { url = "https://files.pythonhosted.org/packages/ee/dc/7ba6c789d05c310e294f85329efac1bf5b450338d2542498db1491a264df/multidict-6.4.4-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5bce06b83be23225be1905dcdb6b789064fae92499fbc458f59a8c0e68718601", size = 222597 },
    { url = "https://files.pythonhosted.org/packages/24/4f/34eadbbf401b03768dba439be0fb94b0d187facae9142821a3d5599ccb3b/multidict-6.4.4-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:66ed0731f8e5dfd8369a883b6e564aca085fb9289aacabd9decd70568b9a30de", size = 228253 },
    { url = "https://files.pythonhosted.org/packages/c0/e6/493225a3cdb0d8d80d43a94503fc313536a07dae54a3f030d279e629a2bc/multidict-6.4.4-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:329ae97fc2f56f44d91bc47fe0972b1f52d21c4b7a2ac97040da02577e2daca2", size = 226146 },
    { url = "https://files.pythonhosted.org/packages/2f/70/e411a7254dc3bff6f7e6e004303b1b0591358e9f0b7c08639941e0de8bd6/multidict-6.4.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c27e5dcf520923d6474d98b96749e6805f7677e93aaaf62656005b8643f907ab", size = 220585 },
    { url = "https://files.pythonhosted.org/packages/08/8f/beb3ae7406a619100d2b1fb0022c3bb55a8225ab53c5663648ba50dfcd56/multidict-6.4.4-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:058cc59b9e9b143cc56715e59e22941a5d868c322242278d28123a5d09cdf6b0", size = 212080 },
    { url = "https://files.pythonhosted.org/packages/9c/ec/355124e9d3d01cf8edb072fd14947220f357e1c5bc79c88dff89297e9342/multidict-6.4.4-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:69133376bc9a03f8c47343d33f91f74a99c339e8b58cea90433d8e24bb298031", size = 226558 },
    { url = "https://files.pythonhosted.org/packages/fd/22/d2b95cbebbc2ada3be3812ea9287dcc9712d7f1a012fad041770afddb2ad/multidict-6.4.4-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d6b15c55721b1b115c5ba178c77104123745b1417527ad9641a4c5e2047450f0", size = 212168 },
    { url = "https://files.pythonhosted.org/packages/4d/c5/62bfc0b2f9ce88326dbe7179f9824a939c6c7775b23b95de777267b9725c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:a887b77f51d3d41e6e1a63cf3bc7ddf24de5939d9ff69441387dfefa58ac2e26", size = 217970 },
    { url = "https://files.pythonhosted.org/packages/79/74/977cea1aadc43ff1c75d23bd5bc4768a8fac98c14e5878d6ee8d6bab743c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:632a3bf8f1787f7ef7d3c2f68a7bde5be2f702906f8b5842ad6da9d974d0aab3", size = 226980 },
    { url = "https://files.pythonhosted.org/packages/48/fc/cc4a1a2049df2eb84006607dc428ff237af38e0fcecfdb8a29ca47b1566c/multidict-6.4.4-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:a145c550900deb7540973c5cdb183b0d24bed6b80bf7bddf33ed8f569082535e", size = 220641 },
    { url = "https://files.pythonhosted.org/packages/3b/6a/a7444d113ab918701988d4abdde373dbdfd2def7bd647207e2bf645c7eac/multidict-6.4.4-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:cc5d83c6619ca5c9672cb78b39ed8542f1975a803dee2cda114ff73cbb076edd", size = 221728 },
    { url = "https://files.pythonhosted.org/packages/2b/b0/fdf4c73ad1c55e0f4dbbf2aa59dd37037334091f9a4961646d2b7ac91a86/multidict-6.4.4-cp313-cp313t-win32.whl", hash = "sha256:3312f63261b9df49be9d57aaa6abf53a6ad96d93b24f9cc16cf979956355ce6e", size = 41913 },
    { url = "https://files.pythonhosted.org/packages/8e/92/27989ecca97e542c0d01d05a98a5ae12198a243a9ee12563a0313291511f/multidict-6.4.4-cp313-cp313t-win_amd64.whl", hash = "sha256:ba852168d814b2c73333073e1c7116d9395bea69575a01b0b3c89d2d5a87c8fb", size = 46112 },
    { url = "https://files.pythonhosted.org/packages/84/5d/e17845bb0fa76334477d5de38654d27946d5b5d3695443987a094a71b440/multidict-6.4.4-py3-none-any.whl", hash = "sha256:bd4557071b561a8b3b6075c3ce93cf9bfb6182cb241805c3d66ced3b75eff4ac", size = 10481 },
]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/6e/371856a3fb9d31ca8dac321cda606860fa4548858c0cc45d9d1d4ca2628b/mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558", size = 6343 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505", size = 4963 },
]

[[package]]
name = "narwhals"
version = "1.48.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/cd/7395d6c247e821cba6243e9f7ed202fae3fefef643c96581b5ecab927bad/narwhals-1.48.0.tar.gz", hash = "sha256:7243b456cbdb60edb148731a8f9b203f473a373a249ad66c699362508730e63f", size = 515112 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/75/72/5406044d4c251f3d8f78cec05b74839d0332d34c9e94b59120f3697ecf48/narwhals-1.48.0-py3-none-any.whl", hash = "sha256:2bbddc3adeed0c5b15ead8fe61f1d5e459f00c1d2fa60921e52a0f9bdc06077d", size = 376866 },
]

[[package]]
name = "numpy"
version = "2.2.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/dc/b2/ce4b867d8cd9c0ee84938ae1e6a6f7926ebf928c9090d036fc3c6a04f946/numpy-2.2.5.tar.gz", hash = "sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291", size = 20273920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e2/a0/0aa7f0f4509a2e07bd7a509042967c2fab635690d4f48c6c7b3afd4f448c/numpy-2.2.5-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4", size = 20935102 },
    { url = "https://files.pythonhosted.org/packages/7e/e4/a6a9f4537542912ec513185396fce52cdd45bdcf3e9d921ab02a93ca5aa9/numpy-2.2.5-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f", size = 14191709 },
    { url = "https://files.pythonhosted.org/packages/be/65/72f3186b6050bbfe9c43cb81f9df59ae63603491d36179cf7a7c8d216758/numpy-2.2.5-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9", size = 5149173 },
    { url = "https://files.pythonhosted.org/packages/e5/e9/83e7a9432378dde5802651307ae5e9ea07bb72b416728202218cd4da2801/numpy-2.2.5-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191", size = 6684502 },
    { url = "https://files.pythonhosted.org/packages/ea/27/b80da6c762394c8ee516b74c1f686fcd16c8f23b14de57ba0cad7349d1d2/numpy-2.2.5-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372", size = 14084417 },
    { url = "https://files.pythonhosted.org/packages/aa/fc/ebfd32c3e124e6a1043e19c0ab0769818aa69050ce5589b63d05ff185526/numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d", size = 16133807 },
    { url = "https://files.pythonhosted.org/packages/bf/9b/4cc171a0acbe4666f7775cfd21d4eb6bb1d36d3a0431f48a73e9212d2278/numpy-2.2.5-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7", size = 15575611 },
    { url = "https://files.pythonhosted.org/packages/a3/45/40f4135341850df48f8edcf949cf47b523c404b712774f8855a64c96ef29/numpy-2.2.5-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73", size = 17895747 },
    { url = "https://files.pythonhosted.org/packages/f8/4c/b32a17a46f0ffbde8cc82df6d3daeaf4f552e346df143e1b188a701a8f09/numpy-2.2.5-cp313-cp313-win32.whl", hash = "sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b", size = 6309594 },
    { url = "https://files.pythonhosted.org/packages/13/ae/72e6276feb9ef06787365b05915bfdb057d01fceb4a43cb80978e518d79b/numpy-2.2.5-cp313-cp313-win_amd64.whl", hash = "sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471", size = 12638356 },
    { url = "https://files.pythonhosted.org/packages/79/56/be8b85a9f2adb688e7ded6324e20149a03541d2b3297c3ffc1a73f46dedb/numpy-2.2.5-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6", size = 20963778 },
    { url = "https://files.pythonhosted.org/packages/ff/77/19c5e62d55bff507a18c3cdff82e94fe174957bad25860a991cac719d3ab/numpy-2.2.5-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba", size = 14207279 },
    { url = "https://files.pythonhosted.org/packages/75/22/aa11f22dc11ff4ffe4e849d9b63bbe8d4ac6d5fae85ddaa67dfe43be3e76/numpy-2.2.5-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133", size = 5199247 },
    { url = "https://files.pythonhosted.org/packages/4f/6c/12d5e760fc62c08eded0394f62039f5a9857f758312bf01632a81d841459/numpy-2.2.5-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376", size = 6711087 },
    { url = "https://files.pythonhosted.org/packages/ef/94/ece8280cf4218b2bee5cec9567629e61e51b4be501e5c6840ceb593db945/numpy-2.2.5-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19", size = 14059964 },
    { url = "https://files.pythonhosted.org/packages/39/41/c5377dac0514aaeec69115830a39d905b1882819c8e65d97fc60e177e19e/numpy-2.2.5-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0", size = 16121214 },
    { url = "https://files.pythonhosted.org/packages/db/54/3b9f89a943257bc8e187145c6bc0eb8e3d615655f7b14e9b490b053e8149/numpy-2.2.5-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a", size = 15575788 },
    { url = "https://files.pythonhosted.org/packages/b1/c4/2e407e85df35b29f79945751b8f8e671057a13a376497d7fb2151ba0d290/numpy-2.2.5-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066", size = 17893672 },
    { url = "https://files.pythonhosted.org/packages/29/7e/d0b44e129d038dba453f00d0e29ebd6eaf2f06055d72b95b9947998aca14/numpy-2.2.5-cp313-cp313t-win32.whl", hash = "sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e", size = 6377102 },
    { url = "https://files.pythonhosted.org/packages/63/be/b85e4aa4bf42c6502851b971f1c326d583fcc68227385f92089cf50a7b45/numpy-2.2.5-cp313-cp313t-win_amd64.whl", hash = "sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8", size = 12750096 },
]

[[package]]
name = "openai"
version = "1.84.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/a3/128caf24e116f48fad3e4d5122cdf84db06c5127911849d51663c66158c8/openai-1.84.0.tar.gz", hash = "sha256:4caa43bdab262cc75680ce1a2322cfc01626204074f7e8d9939ab372acf61698", size = 467066 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/10/f245db006a860dbc1f2e2c8382e0a1762c7753e7971ba43a1dc3f3ec1404/openai-1.84.0-py3-none-any.whl", hash = "sha256:7ec4436c3c933d68dc0f5a0cef0cb3dbc0864a54d62bddaf2ed5f3d521844711", size = 725512 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "parso"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/66/94/68e2e17afaa9169cf6412ab0f28623903be73d1b32e208d9e8e541bb086d/parso-0.8.4.tar.gz", hash = "sha256:eb3a7b58240fb99099a345571deecc0f9540ea5f4dd2fe14c2a99d6b281ab92d", size = 400609 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c6/ac/dac4a63f978e4dcb3c6d3a78c4d8e0192a113d288502a1216950c41b1027/parso-0.8.4-py2.py3-none-any.whl", hash = "sha256:a418670a20291dacd2dddc80c377c5c3791378ee1e8d12bffc35420643d43f18", size = 103650 },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191 },
]

[[package]]
name = "pexpect"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "ptyprocess" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/92/cc564bf6381ff43ce1f4d06852fc19a2f11d180f23dc32d9588bee2f149d/pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f", size = 166450 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/c3/059298687310d527a58bb01f3b1965787ee3b40dce76752eda8b44e9a2c5/pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523", size = 63772 },
]

[[package]]
name = "platformdirs"
version = "4.3.8"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/8b/3c73abc9c759ecd3f1f7ceff6685840859e8070c4d947c93fae71f6a0bf2/platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc", size = 21362 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/39/979e8e21520d4e47a0bbe349e2713c0aac6f3d853d0e5b34d76206c439aa/platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4", size = 18567 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810 },
]

[[package]]
name = "propcache"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/c8/fdc6686a986feae3541ea23dcaa661bd93972d3940460646c6bb96e21c40/propcache-0.3.1.tar.gz", hash = "sha256:40d980c33765359098837527e18eddefc9a24cea5b45e078a7f3bb5b032c6ecf", size = 43651 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/60/f645cc8b570f99be3cf46714170c2de4b4c9d6b827b912811eff1eb8a412/propcache-0.3.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:f1528ec4374617a7a753f90f20e2f551121bb558fcb35926f99e3c42367164b8", size = 77865 },
    { url = "https://files.pythonhosted.org/packages/6f/d4/c1adbf3901537582e65cf90fd9c26fde1298fde5a2c593f987112c0d0798/propcache-0.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dc1915ec523b3b494933b5424980831b636fe483d7d543f7afb7b3bf00f0c10f", size = 45452 },
    { url = "https://files.pythonhosted.org/packages/d1/b5/fe752b2e63f49f727c6c1c224175d21b7d1727ce1d4873ef1c24c9216830/propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a110205022d077da24e60b3df8bcee73971be9575dec5573dd17ae5d81751111", size = 44800 },
    { url = "https://files.pythonhosted.org/packages/62/37/fc357e345bc1971e21f76597028b059c3d795c5ca7690d7a8d9a03c9708a/propcache-0.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d249609e547c04d190e820d0d4c8ca03ed4582bcf8e4e160a6969ddfb57b62e5", size = 225804 },
    { url = "https://files.pythonhosted.org/packages/0d/f1/16e12c33e3dbe7f8b737809bad05719cff1dccb8df4dafbcff5575002c0e/propcache-0.3.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5ced33d827625d0a589e831126ccb4f5c29dfdf6766cac441d23995a65825dcb", size = 230650 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/018b9f2ed876bf5091e60153f727e8f9073d97573f790ff7cdf6bc1d1fb8/propcache-0.3.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4114c4ada8f3181af20808bedb250da6bae56660e4b8dfd9cd95d4549c0962f7", size = 234235 },
    { url = "https://files.pythonhosted.org/packages/45/5f/3faee66fc930dfb5da509e34c6ac7128870631c0e3582987fad161fcb4b1/propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:975af16f406ce48f1333ec5e912fe11064605d5c5b3f6746969077cc3adeb120", size = 228249 },
    { url = "https://files.pythonhosted.org/packages/62/1e/a0d5ebda5da7ff34d2f5259a3e171a94be83c41eb1e7cd21a2105a84a02e/propcache-0.3.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a34aa3a1abc50740be6ac0ab9d594e274f59960d3ad253cd318af76b996dd654", size = 214964 },
    { url = "https://files.pythonhosted.org/packages/db/a0/d72da3f61ceab126e9be1f3bc7844b4e98c6e61c985097474668e7e52152/propcache-0.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9cec3239c85ed15bfaded997773fdad9fb5662b0a7cbc854a43f291eb183179e", size = 222501 },
    { url = "https://files.pythonhosted.org/packages/18/6d/a008e07ad7b905011253adbbd97e5b5375c33f0b961355ca0a30377504ac/propcache-0.3.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:05543250deac8e61084234d5fc54f8ebd254e8f2b39a16b1dce48904f45b744b", size = 217917 },
    { url = "https://files.pythonhosted.org/packages/98/37/02c9343ffe59e590e0e56dc5c97d0da2b8b19fa747ebacf158310f97a79a/propcache-0.3.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5cb5918253912e088edbf023788de539219718d3b10aef334476b62d2b53de53", size = 217089 },
    { url = "https://files.pythonhosted.org/packages/53/1b/d3406629a2c8a5666d4674c50f757a77be119b113eedd47b0375afdf1b42/propcache-0.3.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f3bbecd2f34d0e6d3c543fdb3b15d6b60dd69970c2b4c822379e5ec8f6f621d5", size = 228102 },
    { url = "https://files.pythonhosted.org/packages/cd/a7/3664756cf50ce739e5f3abd48febc0be1a713b1f389a502ca819791a6b69/propcache-0.3.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:aca63103895c7d960a5b9b044a83f544b233c95e0dcff114389d64d762017af7", size = 230122 },
    { url = "https://files.pythonhosted.org/packages/35/36/0bbabaacdcc26dac4f8139625e930f4311864251276033a52fd52ff2a274/propcache-0.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5a0a9898fdb99bf11786265468571e628ba60af80dc3f6eb89a3545540c6b0ef", size = 226818 },
    { url = "https://files.pythonhosted.org/packages/cc/27/4e0ef21084b53bd35d4dae1634b6d0bad35e9c58ed4f032511acca9d4d26/propcache-0.3.1-cp313-cp313-win32.whl", hash = "sha256:3a02a28095b5e63128bcae98eb59025924f121f048a62393db682f049bf4ac24", size = 40112 },
    { url = "https://files.pythonhosted.org/packages/a6/2c/a54614d61895ba6dd7ac8f107e2b2a0347259ab29cbf2ecc7b94fa38c4dc/propcache-0.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:813fbb8b6aea2fc9659815e585e548fe706d6f663fa73dff59a1677d4595a037", size = 44034 },
    { url = "https://files.pythonhosted.org/packages/5a/a8/0a4fd2f664fc6acc66438370905124ce62e84e2e860f2557015ee4a61c7e/propcache-0.3.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:a444192f20f5ce8a5e52761a031b90f5ea6288b1eef42ad4c7e64fef33540b8f", size = 82613 },
    { url = "https://files.pythonhosted.org/packages/4d/e5/5ef30eb2cd81576256d7b6caaa0ce33cd1d2c2c92c8903cccb1af1a4ff2f/propcache-0.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:0fbe94666e62ebe36cd652f5fc012abfbc2342de99b523f8267a678e4dfdee3c", size = 47763 },
    { url = "https://files.pythonhosted.org/packages/87/9a/87091ceb048efeba4d28e903c0b15bcc84b7c0bf27dc0261e62335d9b7b8/propcache-0.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:f011f104db880f4e2166bcdcf7f58250f7a465bc6b068dc84c824a3d4a5c94dc", size = 47175 },
    { url = "https://files.pythonhosted.org/packages/3e/2f/854e653c96ad1161f96194c6678a41bbb38c7947d17768e8811a77635a08/propcache-0.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3e584b6d388aeb0001d6d5c2bd86b26304adde6d9bb9bfa9c4889805021b96de", size = 292265 },
    { url = "https://files.pythonhosted.org/packages/40/8d/090955e13ed06bc3496ba4a9fb26c62e209ac41973cb0d6222de20c6868f/propcache-0.3.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8a17583515a04358b034e241f952f1715243482fc2c2945fd99a1b03a0bd77d6", size = 294412 },
    { url = "https://files.pythonhosted.org/packages/39/e6/d51601342e53cc7582449e6a3c14a0479fab2f0750c1f4d22302e34219c6/propcache-0.3.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5aed8d8308215089c0734a2af4f2e95eeb360660184ad3912686c181e500b2e7", size = 294290 },
    { url = "https://files.pythonhosted.org/packages/3b/4d/be5f1a90abc1881884aa5878989a1acdafd379a91d9c7e5e12cef37ec0d7/propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d8e309ff9a0503ef70dc9a0ebd3e69cf7b3894c9ae2ae81fc10943c37762458", size = 282926 },
    { url = "https://files.pythonhosted.org/packages/57/2b/8f61b998c7ea93a2b7eca79e53f3e903db1787fca9373af9e2cf8dc22f9d/propcache-0.3.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b655032b202028a582d27aeedc2e813299f82cb232f969f87a4fde491a233f11", size = 267808 },
    { url = "https://files.pythonhosted.org/packages/11/1c/311326c3dfce59c58a6098388ba984b0e5fb0381ef2279ec458ef99bd547/propcache-0.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9f64d91b751df77931336b5ff7bafbe8845c5770b06630e27acd5dbb71e1931c", size = 290916 },
    { url = "https://files.pythonhosted.org/packages/4b/74/91939924b0385e54dc48eb2e4edd1e4903ffd053cf1916ebc5347ac227f7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:19a06db789a4bd896ee91ebc50d059e23b3639c25d58eb35be3ca1cbe967c3bf", size = 262661 },
    { url = "https://files.pythonhosted.org/packages/c2/d7/e6079af45136ad325c5337f5dd9ef97ab5dc349e0ff362fe5c5db95e2454/propcache-0.3.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:bef100c88d8692864651b5f98e871fb090bd65c8a41a1cb0ff2322db39c96c27", size = 264384 },
    { url = "https://files.pythonhosted.org/packages/b7/d5/ba91702207ac61ae6f1c2da81c5d0d6bf6ce89e08a2b4d44e411c0bbe867/propcache-0.3.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:87380fb1f3089d2a0b8b00f006ed12bd41bd858fabfa7330c954c70f50ed8757", size = 291420 },
    { url = "https://files.pythonhosted.org/packages/58/70/2117780ed7edcd7ba6b8134cb7802aada90b894a9810ec56b7bb6018bee7/propcache-0.3.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e474fc718e73ba5ec5180358aa07f6aded0ff5f2abe700e3115c37d75c947e18", size = 290880 },
    { url = "https://files.pythonhosted.org/packages/4a/1f/ecd9ce27710021ae623631c0146719280a929d895a095f6d85efb6a0be2e/propcache-0.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:17d1c688a443355234f3c031349da69444be052613483f3e4158eef751abcd8a", size = 287407 },
    { url = "https://files.pythonhosted.org/packages/3e/66/2e90547d6b60180fb29e23dc87bd8c116517d4255240ec6d3f7dc23d1926/propcache-0.3.1-cp313-cp313t-win32.whl", hash = "sha256:359e81a949a7619802eb601d66d37072b79b79c2505e6d3fd8b945538411400d", size = 42573 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/50ad8599399d1861b4d2b6b45271f0ef6af1b09b0a2386a46dbaf19c9535/propcache-0.3.1-cp313-cp313t-win_amd64.whl", hash = "sha256:e7fb9a84c9abbf2b2683fa3e7b0d7da4d8ecf139a1c635732a8bda29c5214b0e", size = 46757 },
    { url = "https://files.pythonhosted.org/packages/b8/d3/c3cb8f1d6ae3b37f83e1de806713a9b3642c5895f0215a62e1a4bd6e5e34/propcache-0.3.1-py3-none-any.whl", hash = "sha256:9a8ecf38de50a7f518c21568c80f985e776397b902f1ce0b01f799aba1608b40", size = 12376 },
]

[[package]]
name = "proto-plus"
version = "1.26.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "protobuf" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f4/ac/87285f15f7cce6d4a008f33f1757fb5a13611ea8914eb58c3d0d26243468/proto_plus-1.26.1.tar.gz", hash = "sha256:21a515a4c4c0088a773899e23c7bbade3d18f9c66c73edd4c7ee3816bc96a012", size = 56142 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl", hash = "sha256:13285478c2dcf2abb829db158e1047e2f1e8d63a077d94263c2b88b043c75a66", size = 50163 },
]

[[package]]
name = "protobuf"
version = "5.29.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/17/7d/b9dca7365f0e2c4fa7c193ff795427cfa6290147e5185ab11ece280a18e7/protobuf-5.29.4.tar.gz", hash = "sha256:4f1dfcd7997b31ef8f53ec82781ff434a28bf71d9102ddde14d076adcfc78c99", size = 424902 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/b2/043a1a1a20edd134563699b0e91862726a0dc9146c090743b6c44d798e75/protobuf-5.29.4-cp310-abi3-win32.whl", hash = "sha256:13eb236f8eb9ec34e63fc8b1d6efd2777d062fa6aaa68268fb67cf77f6839ad7", size = 422709 },
    { url = "https://files.pythonhosted.org/packages/79/fc/2474b59570daa818de6124c0a15741ee3e5d6302e9d6ce0bdfd12e98119f/protobuf-5.29.4-cp310-abi3-win_amd64.whl", hash = "sha256:bcefcdf3976233f8a502d265eb65ea740c989bacc6c30a58290ed0e519eb4b8d", size = 434506 },
    { url = "https://files.pythonhosted.org/packages/46/de/7c126bbb06aa0f8a7b38aaf8bd746c514d70e6a2a3f6dd460b3b7aad7aae/protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl", hash = "sha256:307ecba1d852ec237e9ba668e087326a67564ef83e45a0189a772ede9e854dd0", size = 417826 },
    { url = "https://files.pythonhosted.org/packages/a2/b5/bade14ae31ba871a139aa45e7a8183d869efe87c34a4850c87b936963261/protobuf-5.29.4-cp38-abi3-manylinux2014_aarch64.whl", hash = "sha256:aec4962f9ea93c431d5714ed1be1c93f13e1a8618e70035ba2b0564d9e633f2e", size = 319574 },
    { url = "https://files.pythonhosted.org/packages/46/88/b01ed2291aae68b708f7d334288ad5fb3e7aa769a9c309c91a0d55cb91b0/protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl", hash = "sha256:d7d3f7d1d5a66ed4942d4fefb12ac4b14a29028b209d4bfb25c68ae172059922", size = 319672 },
    { url = "https://files.pythonhosted.org/packages/12/fb/a586e0c973c95502e054ac5f81f88394f24ccc7982dac19c515acd9e2c93/protobuf-5.29.4-py3-none-any.whl", hash = "sha256:3fde11b505e1597f71b875ef2fc52062b6a9740e5f7c8997ce878b6009145862", size = 172551 },
]

[[package]]
name = "psutil"
version = "7.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2a/80/336820c1ad9286a4ded7e845b2eccfcb27851ab8ac6abece774a6ff4d3de/psutil-7.0.0.tar.gz", hash = "sha256:7be9c3eba38beccb6495ea33afd982a44074b78f28c434a1f51cc07fd315c456", size = 497003 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ed/e6/2d26234410f8b8abdbf891c9da62bee396583f713fb9f3325a4760875d22/psutil-7.0.0-cp36-abi3-macosx_10_9_x86_64.whl", hash = "sha256:101d71dc322e3cffd7cea0650b09b3d08b8e7c4109dd6809fe452dfd00e58b25", size = 238051 },
    { url = "https://files.pythonhosted.org/packages/04/8b/30f930733afe425e3cbfc0e1468a30a18942350c1a8816acfade80c005c4/psutil-7.0.0-cp36-abi3-macosx_11_0_arm64.whl", hash = "sha256:39db632f6bb862eeccf56660871433e111b6ea58f2caea825571951d4b6aa3da", size = 239535 },
    { url = "https://files.pythonhosted.org/packages/2a/ed/d362e84620dd22876b55389248e522338ed1bf134a5edd3b8231d7207f6d/psutil-7.0.0-cp36-abi3-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1fcee592b4c6f146991ca55919ea3d1f8926497a713ed7faaf8225e174581e91", size = 275004 },
    { url = "https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b1388a4f6875d7e2aff5c4ca1cc16c545ed41dd8bb596cefea80111db353a34", size = 277986 },
    { url = "https://files.pythonhosted.org/packages/eb/a2/709e0fe2f093556c17fbafda93ac032257242cabcc7ff3369e2cb76a97aa/psutil-7.0.0-cp36-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5f098451abc2828f7dc6b58d44b532b22f2088f4999a937557b603ce72b1993", size = 279544 },
    { url = "https://files.pythonhosted.org/packages/50/e6/eecf58810b9d12e6427369784efe814a1eec0f492084ce8eb8f4d89d6d61/psutil-7.0.0-cp37-abi3-win32.whl", hash = "sha256:ba3fcef7523064a6c9da440fc4d6bd07da93ac726b5733c29027d7dc95b39d99", size = 241053 },
    { url = "https://files.pythonhosted.org/packages/50/1b/6921afe68c74868b4c9fa424dad3be35b095e16687989ebbb50ce4fceb7c/psutil-7.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:4cf3d4eb1aa9b348dec30105c55cd9b7d4629285735a102beb4441e38db90553", size = 244885 },
]

[[package]]
name = "ptyprocess"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/20/e5/16ff212c1e452235a90aeb09066144d0c5a6a8c0834397e03f5224495c4e/ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220", size = 70762 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/22/a6/858897256d0deac81a172289110f31629fc4cee19b6f01283303e18c8db3/ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35", size = 13993 },
]

[[package]]
name = "pure-eval"
version = "0.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/05/0a34433a064256a578f1783a10da6df098ceaa4a57bbeaa96a6c0352786b/pure_eval-0.2.3.tar.gz", hash = "sha256:5f4e983f40564c576c7c8635ae88db5956bb2229d7e9237d03b3c0b0190eaf42", size = 19752 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8e/37/efad0257dc6e593a18957422533ff0f87ede7c9c6ea010a2177d738fb82f/pure_eval-0.2.3-py3-none-any.whl", hash = "sha256:1db8e35b67b3d218d818ae653e27f06c3aa420901fa7b081ca98cbedc874e0d0", size = 11842 },
]

[[package]]
name = "pyasn1"
version = "0.6.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ba/e9/01f1a64245b89f039897cb0130016d79f77d52669aae6ee7b159a6c4c018/pyasn1-0.6.1.tar.gz", hash = "sha256:6f580d2bdd84365380830acf45550f2511469f673cb4a5ae3857a3170128b034", size = 145322 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl", hash = "sha256:0d632f46f2ba09143da3a8afe9e33fb6f92fa2320ab7e886e2d0f7672af84629", size = 83135 },
]

[[package]]
name = "pyasn1-modules"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e9/e6/78ebbb10a8c8e4b61a59249394a4a594c1a7af95593dc933a349c8d00964/pyasn1_modules-0.4.2.tar.gz", hash = "sha256:677091de870a80aae844b1ca6134f54652fa2c8c5a52aa396440ac3106e941e6", size = 307892 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl", hash = "sha256:29253a9207ce32b64c3ac6600edc75368f98473906e8fd1043bd6b5b1de2c14a", size = 181259 },
]

[[package]]
name = "pydantic"
version = "2.11.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/77/ab/5250d56ad03884ab5efd07f734203943c8a8ab40d551e208af81d0257bf2/pydantic-2.11.4.tar.gz", hash = "sha256:32738d19d63a226a52eed76645a98ee07c1f410ee41d93b4afbfa85ed8111c2d", size = 786540 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl", hash = "sha256:d9615eaa9ac5a063471da949c8fc16376a84afb5024688b3ff885693506764eb", size = 443900 },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688 },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808 },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580 },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859 },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810 },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498 },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611 },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924 },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196 },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389 },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223 },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473 },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269 },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921 },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162 },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560 },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777 },
]

[[package]]
name = "pydantic-settings"
version = "2.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/67/1d/42628a2c33e93f8e9acbde0d5d735fa0850f3e6a2f8cb1eb6c40b9a732ac/pydantic_settings-2.9.1.tar.gz", hash = "sha256:c509bf79d27563add44e8446233359004ed85066cd096d8b510f715e6ef5d268", size = 163234 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl", hash = "sha256:59b4f431b1defb26fe620c71a7d3968a710d719f5f4cdbbdb7926edeb770f6ef", size = 44356 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pymdown-extensions"
version = "10.16"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown" },
    { name = "pyyaml" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/0a/c06b542ac108bfc73200677309cd9188a3a01b127a63f20cadc18d873d88/pymdown_extensions-10.16.tar.gz", hash = "sha256:71dac4fca63fabeffd3eb9038b756161a33ec6e8d230853d3cecf562155ab3de", size = 853197 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/d4/10bb14004d3c792811e05e21b5e5dcae805aacb739bd12a0540967b99592/pymdown_extensions-10.16-py3-none-any.whl", hash = "sha256:f5dd064a4db588cb2d95229fc4ee63a1b16cc8b4d0e6145c0899ed8723da1df2", size = 266143 },
]

[[package]]
name = "pyparsing"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bb/22/f1129e69d94ffff626bdb5c835506b3a5b4f3d070f17ea295e12c2c6f60f/pyparsing-3.2.3.tar.gz", hash = "sha256:b9c13f1ab8b3b542f72e28f634bad4de758ab3ce4546e4301970ad6fa77c38be", size = 1088608 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/e7/df2285f3d08fee213f2d041540fa4fc9ca6c2d44cf36d3a035bf2a8d2bcc/pyparsing-3.2.3-py3-none-any.whl", hash = "sha256:a749938e02d6fd0b59b356ca504a24982314bb090c383e3cf201c95ef7e2bfcf", size = 111120 },
]

[[package]]
name = "pyperclip"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/30/23/2f0a3efc4d6a32f3b63cdff36cd398d9701d26cda58e3ab97ac79fb5e60d/pyperclip-1.9.0.tar.gz", hash = "sha256:b7de0142ddc81bfc5c7507eea19da920b92252b548b96186caf94a5e2527d310", size = 20961 }

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "pytest-asyncio"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d0/d4/14f53324cb1a6381bef29d698987625d80052bb33932d8e7cbf9b337b17c/pytest_asyncio-1.0.0.tar.gz", hash = "sha256:d15463d13f4456e1ead2594520216b225a16f781e144f8fdf6c5bb4667c48b3f", size = 46960 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/05/ce271016e351fddc8399e546f6e23761967ee09c8c568bbfbecb0c150171/pytest_asyncio-1.0.0-py3-none-any.whl", hash = "sha256:4f024da9f1ef945e680dc68610b52550e36590a67fd31bb3b4943979a1f90ef3", size = 15976 },
]

[[package]]
name = "pytest-xdist"
version = "3.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "execnet" },
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/dc/865845cfe987b21658e871d16e0a24e871e00884c545f246dd8f6f69edda/pytest_xdist-3.7.0.tar.gz", hash = "sha256:f9248c99a7c15b7d2f90715df93610353a485827bc06eefb6566d23f6400f126", size = 87550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/b2/0e802fde6f1c5b2f7ae7e9ad42b83fd4ecebac18a8a8c2f2f14e39dce6e1/pytest_xdist-3.7.0-py3-none-any.whl", hash = "sha256:7d3fbd255998265052435eb9daa4e99b62e6fb9cfb6efd1f858d4d8c0c7f0ca0", size = 46142 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "python-dotenv"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/88/2c/7bb1416c5620485aa793f2de31d3df393d3686aa8a8506d11e10e13c5baf/python_dotenv-1.1.0.tar.gz", hash = "sha256:41f90bc6f5f177fb41f53e87666db362025010eb28f60a01c9143bfa33a2b2d5", size = 39920 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl", hash = "sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d", size = 20256 },
]

[[package]]
name = "python-json-logger"
version = "3.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz", hash = "sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84", size = 16642 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl", hash = "sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7", size = 15163 },
]

[[package]]
name = "python-multipart"
version = "0.0.20"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f3/87/f44d7c9f274c7ee665a29b885ec97089ec5dc034c7f3fafa03da9e39a09e/python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13", size = 37158 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104", size = 24546 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "readabilipy"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "html5lib" },
    { name = "lxml" },
    { name = "regex" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b8/e4/260a202516886c2e0cc6e6ae96d1f491792d829098886d9529a2439fbe8e/readabilipy-0.3.0.tar.gz", hash = "sha256:e13313771216953935ac031db4234bdb9725413534bfb3c19dbd6caab0887ae0", size = 35491 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dd/46/8a640c6de1a6c6af971f858b2fb178ca5e1db91f223d8ba5f40efe1491e5/readabilipy-0.3.0-py3-none-any.whl", hash = "sha256:d106da0fad11d5fdfcde21f5c5385556bfa8ff0258483037d39ea6b1d6db3943", size = 22158 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "rich"
version = "14.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz", hash = "sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725", size = 224078 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl", hash = "sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0", size = 243229 },
]

[[package]]
name = "rsa"
version = "4.9.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyasn1" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/8a/22b7beea3ee0d44b1916c0c1cb0ee3af23b700b6da9f04991899d0c555d4/rsa-4.9.1.tar.gz", hash = "sha256:e7bdbfdb5497da4c07dfd35530e1a902659db6ff241e39d9953cad06ebd0ae75", size = 29034 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl", hash = "sha256:68635866661c6836b8d39430f97a996acbd61bfa49406748ea243539fe239762", size = 34696 },
]

[[package]]
name = "ruff"
version = "0.11.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ed/da/9c6f995903b4d9474b39da91d2d626659af3ff1eeb43e9ae7c119349dba6/ruff-0.11.13.tar.gz", hash = "sha256:26fa247dc68d1d4e72c179e08889a25ac0c7ba4d78aecfc835d49cbfd60bf514", size = 4282054 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7d/ce/a11d381192966e0b4290842cc8d4fac7dc9214ddf627c11c1afff87da29b/ruff-0.11.13-py3-none-linux_armv6l.whl", hash = "sha256:4bdfbf1240533f40042ec00c9e09a3aade6f8c10b6414cf11b519488d2635d46", size = 10292516 },
    { url = "https://files.pythonhosted.org/packages/78/db/87c3b59b0d4e753e40b6a3b4a2642dfd1dcaefbff121ddc64d6c8b47ba00/ruff-0.11.13-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:aef9c9ed1b5ca28bb15c7eac83b8670cf3b20b478195bd49c8d756ba0a36cf48", size = 11106083 },
    { url = "https://files.pythonhosted.org/packages/77/79/d8cec175856ff810a19825d09ce700265f905c643c69f45d2b737e4a470a/ruff-0.11.13-py3-none-macosx_11_0_arm64.whl", hash = "sha256:53b15a9dfdce029c842e9a5aebc3855e9ab7771395979ff85b7c1dedb53ddc2b", size = 10436024 },
    { url = "https://files.pythonhosted.org/packages/8b/5b/f6d94f2980fa1ee854b41568368a2e1252681b9238ab2895e133d303538f/ruff-0.11.13-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ab153241400789138d13f362c43f7edecc0edfffce2afa6a68434000ecd8f69a", size = 10646324 },
    { url = "https://files.pythonhosted.org/packages/6c/9c/b4c2acf24ea4426016d511dfdc787f4ce1ceb835f3c5fbdbcb32b1c63bda/ruff-0.11.13-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:6c51f93029d54a910d3d24f7dd0bb909e31b6cd989a5e4ac513f4eb41629f0dc", size = 10174416 },
    { url = "https://files.pythonhosted.org/packages/f3/10/e2e62f77c65ede8cd032c2ca39c41f48feabedb6e282bfd6073d81bb671d/ruff-0.11.13-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1808b3ed53e1a777c2ef733aca9051dc9bf7c99b26ece15cb59a0320fbdbd629", size = 11724197 },
    { url = "https://files.pythonhosted.org/packages/bb/f0/466fe8469b85c561e081d798c45f8a1d21e0b4a5ef795a1d7f1a9a9ec182/ruff-0.11.13-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:d28ce58b5ecf0f43c1b71edffabe6ed7f245d5336b17805803312ec9bc665933", size = 12511615 },
    { url = "https://files.pythonhosted.org/packages/17/0e/cefe778b46dbd0cbcb03a839946c8f80a06f7968eb298aa4d1a4293f3448/ruff-0.11.13-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:55e4bc3a77842da33c16d55b32c6cac1ec5fb0fbec9c8c513bdce76c4f922165", size = 12117080 },
    { url = "https://files.pythonhosted.org/packages/5d/2c/caaeda564cbe103bed145ea557cb86795b18651b0f6b3ff6a10e84e5a33f/ruff-0.11.13-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:633bf2c6f35678c56ec73189ba6fa19ff1c5e4807a78bf60ef487b9dd272cc71", size = 11326315 },
    { url = "https://files.pythonhosted.org/packages/75/f0/782e7d681d660eda8c536962920c41309e6dd4ebcea9a2714ed5127d44bd/ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ffbc82d70424b275b089166310448051afdc6e914fdab90e08df66c43bb5ca9", size = 11555640 },
    { url = "https://files.pythonhosted.org/packages/5d/d4/3d580c616316c7f07fb3c99dbecfe01fbaea7b6fd9a82b801e72e5de742a/ruff-0.11.13-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:4a9ddd3ec62a9a89578c85842b836e4ac832d4a2e0bfaad3b02243f930ceafcc", size = 10507364 },
    { url = "https://files.pythonhosted.org/packages/5a/dc/195e6f17d7b3ea6b12dc4f3e9de575db7983db187c378d44606e5d503319/ruff-0.11.13-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:d237a496e0778d719efb05058c64d28b757c77824e04ffe8796c7436e26712b7", size = 10141462 },
    { url = "https://files.pythonhosted.org/packages/f4/8e/39a094af6967faa57ecdeacb91bedfb232474ff8c3d20f16a5514e6b3534/ruff-0.11.13-py3-none-musllinux_1_2_i686.whl", hash = "sha256:26816a218ca6ef02142343fd24c70f7cd8c5aa6c203bca284407adf675984432", size = 11121028 },
    { url = "https://files.pythonhosted.org/packages/5a/c0/b0b508193b0e8a1654ec683ebab18d309861f8bd64e3a2f9648b80d392cb/ruff-0.11.13-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:51c3f95abd9331dc5b87c47ac7f376db5616041173826dfd556cfe3d4977f492", size = 11602992 },
    { url = "https://files.pythonhosted.org/packages/7c/91/263e33ab93ab09ca06ce4f8f8547a858cc198072f873ebc9be7466790bae/ruff-0.11.13-py3-none-win32.whl", hash = "sha256:96c27935418e4e8e77a26bb05962817f28b8ef3843a6c6cc49d8783b5507f250", size = 10474944 },
    { url = "https://files.pythonhosted.org/packages/46/f4/7c27734ac2073aae8efb0119cae6931b6fb48017adf048fdf85c19337afc/ruff-0.11.13-py3-none-win_amd64.whl", hash = "sha256:29c3189895a8a6a657b7af4e97d330c8a3afd2c9c8f46c81e2fc5a31866517e3", size = 11548669 },
    { url = "https://files.pythonhosted.org/packages/ec/bf/b273dd11673fed8a6bd46032c0ea2a04b2ac9bfa9c628756a5856ba113b0/ruff-0.11.13-py3-none-win_arm64.whl", hash = "sha256:b4385285e9179d608ff1d2fb9922062663c658605819a6876d8beef0c30b7f3b", size = 10683928 },
]

[[package]]
name = "sentencepiece"
version = "0.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/d2/b9c7ca067c26d8ff085d252c89b5f69609ca93fb85a00ede95f4857865d4/sentencepiece-0.2.0.tar.gz", hash = "sha256:a52c19171daaf2e697dc6cbe67684e0fa341b1248966f6aebb541de654d15843", size = 2632106 }

[[package]]
name = "shapely"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fb/fe/3b0d2f828ffaceadcdcb51b75b9c62d98e62dd95ce575278de35f24a1c20/shapely-2.1.0.tar.gz", hash = "sha256:2cbe90e86fa8fc3ca8af6ffb00a77b246b918c7cf28677b7c21489b678f6b02e", size = 313617 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8d/77/4e368704b2193e74498473db4461d697cc6083c96f8039367e59009d78bd/shapely-2.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b64423295b563f43a043eb786e7a03200ebe68698e36d2b4b1c39f31dfb50dfb", size = 1830029 },
    { url = "https://files.pythonhosted.org/packages/71/3c/d888597bda680e4de987316b05ca9db07416fa29523beff64f846503302f/shapely-2.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1b5578f45adc25b235b22d1ccb9a0348c8dc36f31983e57ea129a88f96f7b870", size = 1637999 },
    { url = "https://files.pythonhosted.org/packages/03/8d/ee0e23b7ef88fba353c63a81f1f329c77f5703835db7b165e7c0b8b7f839/shapely-2.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1a7e83d383b27f02b684e50ab7f34e511c92e33b6ca164a6a9065705dd64bcb", size = 2929348 },
    { url = "https://files.pythonhosted.org/packages/d1/a7/5c9cb413e4e2ce52c16be717e94abd40ce91b1f8974624d5d56154c5d40b/shapely-2.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:942031eb4d8f7b3b22f43ba42c09c7aa3d843aa10d5cc1619fe816e923b66e55", size = 3048973 },
    { url = "https://files.pythonhosted.org/packages/84/23/45b90c0bd2157b238490ca56ef2eedf959d3514c7d05475f497a2c88b6d9/shapely-2.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:d2843c456a2e5627ee6271800f07277c0d2652fb287bf66464571a057dbc00b3", size = 3873148 },
    { url = "https://files.pythonhosted.org/packages/c0/bc/ed7d5d37f5395166042576f0c55a12d7e56102799464ba7ea3a72a38c769/shapely-2.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8c4b17469b7f39a5e6a7cfea79f38ae08a275427f41fe8b48c372e1449147908", size = 4052655 },
    { url = "https://files.pythonhosted.org/packages/c0/8f/a1dafbb10d20d1c569f2db3fb1235488f624dafe8469e8ce65356800ba31/shapely-2.1.0-cp313-cp313-win32.whl", hash = "sha256:30e967abd08fce49513d4187c01b19f139084019f33bec0673e8dbeb557c45e4", size = 1526600 },
    { url = "https://files.pythonhosted.org/packages/e3/f0/9f8cdf2258d7aed742459cea51c70d184de92f5d2d6f5f7f1ded90a18c31/shapely-2.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:1dc8d4364483a14aba4c844b7bd16a6fa3728887e2c33dfa1afa34a3cf4d08a5", size = 1707115 },
    { url = "https://files.pythonhosted.org/packages/75/ed/32952df461753a65b3e5d24c8efb361d3a80aafaef0b70d419063f6f2c11/shapely-2.1.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:673e073fea099d1c82f666fb7ab0a00a77eff2999130a69357ce11941260d855", size = 1824847 },
    { url = "https://files.pythonhosted.org/packages/ff/b9/2284de512af30b02f93ddcdd2e5c79834a3cf47fa3ca11b0f74396feb046/shapely-2.1.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:6d1513f915a56de67659fe2047c1ad5ff0f8cbff3519d1e74fced69c9cb0e7da", size = 1631035 },
    { url = "https://files.pythonhosted.org/packages/35/16/a59f252a7e736b73008f10d0950ffeeb0d5953be7c0bdffd39a02a6ba310/shapely-2.1.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0d6a7043178890b9e028d80496ff4c79dc7629bff4d78a2f25323b661756bab8", size = 2968639 },
    { url = "https://files.pythonhosted.org/packages/a5/0a/6a20eca7b0092cfa243117e8e145a58631a4833a0a519ec9b445172e83a0/shapely-2.1.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb638378dc3d76f7e85b67d7e2bb1366811912430ac9247ac00c127c2b444cdc", size = 3055713 },
    { url = "https://files.pythonhosted.org/packages/fb/44/eeb0c7583b1453d1cf7a319a1d738e08f98a5dc993fa1ef3c372983e4cb5/shapely-2.1.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:737124e87d91d616acf9a911f74ac55e05db02a43a6a7245b3d663817b876055", size = 3890478 },
    { url = "https://files.pythonhosted.org/packages/5d/6e/37ff3c6af1d408cacb0a7d7bfea7b8ab163a5486e35acb08997eae9d8756/shapely-2.1.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8e6c229e7bb87aae5df82fa00b6718987a43ec168cc5affe095cca59d233f314", size = 4036148 },
    { url = "https://files.pythonhosted.org/packages/c8/6a/8c0b7de3aeb5014a23f06c5e9d3c7852ebcf0d6b00fe660b93261e310e24/shapely-2.1.0-cp313-cp313t-win32.whl", hash = "sha256:a9580bda119b1f42f955aa8e52382d5c73f7957e0203bc0c0c60084846f3db94", size = 1535993 },
    { url = "https://files.pythonhosted.org/packages/a8/91/ae80359a58409d52e4d62c7eacc7eb3ddee4b9135f1db884b6a43cf2e174/shapely-2.1.0-cp313-cp313t-win_amd64.whl", hash = "sha256:e8ff4e5cfd799ba5b6f37b5d5527dbd85b4a47c65b6d459a03d0962d2a9d4d10", size = 1717777 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677 },
]

[[package]]
name = "sqlparse"
version = "0.5.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e5/40/edede8dd6977b0d3da179a342c198ed100dd2aba4be081861ee5911e4da4/sqlparse-0.5.3.tar.gz", hash = "sha256:09f67787f56a0b16ecdbde1bfc7f5d9c3371ca683cfeaa8e6ff60b4807ec9272", size = 84999 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/5c/bfd6bd0bf979426d405cc6e71eceb8701b148b16c21d2dc3c261efc61c7b/sqlparse-0.5.3-py3-none-any.whl", hash = "sha256:cf2196ed3418f3ba5de6af7e82c694a9fbdbfecccdfc72e281548517081f16ca", size = 44415 },
]

[[package]]
name = "sse-starlette"
version = "2.3.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/35/7d8d94eb0474352d55f60f80ebc30f7e59441a29e18886a6425f0bccd0d3/sse_starlette-2.3.3.tar.gz", hash = "sha256:fdd47c254aad42907cfd5c5b83e2282be15be6c51197bf1a9b70b8e990522072", size = 17499 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/20/52fdb5ebb158294b0adb5662235dd396fc7e47aa31c293978d8d8942095a/sse_starlette-2.3.3-py3-none-any.whl", hash = "sha256:8b0a0ced04a329ff7341b01007580dd8cf71331cc21c0ccea677d500618da1e0", size = 10235 },
]

[[package]]
name = "stack-data"
version = "0.6.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "asttokens" },
    { name = "executing" },
    { name = "pure-eval" },
]
sdist = { url = "https://files.pythonhosted.org/packages/28/e3/55dcc2cfbc3ca9c29519eb6884dd1415ecb53b0e934862d3559ddcb7e20b/stack_data-0.6.3.tar.gz", hash = "sha256:836a778de4fec4dcd1dcd89ed8abff8a221f58308462e1c4aa2a3cf30148f0b9", size = 44707 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f1/7b/ce1eafaf1a76852e2ec9b22edecf1daa58175c090266e9f6c64afcd81d91/stack_data-0.6.3-py3-none-any.whl", hash = "sha256:d5558e0c25a4cb0853cddad3d77da9891a08cb85dd9f9f91b9f8cd66e511e695", size = 24521 },
]

[[package]]
name = "starlette"
version = "0.46.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ce/20/08dfcd9c983f6a6f4a1000d934b9e6d626cff8d2eeb77a89a68eef20a2b7/starlette-0.46.2.tar.gz", hash = "sha256:7f7361f34eed179294600af672f565727419830b54b7b084efe44bb82d2fccd5", size = 2580846 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/0c/9d30a4ebeb6db2b25a841afbb80f6ef9a854fc3b41be131d249a977b4959/starlette-0.46.2-py3-none-any.whl", hash = "sha256:595633ce89f8ffa71a015caed34a5b2dc1c0cdb3f0f1fbd1e69339cf2abeec35", size = 72037 },
]

[[package]]
name = "tabulate"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ec/fe/802052aecb21e3797b8f7902564ab6ea0d60ff8ca23952079064155d1ae1/tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c", size = 81090 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f", size = 35252 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tomlkit"
version = "0.13.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/18/0bbf3884e9eaa38819ebe46a7bd25dcd56b67434402b66a58c4b8e552575/tomlkit-0.13.3.tar.gz", hash = "sha256:430cf247ee57df2b94ee3fbe588e71d362a941ebb545dec29b53961d61add2a1", size = 185207 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl", hash = "sha256:c89c649d79ee40629a9fda55f8ace8c6a1b42deb912b2a8fd8d942ddadb606b0", size = 38901 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "traitlets"
version = "5.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/eb/79/72064e6a701c2183016abbbfedaba506d81e30e232a68c9f0d6f6fcd1574/traitlets-5.14.3.tar.gz", hash = "sha256:9ed0579d3502c94b4b3732ac120375cda96f923114522847de4b3bb98b96b6b7", size = 161621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/c0/8f5d070730d7836adc9c9b6408dec68c6ced86b304a9b26a14df072a6e8c/traitlets-5.14.3-py3-none-any.whl", hash = "sha256:b74e89e397b1ed28cc831db7aea759ba6640cb3de13090ca145426688ff1ac4f", size = 85359 },
]

[[package]]
name = "typer"
version = "0.15.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/1a/5f36851f439884bcfe8539f6a20ff7516e7b60f319bbaf69a90dc35cc2eb/typer-0.15.3.tar.gz", hash = "sha256:818873625d0569653438316567861899f7e9972f2e6e0c16dab608345ced713c", size = 101641 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/48/20/9d953de6f4367163d23ec823200eb3ecb0050a2609691e512c8b95827a9b/typer-0.15.3-py3-none-any.whl", hash = "sha256:c86a65ad77ca531f03de08d1b9cb67cd09ad02ddddf4b34745b5008f43b239bd", size = 45253 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "uritemplate"
version = "4.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/60/f174043244c5306c9988380d2cb10009f91563fc4b31293d27e17201af56/uritemplate-4.2.0.tar.gz", hash = "sha256:480c2ed180878955863323eea31b0ede668795de182617fef9c6ca09e6ec9d0e", size = 33267 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a9/99/3ae339466c9183ea5b8ae87b34c0b897eda475d2aec2307cae60e5cd4f29/uritemplate-4.2.0-py3-none-any.whl", hash = "sha256:962201ba1c4edcab02e60f9a0d3821e82dfc5d2d6662a21abd533879bdb8a686", size = 11488 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "uvicorn"
version = "0.34.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a6/ae/9bbb19b9e1c450cf9ecaef06463e40234d98d95bf572fab11b4f19ae5ded/uvicorn-0.34.2.tar.gz", hash = "sha256:0e929828f6186353a80b58ea719861d2629d766293b6d19baf086ba31d4f3328", size = 76815 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/4b/4cef6ce21a2aaca9d852a6e84ef4f135d99fcd74fa75105e2fc0c8308acd/uvicorn-0.34.2-py3-none-any.whl", hash = "sha256:deb49af569084536d269fe0a6d67e3754f104cf03aba7c11c40f01aadf33c403", size = 62483 },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166 },
]

[[package]]
name = "webencodings"
version = "0.5.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/02/ae6ceac1baeda530866a85075641cec12989bd8d31af6d5ab4a3e8c92f47/webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923", size = 9721 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78", size = 11774 },
]

[[package]]
name = "websockets"
version = "15.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/e6/26d09fab466b7ca9c7737474c52be4f76a40301b08362eb2dbc19dcc16c1/websockets-15.0.1.tar.gz", hash = "sha256:82544de02076bafba038ce055ee6412d68da13ab47f0c60cab827346de828dee", size = 177016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/9f/51f0cf64471a9d2b4d0fc6c534f323b664e7095640c34562f5182e5a7195/websockets-15.0.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:ee443ef070bb3b6ed74514f5efaa37a252af57c90eb33b956d35c8e9c10a1931", size = 175440 },
    { url = "https://files.pythonhosted.org/packages/8a/05/aa116ec9943c718905997412c5989f7ed671bc0188ee2ba89520e8765d7b/websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:5a939de6b7b4e18ca683218320fc67ea886038265fd1ed30173f5ce3f8e85675", size = 173098 },
    { url = "https://files.pythonhosted.org/packages/ff/0b/33cef55ff24f2d92924923c99926dcce78e7bd922d649467f0eda8368923/websockets-15.0.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:746ee8dba912cd6fc889a8147168991d50ed70447bf18bcda7039f7d2e3d9151", size = 173329 },
    { url = "https://files.pythonhosted.org/packages/31/1d/063b25dcc01faa8fada1469bdf769de3768b7044eac9d41f734fd7b6ad6d/websockets-15.0.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:595b6c3969023ecf9041b2936ac3827e4623bfa3ccf007575f04c5a6aa318c22", size = 183111 },
    { url = "https://files.pythonhosted.org/packages/93/53/9a87ee494a51bf63e4ec9241c1ccc4f7c2f45fff85d5bde2ff74fcb68b9e/websockets-15.0.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3c714d2fc58b5ca3e285461a4cc0c9a66bd0e24c5da9911e30158286c9b5be7f", size = 182054 },
    { url = "https://files.pythonhosted.org/packages/ff/b2/83a6ddf56cdcbad4e3d841fcc55d6ba7d19aeb89c50f24dd7e859ec0805f/websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f3c1e2ab208db911594ae5b4f79addeb3501604a165019dd221c0bdcabe4db8", size = 182496 },
    { url = "https://files.pythonhosted.org/packages/98/41/e7038944ed0abf34c45aa4635ba28136f06052e08fc2168520bb8b25149f/websockets-15.0.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:229cf1d3ca6c1804400b0a9790dc66528e08a6a1feec0d5040e8b9eb14422375", size = 182829 },
    { url = "https://files.pythonhosted.org/packages/e0/17/de15b6158680c7623c6ef0db361da965ab25d813ae54fcfeae2e5b9ef910/websockets-15.0.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:756c56e867a90fb00177d530dca4b097dd753cde348448a1012ed6c5131f8b7d", size = 182217 },
    { url = "https://files.pythonhosted.org/packages/33/2b/1f168cb6041853eef0362fb9554c3824367c5560cbdaad89ac40f8c2edfc/websockets-15.0.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:558d023b3df0bffe50a04e710bc87742de35060580a293c2a984299ed83bc4e4", size = 182195 },
    { url = "https://files.pythonhosted.org/packages/86/eb/20b6cdf273913d0ad05a6a14aed4b9a85591c18a987a3d47f20fa13dcc47/websockets-15.0.1-cp313-cp313-win32.whl", hash = "sha256:ba9e56e8ceeeedb2e080147ba85ffcd5cd0711b89576b83784d8605a7df455fa", size = 176393 },
    { url = "https://files.pythonhosted.org/packages/1b/6c/c65773d6cab416a64d191d6ee8a8b1c68a09970ea6909d16965d26bfed1e/websockets-15.0.1-cp313-cp313-win_amd64.whl", hash = "sha256:e09473f095a819042ecb2ab9465aee615bd9c2028e4ef7d933600a8401c79561", size = 176837 },
    { url = "https://files.pythonhosted.org/packages/fa/a8/5b41e0da817d64113292ab1f8247140aac61cbf6cfd085d6a0fa77f4984f/websockets-15.0.1-py3-none-any.whl", hash = "sha256:f7a866fbc1e97b5c617ee4116daaa09b722101d4a3c170c787450ba409f9736f", size = 169743 },
]

[[package]]
name = "yarl"
version = "1.20.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/62/51/c0edba5219027f6eab262e139f73e2417b0f4efffa23bf562f6e18f76ca5/yarl-1.20.0.tar.gz", hash = "sha256:686d51e51ee5dfe62dec86e4866ee0e9ed66df700d55c828a615640adc885307", size = 185258 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0f/6f/514c9bff2900c22a4f10e06297714dbaf98707143b37ff0bcba65a956221/yarl-1.20.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:2137810a20b933b1b1b7e5cf06a64c3ed3b4747b0e5d79c9447c00db0e2f752f", size = 145030 },
    { url = "https://files.pythonhosted.org/packages/4e/9d/f88da3fa319b8c9c813389bfb3463e8d777c62654c7168e580a13fadff05/yarl-1.20.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:447c5eadd750db8389804030d15f43d30435ed47af1313303ed82a62388176d3", size = 96894 },
    { url = "https://files.pythonhosted.org/packages/cd/57/92e83538580a6968b2451d6c89c5579938a7309d4785748e8ad42ddafdce/yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:42fbe577272c203528d402eec8bf4b2d14fd49ecfec92272334270b850e9cd7d", size = 94457 },
    { url = "https://files.pythonhosted.org/packages/e9/ee/7ee43bd4cf82dddd5da97fcaddb6fa541ab81f3ed564c42f146c83ae17ce/yarl-1.20.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:18e321617de4ab170226cd15006a565d0fa0d908f11f724a2c9142d6b2812ab0", size = 343070 },
    { url = "https://files.pythonhosted.org/packages/4a/12/b5eccd1109e2097bcc494ba7dc5de156e41cf8309fab437ebb7c2b296ce3/yarl-1.20.0-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:4345f58719825bba29895011e8e3b545e6e00257abb984f9f27fe923afca2501", size = 337739 },
    { url = "https://files.pythonhosted.org/packages/7d/6b/0eade8e49af9fc2585552f63c76fa59ef469c724cc05b29519b19aa3a6d5/yarl-1.20.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5d9b980d7234614bc4674468ab173ed77d678349c860c3af83b1fffb6a837ddc", size = 351338 },
    { url = "https://files.pythonhosted.org/packages/45/cb/aaaa75d30087b5183c7b8a07b4fb16ae0682dd149a1719b3a28f54061754/yarl-1.20.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af4baa8a445977831cbaa91a9a84cc09debb10bc8391f128da2f7bd070fc351d", size = 353636 },
    { url = "https://files.pythonhosted.org/packages/98/9d/d9cb39ec68a91ba6e66fa86d97003f58570327d6713833edf7ad6ce9dde5/yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:123393db7420e71d6ce40d24885a9e65eb1edefc7a5228db2d62bcab3386a5c0", size = 348061 },
    { url = "https://files.pythonhosted.org/packages/72/6b/103940aae893d0cc770b4c36ce80e2ed86fcb863d48ea80a752b8bda9303/yarl-1.20.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ab47acc9332f3de1b39e9b702d9c916af7f02656b2a86a474d9db4e53ef8fd7a", size = 334150 },
    { url = "https://files.pythonhosted.org/packages/ef/b2/986bd82aa222c3e6b211a69c9081ba46484cffa9fab2a5235e8d18ca7a27/yarl-1.20.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4a34c52ed158f89876cba9c600b2c964dfc1ca52ba7b3ab6deb722d1d8be6df2", size = 362207 },
    { url = "https://files.pythonhosted.org/packages/14/7c/63f5922437b873795d9422cbe7eb2509d4b540c37ae5548a4bb68fd2c546/yarl-1.20.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:04d8cfb12714158abf2618f792c77bc5c3d8c5f37353e79509608be4f18705c9", size = 361277 },
    { url = "https://files.pythonhosted.org/packages/81/83/450938cccf732466953406570bdb42c62b5ffb0ac7ac75a1f267773ab5c8/yarl-1.20.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7dc63ad0d541c38b6ae2255aaa794434293964677d5c1ec5d0116b0e308031f5", size = 364990 },
    { url = "https://files.pythonhosted.org/packages/b4/de/af47d3a47e4a833693b9ec8e87debb20f09d9fdc9139b207b09a3e6cbd5a/yarl-1.20.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d02b591a64e4e6ca18c5e3d925f11b559c763b950184a64cf47d74d7e41877", size = 374684 },
    { url = "https://files.pythonhosted.org/packages/62/0b/078bcc2d539f1faffdc7d32cb29a2d7caa65f1a6f7e40795d8485db21851/yarl-1.20.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:95fc9876f917cac7f757df80a5dda9de59d423568460fe75d128c813b9af558e", size = 382599 },
    { url = "https://files.pythonhosted.org/packages/74/a9/4fdb1a7899f1fb47fd1371e7ba9e94bff73439ce87099d5dd26d285fffe0/yarl-1.20.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bb769ae5760cd1c6a712135ee7915f9d43f11d9ef769cb3f75a23e398a92d384", size = 378573 },
    { url = "https://files.pythonhosted.org/packages/fd/be/29f5156b7a319e4d2e5b51ce622b4dfb3aa8d8204cd2a8a339340fbfad40/yarl-1.20.0-cp313-cp313-win32.whl", hash = "sha256:70e0c580a0292c7414a1cead1e076c9786f685c1fc4757573d2967689b370e62", size = 86051 },
    { url = "https://files.pythonhosted.org/packages/52/56/05fa52c32c301da77ec0b5f63d2d9605946fe29defacb2a7ebd473c23b81/yarl-1.20.0-cp313-cp313-win_amd64.whl", hash = "sha256:4c43030e4b0af775a85be1fa0433119b1565673266a70bf87ef68a9d5ba3174c", size = 92742 },
    { url = "https://files.pythonhosted.org/packages/d4/2f/422546794196519152fc2e2f475f0e1d4d094a11995c81a465faf5673ffd/yarl-1.20.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:b6c4c3d0d6a0ae9b281e492b1465c72de433b782e6b5001c8e7249e085b69051", size = 163575 },
    { url = "https://files.pythonhosted.org/packages/90/fc/67c64ddab6c0b4a169d03c637fb2d2a212b536e1989dec8e7e2c92211b7f/yarl-1.20.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8681700f4e4df891eafa4f69a439a6e7d480d64e52bf460918f58e443bd3da7d", size = 106121 },
    { url = "https://files.pythonhosted.org/packages/6d/00/29366b9eba7b6f6baed7d749f12add209b987c4cfbfa418404dbadc0f97c/yarl-1.20.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:84aeb556cb06c00652dbf87c17838eb6d92cfd317799a8092cee0e570ee11229", size = 103815 },
    { url = "https://files.pythonhosted.org/packages/28/f4/a2a4c967c8323c03689383dff73396281ced3b35d0ed140580825c826af7/yarl-1.20.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f166eafa78810ddb383e930d62e623d288fb04ec566d1b4790099ae0f31485f1", size = 408231 },
    { url = "https://files.pythonhosted.org/packages/0f/a1/66f7ffc0915877d726b70cc7a896ac30b6ac5d1d2760613603b022173635/yarl-1.20.0-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:5d3d6d14754aefc7a458261027a562f024d4f6b8a798adb472277f675857b1eb", size = 390221 },
    { url = "https://files.pythonhosted.org/packages/41/15/cc248f0504610283271615e85bf38bc014224122498c2016d13a3a1b8426/yarl-1.20.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2a8f64df8ed5d04c51260dbae3cc82e5649834eebea9eadfd829837b8093eb00", size = 411400 },
    { url = "https://files.pythonhosted.org/packages/5c/af/f0823d7e092bfb97d24fce6c7269d67fcd1aefade97d0a8189c4452e4d5e/yarl-1.20.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d9949eaf05b4d30e93e4034a7790634bbb41b8be2d07edd26754f2e38e491de", size = 411714 },
    { url = "https://files.pythonhosted.org/packages/83/70/be418329eae64b9f1b20ecdaac75d53aef098797d4c2299d82ae6f8e4663/yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c366b254082d21cc4f08f522ac201d0d83a8b8447ab562732931d31d80eb2a5", size = 404279 },
    { url = "https://files.pythonhosted.org/packages/19/f5/52e02f0075f65b4914eb890eea1ba97e6fd91dd821cc33a623aa707b2f67/yarl-1.20.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:91bc450c80a2e9685b10e34e41aef3d44ddf99b3a498717938926d05ca493f6a", size = 384044 },
    { url = "https://files.pythonhosted.org/packages/6a/36/b0fa25226b03d3f769c68d46170b3e92b00ab3853d73127273ba22474697/yarl-1.20.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9c2aa4387de4bc3a5fe158080757748d16567119bef215bec643716b4fbf53f9", size = 416236 },
    { url = "https://files.pythonhosted.org/packages/cb/3a/54c828dd35f6831dfdd5a79e6c6b4302ae2c5feca24232a83cb75132b205/yarl-1.20.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:d2cbca6760a541189cf87ee54ff891e1d9ea6406079c66341008f7ef6ab61145", size = 402034 },
    { url = "https://files.pythonhosted.org/packages/10/97/c7bf5fba488f7e049f9ad69c1b8fdfe3daa2e8916b3d321aa049e361a55a/yarl-1.20.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:798a5074e656f06b9fad1a162be5a32da45237ce19d07884d0b67a0aa9d5fdda", size = 407943 },
    { url = "https://files.pythonhosted.org/packages/fd/a4/022d2555c1e8fcff08ad7f0f43e4df3aba34f135bff04dd35d5526ce54ab/yarl-1.20.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:f106e75c454288472dbe615accef8248c686958c2e7dd3b8d8ee2669770d020f", size = 423058 },
    { url = "https://files.pythonhosted.org/packages/4c/f6/0873a05563e5df29ccf35345a6ae0ac9e66588b41fdb7043a65848f03139/yarl-1.20.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:3b60a86551669c23dc5445010534d2c5d8a4e012163218fc9114e857c0586fdd", size = 423792 },
    { url = "https://files.pythonhosted.org/packages/9e/35/43fbbd082708fa42e923f314c24f8277a28483d219e049552e5007a9aaca/yarl-1.20.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:3e429857e341d5e8e15806118e0294f8073ba9c4580637e59ab7b238afca836f", size = 422242 },
    { url = "https://files.pythonhosted.org/packages/ed/f7/f0f2500cf0c469beb2050b522c7815c575811627e6d3eb9ec7550ddd0bfe/yarl-1.20.0-cp313-cp313t-win32.whl", hash = "sha256:65a4053580fe88a63e8e4056b427224cd01edfb5f951498bfefca4052f0ce0ac", size = 93816 },
    { url = "https://files.pythonhosted.org/packages/3f/93/f73b61353b2a699d489e782c3f5998b59f974ec3156a2050a52dfd7e8946/yarl-1.20.0-cp313-cp313t-win_amd64.whl", hash = "sha256:53b2da3a6ca0a541c1ae799c349788d480e5144cac47dba0266c7cb6c76151fe", size = 101093 },
    { url = "https://files.pythonhosted.org/packages/ea/1f/70c57b3d7278e94ed22d85e09685d3f0a38ebdd8c5c73b65ba4c0d0fe002/yarl-1.20.0-py3-none-any.whl", hash = "sha256:5d0fe6af927a47a230f31e6004621fd0959eaa915fc62acfafa67ff7229a3124", size = 46124 },
]

[[package]]
name = "yoyo-migrations"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "importlib-metadata" },
    { name = "sqlparse" },
    { name = "tabulate" },
]
wheels = [
    { url = "https://files.pythonhosted.org/packages/8c/5d/9ef7f808ea955eca9f08043c65bdc81a4694e784c978b24ad72022974a97/yoyo_migrations-9.0.0-py3-none-any.whl", hash = "sha256:fc65d3a6d9449c1c54d64ff2ff98e32a27da356057c60e3471010bfb19ede081", size = 49002 },
]

[[package]]
name = "zipp"
version = "3.23.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e3/02/0f2892c661036d50ede074e376733dca2ae7c6eb617489437771209d4180/zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166", size = 25547 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2e/54/647ade08bf0db230bfea292f893923872fd20be6ac6f53b2b936ba839d75/zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e", size = 10276 },
]
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="prompt_service_notebook.py"&amp;amp;amp;gt;import marimo

__generated_with = "0.14.12"
app = marimo.App(width="medium")


@app.cell
def _():
    from repository.database import SQLite3Database
    from repository.prompt_service import PromptService
    from repository.prompt_models import (
        Prompt,
        PromptType,
        PromptPlanStatus,
        CmdCategory,
    )

    return (
        CmdCategory,
        Prompt,
        PromptPlanStatus,
        PromptService,
        PromptType,
        SQLite3Database,
    )


@app.cell
def _(PromptService, SQLite3Database):
    db = SQLite3Database("data/collect.db")
    with db.get_connection() as conn:
        ps = PromptService(conn)
    return (ps,)


@app.cell
def _(ps):
    cmds = ps.load_cmds_from_disk()
    plans = ps.load_plans_from_disk()
    return cmds, plans


@app.cell
def _(cmds):
    print(f"Num cmds: {len(cmds.loaded_prompts)}\n")
    for cmd in cmds.loaded_prompts:
        print(cmd.name)
    return


@app.cell
def _(plans):
    print(f"Num plans: {len(plans.loaded_prompts)}\n")
    for plan in plans.loaded_prompts:
        print(plan.name)
    return


@app.cell
def _():
    db_name = "collect_completed_add_claude_sdk_processing.md"
    result = db_name.split("_")
    print(result)
    return db_name, result


@app.cell
def _(result):
    print(f"project name: {result[0]}")
    print(f"plan status: {result[1]}")
    return


@app.cell
def _(result):
    namelist = result[2:]
    print(namelist)
    return (namelist,)


@app.cell
def _(namelist):
    newname = ""
    for word in namelist:
        if not word.endswith(".md"):
            newname = newname + word + "_"
        else:
            newname = newname + word
    print(newname)
    return


@app.cell
def _(db_name):
    print(db_name.split("_")[2:])
    return


@app.cell
def _(PromptType):
    def parse_db_name(db_name: str, prompt_type: PromptType) -&amp;amp;amp;amp;gt; str:
        ls = db_name.split("_")
        filename = ""
        if prompt_type == PromptType.PLAN:
            project = ls[0]
            plan_status = ls[1]
            print(f"project: {project}")
            print(f"plan status: {plan_status}")

            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

        if prompt_type == PromptType.CMD:
            cmd_dir = ls[0]
            print(f"cmd/dir: {cmd_dir}")
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            print(f"file name: {filename}")

            return filename

    return (parse_db_name,)


@app.cell
def _(PromptType, db_name, parse_db_name):
    parse_db_name(db_name, PromptType.PLAN)
    return


@app.cell
def _(PromptType, parse_db_name):
    parse_db_name("tools_create_database.md", PromptType.CMD)
    return


@app.cell
def _(CmdCategory, Prompt, PromptPlanStatus, PromptType, ps):
    def new_cmd_prompt(prompt_content: str) -&amp;amp;amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;amp;amp;gt; Prompt:
        return ps.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="Makefile"&amp;amp;amp;gt;PROJECT_NAME := collect

marimo:
	uv run marimo edit

.PHONY: movetools
movetools:
	./movetools


.PHONY: ensuregithub
ensuregithub:
	./tools/ensure-github-url

lint:
	ruff check .

format:
	black .

test: 
	uv run pytest -v -s -n auto

test-fast:
	uv run pytest -v -n auto -m "not slow"

test-slow:
	uv run pytest -v -s -m slow

test-single:
	uv run pytest -v -s

check: 
	make lint
	make format
	make movetools
	make ensuregithub

migrate:
	uv run yoyo apply --config yoyo.ini --batch


&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".mcp.json"&amp;amp;amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "/Users/benjaminmetz/.local/bin/uv",
			"args": [
				"--directory",
				"/Users/benjaminmetz/python/collect",
				"run",
				"collect.py"
			]
		}
	}
}
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="pyproject.toml"&amp;amp;amp;gt;[project]
name = "collect"
version = "0.1.0"
description = "development toolkit for all the things.."
readme = "README.md"
requires-python = "&amp;amp;amp;amp;gt;=3.13"
dependencies = [
    "aiohttp&amp;amp;amp;amp;gt;=3.12.11",
    "anthropic&amp;amp;amp;amp;gt;=0.50.0",
    "beautifulsoup4&amp;amp;amp;amp;gt;=4.13.4",
    "black&amp;amp;amp;amp;gt;=25.1.0",
    "fastapi&amp;amp;amp;amp;gt;=0.116.1",
    "google-ai-generativelanguage&amp;amp;amp;amp;gt;=0.6.15",
    "google-api-python-client&amp;amp;amp;amp;gt;=2.169.0",
    "google-auth-httplib2&amp;amp;amp;amp;gt;=0.2.0",
    "google-cloud-aiplatform[tokenization]&amp;amp;amp;amp;gt;=1.91.0",
    "google-cloud-secret-manager&amp;amp;amp;amp;gt;=2.23.3",
    "google-genai&amp;amp;amp;amp;gt;=1.13.0",
    "google-generativeai&amp;amp;amp;amp;gt;=0.8.5",
    "html-to-markdown&amp;amp;amp;amp;gt;=1.3.2",
    "html5lib&amp;amp;amp;amp;gt;=1.1",
    "httplib2&amp;amp;amp;amp;gt;=0.22.0",
    "httpx&amp;amp;amp;amp;gt;=0.28.1",
    "ipython&amp;amp;amp;amp;gt;=9.4.0",
    "lxml&amp;amp;amp;amp;gt;=5.4.0",
    "marimo&amp;amp;amp;amp;gt;=0.14.12",
    "markdownify&amp;amp;amp;amp;gt;=1.1.0",
    "mcp[cli]&amp;amp;amp;amp;gt;=1.7.1",
    "openai&amp;amp;amp;amp;gt;=1.59.4",
    "pathspec&amp;amp;amp;amp;gt;=0.12.1",
    "pyperclip&amp;amp;amp;amp;gt;=1.9.0",
    "pytest&amp;amp;amp;amp;gt;=8.3.5",
    "pytest-asyncio&amp;amp;amp;amp;gt;=0.26.0",
    "pytest-xdist&amp;amp;amp;amp;gt;=3.6.1",
    "python-json-logger&amp;amp;amp;amp;gt;=3.3.0",
    "readabilipy&amp;amp;amp;amp;gt;=0.3.0",
    "ruff&amp;amp;amp;amp;gt;=0.11.9",
    "tiktoken&amp;amp;amp;amp;gt;=0.9.0",
    "uvicorn&amp;amp;amp;amp;gt;=0.34.2",
    "yoyo-migrations&amp;amp;amp;amp;gt;=9.0.0",
]

[tool.pytest.ini_options]
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["."]
filterwarnings = [
    "ignore::UserWarning:google.auth._default"
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests"
]
# Parallel test execution settings
addopts = [
    "--strict-markers",  # Ensure all marks are registered
    "--tb=short",       # Shorter traceback format
    "--dist=worksteal", # Better work distribution for uneven test times
]
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="build_source_xml.py"&amp;amp;amp;gt;#!/usr/bin/env python3
"""
Build Source XML Script

Creates a source.xml file containing all project files, respecting .gitignore patterns
and excluding specified directories.

Usage:
    uv run build_source_xml.py [--output OUTPUT] [--dry-run] [--verbose]
"""

import argparse
import os
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Set, Iterator
import pathspec
import sys
from dataclasses import dataclass


@dataclass
class ProcessingStats:
    """Statistics for file processing."""
    files_processed: int = 0
    files_skipped: int = 0
    total_size: int = 0
    directories_excluded: int = 0


class GitignoreFilter:
    """Handles .gitignore pattern matching."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.spec = self._load_gitignore()
    
    def _load_gitignore(self) -&amp;amp;amp;amp;gt; pathspec.PathSpec:
        """Load and parse .gitignore file."""
        gitignore_path = self.project_root / '.gitignore'
        patterns = []
        
        if gitignore_path.exists():
            with open(gitignore_path, 'r', encoding='utf-8') as f:
                patterns = f.read().splitlines()
        
        # Add standard Python cache patterns if not present
        additional_patterns = [
            '__pycache__/',
            '*.pyc',
            '*.pyo',
            '.pytest_cache/',
            '.coverage',
            'htmlcov/',
        ]
        
        return pathspec.PathSpec.from_lines('gitwildmatch', patterns + additional_patterns)
    
    def should_ignore(self, path: Path) -&amp;amp;amp;amp;gt; bool:
        """Check if path should be ignored according to gitignore rules."""
        relative_path = path.relative_to(self.project_root)
        return self.spec.match_file(str(relative_path))


class FileProcessor:
    """Processes individual files for XML generation."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
    
    def is_text_file(self, file_path: Path) -&amp;amp;amp;amp;gt; bool:
        """Check if file is likely a text file."""
        try:
            # Check file extension first
            text_extensions = {
                '.py', '.md', '.txt', '.yml', '.yaml', '.json', '.toml', '.ini',
                '.sql', '.sh', '.lua', '.js', '.ts', '.html', '.css', '.xml',
                '.cfg', '.conf', '.gitignore', '.env', '.lock'
            }
            
            if file_path.suffix.lower() in text_extensions:
                return True
            
            # For files without extension, try to read a small sample
            if not file_path.suffix:
                try:
                    with open(file_path, 'rb') as f:
                        sample = f.read(1024)
                        # Check if sample contains null bytes (binary indicator)
                        return b'\x00' not in sample
                except:
                    return False
            
            return False
        except:
            return False
    
    def read_file_content(self, file_path: Path) -&amp;amp;amp;amp;gt; str:
        """Read file content with proper encoding handling."""
        encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                return f"[ERROR: Could not read file - {str(e)}]"
        
        return "[ERROR: Could not decode file with any supported encoding]"
    
    def get_relative_path(self, file_path: Path) -&amp;amp;amp;amp;gt; str:
        """Get relative path from project root."""
        return str(file_path.relative_to(self.project_root))


class SourceXMLBuilder:
    """Main class for building the source XML."""
    
    def __init__(self, project_root: Path, output_file: str = 'source.xml'):
        self.project_root = project_root
        self.output_file = output_file
        self.gitignore_filter = GitignoreFilter(project_root)
        self.file_processor = FileProcessor(project_root)
        self.stats = ProcessingStats()
        
        # Directories to exclude
        self.excluded_dirs = {
            'data',
            '.claude/commands',
            '.gemini/commands', 
            'tools',
            'guides',
            '__pycache__',
            '.pytest_cache',
            '.git',
            '.venv',
            'venv',
            'env',
            'htmlcov',
            'build',
            'dist',
            '*.egg-info'
        }
    
    def should_exclude_directory(self, dir_path: Path) -&amp;amp;amp;amp;gt; bool:
        """Check if directory should be excluded."""
        relative_path = str(dir_path.relative_to(self.project_root))
        
        # Check exact matches
        if relative_path in self.excluded_dirs:
            return True
        
        # Check if any part of the path matches excluded directories
        path_parts = Path(relative_path).parts
        for part in path_parts:
            if part in self.excluded_dirs:
                return True
        
        # Check specific path patterns
        excluded_patterns = [
            'data',
            '.claude/commands',
            '.gemini/commands',
            'tools',
            'guides'
        ]
        
        for pattern in excluded_patterns:
            if relative_path == pattern or relative_path.startswith(pattern + '/'):
                return True
        
        return False
    
    def collect_files(self) -&amp;amp;amp;amp;gt; Iterator[Path]:
        """Collect all files that should be included in the XML."""
        for root, dirs, files in os.walk(self.project_root):
            root_path = Path(root)
            
            # Skip if current directory should be excluded
            if self.should_exclude_directory(root_path):
                self.stats.directories_excluded += 1
                dirs.clear()  # Don't recurse into this directory
                continue
            
            # Filter out excluded directories from further traversal
            dirs[:] = [d for d in dirs if not self.should_exclude_directory(root_path / d)]
            
            for file_name in files:
                file_path = root_path / file_name
                
                # Skip if file matches gitignore patterns
                if self.gitignore_filter.should_ignore(file_path):
                    self.stats.files_skipped += 1
                    continue
                
                # Skip if not a text file
                if not self.file_processor.is_text_file(file_path):
                    self.stats.files_skipped += 1
                    continue
                
                yield file_path
    
    def build_xml(self, dry_run: bool = False, verbose: bool = False) -&amp;amp;amp;amp;gt; str:
        """Build the XML content."""
        if verbose:
            print(f"Scanning project: {self.project_root}")
        
        # Create root element
        root = ET.Element('source_code')
        root.set('project', str(self.project_root.name))
        
        files_to_process = list(self.collect_files())
        
        if dry_run:
            print(f"Would process {len(files_to_process)} files:")
            for file_path in files_to_process:
                rel_path = self.file_processor.get_relative_path(file_path)
                print(f"  - {rel_path}")
            return ""
        
        for file_path in files_to_process:
            if verbose:
                rel_path = self.file_processor.get_relative_path(file_path)
                print(f"Processing: {rel_path}")
            
            # Create file element
            file_element = ET.SubElement(root, 'file')
            rel_path = self.file_processor.get_relative_path(file_path)
            file_element.set('path', rel_path)
            
            # Read and add file content
            content = self.file_processor.read_file_content(file_path)
            file_element.text = content
            
            self.stats.files_processed += 1
            self.stats.total_size += len(content)
        
        # Create XML string
        ET.indent(root, space="  ", level=0)
        xml_str = ET.tostring(root, encoding='unicode', xml_declaration=True)
        
        return xml_str
    
    def write_xml(self, xml_content: str) -&amp;amp;amp;amp;gt; None:
        """Write XML content to file."""
        output_path = self.project_root / self.output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(xml_content)
        
        print(f"Source XML written to: {output_path}")
    
    def print_stats(self) -&amp;amp;amp;amp;gt; None:
        """Print processing statistics."""
        print(f"\nProcessing Statistics:")
        print(f"  Files processed: {self.stats.files_processed}")
        print(f"  Files skipped: {self.stats.files_skipped}")
        print(f"  Directories excluded: {self.stats.directories_excluded}")
        print(f"  Total content size: {self.stats.total_size:,} characters")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Build source code XML file')
    parser.add_argument('--output', '-o', default='source.xml', 
                       help='Output file name (default: source.xml)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show which files would be processed without creating output')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output showing file processing')
    
    args = parser.parse_args()
    
    # Get project root (current directory)
    project_root = Path.cwd()
    
    # Create builder
    builder = SourceXMLBuilder(project_root, args.output)
    
    try:
        # Build XML
        xml_content = builder.build_xml(dry_run=args.dry_run, verbose=args.verbose)
        
        if not args.dry_run:
            # Write to file
            builder.write_xml(xml_content)
        
        # Print statistics
        builder.print_stats()
        
    except KeyboardInterrupt:
        print("\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="initial_load.py"&amp;amp;amp;gt;#!/usr/bin/env python3
"""
Initial load script for loading plans and commands from disk into database
Uses PromptService to load from .claude/commands,
.gemini/commands, and _docs/plans
"""
import sys
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import PromptLoadResult, PromptCreateResult


def load_commands_to_db(service: PromptService) -&amp;amp;amp;amp;gt; PromptLoadResult:
    """
    Load commands from .claude and .gemini directories and save to database
    This is primarily for an initial load or to clean and restart from disk

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📁 Loading commands from disk...")

    # Load commands from filesystem
    cmd_result: PromptLoadResult = service.load_cmds_from_disk()

    if cmd_result.errors:
        print(
            f"⚠️  Found {len(cmd_result.errors)
                           } errors while loading commands:"
        )
        for error in cmd_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not cmd_result.loaded_prompts:
        print("ℹ️  No commands found to load")
        return cmd_result

    print(f"Found: {len(cmd_result.loaded_prompts)} to save to database")

    # Save commands to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        cmd_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save command: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (cmd_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=cmd_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def load_plans_to_db(service: PromptService) -&amp;amp;amp;amp;gt; PromptLoadResult:
    """Load plans from _docs/plans directories and save to database

    Returns:
        PromptLoadResult: Combined result with loading and saving information
    """
    print("📋 Loading plans from disk...")

    # Load plans from filesystem
    plan_result: PromptLoadResult = service.load_plans_from_disk()

    if plan_result.errors:
        print(
            f"⚠️  Found {len(plan_result.errors)
                           } errors while loading plans:"
        )
        for error in plan_result.errors:
            print(f"   ❌ {error.filename}: {error.error_message}")

    if not plan_result.loaded_prompts:
        print("ℹ️  No plans found to load")
        return plan_result

    print(
        f"📄 Found {len(plan_result.loaded_prompts)
                     } plans to save to database"
    )

    # Save plans to database
    save_results: List[PromptCreateResult] = service.bulk_save_in_db(
        plan_result.loaded_prompts
    )

    # Track save results
    save_success_count = 0
    save_errors = []

    for result in save_results:
        if result.success:
            save_success_count += 1
        else:
            print(f"   ❌ Failed to save plan: {result.error_message}")
            # Convert PromptCreateResult errors to LoadError format for consistency
            from repository.prompt_models import LoadError

            save_errors.append(
                LoadError(
                    filename=f"database_save_{result.prompt_id}",
                    error_message=result.error_message or "Unknown save error",
                    error_type=result.error_type or "SaveError",
                )
            )

    # Return updated PromptLoadResult with combined errors
    all_errors = (plan_result.errors or []) + save_errors

    return PromptLoadResult(
        loaded_prompts=plan_result.loaded_prompts,
        errors=all_errors if all_errors else None,
    )


def main():
    """Main function to orchestrate the complete loading process"""
    print("🚀 Starting initial data load from disk to database...")
    print("=" * 60)

    try:
        # Initialize database connection
        database = SQLite3Database("data/collect.db")

        with database.get_connection() as conn:
            # Create PromptService instance
            service = PromptService(conn)

            # Track overall statistics
            total_loaded = 0
            total_errors = 0

            # Load commands
            cmd_result: PromptLoadResult = load_commands_to_db(service)
            cmd_loaded = len(cmd_result.loaded_prompts)
            cmd_errors = len(cmd_result.errors) if cmd_result.errors else 0

            total_loaded += cmd_loaded
            total_errors += cmd_errors

            print(f"✅ Commands: {cmd_loaded} loaded, {cmd_errors} errors")
            print()

            # Load plans
            plan_result: PromptLoadResult = load_plans_to_db(service)
            plan_loaded = len(plan_result.loaded_prompts)
            plan_errors = len(plan_result.errors) if plan_result.errors else 0

            total_loaded += plan_loaded
            total_errors += plan_errors

            print(f"✅ Plans: {plan_loaded} loaded, {plan_errors} errors")
            print()

            # Print final summary
            print("=" * 60)
            print("📊 FINAL SUMMARY:")
            print(f"   ✅ Total items loaded: {total_loaded}")
            print(f"   ❌ Total errors: {total_errors}")

            if total_errors == 0:
                print("🎉 All data loaded successfully!")
            else:
                print(
                    f"⚠️  Completed with {
                      total_errors} errors - check output above"
                )

    except Exception as e:
        print(f"💥 Fatal error during loading process: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        sys.exit(1)


if __name__ == "__main__":
    main()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".sync_cache.json"&amp;amp;amp;gt;{
  ".claude/commands/go/go_build_endpoint_test.md": {
    "sha": "bb6aa9bffab635d3455caf7cfa59fc4f43036aac",
    "path": ".claude/commands/go/go_build_endpoint_test.md",
    "last_synced": 1754870996.5877829,
    "converted": true
  },
  ".claude/commands/archive/build_context.md": {
    "sha": "d976db85985179f771925095a73d98142d3ab30b",
    "path": ".claude/commands/archive/build_context.md",
    "last_synced": 1754870996.598152,
    "converted": true
  },
  ".claude/commands/commit.md": {
    "sha": "891c52b5b372e6fffd161dcf9b55930755d9fbf3",
    "path": ".claude/commands/commit.md",
    "last_synced": 1754870996.614573,
    "converted": true
  },
  ".claude/commands/go/create_go_structs.md": {
    "sha": "56a5e5dc2abb511b98b2e1147d428551610aa49e",
    "path": ".claude/commands/go/create_go_structs.md",
    "last_synced": 1754870996.625781,
    "converted": true
  },
  ".claude/commands/diff_code_review.md": {
    "sha": "6ef1818de14d6d1e994d9fd4841bc5486c4d6f20",
    "path": ".claude/commands/diff_code_review.md",
    "last_synced": 1754870996.635034,
    "converted": true
  },
  ".claude/commands/convert_to_toml.md": {
    "sha": "7bdd42b1ffeee07bd06ed5b4b97e99381c8052e9",
    "path": ".claude/commands/convert_to_toml.md",
    "last_synced": 1754870996.642501,
    "converted": true
  },
  ".claude/commands/create_checklist_3.md": {
    "sha": "27f4fa9841078c71d9490d9ef2c967970c053bbf",
    "path": ".claude/commands/create_checklist_3.md",
    "last_synced": 1754870996.688289,
    "converted": true
  },
  ".claude/commands/go/create_go_structsV1.md": {
    "sha": "d7e28d7eb8dee775f9416574d8f06ef11ca42158",
    "path": ".claude/commands/go/create_go_structsV1.md",
    "last_synced": 1754870996.692773,
    "converted": true
  },
  ".claude/commands/go/go_update_config.md": {
    "sha": "3cbaf390b5abd45230dde54705958bd873f16ebc",
    "path": ".claude/commands/go/go_update_config.md",
    "last_synced": 1754871010.753632,
    "converted": true
  },
  ".claude/commands/mcp/copy_to_clipboard.md": {
    "sha": "aad8af9420d4a80933aaf12accfe7a8450679f32",
    "path": ".claude/commands/mcp/copy_to_clipboard.md",
    "last_synced": 1754871011.429687,
    "converted": true
  },
  ".claude/commands/mcp/get_docs.md": {
    "sha": "9e27863768032379a1798cf6da1a881720ed65bf",
    "path": ".claude/commands/mcp/get_docs.md",
    "last_synced": 1754871012.238699,
    "converted": true
  },
  ".claude/commands/model_code_review.md": {
    "sha": "8bc222f49f8587a01362d06ff6475cb8e46ace2e",
    "path": ".claude/commands/model_code_review.md",
    "last_synced": 1754871023.324787,
    "converted": true
  },
  ".claude/commands/pr.md": {
    "sha": "0e8a152f358b515f158d5025428db18847101fc0",
    "path": ".claude/commands/pr.md",
    "last_synced": 1754871024.7506702,
    "converted": true
  },
  ".claude/commands/prime_webapp.md": {
    "sha": "c6f4cfe53f2aa92e1ac5661138989c9f9ff3ec42",
    "path": ".claude/commands/prime_webapp.md",
    "last_synced": 1754871038.662535,
    "converted": true
  },
  ".claude/commands/python/python_update_config.md": {
    "sha": "8562257de3a622c1aefb06be4c78a4ce9c580e2a",
    "path": ".claude/commands/python/python_update_config.md",
    "last_synced": 1754871042.126036,
    "converted": true
  },
  ".claude/commands/read.md": {
    "sha": "524139e35449b4f5c0a88b206699cec5d2f1409b",
    "path": ".claude/commands/read.md",
    "last_synced": 1754871043.723397,
    "converted": true
  },
  ".claude/commands/response_in_markdown.md": {
    "sha": "a540be858d83e4b65df8dd2a23e3656bbdce8ee7",
    "path": ".claude/commands/response_in_markdown.md",
    "last_synced": 1754871049.784717,
    "converted": true
  },
  ".claude/commands/runplan.md": {
    "sha": "08d9b2d8c3bd445c55170adec7e80b9d3733e27a",
    "path": ".claude/commands/runplan.md",
    "last_synced": 1754871056.6670442,
    "converted": true
  },
  ".claude/commands/test_runner.md": {
    "sha": "28e088b2d155b9eca391add1bc75cdf701a3a4d8",
    "path": ".claude/commands/test_runner.md",
    "last_synced": 1754871059.413824,
    "converted": true
  },
  ".claude/commands/tools/create_database.md": {
    "sha": "fcfeeb2b78e39ecf0a1b8b4a85735dd83fa1c8f1",
    "path": ".claude/commands/tools/create_database.md",
    "last_synced": 1754871070.783872,
    "converted": true
  },
  ".claude/commands/tools/extract.md": {
    "sha": "e8946fb23beb24cd8e8689c48cd21b155dfa10a6",
    "path": ".claude/commands/tools/extract.md",
    "last_synced": 1754871074.061621,
    "converted": true
  }
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="llmrunner.py"&amp;amp;amp;gt;import asyncio
from datetime import datetime

from pydantic import BaseModel
from typing import Dict, Union, List, Optional, Any

from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP
from models.gemini_mcp import GeminiMCP
from models.openai_mpc import OpenAIMCP
from models.xai_mcp import XaiMCP


class ModelsToMCP(BaseModel):
    model_config = {"arbitrary_types_allowed": True}
    models_to_mcp: Dict[str, Union[GeminiMCP, AnthropicMCP, OpenAIMCP, XaiMCP]]


class ModelResult(BaseModel):
    model: str
    timestamp: str
    success: bool
    actual_model: Optional[str] = None
    duration_seconds: Optional[float] = None
    response: Optional[Any] = None
    error: Optional[str] = None


class LLMRunnerResults(BaseModel):
    successful_results: List[ModelResult]
    failed_results: List[ModelResult]
    total_models: int
    success_count: int
    failure_count: int


async def llmrunner(prompt: str, models_to_mcp: ModelsToMCP) -&amp;amp;amp;amp;gt; LLMRunnerResults:

    async def call_model(model_name: str) -&amp;amp;amp;amp;gt; dict:
        try:
            start_time = datetime.now()
            iso_time = start_time.isoformat()

            mcp_instance = models_to_mcp.models_to_mcp[model_name]
            print(f"sending to --&amp;amp;amp;amp;gt; {model_name} : at -&amp;amp;amp;amp;gt; {iso_time}")
            response = mcp_instance.send_message(prompt, model=model_name)
            end_time = datetime.now()

            result = ModelResult(
                model=model_name,
                actual_model=model_name,
                timestamp=iso_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                response=response,
                success=True,
            )

            return result

        except Exception as e:
            error_result = ModelResult(
                success=False,
                error=str(e),
                model=model_name,
                timestamp=datetime.now().isoformat(),
            )

            return error_result

    print(
        f"starting runner for: {
          len(models_to_mcp.models_to_mcp.keys())} models -&amp;amp;amp;amp;gt;"
    )
    tasks = [call_model(model) for model in models_to_mcp.models_to_mcp.keys()]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful_results = [
        r for r in results if isinstance(r, ModelResult) and r.success
    ]

    failed_results = [
        r for r in results if isinstance(r, ModelResult) and not r.success
    ]

    return LLMRunnerResults(
        successful_results=successful_results,
        failed_results=failed_results,
        total_models=len(models_to_mcp.models_to_mcp),
        success_count=len(successful_results),
        failure_count=len(failed_results),
    )


def code_review_models_to_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)

    anthropic_model = config.anthropic_default_code_review_model
    gemini_model = config.gemini_default_code_review_model
    xai_model = config.xai_default_code_review_model
    openai_model = config.openai_default_code_review_model

    gemini_mcp = GeminiMCP(config, secret_mgr, gemini_model)
    openai_mcp = OpenAIMCP(config, secret_mgr, openai_model)
    xai_mcp = XaiMCP(config, secret_mgr, xai_model)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, anthropic_model)

    model_mcps = {
        gemini_model: gemini_mcp,
        openai_model: openai_mcp,
        xai_model: xai_mcp,
        anthropic_model: anthropic_mcp,
    }

    return ModelsToMCP(models_to_mcp=model_mcps)
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="README.md"&amp;amp;amp;gt;**Collect** is a command-line toolkit built with Python that functions as an MCP (Model Context Protocol) server. It is designed to assist with AI-driven development by providing tools to fetch web content, process it, and coordinate analysis across multiple AI models.

*   **Multi-Model Integration**: Interact with models from Google (Gemini), Anthropic (Claude), OpenAI (GPT), and XAI (Grok) through a single interface.
*   **Content Processing**: Fetch content from URLs and convert HTML to clean markdown or plain text.
*   **Code &amp;amp;amp;amp;amp; Diff Analysis**: Perform code reviews on files or git diffs using any of the integrated AI models.
*   **Secure Configuration**: Utilizes Google Cloud Secret Manager for API key storage.
*   **Prompt Management**: A version-controlled system for managing and synchronizing prompts between the local filesystem and a SQLite database.
*   **Token Utilities**: Tools to count token usage for various models to manage costs and context windows.

### MCP Server Configuration

#### For Claude Code

To enable Claude Code to use the `collect` MCP server, create a `.mcp.json` file in your project's root directory:

1.  **Create the Configuration File**: In the root of your project where you want to use the collect tools, create a file named `.mcp.json`.
2.  **Add Configuration**: Add the following JSON configuration:

```json
{
  "mcpServers": {
    "collect": {
      "command": "/path/to/.local/bin/uv",
      "args": [
        "--directory",
        "/path/to/collect",
        "run",
        "collect.py"
      ]
    }
  }
}
```

Replace `/path/to/.local/bin/uv` with the full path to your `uv` binary (you can find this with `which uv`), and `/path/to/collect` with the full path to your collect repository.

#### For Gemini CLI

To enable the Gemini CLI to automatically start the `collect` MCP server, you need to configure a `.gemini/settings.json` file in your project's root directory:

1.  **Create the Directory**: If it doesn't already exist, create a `.gemini` directory in the root of the `collect` project.
2.  **Create the Settings File**: Inside the `.gemini` directory, create a file named `settings.json`.
3.  **Add Configuration**: Paste the following JSON configuration into the `settings.json` file.

```json
{
  "mcpServers": {
    "collect": {
      "command": "uv",
      "args": [
        "run",
        "python",
        "collect.py"
      ],
      "workingDirectory": "/Users/benjaminmetz/python/collect",
      "enabled": true
    }
  }
}
```

This configuration tells the Gemini CLI how to launch the `collect` server, specifying the command, arguments, and working directory.

### Command Category System

The command category system dynamically creates categories based on subdirectories configured in the `.env` file. This approach allows for easy extension of command categories without code changes.

#### How Categories Are Created

1. **Configuration**: Command subdirectories are defined in the `.env` file:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
   ```

2. **Dynamic Enum Generation**: The `create_cmd_category_enum()` function in `repository/prompt_models.py` reads the `COMMAND_SUBDIRS` from the `.env` file via the `Config` class and dynamically creates a `CmdCategory` enum at runtime.

3. **Directory Management**: When the `PromptService` initializes, the `cmd_check_dirs()` function in `repository/prompt_service.py`:
   - Reads the subdirectory list from the config
   - Checks for the existence of each configured subdirectory under both `.claude/commands/` and `.gemini/commands/`
   - Automatically creates any missing directories
   - Each subdirectory becomes a valid command category

4. **Category Assignment**: When loading commands from disk:
   - Files directly in `.claude/commands/` or `.gemini/commands/` are assigned the `UNCATEGORIZED` category
   - Files in subdirectories are assigned the category matching the subdirectory name
   - The category is stored as part of the prompt's metadata in the database

#### Adding New Categories

To add new command categories:
1. Update the `COMMAND_SUBDIRS` line in the `.env` file with your new category
2. The system will automatically create the directories and recognize them as valid categories on the next run
3. Commands placed in those directories will be tagged with the new category

#### Example

To add a "rust" category:
1. Edit `.env`:
   ```
   COMMAND_SUBDIRS=archive,go,js,mcp,python,tools,rust
   ```
2. Restart the service or run the prompt service
3. The system will create:
   - `.claude/commands/rust/`
   - `.gemini/commands/rust/`
4. Any `.md` files placed in these directories will be categorized as "rust" commands

#### Current Directory Structure
Based on the `.env` configuration (`COMMAND_SUBDIRS=archive,go,js,mcp,python,tools`), the directory structure is:

```
.claude/
└── commands/
    ├── archive/          # Archived commands
    ├── go/               # Go-specific commands
    ├── js/               # JavaScript commands
    ├── mcp/              # MCP server commands
    ├── python/           # Python-specific commands
    └── tools/            # Tool-related commands

.gemini/
└── commands/
    ├── archive/
    ├── go/
    ├── js/
    ├── mcp/
    ├── python/
    └── tools/
```

Note: Files placed directly in `.claude/commands/` or `.gemini/commands/` (not in subdirectories) are automatically assigned the `UNCATEGORIZED` category.

### Prompt Management System

The project includes a system for managing prompts **that is very much under construction**. Prompts are categorized as either **Commands** (`CMD`) or **Plans** (`PLAN`). This system, located in the `repository/` directory, uses a SQLite database to store and version prompts, while also synchronizing them with the local filesystem.

*   **Core Components**:
    *   `prompt_service.py`: The main service class that orchestrates loading, saving, versioning, and flattening prompts.
    *   `prompt_models.py`: Defines the Pydantic data models for prompts, including `Prompt`, `PromptData`, and various status enums like `PromptType` and `PromptPlanStatus`.
    *   `database.py`: Manages the connection to the `collect.db` SQLite database.
    *   `20250727_01_create-prompt-tables.sql`: The database migration file that defines the schema for the `prompt` and `prompt_history` tables.

*   **Synchronization Workflow**:
    1.  **Loading from Disk**: The `PromptService` can load prompts from predefined directories (`.claude/commands`, `.gemini/commands`, and `_docs/plans`).
    2.  **Database Persistence**: Loaded prompts are saved to the SQLite database. The service checks for existing prompts by name. If a prompt already exists and its content has changed (verified via a SHA256 hash), a new version is created in the `prompt_history` table, and the main `prompt` table is updated.
    3.  **Flattening to Disk**: The service can "flatten" the prompts from the database back to the filesystem, ensuring that the local files are consistent with the database state. This is useful for maintaining a clear and organized prompt library.

*   **Versioning**:
    *   Every time a prompt's content is updated, its `version` number is incremented.
    *   A complete record of all versions is stored in the `prompt_history` table, including a timestamp and a change summary. This allows for a full audit trail of how a prompt has evolved.

This system ensures that prompts are treated as version-controlled assets within the project, providing a structured and auditable way to manage the instructions given to the AI models.
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="api.py"&amp;amp;amp;gt;#!/usr/bin/env python

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api import prompt_api_router
from config import Config
import uvicorn

import sys
import logging
from pythonjsonlogger.json import JsonFormatter
from contextlib import asynccontextmanager


# Configure JSON logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(stream=sys.stdout)
handler.setFormatter(JsonFormatter())
handler.setLevel(logging.INFO)

logger.addHandler(handler)

# Load configuration
config = Config()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting prompt API service...")
    app.state.db_path = config.db_path
    app.state.config = config
    logger.info(f"Database path set to: {app.state.db_path}")
    logger.info(f"Service running on port: {config.port}")

    yield

    # Shutdown
    logger.info("Shutting down prompt API service...")


app = FastAPI(
    title="Prompt Service API",
    description="HTTP API for managing prompts and plans",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:*", "http://127.0.0.1:*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

app.include_router(prompt_api_router, tags=["prompt_api"])


def main():
    uvicorn.run(app, host="0.0.0.0", port=int(config.port))


if __name__ == "__main__":
    main()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="yoyo.ini"&amp;amp;amp;gt;[DEFAULT]
sources = migrations
database = sqlite:///data/collect.db
batch_mode = on
verbosity = 0

&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="secret_manager.py"&amp;amp;amp;gt;from google.cloud import secretmanager


class SecretManager:
    def __init__(
        self,
        project_id: str,
    ) -&amp;amp;amp;amp;gt; None:
        self.project_id = project_id
        self.gcp_client = secretmanager.SecretManagerServiceClient()

    def get_secret(self, secret_name: str) -&amp;amp;amp;amp;gt; str:
        response = self.gcp_client.access_secret_version(request={"name": secret_name})
        return response.payload.data.decode("UTF-8").strip()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".gitignore"&amp;amp;amp;gt;# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
venv/
env/

# Environment variables
.env.local
.env.*.local

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Test coverage
.coverage
htmlcov/
.pytest_cache/
.tox/

# Temporary files
:w
*.tmp
*.temp

# Python version management
.python-version

# UV lock file (optional - some prefer to track this)
# uv.lock
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".env"&amp;amp;amp;gt;PORT=8081
GCP_PROJECT_ID=482777410016
DB_PATH=data/collect.db
ANTHROPIC_API_KEY_PATH=projects/482777410016/secrets/AnthropicMCP/versions/1
ANTHROPIC_MODEL_OPUS=claude-opus-4-20250514
ANTHROPIC_MODEL_SONNET=claude-sonnet-4-20250514
GEMINI_API_KEY_PATH=projects/482777410016/secrets/GeminiTest/versions/1
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/
XAI_API_KEY_PATH=projects/482777410016/secrets/XAI_API_KEY_ELEPHNT/versions/1
GROK_SYSTEM_PROMPT=You are a helpful assistant that can answer questions and help with tasks.
OPENAI_API_KEY_PATH=projects/482777410016/secrets/OpenAIMCP/versions/1

# default code review models for running the code review loop
OPENAI_DEFAULT_CODE_REVIEW_MODEL=o3-mini-2025-01-31
GEMINI_DEFAULT_CODE_REVIEW_MODEL=gemini-2.5-flash-preview-05-20
ANTHROPIC_DEFAULT_CODE_REVIEW_MODEL=claude-opus-4-20250514
XAI_DEFAULT_CODE_REVIEW_MODEL=grok-3-mini-fast-latest

# Command subdirectories
COMMAND_SUBDIRS=archive,go,js,mcp,python,tools
GITHUB_URL=https://github.com/austere-labs/collect
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="filterModel.md"&amp;amp;amp;gt;# Gemini Model Filtering Implementation Guide

## Overview

This document describes how to filter Gemini models by version number (2.0, 2.5, etc.) and extract input token limits from the API response.

## Current State

The `GeminiMCP.get_model_list()` method has been updated to return the full API response instead of just model names:

```python
def get_model_list(self) -&amp;amp;amp;amp;gt; Dict:
    # ... API call logic ...
    model_data = response.json()
    return model_data  # Returns full response with all model metadata
```

## Implementation Plan

### 1. Add Filtering Methods to GeminiMCP Class

Add these methods to the `GeminiMCP` class in `models/gemini_mcp.py`:

```python
def filter_models_by_version(self, versions: list[str]) -&amp;amp;amp;amp;gt; list[dict]:
    """
    Filter models by version numbers and include token limits.
    
    Args:
        versions: List of version strings (e.g., ['2.0', '2.5'])
    
    Returns:
        List of dicts with model info including inputTokenLimit
    """
    all_models = self.get_model_list()
    filtered_models = []
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Check if model matches any requested version
        for version in versions:
            if version in model_name:
                model_info = {
                    'name': model_name,
                    'displayName': model.get('displayName', ''),
                    'inputTokenLimit': model.get('inputTokenLimit', 0),
                    'outputTokenLimit': model.get('outputTokenLimit', 0),
                    'description': model.get('description', ''),
                    'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
                }
                filtered_models.append(model_info)
                break
    
    return filtered_models

def get_models_with_token_info(self) -&amp;amp;amp;amp;gt; list[dict]:
    """
    Get all models with their token limit information.
    
    Returns:
        List of models sorted by inputTokenLimit (descending)
    """

    all_models = self.get_model_list()
    models_with_tokens = []

    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Only include models with token limit info
        if input_limit &amp;amp;amp;amp;gt; 0:
            models_with_tokens.append({
                'name': model_name,
                'inputTokenLimit': input_limit,
                'outputTokenLimit': model.get('outputTokenLimit', 0)
            })
    
    # Sort by input token limit (highest first)
    models_with_tokens.sort(key=lambda x: x['inputTokenLimit'], reverse=True)
    return models_with_tokens
```

### 2. Advanced Filtering Function (Standalone)

For more complex filtering needs, you can use this standalone function:

```python
def filter_gemini_models(models_data: dict, 
                        versions: list[str] = None,
                        min_input_tokens: int = None,
                        max_input_tokens: int = None,
                        generation_methods: list[str] = None) -&amp;amp;amp;amp;gt; list[dict]:
    """
    Advanced filtering with multiple criteria.
    
    Args:
        models_data: Response from get_model_list()
        versions: Filter by version numbers (optional)
        min_input_tokens: Minimum inputTokenLimit (optional)
        max_input_tokens: Maximum inputTokenLimit (optional)
        generation_methods: Required generation methods (optional)
    
    Returns:
        Filtered list of model information
    """
    filtered_models = []
    
    for model in models_data.get('models', []):
        model_name = model['name'].split('/')[-1]
        input_limit = model.get('inputTokenLimit', 0)
        
        # Apply version filter
        if versions:
            if not any(ver in model_name for ver in versions):
                continue
        
        # Apply token limit filters
        if min_input_tokens and input_limit &amp;amp;amp;amp;lt; min_input_tokens:
            continue
        if max_input_tokens and input_limit &amp;amp;amp;amp;gt; max_input_tokens:
            continue
        
        # Apply generation method filter
        if generation_methods:
            supported_methods = model.get('supportedGenerationMethods', [])
            if not all(method in supported_methods for method in generation_methods):
                continue
        
        # Model passed all filters
        model_info = {
            'name': model_name,
            'displayName': model.get('displayName', ''),
            'inputTokenLimit': input_limit,
            'outputTokenLimit': model.get('outputTokenLimit', 0),
            'description': model.get('description', ''),
            'supportedGenerationMethods': model.get('supportedGenerationMethods', [])
        }
        filtered_models.append(model_info)
    
    return filtered_models
```

## Usage Examples

### Basic Version Filtering

```python
def test_filter_by_version(gemini_mcp):
    # Get models for versions 2.0 and 2.5
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    print(f"Found {len(filtered)} models:")
    for model in filtered:
        print(f"- {model['name']}: {model['inputTokenLimit']:,} input tokens")
```

### Get Models with Token Info

```python
def test_models_with_tokens(gemini_mcp):
    models = gemini_mcp.get_models_with_token_info()
    
    print("Models by input token limit:")
    for model in models[:10]:  # Top 10 models
        print(f"- {model['name']}: {model['inputTokenLimit']:,} tokens")
```

### Advanced Filtering

```python
def test_advanced_filtering(gemini_mcp):
    all_models = gemini_mcp.get_model_list()
    
    # Find 2.5 models with at least 100k input tokens
    filtered = filter_gemini_models(
        all_models,
        versions=['2.5'],
        min_input_tokens=100000,
        generation_methods=['generateContent']
    )
    
    print("High-capacity 2.5 models:")
    for model in filtered:
        print(f"- {model['name']}")
        print(f"  Input limit: {model['inputTokenLimit']:,}")
        print(f"  Output limit: {model['outputTokenLimit']:,}")
```

### Grouping Models by Version

```python
def group_models_by_version(gemini_mcp):
    from collections import defaultdict
    
    all_models = gemini_mcp.get_model_list()
    version_groups = defaultdict(list)
    
    for model in all_models.get('models', []):
        model_name = model['name'].split('/')[-1]
        
        # Extract version pattern
        if '2.5' in model_name:
            version = '2.5'
        elif '2.0' in model_name:
            version = '2.0'
        elif '1.5' in model_name:
            version = '1.5'
        elif '1.0' in model_name:
            version = '1.0'
        else:
            version = 'other'
        
        version_groups[version].append({
            'name': model_name,
            'inputTokenLimit': model.get('inputTokenLimit', 0)
        })
    
    # Display grouped results
    for version, models in sorted(version_groups.items()):
        print(f"\nVersion {version} ({len(models)} models):")
        for model in sorted(models, key=lambda x: x['inputTokenLimit'], reverse=True)[:3]:
            print(f"  - {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Expected Output Format

When filtering models, you'll get results like:

```
Found 15 models:
- gemini-2.5-pro: 2,000,000 input tokens
- gemini-2.5-flash: 1,000,000 input tokens
- gemini-2.5-flash-preview-05-20: 1,000,000 input tokens
- gemini-2.0-flash: 32,768 input tokens
- gemini-2.0-flash-exp: 32,768 input tokens
- gemini-2.0-pro-exp: 32,768 input tokens
```

## API Response Structure

The Gemini API returns model data in this format:

```json
{
  "models": [
    {
      "name": "models/gemini-2.5-flash",
      "displayName": "Gemini 2.5 Flash",
      "description": "Fast and versatile multimodal model",
      "inputTokenLimit": 1000000,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ]
    }
    // ... more models
  ]
}
```

## Testing the Implementation

Add this test to `models/test_gemini_mcp.py`:

```python
def test_filter_models_by_version(gemini_mcp):
    # Test filtering for 2.0 and 2.5 versions
    filtered = gemini_mcp.filter_models_by_version(['2.0', '2.5'])
    
    assert len(filtered) &amp;amp;amp;amp;gt; 0
    assert all('2.0' in m['name'] or '2.5' in m['name'] for m in filtered)
    assert all('inputTokenLimit' in m for m in filtered)
    
    # Print results for verification
    print(f"\nFound {len(filtered)} models for versions 2.0 and 2.5:")
    for model in sorted(filtered, key=lambda x: x['inputTokenLimit'], reverse=True):
        print(f"  {model['name']}: {model['inputTokenLimit']:,} tokens")
```

## Notes

1. **Token Limits**: Not all models return `inputTokenLimit`. Handle missing values gracefully.
2. **Model Names**: The API returns full names like "models/gemini-2.5-flash". We extract just the model part.
3. **Sorting**: Consider sorting results by token limit, name, or version for consistent output.
4. **Caching**: For production use, consider caching the model list as it doesn't change frequently.
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="test_llmrunner.py"&amp;amp;amp;gt;import pytest

from llmrunner import (
    llmrunner,
    code_review_models_to_mcp,
    ModelResult,
    LLMRunnerResults,
)


@pytest.fixture
def models_to_mcp():
    return code_review_models_to_mcp()


@pytest.mark.asyncio
async def test_llmrunner(models_to_mcp):
    prompt = "What is 2 + 2?"
    result = await llmrunner(prompt, models_to_mcp)

    assert isinstance(result, LLMRunnerResults)
    assert isinstance(result.successful_results, list)
    assert isinstance(result.failed_results, list)
    assert isinstance(result.total_models, int)
    assert isinstance(result.success_count, int)
    assert isinstance(result.failure_count, int)

    assert result.total_models == len(models_to_mcp.models_to_mcp)
    assert result.success_count + result.failure_count == result.total_models

    for success_result in result.successful_results:
        assert isinstance(success_result, ModelResult)
        assert success_result.success is True
        assert success_result.model is not None
        assert success_result.timestamp is not None
        assert success_result.response is not None
        assert success_result.duration_seconds is not None

    for failed_result in result.failed_results:
        assert isinstance(failed_result, ModelResult)
        assert failed_result.success is False
        assert failed_result.model is not None
        assert failed_result.timestamp is not None
        assert failed_result.error is not None

    print(f"Total models: {result.total_models}")
    print(f"Successful: {result.success_count}")
    print(f"Failed: {result.failure_count}")

    for failed_result in result.failed_results:
        print(
            f"Failed model: {
              failed_result.model} - Error: {failed_result.error}"
        )

    for success_result in result.successful_results:
        print(f"Successful model: {success_result.model}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="GEMINI.md"&amp;amp;amp;gt;# Gemini Code Assistant Context

This document provides context for the Gemini Code Assistant to understand the project structure, conventions, and important files.

## Project Overview

This project is a Python-based MCP (Model Context Protocol) server named "Collect". Its primary purpose is to fetch web content, process it, and facilitate multi-model AI analysis workflows. It provides a unified interface to interact with various AI models (OpenAI, Anthropic, Gemini, XAI) for tasks like code review. The server is built using the `mcp` library and exposes several tools for fetching URLs, converting HTML to markdown, counting tokens, and more. It also includes a database layer using SQLite for data persistence.

**Key Technologies:**

*   **Programming Language:** Python
*   **Framework:** `mcp` (Model Context Protocol)
*   **Key Libraries:** 
    *   `httpx` for asynchronous HTTP requests.
    *   `anthropic`, `openai`, `google-cloud-aiplatform` for interacting with various LLMs.
    *   `readabilipy`, `markdownify`, `beautifulsoup4` for HTML processing.
    *   `pyperclip` for clipboard integration.
    *   `yoyo-migrations` for database schema management.
*   **Package Manager:** `uv`
*   **Testing:** `pytest` with `pytest-asyncio` and `pytest-xdist` for parallel testing.
*   **Linting/Formatting:** `ruff` and `black`.
*   **Database:** SQLite.

## Building and Running

*   **Install Dependencies:** `uv sync`
*   **Run the Server:** `python collect.py`
*   **Run Tests:** `uv run pytest -v -s -n auto`
*   **Run Linter:** `ruff check .`
*   **Run Formatter:** `black .`
*   **Apply Database Migrations:** `uv run yoyo apply --config yoyo.ini --batch`

## Development Conventions

*   **Testing:** Tests are written using `pytest` and are located in files like `test_collect.py`. Asynchronous functions are tested using `@pytest.mark.asyncio`. The project uses `pytest-xdist` for parallel test execution.
*   **Linting and Formatting:** The project uses `ruff` for linting and `black` for formatting. These are run via the `Makefile`.
*   **Configuration:** Project configuration is managed in `config.py`, which loads environment variables from a `.env` file.
*   **Secrets Management:** API keys and other secrets are managed through Google Cloud Secret Manager, as indicated in `secret_manager.py` and `config.py`.
*   **Database:** The project uses SQLite for its database. The database connection logic is in `repository/database.py`. Migrations are handled by `yoyo-migrations`.

## Key Files

*   **`collect.py`:** The main entry point of the MCP server. It defines the available tools, such as `fetch_urls`, `run_code_review`, and `to_markdown`.
*   **`pyproject.toml`:** Defines the project's dependencies and development tool configurations.
*   **`Makefile`:** Provides convenient commands for common development tasks like testing, linting, and formatting.
*   **`config.py`:** Handles the project's configuration by loading environment variables from a `.env` file.
*   **`reviewer/code_review.py`:** Contains the logic for the code review functionality. It takes a diff file, sends it to multiple LLMs, and then formats and saves the results.
*   **`models/`:** This directory contains modules for interacting with different AI models (e.g., `anthropic_mpc.py`, `openai_mpc.py`).
*   **`repository/database.py`:** Contains the logic for connecting to the SQLite database.
*   **`migrations/`:** This directory contains the SQL migration files for the database schema.
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="movetools"&amp;amp;amp;gt;#!/bin/bash

# Enhanced setup script to copy tools to user bin directory with colorful output
# This script should be run from the collect project home directory

# Color definitions for enhanced output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly BOLD='\033[1m'
readonly NC='\033[0m' # No Color

# Set default target directory (configurable)
TARGET_DIR=${1:-~/bin}

# Expand tilde to home directory
TARGET_DIR="${TARGET_DIR/#\~/$HOME}"

# Get the directory where this script is located (project home)
PROJECT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; pwd )"
TOOLS_DIR="$PROJECT_DIR/tools"

# Enhanced printing functions
print_header() {
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BOLD}${WHITE}  🔧 MOVETOOLS - Tool Installation Script${NC}"
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
}

print_info() {
    echo -e "${CYAN}ℹ${NC}  $1"
}

print_success() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC}  $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_step() {
    echo -e "\n${BOLD}${PURPLE}▶${NC} ${BOLD}$1${NC}"
}

print_item() {
    echo -e "   ${GREEN}•${NC} $1"
}

# Function to display usage
usage() {
    print_header
    echo -e "${BOLD}USAGE:${NC}"
    echo -e "  $0 [target_directory]"
    echo ""
    echo -e "${BOLD}DESCRIPTION:${NC}"
    echo -e "  ${BOLD}MOVETOOLS${NC} is a comprehensive installation and backup script for the collect project."
    echo -e "  It performs two main functions:"
    echo ""
    echo -e "  ${BOLD}1. Tool Installation:${NC}"
    echo -e "     • Copies all executable tools from the tools/ directory to your bin directory"
    echo -e "     • Makes all copied tools executable with proper permissions"
    echo -e "     • Validates PATH configuration and provides setup guidance"
    echo -e "     • Provides colorful visual feedback throughout the process"
    echo ""
    echo -e "  ${BOLD}2. Dotfiles Backup:${NC}"
    echo -e "     • Backs up your .zshrc configuration to the project's dotfiles/ directory"
    echo -e "     • Copies your Ghostty terminal configuration (~/.config/ghostty)"
    echo -e "     • Backs up your Neovim init.lua configuration (~/.config/nvim/init.lua)"
    echo -e "     • Creates organized dotfiles structure for version control"
    echo ""
    echo -e "${BOLD}ARGUMENTS:${NC}"
    echo -e "  ${CYAN}target_directory${NC}    Optional. Directory to install tools (default: ~/bin)"
    echo ""
    echo -e "${BOLD}OPTIONS:${NC}"
    echo -e "  ${YELLOW}--llm${NC}              Display comprehensive usage information (LLM-friendly)"
    echo -e "  ${YELLOW}--help, -h${NC}         Display this usage information"
    echo ""
    echo -e "${BOLD}FEATURES:${NC}"
    echo -e "  • Enhanced colorful terminal output with status indicators"
    echo -e "  • Automatic directory creation with proper error handling"
    echo -e "  • Tool validation and permission management"
    echo -e "  • PATH verification with configuration suggestions"
    echo -e "  • Comprehensive dotfiles backup across multiple applications"
    echo -e "  • Installation summary with detailed feedback"
    echo ""
    echo -e "${BOLD}EXAMPLES:${NC}"
    echo -e "  ${GREEN}$0${NC}                 # Install tools to ~/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 ~/.local/bin${NC}    # Install tools to ~/.local/bin and backup dotfiles"
    echo -e "  ${GREEN}$0 --llm${NC}           # Show comprehensive help for AI assistants"
    echo -e "  ${GREEN}$0 --help${NC}          # Show this help message"
    echo ""
    echo -e "${BOLD}DIRECTORY STRUCTURE:${NC}"
    echo -e "  ${CYAN}tools/${NC}              # Source directory containing executable tools"
    echo -e "  ${CYAN}dotfiles/zshrc${NC}      # Backed up shell configuration"
    echo -e "  ${CYAN}dotfiles/ghostty/${NC}   # Backed up terminal configuration"
    echo -e "  ${CYAN}dotfiles/nvim/${NC}      # Backed up Neovim configuration"
    echo ""
    echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    exit 0
}

# Check for help flags
if [ $# -eq 1 ] &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ([ "$1" = "--llm" ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]); then
    usage
fi

# Display header
print_header

# Display configuration
print_step "Configuration"
print_info "Project directory: ${BOLD}$PROJECT_DIR${NC}"
print_info "Tools directory: ${BOLD}$TOOLS_DIR${NC}"
print_info "Target directory: ${BOLD}$TARGET_DIR${NC}"

# Check if tools directory exists
print_step "Validation"
if [ ! -d "$TOOLS_DIR" ]; then
    print_error "Tools directory not found at $TOOLS_DIR"
    exit 1
fi
print_success "Tools directory found"

# Create target directory if it doesn't exist
if [ ! -d "$TARGET_DIR" ]; then
    print_info "Creating target directory..."
    mkdir -p "$TARGET_DIR"
    if [ $? -eq 0 ]; then
        print_success "Target directory created"
    else
        print_error "Failed to create target directory"
        exit 1
    fi
else
    print_success "Target directory exists"
fi

# Copy all files from tools directory
print_step "Copying Tools"
tool_count=0
copied_tools=()

for tool in "$TOOLS_DIR"/*; do
    if [ -f "$tool" ]; then
        tool_name=$(basename "$tool")
        # Skip CLAUDE.md file
        if [ "$tool_name" = "CLAUDE.md" ]; then
            continue
        fi
        if cp "$tool" "$TARGET_DIR/" 2&amp;amp;amp;amp;gt;/dev/null; then
            print_item "Copied ${BOLD}$tool_name${NC}"
            copied_tools+=("$tool_name")
            ((tool_count++))
        else
            print_error "Failed to copy $tool_name"
        fi
    fi
done

if [ $tool_count -eq 0 ]; then
    print_warning "No tools found in $TOOLS_DIR"
    exit 0
fi

# Make all copied tools executable
print_step "Setting Permissions"
executable_count=0
for tool_name in "${copied_tools[@]}"; do
    tool_path="$TARGET_DIR/$tool_name"
    if chmod u+x "$tool_path" 2&amp;amp;amp;amp;gt;/dev/null; then
        print_item "Made ${BOLD}$tool_name${NC} executable"
        ((executable_count++))
    else
        print_error "Failed to make $tool_name executable"
    fi
done

# Display completion summary
print_step "Summary"
print_success "Installation complete!"
print_info "${BOLD}$tool_count${NC} tools copied and ${BOLD}$executable_count${NC} made executable"
print_info "Tools are now available in: ${BOLD}$TARGET_DIR${NC}"

# List installed tools
if [ ${#copied_tools[@]} -gt 0 ]; then
    echo ""
    print_info "${BOLD}Installed tools:${NC}"
    for tool_name in "${copied_tools[@]}"; do
        echo -e "   ${GREEN}▸${NC} ${BOLD}$tool_name${NC}"
    done
fi

# Check if target directory is in PATH
echo ""
if [[ ":$PATH:" != *":$TARGET_DIR:"* ]]; then
    print_warning "Target directory is not in your PATH"
    echo -e "${YELLOW}💡${NC} To use these tools from anywhere, add this line to your ${BOLD}~/.bashrc${NC} or ${BOLD}~/.zshrc${NC}:"
    echo -e "   ${CYAN}export PATH=\"$TARGET_DIR:\$PATH\"${NC}"
else
    print_success "Target directory is already in your PATH"
fi

# Copy dotfiles section
print_step "Copying Dotfiles"
DOTFILES_DIR="$PROJECT_DIR/dotfiles"

# Ensure dotfiles directory exists
if [ ! -d "$DOTFILES_DIR" ]; then
    print_info "Creating dotfiles directory..."
    mkdir -p "$DOTFILES_DIR"
    if [ $? -eq 0 ]; then
        print_success "Dotfiles directory created"
    else
        print_error "Failed to create dotfiles directory"
    fi
else
    print_success "Dotfiles directory exists"
fi

# Copy .zshrc
if [ -f "$HOME/.zshrc" ]; then
    if cp "$HOME/.zshrc" "$DOTFILES_DIR/.zshrc" 2&amp;amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}.zshrc${NC} from home directory"
    else
        print_error "Failed to copy .zshrc"
    fi
else
    print_warning ".zshrc not found in home directory"
fi

# Copy ghostty config
GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"
if [ -d "$GHOSTTY_CONFIG_DIR" ]; then
    # Create ghostty subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/ghostty"
    if cp -r "$GHOSTTY_CONFIG_DIR"/* "$DOTFILES_DIR/ghostty/" 2&amp;amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}ghostty config${NC} from ~/.config/ghostty"
    else
        print_error "Failed to copy ghostty config"
    fi
else
    print_warning "Ghostty config directory not found at ~/.config/ghostty"
fi

# Copy nvim init.lua
NVIM_CONFIG="$HOME/.config/nvim/init.lua"
if [ -f "$NVIM_CONFIG" ]; then
    # Create nvim subdirectory in dotfiles
    mkdir -p "$DOTFILES_DIR/nvim"
    if cp "$NVIM_CONFIG" "$DOTFILES_DIR/nvim/init.lua" 2&amp;amp;amp;amp;gt;/dev/null; then
        print_item "Copied ${BOLD}nvim init.lua${NC} from ~/.config/nvim"
    else
        print_error "Failed to copy nvim init.lua"
    fi
else
    print_warning "Nvim init.lua not found at ~/.config/nvim/init.lua"
fi

echo -e "\n${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}${GREEN}🎉 Setup completed successfully!${NC}"
echo -e "${BOLD}${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="CLAUDE.md"&amp;amp;amp;gt;---
allowed-tools: Bash(tools/*)
description: scripts that can be perused and run where appropriate see the examples in this prompt
---

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

**Setup:**
```bash
uv sync
```

**Testing:**
### Use `pytest` for all testing in this project.
### When running all tests, use the Makefile and run test-fast:
### here is an example

```bash
make test-fast
```
### OR use the following bash command:
```bash
uv run pytest -v -n auto -m "not slow"
```


## IMPORTANT: Always Always use uv run when running tests
### Here is an example
```bash
uv run pytest test_collect.py::test_function_name -v -s
# Run specific test: pytest test_collect.py::test_function_name -v -s
```
**Code Quality:**
```bash
make lint     # Run ruff check
make format   # Run black formatter
make check    # Run all: lint, format, test
```

**Run MCP Server:**
```bash
uv run collect.py
```

**Plan Management:**
```bash
# Sync plans from filesystem to database
uv run -m repository.plan_service

# Test plan service functionality (uses separate test database)
uv run pytest repository/test_plan_service.py -v -s

# Test plan database operations
uv run pytest repository/test_plan_service.py::test_sync_plans -v -s

# Set up test database manually (optional - done automatically by tests)
uv run yoyo apply --config yoyo-test-plans.ini --batch

# Reset test database to clean state
rm data/test_plans.db &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; uv run yoyo apply --config yoyo-test-plans.ini --batch

# Test database management utilities
uv run python repository/test_database_setup.py setup
uv run python repository/test_database_setup.py reset
uv run python repository/test_database_setup.py cleanup
```

## Planning System

This project uses a structured planning approach for feature development with plans organized in `_docs/plans/`. Plans progress through three stages:

### Plan Lifecycle
1. **`drafts/`** - Initial plans under development or consideration
2. **`approved/`** - Reviewed plans ready for implementation
3. **`completed/`** - Implemented plans with results documented

### Plan Document Format

#### Draft/Approved Plans Should Include:
```markdown
# Plan: [Clear Action-Oriented Title]

## Overview
Brief description of what needs to be implemented and why

## Implementation Steps
### 1. [Step Name]
Detailed implementation instructions including:
- Specific file locations and line numbers
- Function signatures with type hints
- Implementation pseudo-code or actual code
- Error handling approach

### 2. [Next Step]
...

## Key Features
- List of main features/capabilities
- Expected benefits

## Testing Considerations
- Test scenarios to implement
- Edge cases to handle
- Performance considerations

## Example Usage
```python
# Code examples demonstrating the feature
```
```

#### Completed Plans Add:
- **Status**: COMPLETED (YYYY-MM-DD)
- **Implementation Summary**: What was actually done
- **Results**: Outcomes, verification, test results
- **Files Modified**: List with ✅/❌ status indicators

### Plan Naming Conventions
- Use descriptive names with underscores: `add_improve_prompt.md`
- Start with action verbs: add, fix, implement, create, update
- Keep names concise but clear about the purpose

### Automated Plan Processing

The repository includes tools for automated plan implementation:

```python
# Build git worktrees for all approved plans
from mcp__collect__build_worktrees import build_worktrees
result = await build_worktrees(auto_process=True)  # Uses Claude Code SDK

# Sync plans between filesystem and database
from repository.plan_service import PlanService
service = PlanService(conn)
result = service.sync_plans()  # Loads plans into SQLite with JSONB
```

Branch names are automatically derived from plan filenames:
- `add_improve_prompt.md` → `feature/add-improve-prompt`
- Underscores become hyphens, `feature/` prefix added

### Plan Management Database

Plans are tracked in `data/plans.db` with:
- **plans** table: Current state with JSONB data field
- **plan_history**: Audit trail of changes
- **plan_metrics**: Analytics and performance data

Use the PlanService class to:
- Load plans from disk to database
- Track plan status changes
- Detect content changes via SHA256 hashing
- Query plans by status, tags, or content

## Architecture Overview

This is an MCP (Model Context Protocol) server that provides web content fetching and multi-model AI analysis tools. The architecture follows these key patterns:

### Core Structure
- **collect.py**: Main MCP server entry point with FastMCP tool definitions
- **fetcher.py**: Handles URL fetching and content processing with clipboard integration
- **config.py**: Environment-based configuration with dotenv support
- **secret_manager.py**: Google Cloud Secret Manager integration for API keys

### Models Package
The `models/` directory contains unified API wrappers for different AI providers:
- **anthropic_mpc.py**: Anthropic Claude API integration
- **openai_mpc.py**: OpenAI API integration  
- **gemini_mcp.py**: Google Gemini API integration
- **xai_mcp.py**: XAI/Grok API integration

Each model wrapper follows the same pattern: configuration injection, secret management, and standardized methods like `send_message()`, `count_tokens()`, and `get_model_list()`.

### Packages
Additional specialized packages provide focused functionality:
- **reviewer/**: Code review automation system
  - `code_review.py`: CodeReviewer class for analyzing code diffs
  - Supports both file-based and git diff reviews
  - Generates individual model reviews and consolidated summaries
- **repository/**: Plan management and database operations
  - `plan_service.py`: PlanService class for filesystem-to-database sync
  - `plan_models.py`: Pydantic models for plan data with JSONB support
  - `database.py`: SQLite connection management with custom datetime adapters
  - Supports plan lifecycle tracking and content change detection


### Key Features
- **Async token counting**: All providers support async token counting with proper chunking
- **Multi-model workflows**: Send content to all AI models concurrently via `multi_model_code_review()`
- **Content processing**: HTML-to-markdown conversion using readabilipy and markdownify
- **Automatic chunking**: Handles large content (&amp;amp;amp;amp;gt;25k tokens) with intelligent splitting
- **Code review system**: Automated code review via `run_code_review()` and `run_git_diff_review()` tools
- **Prompt engineering**: Generate optimized AI prompts using Anthropic's experimental API via `generate_prompt()`
- **Documentation extraction**: Intelligent section extraction from web docs using `get_docs()` with AI filtering
- **Clipboard integration**: Direct content copying with `copy_clipboard()` and automatic clipboard support in fetchers
- **Enhanced model features**: Gemini model listing with token limits, unified `extract_text()` methods across all providers

### Configuration
Environment variables are loaded from `.env` file:
- GCP_PROJECT_ID (required)
- API key paths for Google Cloud Secret Manager:
  - ANTHROPIC_API_KEY_PATH
  - OPENAI_API_KEY_PATH
  - GEMINI_API_KEY_PATH
  - XAI_API_KEY_PATH
- Default model names for each provider
- Code review model configurations

### Directory Structure
```
collect/
├── data/
│   ├── prompts.db      # Original prompts database
│   └── plans.db        # Plan management database
├── _docs/
│   └── plans/
│       ├── drafts/     # Plans under development
│       ├── approved/   # Plans ready for implementation
│       └── completed/  # Implemented plans with results
├── migrations/         # Database migrations for prompts.db
├── migrations-plans/   # Database migrations for plans.db
├── repository/         # Plan management system
│   ├── plan_service.py # Plan filesystem-to-database sync
│   ├── plan_models.py  # Pydantic models for plan data
│   └── database.py     # SQLite connection management
├── models/            # AI provider API wrappers
└── reviewer/          # Code review automation
```

### Testing Strategy
- **IMPORTANT**:  When writing and designing tests, we only want live direct integration tests. Please only create live direct integration testing. Please do not use mocks. 

## Rules
- **IMPORTANT**: YOU MUST always use `uv run` to run tests.

## Workflow Rules
- I do not want a pr created if I don't have a branch already

## Tools

###IMPORTANT: 
I have a directory from the main project directory called: tools/* wherein there scripts stored that you can use
use

- All of my tools in this directory are on my path and can be called directly.
- You use these tools and see what they do by simply calling the tool name with `--llm`

Example 1:
```bash
extract --llm
```

Example 2: 
```bash
createdb --llm
```


&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="fetcher.py"&amp;amp;amp;gt;from typing import List
from mcp.server.fastmcp import Context
import pyperclip
import httpx
from models.anthropic_mpc import AnthropicMCP


class Fetcher:
    def __init__(self, ctx: Context = None) -&amp;amp;amp;amp;gt; None:
        self.ctx = ctx

    async def get(self, url: str) -&amp;amp;amp;amp;gt; str:
        """
        Fetch content from a single URL.

        Args:
            url: URL to fetch content from

        Returns:
            Content from the URL as a string
        """

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                content = response.text

                return content

            except httpx.HTTPError as e:
                return f"Error fetching {url}: {str(e)}"
            except Exception as e:
                return f"Error fetching {url}: {str(e)}"

    async def fetch_urls(self, urls: List[str]) -&amp;amp;amp;amp;gt; str:
        """
        Fetch content from multiple URLs and concatenate their responses.
        If token count exceeds 25000, content is split into chunks.

        Args:
            urls: List of URLs to fetch content from
            ctx: Optional context object for progress reporting

        Returns:
            Either concatenated content from all URLs as a string,
            or a list of content chunks if token count exceeds 25000
        """

        results = []

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for i, url in enumerate(urls):
                if self.ctx:
                    self.ctx.info(f"Fetching content from {url}")
                    await self.ctx.report_progress(i, len(urls))

                try:
                    response = await client.get(url)
                    response.raise_for_status()

                    results.append(f"\n\n--- Content from {url} --\n\n")
                    results.append(response.text)

                except httpx.HTTPError as e:
                    results.append(f"\n\n --- Error fetching {url}: {str(e)} ---\n\n")
                except Exception as e:
                    results.append(f"\n\n--- error fetching {url}: {str(e)} ---\n\n")

        if self.ctx:
            self.ctx.info("all urls processed")
            await self.ctx.report_progress(len(urls), len(urls))

        content = "".join(results)

        # Copy original content to clipboard
        pyperclip.copy(content)

        # Otherwise return the original content
        return content

    async def chunk_by_token_count(text: str, max_tokens: int = 25000) -&amp;amp;amp;amp;gt; List[str]:
        """
        Split text into chunks that are each under the specified token count.

        Args:
            text: The text to chunk
            max_tokens: Maximum tokens per chunk

        Returns:
            List of text chunks, each under max_tokens
        """

        # If text is short enough, return as a single chunk
        anthropic_mcp = AnthropicMCP()
        token_count = await anthropic_mcp.count_tokens(text, None)
        if token_count &amp;amp;amp;amp;lt;= max_tokens:
            return [text]

        # Split text into paragraphs as a starting point
        paragraphs = text.split("\n\n")
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0

        for paragraph in paragraphs:
            paragraph_tokens = await anthropic_mcp.count_tokens(
                paragraph + "\n\n", None
            )

            # If adding this paragraph would exceed the limit,
            # start a new chunk
            if current_chunk_tokens + paragraph_tokens &amp;amp;amp;amp;gt; max_tokens:
                # If the paragraph alone exceeds the limit, we split it further
                if paragraph_tokens &amp;amp;amp;amp;gt; max_tokens:
                    # Split by sentences or just characters if needed
                    sentences = paragraph.split(". ")
                    for sentence in sentences:
                        sentence_tokens = await anthropic_mcp.count_tokens(
                            sentence + ". ", None
                        )
                        if current_chunk_tokens + sentence_tokens &amp;amp;amp;amp;gt; max_tokens:
                            if current_chunk:
                                chunks.append("".join(current_chunk))
                            current_chunk = [sentence + ". "]
                            current_chunk_tokens = sentence_tokens
                        else:
                            current_chunk.append(sentence + ". ")
                            current_chunk_tokens += sentence_tokens
                else:
                    # Save the current chunk and start a new one
                    chunks.append("".join(current_chunk))
                    current_chunk = [paragraph + "\n\n"]
                    current_chunk_tokens = paragraph_tokens
            else:
                # Add paragraph to current chunk
                current_chunk.append(paragraph + "\n\n")
                current_chunk_tokens += paragraph_tokens

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append("".join(current_chunk))

        return chunks
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=";"&amp;amp;amp;gt;from typing import Listfrom pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""
    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""
    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""
    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "&amp;amp;amp;amp;lt;improved prompt&amp;amp;amp;amp;gt;",
                        "type": "text"
                    }
                ],
                "role": "user"
            },
            {
                "content": [
                    {
                        "text": "&amp;amp;amp;amp;lt;assistant prefill&amp;amp;amp;amp;gt;",
                        "type": "text"
                    }
                ],
                "role": "assistant"
            }
        ],
        "system": "",
        "usage": [
            {
                "input_tokens": 490,
                "output_tokens": 661
            }
        ]
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_diff.md"&amp;amp;amp;gt;# Test Code Review

## Diff

```diff
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,5 +1,8 @@
 def calculate_total(items):
+    if not items:
+        return 0
+        
     total = 0
     for item in items:
-        total += item.price
+        total += item.get('price', 0)
     return total
```&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/code_review.py"&amp;amp;amp;gt;import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -&amp;amp;amp;amp;gt; str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -&amp;amp;amp;amp;gt; str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
        self, output_dir: str, timestamp: str, failed_results: List
    ) -&amp;amp;amp;amp;gt; str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, "w", encoding="utf-8") as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -&amp;amp;amp;amp;gt; str:
        """Read and return content from input file."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -&amp;amp;amp;amp;gt; str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 🔴/🟡/🟢 | ... | ... |

Use emojis: 🔴 Critical, 🟡 Important, 🟢 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
        self, timestamp: str, from_file: str, results: LLMRunnerResults
    ) -&amp;amp;amp;amp;gt; Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": [],
        }

    def write_successful_results(
        self,
        results: LLMRunnerResults,
        output_dir: str,
        timestamp: str,
        summary: Dict[str, Any],
    ) -&amp;amp;amp;amp;gt; None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(result, response_text)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
        self, output_dir: str, timestamp: str, summary: Dict[str, Any]
    ) -&amp;amp;amp;amp;gt; str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
        self, from_file: str, to_file: Optional[str] = None
    ) -&amp;amp;amp;amp;gt; Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }

    async def review_diff_from_git(
        self, to_file: Optional[str] = None, staged_only: bool = True
    ) -&amp;amp;amp;amp;gt; Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ["git", "diff", "--staged"],
                    capture_output=True,
                    text=True,
                    check=True,
                )
            else:
                result = subprocess.run(
                    ["git", "diff"], capture_output=True, text=True, check=True
                )

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception("Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_code_review_live.py"&amp;amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"

        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir

        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] &amp;amp;amp;amp;gt;= 1
        assert (
            summary["successful_reviews"] + summary["failed_reviews"]
            == summary["total_models"]
        )
        assert "timestamp" in summary

        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) &amp;amp;amp;amp;gt;= 1  # At least summary file should exist

        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1

        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, "r") as f:
            summary_data = json.load(f)

        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]

        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)

            # Verify file contains expected content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                assert "Code Review" in content
                assert "**Model**:" in content
                assert "**Timestamp**:" in content
                assert "**Duration**:" in content
                assert "Generated by" in content

        # Print results for visibility
        print("\n✅ Live code review test completed successfully!")
        print("📊 Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")

        if summary["successful_reviews"] &amp;amp;amp;amp;gt; 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")

        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(
                temp_output_dir, staged_only=False
            )

            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"

            print("\n✅ Git diff review completed!")
            print(
                f"📊 Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed"
            )

        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"

        prompt = reviewer.create_code_review_prompt(test_content)

        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "🔴" in prompt and "🟡" in prompt and "🟢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] &amp;amp;amp;amp;gt; 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Should contain actual review content, not just raw API response
            assert len(content) &amp;amp;amp;amp;gt; 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON

            print(f"✅ Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/__init__.py"&amp;amp;amp;gt;# Reviewer package for code review functionality
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_code_review.py"&amp;amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import Mock, patch, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


# Module-level fixtures available to all test classes
@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def reviewer(temp_dir):
    """Create a CodeReviewer instance with temp directory."""
    return CodeReviewer(output_dir=temp_dir)


@pytest.fixture
def sample_diff_content():
    """Sample diff content for testing."""
    return """
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,6 @@
 def hello():
-    print("Hello")
+    print("Hello World")
+    return "greeting"
+
+def goodbye():
+    print("Goodbye")
"""


@pytest.fixture
def mock_model_result():
    """Create a mock successful model result."""
    return ModelResult(
        model="test-model",
        timestamp="2024-01-01T12:00:00",
        success=True,
        actual_model="test-model",
        duration_seconds=2.5,
        response={
            "choices": [
                {
                    "message": {
                        "content": "# Code Review\n\nThis is a test review response."
                    }
                }
            ]
        },
    )


@pytest.fixture
def mock_failed_result():
    """Create a mock failed model result."""
    return ModelResult(
        model="failed-model",
        timestamp="2024-01-01T12:00:00",
        success=False,
        error="API timeout error",
    )


@pytest.fixture
def mock_llm_results(mock_model_result, mock_failed_result):
    """Create mock LLMRunnerResults."""
    return LLMRunnerResults(
        successful_results=[mock_model_result],
        failed_results=[mock_failed_result],
        total_models=2,
        success_count=1,
        failure_count=1,
    )


class TestCodeReviewer:
    """Test suite for CodeReviewer class."""


class TestInitialization:
    """Test CodeReviewer initialization."""

    def test_default_initialization(self):
        """Test CodeReviewer with default parameters."""
        reviewer = CodeReviewer()
        assert reviewer.output_dir == "codereview"

    def test_custom_output_dir(self):
        """Test CodeReviewer with custom output directory."""
        custom_dir = "/tmp/custom_reviews"
        reviewer = CodeReviewer(output_dir=custom_dir)
        assert reviewer.output_dir == custom_dir


class TestExtractResponseText:
    """Test response text extraction from different model formats."""

    def test_extract_gemini_response(self, reviewer):
        """Test extracting text from Gemini response format."""
        gemini_response = {
            "candidates": [
                {"content": {"parts": [{"text": "This is a Gemini response"}]}}
            ]
        }
        result = reviewer.extract_response_text(gemini_response)
        assert result == "This is a Gemini response"

    def test_extract_openai_response(self, reviewer):
        """Test extracting text from OpenAI/XAI response format."""
        openai_response = {
            "choices": [{"message": {"content": "This is an OpenAI response"}}]
        }
        result = reviewer.extract_response_text(openai_response)
        assert result == "This is an OpenAI response"

    def test_extract_anthropic_response(self, reviewer):
        """Test extracting text from Anthropic response format."""
        anthropic_response = {"content": [{"text": "This is an Anthropic response"}]}
        result = reviewer.extract_response_text(anthropic_response)
        assert result == "This is an Anthropic response"

    def test_extract_non_dict_response(self, reviewer):
        """Test extracting text from non-dictionary response."""
        simple_response = "Simple string response"
        result = reviewer.extract_response_text(simple_response)
        assert result == "Simple string response"

    def test_extract_unknown_format(self, reviewer):
        """Test extracting text from unknown response format."""
        unknown_response = {"unknown": "format"}
        result = reviewer.extract_response_text(unknown_response)
        assert result == str(unknown_response)

    def test_extract_empty_gemini_response(self, reviewer):
        """Test extracting from empty Gemini response."""
        empty_response = {"candidates": []}
        result = reviewer.extract_response_text(empty_response)
        assert result == str(empty_response)


class TestMarkdownContent:
    """Test markdown content creation."""

    def test_create_markdown_content(self, reviewer, mock_model_result):
        """Test creating markdown content for a model result."""
        response_text = "Test review content"
        result = reviewer.create_markdown_content(mock_model_result, response_text)

        assert "# Code Review - test-model" in result
        assert "**Model**: test-model" in result
        assert "**Timestamp**: 2024-01-01T12:00:00" in result
        assert "**Duration**: 2.50 seconds" in result
        assert "Test review content" in result
        assert "*Generated by test-model via MCP Code Review Tool*" in result


class TestFileOperations:
    """Test file reading and writing operations."""

    def test_read_input_file_success(self, reviewer, temp_dir, sample_diff_content):
        """Test successfully reading an input file."""
        test_file = os.path.join(temp_dir, "test_diff.md")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        result = reviewer.read_input_file(test_file)
        assert result == sample_diff_content

    def test_read_input_file_not_found(self, reviewer):
        """Test reading a non-existent file."""
        with pytest.raises(FileNotFoundError, match="Input file .* not found"):
            reviewer.read_input_file("/nonexistent/file.md")

    def test_write_error_file(self, reviewer, temp_dir, mock_failed_result):
        """Test writing error file for failed results."""
        timestamp = "20240101_120000"
        failed_results = [mock_failed_result]

        error_filename = reviewer.write_error_file(temp_dir, timestamp, failed_results)

        assert error_filename == "errors_20240101_120000.md"
        error_path = os.path.join(temp_dir, error_filename)
        assert os.path.exists(error_path)

        with open(error_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review Errors" in content
        assert "**Failed Models**: 1" in content
        assert "### failed-model" in content
        assert "API timeout error" in content

    def test_write_summary_file(self, reviewer, temp_dir):
        """Test writing summary JSON file."""
        timestamp = "20240101_120000"
        summary = {
            "timestamp": timestamp,
            "input_file": "test.md",
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": ["model1_20240101_120000.md"],
        }

        summary_filename = reviewer.write_summary_file(temp_dir, timestamp, summary)

        assert summary_filename == "summary_20240101_120000.json"
        summary_path = os.path.join(temp_dir, summary_filename)
        assert os.path.exists(summary_path)

        with open(summary_path, "r", encoding="utf-8") as f:
            loaded_summary = json.load(f)

        assert loaded_summary == summary


class TestPromptCreation:
    """Test code review prompt creation."""

    def test_create_code_review_prompt(self, reviewer, sample_diff_content):
        """Test creating a comprehensive code review prompt."""
        prompt = reviewer.create_code_review_prompt(sample_diff_content)

        assert "comprehensive code review" in prompt
        assert sample_diff_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Suggestions" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt


class TestSummaryCreation:
    """Test summary creation and management."""

    def test_create_summary(self, reviewer, mock_llm_results):
        """Test creating a summary dictionary."""
        timestamp = "20240101_120000"
        from_file = "test.md"

        summary = reviewer.create_summary(timestamp, from_file, mock_llm_results)

        expected_summary = {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": [],
        }

        assert summary == expected_summary

    def test_write_successful_results(self, reviewer, temp_dir, mock_llm_results):
        """Test writing successful model results to files."""
        timestamp = "20240101_120000"
        summary = {"output_files": []}

        reviewer.write_successful_results(
            mock_llm_results, temp_dir, timestamp, summary
        )

        # Check that file was created
        expected_filename = "test-model_20240101_120000.md"
        expected_path = os.path.join(temp_dir, expected_filename)
        assert os.path.exists(expected_path)

        # Check that summary was updated
        assert expected_filename in summary["output_files"]

        # Check file content
        with open(expected_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review - test-model" in content
        assert "This is a test review response." in content


class TestReviewCode:
    """Test the main review_code method."""

    @pytest.mark.asyncio
    async def test_review_code_success(
        self, reviewer, temp_dir, sample_diff_content, mock_llm_results
    ):
        """Test successful code review execution."""
        # Create input file
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Mock dependencies
        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # Verify result structure
            assert result["status"] == "completed"
            assert result["output_directory"] == temp_dir
            # 1 success + 1 error + 1 summary
            assert result["files_created"] == 3

            # Verify files were created
            assert os.path.exists(
                os.path.join(temp_dir, "test-model_20240101_120000.md")
            )
            assert os.path.exists(os.path.join(temp_dir, "errors_20240101_120000.md"))
            assert os.path.exists(
                os.path.join(temp_dir, "summary_20240101_120000.json")
            )

    @pytest.mark.asyncio
    async def test_review_code_file_not_found(self, reviewer, temp_dir):
        """Test review_code with non-existent input file."""
        with pytest.raises(FileNotFoundError):
            await reviewer.review_code("/nonexistent/file.md", temp_dir)

    @pytest.mark.asyncio
    async def test_review_code_no_failures(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code with only successful results."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Create results with no failures
        success_only_results = LLMRunnerResults(
            successful_results=[
                ModelResult(
                    model="test-model",
                    timestamp="2024-01-01T12:00:00",
                    success=True,
                    actual_model="test-model",
                    duration_seconds=2.5,
                    response={"choices": [{"message": {"content": "Review content"}}]},
                )
            ],
            failed_results=[],
            total_models=1,
            success_count=1,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_only_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # 1 success + 1 summary (no error file)
            assert result["files_created"] == 2
            assert "error_file" not in result["summary"]


class TestReviewDiffFromGit:
    """Test git diff review functionality."""

    @pytest.mark.asyncio
    async def test_review_diff_from_git_staged(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing staged git diff."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            # Setup mocks
            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess call
            mock_subprocess.assert_called_once_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"
            assert result["summary"]["input_file"] == "git diff"

    @pytest.mark.asyncio
    async def test_review_diff_from_git_all_changes(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing all git changes."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=False)

            mock_subprocess.assert_called_once_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

            assert result["summary"]["source"] == "git_diff_all"

    @pytest.mark.asyncio
    async def test_review_diff_no_changes(self, reviewer, temp_dir):
        """Test git diff with no changes."""
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value.stdout = ""
            mock_subprocess.return_value.check_returncode = Mock()

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_not_found(self, reviewer, temp_dir):
        """Test git diff when git is not installed."""
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_error(self, reviewer, temp_dir):
        """Test git diff with git command error."""
        import subprocess

        with patch(
            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
        ):
            with pytest.raises(Exception, match="Git diff failed"):
                await reviewer.review_diff_from_git(temp_dir)


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_extract_response_malformed_gemini(self, reviewer):
        """Test extracting from malformed Gemini response."""
        malformed = {"candidates": [{"content": {"parts": []}}]}  # Empty parts
        result = reviewer.extract_response_text(malformed)
        assert result == str(malformed)

    def test_extract_response_malformed_openai(self, reviewer):
        """Test extracting from malformed OpenAI response."""
        malformed = {"choices": [{"message": {}}]}  # Missing content
        result = reviewer.extract_response_text(malformed)
        assert result == ""

    def test_extract_response_empty_anthropic(self, reviewer):
        """Test extracting from empty Anthropic response."""
        empty = {"content": []}
        result = reviewer.extract_response_text(empty)
        assert result == str(empty)

    @pytest.mark.asyncio
    async def test_review_code_with_default_output_dir(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code using default output directory from None."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        success_results = LLMRunnerResults(
            successful_results=[],
            failed_results=[],
            total_models=0,
            success_count=0,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
            patch("os.makedirs") as mock_makedirs,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            # Test with None to_file (should use default)
            result = await reviewer.review_code(input_file, None)

            # Should use reviewer's default output_dir
            mock_makedirs.assert_called_with(temp_dir, exist_ok=True)
            assert result["output_directory"] == temp_dir


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_code_review_integration.py"&amp;amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import patch, MagicMock, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


class TestCodeReviewIntegration:
    """Test integration between .claude/commands/model_code_review.md and code_review.py"""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_diff_content(self):
        """Sample diff content for testing"""
        return """diff --git a/src/auth.py b/src/auth.py
index 1234567..abcdefg 100644
--- a/src/auth.py
+++ b/src/auth.py
@@ -1,5 +1,10 @@
 def authenticate_user(username, password):
-    # Simple authentication
-    return username == "admin" and password == "secret"
+    # Improved authentication with validation
+    if not username or not password:
+        return False
+    
+    # TODO: Add proper password hashing
+    return username == "admin" and password == "secret123"
 
 def get_user_role(username):
     return "admin" if username == "admin" else "user"
"""

    @pytest.fixture
    def sample_diff_file(self, temp_dir, sample_diff_content):
        """Create sample diff file for testing"""
        diff_file = os.path.join(temp_dir, "test_diff.md")
        with open(diff_file, "w") as f:
            f.write(sample_diff_content)
        return diff_file

    @pytest.fixture
    def mock_llm_results(self):
        """Mock LLM runner results matching expected format"""
        successful_results = [
            ModelResult(
                model="claude-3-5-sonnet",
                success=True,
                response={
                    "content": [
                        {
                            "text": "## Code Review Analysis\n\n### Security Issues\n🔴 **Critical**: Hardcoded password in authentication logic\n\n### Recommendations\n- Implement proper password hashing\n- Add input validation"
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:00",
                duration_seconds=2.5,
                error=None,
            ),
            ModelResult(
                model="gpt-4-turbo",
                success=True,
                response={
                    "choices": [
                        {
                            "message": {
                                "content": "## Security Analysis\n\n🔴 **High Risk**: Authentication uses plaintext password comparison\n🟡 **Medium**: Missing input validation for empty credentials"
                            }
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:05",
                duration_seconds=3.1,
                error=None,
            ),
        ]

        failed_results = [
            ModelResult(
                model="gemini-pro",
                success=False,
                response=None,
                timestamp="2024-01-01T12:00:10",
                duration_seconds=0,
                error="API rate limit exceeded",
            )
        ]

        return LLMRunnerResults(
            successful_results=successful_results,
            failed_results=failed_results,
            total_models=3,
            success_count=2,
            failure_count=1,
        )

    @pytest.mark.asyncio
    async def test_code_review_from_file_integration(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test the complete file-based code review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config", "gpt": "config"}

            # Initialize reviewer with temp directory
            reviewer = CodeReviewer(output_dir=temp_dir)

            # Run review (simulates mcp__collect__run_code_review)
            result = await reviewer.review_code(sample_diff_file, temp_dir)

            # Verify return structure matches command expectations
            assert result["status"] == "completed"
            assert "summary" in result
            assert "output_directory" in result
            assert "files_created" in result

            # Verify output files were created as documented in command
            files = os.listdir(temp_dir)

            # Should have individual model reviews
            claude_files = [f for f in files if f.startswith("claude-3-5-sonnet")]
            gpt_files = [f for f in files if f.startswith("gpt-4-turbo")]
            assert len(claude_files) == 1
            assert len(gpt_files) == 1

            # Should have summary file
            summary_files = [f for f in files if f.startswith("summary_")]
            assert len(summary_files) == 1

            # Should have errors file for failed models
            error_files = [f for f in files if f.startswith("errors_")]
            assert len(error_files) == 1

            # Verify summary JSON structure
            summary_file = os.path.join(temp_dir, summary_files[0])
            with open(summary_file, "r") as f:
                summary_data = json.load(f)

            assert summary_data["total_models"] == 3
            assert summary_data["successful_reviews"] == 2
            assert summary_data["failed_reviews"] == 1
            assert len(summary_data["output_files"]) == 2

    @pytest.mark.asyncio
    async def test_git_diff_review_integration(self, temp_dir, mock_llm_results):
        """Test git diff review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
            patch("subprocess.run") as mock_subprocess,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config"}

            # Mock git diff output
            mock_subprocess.return_value = MagicMock(
                stdout="diff --git a/test.py b/test.py\n+def new_function():\n+    pass",
                returncode=0,
            )

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Test staged-only review (Option A from command)
            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess called with correct git command
            mock_subprocess.assert_called_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"

            # Test all changes review
            await reviewer.review_diff_from_git(temp_dir, staged_only=False)
            mock_subprocess.assert_called_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

    def test_claude_command_workflow_documentation(self):
        """Verify that the Claude command documentation matches code_review.py capabilities"""
        reviewer = CodeReviewer()

        # Test that CodeReviewer has methods mentioned in command
        assert hasattr(
            reviewer, "review_code"
        ), "Should support file-based review (Option B)"
        assert hasattr(
            reviewer, "review_diff_from_git"
        ), "Should support git diff review (Option A)"

        # Test that review_code signature matches command usage
        import inspect

        sig = inspect.signature(reviewer.review_code)
        assert "from_file" in sig.parameters, "Should accept from_file parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

        # Test that review_diff_from_git signature matches command usage
        sig = inspect.signature(reviewer.review_diff_from_git)
        assert "staged_only" in sig.parameters, "Should accept staged_only parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

    def test_output_file_naming_convention(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test that output files follow naming convention documented in command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {}

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Mock timestamp for predictable filenames
            with patch("reviewer.code_review.datetime") as mock_datetime:
                mock_datetime.now.return_value.strftime.return_value = "20241201_143052"

                # Run async test
                import asyncio

                asyncio.run(reviewer.review_code(sample_diff_file, temp_dir))

            files = os.listdir(temp_dir)

            # Verify naming matches documentation: {model}_YYYYMMDD_HHMMSS.md
            expected_patterns = [
                "claude-3-5-sonnet_20241201_143052.md",
                "gpt-4-turbo_20241201_143052.md",
                "summary_20241201_143052.json",
                "errors_20241201_143052.md",
            ]

            for pattern in expected_patterns:
                assert pattern in files, f"Expected file {pattern} not found in {files}"

    def test_prompt_structure_matches_command_requirements(self):
        """Test that code review prompt includes all sections mentioned in command"""
        reviewer = CodeReviewer()
        prompt = reviewer.create_code_review_prompt("sample code")

        # Verify prompt includes all required sections from command documentation
        required_sections = [
            "Overall Assessment",
            "Issues Found",
            "Security vulnerabilities",
            "Bugs and logic errors",
            "Performance issues",
            "Code quality problems",
            "Testing gaps",
            "Suggestions for Improvement",
            "Positive Aspects",
            "Risk Assessment",
            "Summary Table",
        ]

        for section in required_sections:
            assert section in prompt, f"Prompt missing required section: {section}"

        # Verify emoji risk indicators are included
        risk_emojis = ["🔴", "🟡", "🟢"]
        for emoji in risk_emojis:
            assert emoji in prompt, f"Prompt missing risk emoji: {emoji}"

    @pytest.mark.asyncio
    async def test_error_handling_matches_command_expectations(self, temp_dir):
        """Test error handling for scenarios mentioned in command troubleshooting"""
        reviewer = CodeReviewer(output_dir=temp_dir)

        # Test "No git changes found" scenario
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value = MagicMock(stdout="", returncode=0)

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

        # Test file not found scenario
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_dir)

        # Test git not available scenario
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    def test_response_text_extraction_supports_all_models(self):
        """Test that response extraction works for all model formats mentioned in command"""
        reviewer = CodeReviewer()

        # Test Anthropic Claude format
        anthropic_response = {"content": [{"text": "Claude review content"}]}
        assert (
            reviewer.extract_response_text(anthropic_response)
            == "Claude review content"
        )

        # Test OpenAI GPT format
        openai_response = {"choices": [{"message": {"content": "GPT review content"}}]}
        assert reviewer.extract_response_text(openai_response) == "GPT review content"

        # Test Google Gemini format
        gemini_response = {
            "candidates": [{"content": {"parts": [{"text": "Gemini review content"}]}}]
        }
        assert (
            reviewer.extract_response_text(gemini_response) == "Gemini review content"
        )

        # Test fallback for XAI Grok or unknown formats
        unknown_response = "Direct string response"
        assert (
            reviewer.extract_response_text(unknown_response) == "Direct string response"
        )
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview_live/errors_20250601_085957.md"&amp;amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_085957
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.646644

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.833688

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.262231

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.434107

            &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview_live/summary_20250601_085957.json"&amp;amp;amp;gt;{
  "timestamp": "20250601_085957",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_085957.md"
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview/errors_20250601_084959.md"&amp;amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084959
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.675555

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.837903

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.045514

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.245033

            &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview/summary_20250601_084601.json"&amp;amp;amp;gt;{
  "timestamp": "20250601_084601",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084601.md"
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview/summary_20250601_084959.json"&amp;amp;amp;gt;{
  "timestamp": "20250601_084959",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084959.md"
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/test_codereview/errors_20250601_084601.md"&amp;amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084601
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:43:35.383459

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:44:24.400527

            
                ### grok-3
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:45:01.153872

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 504 Deadline Exceeded
                - **Timestamp**: 2025-06-01T08:46:01.157505

            &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/live_test_codereview/grok-3-mini-fast-latest_20250601_091219.md"&amp;amp;amp;gt;
            # Code Review - grok-3-mini-fast-latest

            **Model**: grok-3-mini-fast-latest
            **Timestamp**: 2025-06-01T09:11:32.476330
            **Duration**: 17.72 seconds

            ---

            Below is a comprehensive code review of the provided diff for the `calculate_total` function in `test.py`, based on the instructions. I'll address each section step by step, focusing on actionable feedback to improve code quality, security, and maintainability.

---

### 1. **Overall Assessment**

- **Brief summary of what changed and why**: The changes enhance the robustness of the `calculate_total` function. An explicit check for an empty `items` list was added to return 0, preventing potential runtime errors. Additionally, the price access was modified from direct attribute access (`item.price`) to a dictionary-based approach (`item.get('price', 0)`), likely to handle cases where `items` are dictionaries rather than custom objects. This suggests a shift in data structure (e.g., from object-oriented to dictionary-based data), possibly to accommodate more flexible input formats or to avoid `AttributeError` when `price` is missing.

- **Impact on the codebase**: The scope is narrow, affecting only this function, but the significance is moderate. It improves error handling and reduces the risk of crashes in edge cases (e.g., empty lists or missing keys), making the code more resilient. However, if this function is used across the application, the change could introduce inconsistencies if other parts of the codebase still expect `items` to be objects with attributes. Overall, it aligns with defensive programming principles.

- **Alignment with best practices**: The changes are positive, as they address common pitfalls like unhandled edge cases and attribute access errors. Using `dict.get()` with a default value follows best practices for handling optional keys, promoting safer code. However, the function could benefit from additional improvements in type safety, documentation, and testing to fully align with modern Python standards (e.g., PEP 8, type hints).

---

### 2. **Issues Found**

I analyzed the diff for potential security, bugs, performance, code quality, and testing issues. Here's a breakdown:

- **Security vulnerabilities**: 
  - No significant security issues were identified. This function performs a simple summation and doesn't involve user input, network operations, or sensitive data handling, so risks like injection or data exposure are low. However, if `items` comes from an untrusted source (e.g., user input or external API), the function could be vulnerable to malicious data (e.g., if `items` contains non-numeric values). This isn't directly addressed in the change.

- **Bugs and logic errors**:
  - **Edge case handling**: The added empty list check is good, but the function now assumes `items` elements are dictionaries (due to `item.get()`). If `items` contains non-dictionary types (e.g., custom objects or other data structures), a `AttributeError` could still occur when calling `get()`. For example, if `items` includes strings or lists, the code would fail. The original code had a similar issue with attribute access, but this change doesn't fully resolve type mismatches.
  - **Error handling**: No broader error handling is present (e.g., for non-numeric `price` values). If a dictionary has a `price` key with a non-numeric value (e.g., a string), a `TypeError` could occur during addition. The `get()` method defaults to 0 for missing keys, which is helpful, but it doesn't validate the type or value of existing keys.
  - **Logic errors**: The function works for valid inputs but could return incorrect results if `items` is not a list (e.g., a string or None). The iteration (`for item in items`) assumes `items` is iterable, which could raise a `TypeError` if it's not.

- **Performance issues**:
  - No major performance concerns. The function uses a linear O(n) loop, which is efficient for most use cases. The addition of the empty check and `get()` method adds negligible overhead. However, if `items` is very large, the function could benefit from Python's built-in `sum()` function for slight optimization.

- **Code quality problems**:
  - **Readability and maintainability**: The code is straightforward, but the shift from attribute access to dictionary access could confuse maintainers if the data structure isn't documented. There's no type hinting, docstring, or comments, making it harder to understand the function's intent or constraints. Code style is mostly fine, but the indentation in the diff looks consistent.
  - **Complexity**: Low cyclomatic complexity, which is good. However, the function could be simplified using Python's `sum()` with a generator expression, reducing code duplication.

- **Testing gaps**:
  - No tests are provided or referenced in the diff, so it's unclear if the changes are covered. Potential gaps include:
    - Tests for edge cases like empty lists, missing keys, non-dictionary items, or invalid data types.
    - Regression tests to ensure the function still works for valid inputs (e.g., lists of dictionaries or objects).
    - The added changes improve robustness, but without tests, there's no assurance that they don't introduce new bugs. Unit tests should cover various scenarios to achieve good code coverage.

---

### 3. **Suggestions for Improvement**

Here are specific, actionable recommendations to enhance the code:

- **Code structure and organization**:
  - Add type hints to clarify expected input and output types, e.g., `def calculate_total(items: list[dict]) -&amp;amp;amp;amp;gt; int`. This would make the function more explicit about assuming dictionary-based items and help catch type errors early with tools like mypy.
  - Consider refactoring the function to handle both dictionary and object-based inputs if needed, e.g., by checking the type of each item and using `getattr(item, 'price', 0)` for objects or `item.get('price', 0)` for dicts. This would make the function more flexible.

- **Error handling improvements**:
  - Add input validation at the start of the function, e.g., check if `items` is iterable and raise a custom error (e.g., `ValueError`) if not. Also, handle potential non-numeric `price` values by adding a type check or conversion, e.g., `total += float(item.get('price', 0))` to avoid `TypeError`.
  - For better robustness, use a try-except block around the loop to catch unexpected errors and log them or return a default value, but avoid overusing exceptions for performance reasons.

- **Performance optimizations**:
  - Replace the manual loop with a more concise and potentially faster approach using `sum()`, e.g., `return sum(item.get('price', 0) for item in items or [])`. This handles the empty case automatically and reduces code lines. The `or []` ensures that if `items` is None, it defaults to an empty list, avoiding errors.

- **Better naming and documentation**:
  - Add a docstring to the function describing its purpose, parameters, return value, and any assumptions (e.g., that items are dictionaries). Example:
    ```
    def calculate_total(items):
        """
        Calculate the total price from a list of items.

        Args:
            items (list): A list of dictionaries, each containing a 'price' key.

        Returns:
            int: The sum of all prices, or 0 if no items are provided.

        Raises:
            ValueError: If items is not iterable or contains invalid data.
        """
        # ... rest of the code
    ```
  - Improve variable naming if needed; `total` is fine, but `items` could be more descriptive (e.g., `item_list`) if the context isn't clear.

- **Refactoring opportunities**:
  - Simplify the code using `sum()` as mentioned earlier. If this function is part of a larger module, consider extracting price retrieval into a separate helper function (e.g., `get_price(item)`) to handle different data types, improving reusability and reducing duplication.

---

### 4. **Positive Aspects**

The changes demonstrate good engineering practices and improve the code's reliability:

- **Good patterns and practices**: Adding the empty list check and using `dict.get()` with a default value is a solid example of defensive programming. It prevents common errors like `AttributeError` or index errors, making the function more robust without overcomplicating the logic.
- **Clear, readable code**: The updated logic is straightforward and easy to follow. The changes maintain simplicity while addressing potential issues, which enhances maintainability.
- **Proper error handling**: The use of `get('price', 0)` elegantly handles missing keys, reducing the need for explicit try-except blocks and improving code flow.
- **Well-structured logic**: The function remains concise and focused on a single responsibility (summing prices), adhering to the single-responsibility principle.

Overall, the diff shows thoughtful improvements that make the code more resilient to real-world inputs.

---

### 5. **Risk Assessment**

- **High Risk**: None identified. The changes are localized and don't introduce breaking changes or security threats, but improper handling of data types could lead to runtime errors in certain scenarios.
- **Medium Risk**: Potential type mismatches (e.g., if `items` elements are not dictionaries) could cause errors or incorrect results. Additionally, lack of testing might lead to undetected bugs in production.
- **Low Risk**: Minor issues like missing documentation or code style inconsistencies, which don't affect functionality but could impact long-term maintainability.

---

### Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential type mismatch for `items` elements | 🟡 Important | Assumes items are dictionaries; could raise errors if elements are custom objects or other types. | Add type hints and input validation, e.g., check if item is dict or use a flexible getter function. |
| Missing error handling for non-numeric prices or non-iterable inputs | 🟡 Important | Could raise `TypeError` if `price` is not a number or if `items` is not iterable. | Add type checks or use safe conversions, e.g., `float(item.get('price', 0))`, and validate `items` at the start. |
| Lack of documentation and docstrings | 🟢 Minor | No explanation of function behavior, parameters, or assumptions, reducing readability. | Add a descriptive docstring and consider inline comments for complex logic. |
| Testing gaps for edge cases | 🟡 Important | No visible tests for empty lists, missing keys, or invalid inputs, risking undetected regressions. | Implement unit tests covering various scenarios, e.g., using pytest with cases for empty, valid, and invalid inputs. |
| Opportunity for code simplification | 🟢 Minor | Manual loop could be replaced with `sum()` for conciseness and performance. | Refactor to use `sum(item.get('price', 0) for item in items or [])` to handle edges automatically. |

This review provides a balanced, actionable critique to help refine the code. If you have additional context (e.g., the rest of the codebase or testing framework), I can refine this further!

            ---
            *Generated by grok-3-mini-fast-latest via MCP Code Review Tool*
        &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/live_test_codereview/summary_20250601_091219.json"&amp;amp;amp;gt;{
  "timestamp": "20250601_091219",
  "input_file": "test_diff.md",
  "total_models": 4,
  "successful_reviews": 4,
  "failed_reviews": 0,
  "output_files": [
    "gemini-2.0-flash_20250601_091219.md",
    "o3-mini-2025-01-31_20250601_091219.md",
    "grok-3-mini-fast-latest_20250601_091219.md",
    "claude-opus-4-20250514_20250601_091219.md"
  ]
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/live_test_codereview/o3-mini-2025-01-31_20250601_091219.md"&amp;amp;amp;gt;
            # Code Review - o3-mini-2025-01-31

            **Model**: o3-mini-2025-01-31
            **Timestamp**: 2025-06-01T09:11:23.235642
            **Duration**: 9.24 seconds

            ---

            Below is a comprehensive review of the code diff:

──────────────────────────────
1. Overall Assessment
──────────────────────────────
• Summary of Changes:
 – An early exit is added to handle the case where the items list is empty (returning 0).
 – Instead of directly accessing an attribute (item.price), the code now uses the dictionary “get” method (item.get('price', 0)) to retrieve the price value.
 – This indicates a shift in expectation from an object with a price attribute to a dictionary-like object where "price" is a key.
  
• Impact on the Codebase:
 – The changes are localized to the calculate_total function in test.py.
 – The modifications improve robustness when items is empty and when an item does not contain a "price" key.
 – It may affect other parts of the system if they pass objects with a price attribute rather than dictionaries; integration testing is advised.
  
• Alignment with Best Practices:
 – Handling an empty list immediately is a good practice to avoid unnecessary computation.
 – Using item.get with a default value enhances fault tolerance for missing keys.
 – Code readability is maintained, though further documentation could be added.

──────────────────────────────
2. Issues Found
──────────────────────────────
• Security Vulnerabilities:
 – No apparent security concerns such as injection risks or data exposures. The function’s scope is narrow and does not involve external inputs.

• Bugs and Logic Errors:
 – Type Expectations: Changing from attribute access to dictionary access may break callers expecting objects with attributes. Confirm that all items are dictionaries.
 – Implicit Assumptions: The code assumes every item is a dict-like object. There’s no type-checking, so if an unexpected type is encountered, it might raise an AttributeError or TypeError.
 – Early Exit:
  • The early return for empty items is clear, but if items is None (i.e., not provided), it might lead to unexpected None errors unless the caller guarantees a list.

• Performance Issues:
 – The function’s algorithm is simple and works in O(n) time. There are no performance bottlenecks even for larger lists.
 – Memory usage is minimal.

• Code Quality Problems:
 – Readability is high; however, a brief comment explaining the change in expected item format (from object to dict) could help future maintainers.
 

            ---
            *Generated by o3-mini-2025-01-31 via MCP Code Review Tool*
        &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/live_test_codereview/claude-opus-4-20250514_20250601_091219.md"&amp;amp;amp;gt;
            # Code Review - claude-opus-4-20250514

            **Model**: claude-opus-4-20250514
            **Timestamp**: 2025-06-01T09:11:50.195387
            **Duration**: 29.30 seconds

            ---

            ## Code Review: calculate_total Function Enhancement

### 1. **Overall Assessment**

**Summary of Changes:**
- Added null/empty check for the `items` parameter
- Changed from attribute access (`item.price`) to dictionary access with default value (`item.get('price', 0)`)

**Impact:**
- Improves robustness by handling edge cases (empty input and missing price fields)
- Changes the expected data structure from objects with price attributes to dictionaries
- Prevents potential AttributeError and KeyError exceptions

**Best Practices Alignment:**
- ✅ Defensive programming with input validation
- ✅ Graceful handling of missing data
- ⚠️ Inconsistent data structure assumption (mixing objects and dictionaries)

### 2. **Issues Found**

**Code Quality Problems:**
1. **Inconsistent Data Model**: The change from `item.price` to `item.get('price', 0)` suggests a shift from objects to dictionaries, but this isn't documented or type-hinted
2. **Silent Failure**: Items without prices are silently treated as having price 0, which might mask data quality issues
3. **Missing Type Hints**: No type annotations to clarify expected input/output types
4. **No Documentation**: Function lacks docstring explaining purpose, parameters, and return value

**Potential Logic Issues:**
1. **Type Assumption**: Code assumes items are dictionaries but doesn't validate this
2. **Numeric Type Assumption**: No validation that 'price' values are numeric

### 3. **Suggestions for Improvement**

```python
from typing import List, Dict, Union, Optional
import logging

def calculate_total(items: Optional[List[Dict[str, Union[int, float]]]]) -&amp;amp;amp;amp;gt; Union[int, float]:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dictionaries with 'price' key, or None/empty list
        
    Returns:
        Total price as int or float. Returns 0 for empty/None input.
        
    Raises:
        TypeError: If price value is not numeric
        ValueError: If price value is negative
    """
    if not items:
        return 0
    
    total = 0
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            raise TypeError(f"Item at index {idx} is not a dictionary")
            
        price = item.get('price', None)
        if price is None:
            logging.warning(f"Item at index {idx} has no price field")
            continue
            
        if not isinstance(price, (int, float)):
            raise TypeError(f"Price for item at index {idx} is not numeric: {price}")
            
        if price &amp;amp;amp;amp;lt; 0:
            raise ValueError(f"Price for item at index {idx} is negative: {price}")
            
        total += price
        
    return total
```

**Alternative approach for mixed data types:**
```python
def calculate_total(items: Optional[List[Union[Dict, object]]]) -&amp;amp;amp;amp;gt; Union[int, float]:
    """Handle both dictionary and object inputs."""
    if not items:
        return 0
    
    total = 0
    for item in items:
        if isinstance(item, dict):
            price = item.get('price', 0)
        elif hasattr(item, 'price'):
            price = item.price
        else:
            price = 0
            
        total += price
        
    return total
```

### 4. **Positive Aspects**

- ✅ **Good defensive programming**: Adding the empty check prevents errors on None/empty inputs
- ✅ **Graceful degradation**: Using `.get()` with default prevents KeyError exceptions
- ✅ **Maintains backward compatibility**: Function signature remains unchanged
- ✅ **Simple and readable**: The logic remains straightforward and easy to understand

### 5. **Risk Assessment**

**Medium Risk** 🟡
- **Breaking Change**: Switching from attribute to dictionary access could break existing code that passes objects
- **Data Quality**: Silent handling of missing prices could hide data issues in production
- **Type Safety**: Lack of validation could lead to runtime errors with unexpected data types

**Mitigation Strategies:**
1. Add comprehensive unit tests covering all data type scenarios
2. Implement logging for items with missing prices
3

            ---
            *Generated by claude-opus-4-20250514 via MCP Code Review Tool*
        &amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="reviewer/live_test_codereview/gemini-2.0-flash_20250601_091219.md"&amp;amp;amp;gt;
            # Code Review - gemini-2.0-flash

            **Model**: gemini-2.0-flash
            **Timestamp**: 2025-06-01T09:11:16.427900
            **Duration**: 6.81 seconds

            ---

            ## Test Code Review

### 1. **Overall Assessment**

The diff introduces two key changes to the `calculate_total` function:

1.  A check for an empty `items` list, returning 0 in that case.
2.  A change in how the price is accessed: from `item.price` to `item.get('price', 0)`.

The first change handles a potential edge case, preventing errors when the input is empty. The second change makes the code more robust by handling cases where an item might not have a `price` attribute directly, but rather stores it as a dictionary key. These changes enhance the robustness and reliability of the function. The scope is relatively small, impacting only the `calculate_total` function. These changes generally align with best practices for defensive programming and error handling.

### 2. **Issues Found**

*   **Potential Type Error (🟡)**:  While `item.get('price', 0)` handles the absence of the 'price' key, it assumes the value associated with the 'price' key (if it exists) will be a number that can be added to `total`. If `item['price']` exists but is a string (e.g., "unknown"), a `TypeError` would still occur.
*   **Limited Input Validation (🟡)**: The code assumes that each `item` in `items` is a dictionary. It doesn't validate that `items` is even a list, or that each element within it is a dictionary-like object.

### 3. **Suggestions for Improvement**

*   **Type Validation and Error Handling (Important):**  Implement more robust type validation, either with `isinstance` checks or using a try-except block:

    ```python
    def calculate_total(items):
        if not items:
            return 0

        total = 0
        for item in items:
            try:
                price = item.get('price', 0)
                if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
                total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.
        return total
    ```

*   **Consider a dedicated Item class (Minor):** If the structure of `items` is fixed (i.e., always containing dictionaries with a 'price'), consider defining a dedicated `Item` class with a `price` attribute. This would improve code readability and maintainability.

*   **Add input validation (Minor):** Assert that `items` is a list and each element is either a dictionary or an object with a `get` method.

    ```python
    def calculate_total(items):
        if not isinstance(items, list):
            raise TypeError("items must be a list")

        if not items:
            return 0

        total = 0
        for item in items:
            if not hasattr(item, 'get') and not isinstance(item, dict):
                raise TypeError("Each item must be a dictionary or an object with a 'get' method")
              if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
              total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.

        return total
    ```

### 4. **Positive Aspects**

*   **Handles Empty Input (🟢):** The addition of the `if not items` check is a good practice for handling edge cases and preventing potential errors.
*   **Robust Price Access (🟢):** Using `item.get('price', 0)` is a good way to handle cases where the `price` attribute may not be directly available, providing a default value of 0 if the key is missing.

### 5. **Risk Assessment**

*   **Medium Risk**:  The lack of explicit type validation for the `price` can still lead to runtime errors. Implementing the suggested improvement involving the `try-except` block significantly mitigates this risk.

## Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential Type Error | 🟡 |  If `item

            ---
            *Generated by gemini-2.0-flash via MCP Code Review Tool*
        
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="migrations/20250810_02_add-github-url-to-prompt-history.sql"&amp;amp;amp;gt;-- Add github_url column to prompt_history table to track project association in historical records
-- depends: 20250810_01_add-projects-table

-- Add github_url column to prompt_history table
ALTER TABLE prompt_history ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Add index for efficient queries by github_url
CREATE INDEX IF NOT EXISTS idx_prompt_history_github_url ON prompt_history(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_history_github_url;
-- ALTER TABLE prompt_history DROP COLUMN github_url;&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="migrations/20250727_01_create-prompt-tables.sql"&amp;amp;amp;gt;-- Create prompt tables for prompt storage, versioning, and metrics
-- depends: 

-- Current prompt table
CREATE TABLE IF NOT EXISTS prompt (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    data JSONB NOT NULL,
    version INTEGER DEFAULT 1,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Historical versions table
CREATE TABLE IF NOT EXISTS prompt_history (
    id TEXT,
    version INTEGER,
    data JSONB NOT NULL,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    change_summary TEXT,
    PRIMARY KEY (id, version)
);

-- Metrics time-series table (optimized for prompt tracking)
CREATE TABLE IF NOT EXISTS prompt_metrics (
    prompt_id TEXT,
    version INTEGER,
    metric_name TEXT,
    step INTEGER,
    value REAL,
    timestamp TIMESTAMP,
    PRIMARY KEY (prompt_id, version, metric_name, step)
);

-- Performance-critical indexes
CREATE INDEX IF NOT EXISTS idx_prompt_hash ON prompt(content_hash);
CREATE INDEX IF NOT EXISTS idx_prompt_updated ON prompt(updated_at);
CREATE INDEX IF NOT EXISTS idx_prompt_history_created ON prompt_history(created_at);
CREATE INDEX IF NOT EXISTS idx_prompt_metrics_time ON prompt_metrics(timestamp);

-- Expression indexes on JSONB fields for common queries
CREATE INDEX IF NOT EXISTS idx_prompt_status ON prompt(data -&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; '$.status');
CREATE INDEX IF NOT EXISTS idx_prompt_type ON prompt(data -&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; '$.type');&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="migrations/20250810_01_add-projects-table.sql"&amp;amp;amp;gt;-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/test_database.py"&amp;amp;amp;gt;import pytest
import sqlite3
import os

from repository.database import SQLite3Database


@pytest.fixture
def test_db():
    """Create a test database instance"""
    test_db_path = "test_collect.db"
    db = SQLite3Database(db_path=test_db_path)
    yield db
    # Cleanup
    if os.path.exists(test_db_path):
        os.remove(test_db_path)


def test_database_connection_basic(test_db):
    """Test basic database connection establishment"""
    with test_db.get_connection() as conn:
        assert conn is not None
        assert isinstance(conn, sqlite3.Connection)
        # Test basic query
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_connection_read_only(test_db):
    """Test read-only database connection"""
    with test_db.get_connection(read_only=True) as conn:
        assert conn is not None
        # Should be able to read
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_row_factory(test_db):
    """Test that Row factory is properly configured"""
    with test_db.get_connection() as conn:
        cursor = conn.execute("SELECT 1 as test_col")
        row = cursor.fetchone()
        # Should be able to access by column name
        assert row["test_col"] == 1


def test_database_pragma_settings(test_db):
    """Test that PRAGMA settings are applied correctly"""
    with test_db.get_connection() as conn:
        # Check foreign keys
        cursor = conn.execute("PRAGMA foreign_keys")
        assert cursor.fetchone()[0] == 1

        # Check journal mode
        cursor = conn.execute("PRAGMA journal_mode")
        assert cursor.fetchone()[0] == "wal"

        # Check synchronous mode
        cursor = conn.execute("PRAGMA synchronous")
        assert cursor.fetchone()[0] == 1  # NORMAL = 1


def test_database_context_manager_cleanup():
    """Test that database connections are properly closed"""
    test_db_path = "test_cleanup.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with db.get_connection() as conn:
            conn.execute("SELECT 1")

        # Connection should be closed after context manager exits
        # We can't directly test if it's closed, but we can verify
        # that we can create a new connection successfully
        with db.get_connection() as conn:
            assert conn is not None

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)


def test_database_error_handling():
    """Test error handling and rollback"""
    test_db_path = "test_error.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with pytest.raises(sqlite3.Error):
            with db.get_connection() as conn:
                # This should cause an error
                conn.execute("INVALID SQL STATEMENT")

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/database.py"&amp;amp;amp;gt;import sqlite3

from contextlib import contextmanager
from typing import Generator


class SQLite3Database:
    def __init__(self, db_path: str = "../data/collect.db"):
        self.db_path = db_path

    # Decorator that converts this generator function into a context manager
    @contextmanager
    def get_connection(
        self, read_only: bool = False
    ) -&amp;amp;amp;amp;gt; Generator[sqlite3.Connection, None, None]:
        """Context manager for database connections"""
        # Setup phase: runs when entering 'with' block
        # Enable PARSE_DECLTYPES to use our custom datetime converters
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.row_factory = sqlite3.Row  # enables column access by name

        # Connection optimizations
        conn.execute("PRAGMA foreign_keys = ON")  # enables foreign key support
        if not read_only:
            # enables better concurrency
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL")  # Faster writes
        conn.execute("PRAGMA cache_size = -64000")  # 64MB cache
        # Use memory for temp tables
        conn.execute("PRAGMA temp_store = MEMORY")
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB memory-mapped I/O

        try:
            yield conn  # Pauses here, returns conn to 'with' statement
            # Code inside 'with' block runs here
            if not read_only:
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            # Cleanup phase: always runs when exiting 'with' block
            conn.close()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/test_prompt_service.py"&amp;amp;amp;gt;import pytest
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import Prompt, PromptType, PromptPlanStatus, CmdCategory
from config import Config


@pytest.fixture
def prompt_service():
    """
    ## How It Works

    1. **`with db.get_connection() as conn:`**
       - Opens a database connection using a context manager
       - The `as conn` assigns the connection to the variable
       - When the `with` block exits, `conn.close()` is automatically called

    2. **`cmd_service = CmdsService(conn)`**
       - Creates the service object with the database connection
       - The service can now execute database operations

    3. **`yield cmd_service`**
       - This is pytest fixture syntax that provides the service to the test
       - `yield` pauses execution here while the test runs
       - After the test completes, execution resumes after the `yield`

    4. **Automatic cleanup**
       - When the test finishes, the `with` block exits
       - Database connection is automatically closed
       - Resources are freed

    This pattern ensures **deterministic cleanup** -
    the database connection will always be properly closed regardless of
    whether the test passes or fails.
    """
    config = Config()
    db = SQLite3Database(config.db_path)
    with db.get_connection() as conn:
        cmd_service = PromptService(conn, config)

        yield cmd_service


def test_check_dirs(prompt_service: PromptService):
    result = prompt_service.cmd_check_dirs()
    assert result is True


def test_load_cmds_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_cmds_from_disk()
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def test_load_plans_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_plans_from_disk()

    print(f"\nTotal plans loaded: {len(load_results.loaded_prompts)}")
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def create_test_prompts(prompt_service: PromptService) -&amp;amp;amp;amp;gt; List[Prompt]:
    prompt_content = """
    this is a test prompt for testing database persistence... blah blah
    """

    def new_cmd_prompt(prompt_content: str) -&amp;amp;amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return [new_cmd_prompt(prompt_content), new_plan_prompt(prompt_content)]


def test_save_prompt_in_db(prompt_service: PromptService):
    # create test cmd and plan prompt types
    pls = create_test_prompts(prompt_service)
    cmd_prompt = pls[0]
    plan_prompt = pls[1]

    try:
        # save test prompts in sqlite and verify success
        cmd_result = prompt_service.save_prompt_in_db(cmd_prompt)
        print(f"cmd_result: {cmd_result}")
        assert cmd_result.success is not False

        plan_result = prompt_service.save_prompt_in_db(plan_prompt)
        print(f"plan_result: {plan_result}")
        assert plan_result.success is not False

        # retrieve the saved test prompts from sqlite and verify they
        # match the original test cmd and plan prompts
        print(f"Retrieving cmd prompt with id: {cmd_prompt.id}")
        retrieved_cmd = prompt_service.get_prompt_by_id(cmd_prompt.id)
        print(f"Retrieved cmd: {retrieved_cmd}")
        assert retrieved_cmd is not None

        retrieved_plan = prompt_service.get_prompt_by_id(plan_prompt.id)
        assert retrieved_plan is not None

        # update prompt and increment the version
        updated_text = cmd_prompt.data.content + "UPDATED TEXT"
        cmd_prompt.data.content = updated_text

        update_result = prompt_service.update_prompt_in_db(cmd_prompt)
        assert update_result.success is True

        # retrieve the updated prompt again from the prompt table and
        # validate the changes were persisted/updated
        retrieved_prompt = prompt_service.get_prompt_by_id(cmd_prompt.id)
        assert retrieved_prompt.data.content == updated_text

        # retrieve the prompt by name
        # and validate correct prompt retrieval
        retrieved_prompt_by_name = prompt_service.get_prompt_by_name(cmd_prompt.name)
        assert retrieved_prompt_by_name is not None
        assert retrieved_prompt_by_name.id == cmd_prompt.id

    finally:
        # Clean up test data - this will ALWAYS run, even if test fails
        print("\nCleaning up test prompts...")

        cmd_cleanup = delete_prompt_completely(prompt_service, cmd_prompt.id)
        print(f"CMD cleanup result: {cmd_cleanup}")

        plan_cleanup = delete_prompt_completely(prompt_service, plan_prompt.id)
        print(f"PLAN cleanup result: {plan_cleanup}")


def delete_prompt_completely(prompt_service: PromptService, prompt_id: str):
    """
    DELETE a prompt from tables: prompt, prompt_history and prompt_metrics
    THIS IS FOR INTEGRATION TESTING ONLY - as production code should reserve
    history
    """
    cursor = prompt_service.conn.cursor()
    try:
        # start transaction
        cursor.execute("BEGIN TRANSACTION")

        # delete from prompt_history first (due to composite primary key)
        cursor.execute(
            """
                       DELETE FROM prompt_history
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_history_rows_deleted = cursor.rowcount

        # delete from prompt_metrics table if any exist
        cursor.execute(
            """
                       DELETE FROM prompt_metrics
                       WHERE prompt_id = ?
                       """,
            (prompt_id,),
        )
        prompt_metrics_rows_deleted = cursor.rowcount

        # delete from prompt table (we do this last)
        cursor.execute(
            """
                       DELETE FROM prompt
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_rows_deleted = cursor.rowcount

        prompt_service.conn.commit()
        return {
            "success": True,
            "prompt_rows": prompt_rows_deleted,
            "prompt_history_rows": prompt_history_rows_deleted,
            "prompt_metrics_rows": prompt_metrics_rows_deleted,
        }

    except Exception as e:
        prompt_service.conn.rollback()
        return {"success": False, "error": str(e)}


def test_prompt_loading(prompt_service: PromptService):
    cmds = prompt_service.load_cmds_from_disk()
    print(f"\nTotal commands loaded: {len(cmds.loaded_prompts)}")
    assert len(cmds.errors) == 0

    plans = prompt_service.load_plans_from_disk()
    print(f"\nTotal plans loaded: {len(plans.loaded_prompts)}")
    assert len(plans.errors) == 0

    prompts = cmds.loaded_prompts + plans.loaded_prompts

    results = prompt_service.bulk_save_in_db(prompts)

    bad_results = [result for result in results if not result.success]
    good_results = [result for result in results if result.success]

    print(f"\nGood Result count: {len(good_results)}")
    print(f"\nBad Result count: {len(bad_results)}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/prompt_models.py"&amp;amp;amp;gt;from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
from typing import Optional, List
from config import Config


class PromptPlanStatus(str, Enum):
    """Plan status types"""

    DRAFT = "draft"
    APPROVED = "approved"
    COMPLETED = "completed"


class PromptType(str, Enum):
    """Prompt types"""

    PLAN = "plan"
    CMD = "cmd"


def create_cmd_category_enum():
    """Create CmdCategory enum dynamically from config"""
    try:
        config = Config()
        subdirs = config.command_subdirs
    except Exception:
        # Fallback to default subdirs if config fails
        subdirs = ["archive", "go", "js", "mcp", "python", "tools"]

    # Build enum members dictionary
    members = {}
    for subdir in subdirs:
        members[subdir.upper()] = subdir

    # Always include UNCATEGORIZED as fallback
    members["UNCATEGORIZED"] = "uncategorized"

    # Create enum using the functional API with type=str for JSON serialization
    return Enum("CmdCategory", members, type=str)


# Create the enum instance
CmdCategory = create_cmd_category_enum()


class Project(BaseModel):
    github_url: str
    description: str
    created_at: datetime
    updated_at: datetime


class PromptData(BaseModel):
    """Structured data for prompt JSONB field"""

    type: PromptType
    status: PromptPlanStatus
    project: Optional[str]
    cmd_category: Optional[CmdCategory]
    content: str  # This is the prompt content, in markdown
    description: Optional[str] = None
    # using 'claude' or 'gemini' here to specify the dir it will write to
    # .claude/commands and .gemini/commands respectively
    tags: List[str] = Field(default_factory=list)


class Prompt(BaseModel):
    id: str
    name: str
    github_url: Optional[str]
    data: PromptData  # Structured JSONB data
    version: int
    content_hash: str
    created_at: datetime
    updated_at: datetime


class PromptCreate(BaseModel):
    id: str
    name: str
    data: PromptData
    content_hash: str
    version: Optional[int] = 1


class LoadError(BaseModel):
    filename: str
    error_message: str
    error_type: str


class PromptCreateResult(BaseModel):
    """Result of creating a new prompt"""

    success: bool
    prompt_id: str
    version: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptLoadResult(BaseModel):
    """Result of loading prompts into database"""

    loaded_prompts: List[Prompt]
    errors: Optional[List[LoadError]] = None


class PromptDeleteResult(BaseModel):
    success: bool
    prompt_id: str
    deleted: bool
    rows_affected: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptFlattenResult(BaseModel):
    """Result of flattening a prompt to disk"""

    success: bool
    prompt_id: str
    prompt_name: str
    file_path: str
    cmd_category: str
    error_message: Optional[str] = None
    error_type: Optional[str] = None
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/datetime_adapters.py"&amp;amp;amp;gt;"""Custom datetime adapters for SQLite3 to avoid Python 3.12 deprecation warnings.

This module provides custom adapters and converters for datetime objects when
working with SQLite databases in Python 3.12+, replacing the deprecated default
adapters.
"""

import datetime
import sqlite3


def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format.

    Args:
        val: datetime.datetime object to adapt

    Returns:
        str: ISO 8601 formatted datetime string
    """
    return val.replace(tzinfo=None).isoformat()


def adapt_date_iso(val):
    """Adapt datetime.date to ISO 8601 date format.

    Args:
        val: datetime.date object to adapt

    Returns:
        str: ISO 8601 formatted date string
    """
    return val.isoformat()


def convert_datetime_iso(val):
    """Convert ISO 8601 datetime string to datetime.datetime object.

    Args:
        val: bytes object containing ISO 8601 datetime string

    Returns:
        datetime.datetime: Parsed datetime object
    """
    return datetime.datetime.fromisoformat(val.decode())


def convert_date_iso(val):
    """Convert ISO 8601 date string to datetime.date object.

    Args:
        val: bytes object containing ISO 8601 date string

    Returns:
        datetime.date: Parsed date object
    """
    return datetime.date.fromisoformat(val.decode())


def convert_timestamp(val):
    """Convert Unix timestamp to datetime.datetime object.

    Args:
        val: bytes object containing Unix timestamp

    Returns:
        datetime.datetime: Datetime object from timestamp
    """
    return datetime.datetime.fromtimestamp(int(val))


def register_adapters():
    """Register all custom datetime adapters and converters with sqlite3.

    This function should be called once at application startup to configure
    SQLite to use our custom datetime handling instead of the deprecated
    default handlers.
    """
    # Register adapters (Python -&amp;amp;amp;amp;gt; SQLite)
    sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
    sqlite3.register_adapter(datetime.date, adapt_date_iso)

    # Register converters (SQLite -&amp;amp;amp;amp;gt; Python)
    sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
    sqlite3.register_converter("DATETIME", convert_datetime_iso)
    sqlite3.register_converter("DATE", convert_date_iso)


# Automatically register adapters when module is imported
register_adapters()
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/prompt_service.py"&amp;amp;amp;gt;import sqlite3
from pathlib import Path
import uuid
import json
from datetime import datetime, timezone
import hashlib
from typing import Optional, List, Tuple
from repository.prompt_models import (
    PromptLoadResult,
    LoadError,
    CmdCategory,
    PromptType,
    PromptPlanStatus,
    PromptData,
    Prompt,
    PromptCreateResult,
    PromptDeleteResult,
    PromptFlattenResult,
)
from config import Config


class PromptService:
    def __init__(self, conn: sqlite3.Connection, config: Config):
        self.conn = conn
        self.config = config
        self.plans_check_dirs()
        self.cmd_check_dirs()

    def plans_check_dirs(self) -&amp;amp;amp;amp;gt; bool:
        """Check if all required plan directories exist, create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        # Required directories
        required_dirs = [
            plans_dir,
            plans_dir / "drafts",
            plans_dir / "approved",
            plans_dir / "completed",
        ]

        missing_dirs = []
        created_dirs = []

        for dir_path in required_dirs:
            if not dir_path.exists():
                missing_dirs.append(dir_path)

        if missing_dirs:
            print("📁 Creating missing plan directories:")
            for missing_dir in missing_dirs:
                try:
                    missing_dir.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(missing_dir)
                    print(
                        f"   ✅ Created: {
                            missing_dir.relative_to(project_dir)}"
                    )
                except Exception as e:
                    print(
                        f"   ❌ Failed to create {
                            missing_dir.relative_to(project_dir)}: {e}"
                    )
                    return False

            if created_dirs:
                print(
                    f"📁 Successfully created {
                        len(created_dirs)} directories"
                )
        else:
            print("✅ All required plan directories exist")

        return True

    def cmd_check_dirs(self) -&amp;amp;amp;amp;gt; bool:
        """Check if all required command directories exist,
           create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        claude_dir = project_dir / ".claude"
        gemini_dir = project_dir / ".gemini"

        # Get subdirectories from config
        config = Config()
        subdirs = config.command_subdirs

        # Build required directories
        required_dirs = {
            "claude": [claude_dir / "commands"]
            + [claude_dir / "commands" / subdir for subdir in subdirs],
            "gemini": [gemini_dir / "commands"]
            + [gemini_dir / "commands" / subdir for subdir in subdirs],
        }

        # Check for missing directories by type
        missing_by_type = {"claude": [], "gemini": []}
        for dir_type, dirs in required_dirs.items():
            for dir_path in dirs:
                if not dir_path.exists():
                    missing_by_type[dir_type].append(dir_path)

        # Count total missing
        total_missing = sum(len(dirs) for dirs in missing_by_type.values())

        if total_missing == 0:
            print("✅ All required command directories exist")
            return True

        # Create missing directories
        print(f"📁 Creating {total_missing} missing command directories:")
        created_count = 0
        failed = False

        for dir_type, missing_dirs in missing_by_type.items():
            if missing_dirs:
                print(f"\n   {dir_type.title()} directories:")
                for missing_dir in missing_dirs:
                    try:
                        missing_dir.mkdir(parents=True, exist_ok=True)
                        created_count += 1
                        print(
                            f"   ✅ Created: {
                                missing_dir.relative_to(project_dir)}"
                        )
                    except Exception as e:
                        print(
                            f"   ❌ Failed to create {
                                missing_dir.relative_to(project_dir)}: {e}"
                        )
                        failed = True

        if created_count &amp;amp;amp;amp;gt; 0:
            print(f"\n📁 Successfully created {created_count} directories")

        return not failed

    def _load_cmds_from_directory(
        self, cmds_dir: Path, source: str
    ) -&amp;amp;amp;amp;gt; Tuple[List[Prompt], List[LoadError]]:
        """Load commands from a specific directory

        Args:
            cmds_dir: Path to the commands directory
            source: Source identifier ('claude' or 'gemini')

        Returns:
            Tuple of (prompts list, errors list)
        """
        prompts = []
        errors = []

        if not cmds_dir.exists():
            return prompts, errors

        # Loop through the files in cmds dir and load prompts first
        for file in cmds_dir.iterdir():
            if file.is_file():
                try:
                    # Check if filename adheres to naming rules
                    current_filename = file.name
                    if not self.check_filename(current_filename):
                        # Only rename files during explicit operations, not during loading
                        # Skip file renaming when just loading/reading files
                        print(
                            f"⚠️  File {
                                current_filename} doesn't follow naming convention but will not be renamed during load operation"
                        )

                    prompt_content = file.read_text()
                    prompt = self.new_prompt_model(
                        prompt_content=prompt_content,
                        name=file.name,
                        prompt_type=PromptType.CMD,
                        cmd_category=CmdCategory.UNCATEGORIZED,
                        status=PromptPlanStatus.DRAFT,
                        tags=[source],  # Add source tag
                    )
                    prompts.append(prompt)

                except Exception as e:
                    errors.append(
                        LoadError(
                            filename=str(file),
                            error_message=str(e),
                            error_type=type(e).__name__,
                        )
                    )

        # Then cycle through the subdirs, create Prompt models and append
        for sub_dir in cmds_dir.iterdir():
            if sub_dir.is_dir():
                try:
                    cmd_category = CmdCategory(sub_dir.name.lower())

                    for file in sub_dir.iterdir():
                        try:
                            if file.is_file():
                                # Check if filename adheres to naming rules
                                current_filename = file.name
                                if not self.check_filename(current_filename):
                                    # Normalize the filename
                                    fixed_filename = self.normalize_filename(
                                        current_filename
                                    )

                                    # Create new file path with normalized name
                                    new_file_path = file.parent / fixed_filename

                                    # Rename the file on disk
                                    file.rename(new_file_path)

                                    # Update file reference to the new path
                                    file = new_file_path
                                    print(
                                        f"📝 Renamed: {current_filename} → {
                                            fixed_filename}"
                                    )

                                prompt_content = file.read_text()
                                prompt = self.new_prompt_model(
                                    prompt_content=prompt_content,
                                    name=file.name,
                                    prompt_type=PromptType.CMD,
                                    cmd_category=cmd_category,
                                    status=PromptPlanStatus.DRAFT,
                                    tags=[source],  # Add source tag
                                )
                                prompts.append(prompt)

                        except Exception as e:
                            errors.append(
                                LoadError(
                                    filename=str(file),
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )
                except ValueError:
                    # Skip directories that don't match valid CmdCategory values
                    continue

        return prompts, errors

    def load_cmds_from_disk(self) -&amp;amp;amp;amp;gt; PromptLoadResult:
        """Load commands from both .claude and .gemini directories

        Returns:
            PromptLoadResult: Combined results from both directories
        """
        project_dir = Path(__file__).parent.parent
        claude_cmds_dir = project_dir / ".claude" / "commands"
        gemini_cmds_dir = project_dir / ".gemini" / "commands"

        all_prompts = []
        all_errors = []

        # Load from Claude directory
        claude_prompts, claude_errors = self._load_cmds_from_directory(
            claude_cmds_dir, "claude"
        )
        all_prompts.extend(claude_prompts)
        all_errors.extend(claude_errors)

        # Load from Gemini directory
        gemini_prompts, gemini_errors = self._load_cmds_from_directory(
            gemini_cmds_dir, "gemini"
        )
        all_prompts.extend(gemini_prompts)
        all_errors.extend(gemini_errors)

        return PromptLoadResult(
            loaded_prompts=all_prompts,
            errors=all_errors,
        )

    def load_plans_from_disk(self) -&amp;amp;amp;amp;gt; PromptLoadResult:
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        status_mapping = {
            "drafts": PromptPlanStatus.DRAFT,
            "approved": PromptPlanStatus.APPROVED,
            "completed": PromptPlanStatus.COMPLETED,
        }

        prompts = []
        errors = []

        for subdir in plans_dir.iterdir():
            if subdir.is_dir() and subdir.name in status_mapping:
                cmd_category = None
                status = status_mapping[subdir.name]
                for file in subdir.iterdir():
                    try:
                        if file.is_file():
                            # Check if filename adheres to naming rules
                            current_filename = file.name
                            if not self.check_filename(current_filename):
                                # Normalize the filename
                                fixed_filename = self.normalize_filename(
                                    current_filename
                                )

                                # Create new file path with normalized name
                                new_file_path = file.parent / fixed_filename

                                # Rename the file on disk
                                file.rename(new_file_path)

                                # Update file reference to the new path
                                file = new_file_path
                                print(
                                    f"""📝 Renamed: {current_filename} → {
                                        fixed_filename}
                                      """
                                )

                            prompts.append(
                                self.new_prompt_model(
                                    prompt_content=file.read_text(),
                                    name=file.name,
                                    prompt_type=PromptType.PLAN,
                                    github_url=self.config.github_url,
                                    cmd_category=cmd_category,
                                    status=status,
                                    project=project_dir.name,
                                )
                            )

                    except Exception as e:
                        errors.append(
                            LoadError(
                                filename=str(file),
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

        return PromptLoadResult(loaded_prompts=prompts, errors=errors)

    def normalize_filename(self, filename: str) -&amp;amp;amp;amp;gt; str:
        """Normalize filename to use underscores and ensure .md or .toml extension

        Args:
            filename: The original filename

        Returns:
            Normalized filename with underscores and .md or .toml extension
        """
        # Replace hyphens with underscores
        normalized = filename.replace("-", "_")

        # Check if it already has .md or .toml extension
        if normalized.endswith(".md") or normalized.endswith(".toml"):
            return normalized

        # If it has another extension, replace it with .md
        if "." in normalized:
            normalized = normalized.rsplit(".", 1)[0] + ".md"
        else:
            # No extension, add .md as default
            normalized = normalized + ".md"

        return normalized

    def check_filename(self, filename: str) -&amp;amp;amp;amp;gt; bool:
        """Check if filename adheres to naming rules
        (underscores and .md or .toml extension)

        Args:
            filename: The filename to check

        Returns:
            bool: True if filename follows the rules, False otherwise
        """
        # Check if filename has .md or .toml extension
        if not (filename.endswith(".md") or filename.endswith(".toml")):
            return False

        # Check if filename contains hyphens (should use underscores)
        if "-" in filename:
            return False

        return True

    def new_prompt_model(
        self,
        prompt_content: str,
        name: str,
        prompt_type: PromptType,
        github_url: Optional[str] = None,
        cmd_category: Optional[CmdCategory] = None,
        status: PromptPlanStatus = PromptPlanStatus.DRAFT,
        project: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -&amp;amp;amp;amp;gt; Prompt:
        if prompt_type == PromptType.CMD and not cmd_category:
            raise ValueError("CMD type prompts require a category")

        default_tags = []
        if cmd_category:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                default_tags.append(cmd_category)
            else:
                default_tags.append(cmd_category.value)
        default_tags.append(prompt_type.value)

        # Merge custom tags with default tags
        all_tags = default_tags + (tags if tags else [])

        prompt_data = PromptData(
            type=prompt_type,
            status=status,
            project=project,
            cmd_category=cmd_category,
            content=prompt_content,
            description=description,
            tags=all_tags,
        )

        content_hash = hashlib.sha256(prompt_content.encode("utf-8")).hexdigest()

        timestamp = datetime.now(timezone.utc)

        db_name = self.create_db_name(
            prompt_type=prompt_type,
            prompt_status=status,
            cmd_category=cmd_category,
            project_name=project,
            name=name,
        )

        prompt = Prompt(
            id=str(uuid.uuid4()),
            name=db_name,
            github_url=github_url,
            data=prompt_data,
            version=1,
            content_hash=content_hash,
            created_at=timestamp,
            updated_at=timestamp,
        )

        return prompt

    def create_db_name(
        self,
        prompt_type: PromptType,
        prompt_status: Optional[PromptPlanStatus],
        cmd_category: Optional[CmdCategory],
        project_name: Optional[str],
        name: str,
    ) -&amp;amp;amp;amp;gt; str:
        # in the directory [project]/_docs/plans:
        # there are directories: draft, approved and completed
        # we model those as PromptPlanStatus -&amp;amp;amp;amp;gt; see prompt_models.py
        if prompt_type == PromptType.PLAN:
            create_name = project_name + "_" + prompt_status.value + "_" + name
        if prompt_type == PromptType.CMD:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                create_name = cmd_category + "_" + name
            else:
                create_name = cmd_category.value + "_" + name

        return create_name

    def parse_db_name(self, db_name: str, prompt_type: PromptType) -&amp;amp;amp;amp;gt; str:
        """Extract the original filename from the database name

        Args:
            db_name: The database name
            (e.g., 'collect-approved-update_function.md')
            prompt_type: The type of prompt(PLAN or CMD)

        Returns:
            The original filename(e.g., 'update_function.md')
        """
        # split the name to a list using '_' seperator
        ls = db_name.split("_")
        # rebuild filename from the list of split words
        filename = ""
        if prompt_type == PromptType.PLAN:
            # if prompt type is PLAN: then name will include the project
            # so we need to drop the first 2 words in the db_name
            # example: collect_completed_add_claude_sdk_processing.md
            # ls = [collect, completed, add, claude, sdk, processing.md]
            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

        if prompt_type == PromptType.CMD:
            # if prompt type is CMD: then name will only include the dir/type
            # so we only need to drop the first word in ls
            # example: tools_create_database.md
            # ls = [tools, create, database.md]
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

    def check_exists(self, name: str) -&amp;amp;amp;amp;gt; Tuple[bool, str]:
        """Check if a prompt with the given name already exists

        Args:
            name: The prompt name to check

        Returns:
            Tuple[bool, str]: (exists, prompt_id)
            where exists is True if prompt exists,
            and prompt_id is the ID if found, empty string if not found
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT id FROM prompt WHERE name = ?", (name,))
        result = cursor.fetchone()

        if result:
            return (True, result["id"])  # Found: return True and the prompt ID
        else:
            # Not found: return False and empty string
            return (False, "")

    def save_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Initial prompt creation"
    ) -&amp;amp;amp;amp;gt; PromptCreateResult:
        """Create a new prompt and initialize version history
        if the prompt doesn't exist, if it already exists then
        we call `update_prompt_in_db` with the update.

        Args:
            prompt: Prompt object to create
            change_summary: Description of this change
            (default: "Initial prompt creation")

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            # Validate prompt has required fields
            if not prompt.id or not prompt.name or not prompt.data:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id if prompt.id else "",
                    version=prompt.version if prompt.version else 1,
                    error_message="Prompt missing required fields",
                    error="ValidationError",
                )

            exists, prompt_id = self.check_exists(prompt.name)
            if exists:
                # get prompt from database using prompt_id from the version
                # retrieved from the database
                prompt_from_db = self.get_prompt_by_id(prompt_id)
                # if we don't have a prompt here then we return false
                if prompt_from_db is None:
                    return PromptCreateResult(
                        success=False,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"prompt retrieval failed for {
                            prompt_id}",
                        error="ValueError",
                    )

                # otherwise we have a prompt from the database call
                # and we need to compare hashes to see if there are changes
                if prompt.content_hash == prompt_from_db.content_hash:
                    # if they are the same then the version in the db is the
                    # same as the version on disk so we return success and
                    # do nothing else.
                    return PromptCreateResult(
                        success=True,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"""
                        prompt: {prompt.name} from disk is the same as db
                        """,
                        error="",
                    )

                else:
                    # if we get here then we have changes on disk that are more
                    # current than what is in the database

                    # IMPORTANT: We override the prompt.id here because the
                    # prompt exists already and we don't have a clean way of
                    # storing the uuid with the prompt on disk.
                    # When the prompt model is created from loading from disk,
                    # we DO generate a uuid for the model at that time just in
                    # case the prompt is newly generated from the disk and is
                    # not in the db
                    prompt.id = prompt_from_db.id

                    # Important to note that we will increment the version in
                    # `self.update_prompt_in_db`, we do not increment it here
                    return self.update_prompt_in_db(prompt)

            else:  # prompt doesn't exist in the database
                # if we make it here we have a new prompt and it
                # needs to be saved to the database for the first time
                prompt_jsonb = prompt.data.model_dump_json()

                # Create new cursor for this transaction
                cursor = self.conn.cursor()

                # insert prompt into prompt table
                cursor.execute(
                    """
                    INSERT INTO prompt(
                    id,
                    name,
                    data,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?,?)
                    """,
                    (
                        prompt.id,
                        prompt.name,
                        prompt_jsonb,
                        prompt.version,
                        prompt.content_hash,
                        prompt.created_at,
                        prompt.updated_at,
                        prompt.github_url,
                    ),
                )

                # insert initial version into prompt_history table
                cursor.execute(
                    """
                    INSERT INTO prompt_history(
                    id,
                    version,
                    data,
                    content_hash,
                    created_at,
                    archived_at,
                    change_summary,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                    """,
                    (
                        prompt.id,
                        prompt.version,
                        prompt_jsonb,
                        prompt.content_hash,
                        prompt.created_at,
                        datetime.now(timezone.utc),
                        change_summary,
                        prompt.github_url,
                    ),
                )
                self.conn.commit()

                return PromptCreateResult(
                    success=True, prompt_id=prompt.id, version=prompt.version
                )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def update_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Prompt updated from disk"
    ) -&amp;amp;amp;amp;gt; PromptCreateResult:
        """Update an existing prompt and add to version history

        Args:
            prompt: Prompt object to update
            change_summary: Description of this change

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            cursor = self.conn.cursor()

            # first we get the existing prompt in the database
            current_prompt = self.get_prompt_by_id(prompt.id)
            if not current_prompt:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id,
                    version=prompt.version,
                    error_message=f"Prompt w id {prompt.id} not found",
                    error="NotFoundError",
                )
            # then we increment the version
            prompt.version = current_prompt.version + 1

            # we need to recalculate the hash for the udpated prompt
            # so we can properly compare for changes
            prompt.content_hash = hashlib.sha256(
                prompt.data.content.encode("utf-8")
            ).hexdigest()

            # process the PromptData model to to json
            prompt_jsonb = prompt.data.model_dump_json()

            # then we update the `updated_at` timestamp
            prompt.updated_at = datetime.now(timezone.utc)

            # Update prompt table
            # NOTE: when writing the the jsonb field `data` we use jsonb
            # when reading we use `json(data)`
            cursor.execute(
                """
                UPDATE prompt
                SET name = ?,
                    data = jsonb(?),
                    version = ?,
                    content_hash = ?,
                    updated_at = ?,
                    github_url = ?
                WHERE id = ?
                """,
                (
                    prompt.name,
                    prompt_jsonb,
                    prompt.version,
                    prompt.content_hash,
                    prompt.updated_at,
                    prompt.github_url,
                    prompt.id,
                ),
            )

            # Insert into the updated prompt into prompt_history
            cursor.execute(
                """
                INSERT INTO prompt_history(
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                """,
                (
                    prompt.id,
                    prompt.version,
                    prompt_jsonb,
                    prompt.content_hash,
                    prompt.created_at,
                    datetime.now(timezone.utc),
                    change_summary,
                    prompt.github_url,
                ),
            )

            self.conn.commit()

            return PromptCreateResult(
                success=True, prompt_id=prompt.id, version=prompt.version
            )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def get_prompt_by_id(self, prompt_id: str) -&amp;amp;amp;amp;gt; Optional[Prompt]:
        """Get a prompt by its ID from the database

        Args:
            prompt_id: The ID of the prompt to retrieve

        Returns:
            Optional[Prompt]: The prompt if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE id = ?
            """,
            (prompt_id,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        # Parse the JSONB data back to PromptData
        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        # Create and return the Prompt object
        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def get_prompt_by_name(self, prompt_name: str) -&amp;amp;amp;amp;gt; Optional[Prompt]:
        """
        Get a prompt by name from the database

        Args:
            prompt_name: The name of the prompt to retrieve.
            (should be unique)
        Returns:
            Optional[Prompt]: The prompt if found by name or None otherwise
        """

        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE name = ?
            """,
            (prompt_name,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def delete_prompt_by_id(self, prompt_id: str) -&amp;amp;amp;amp;gt; PromptDeleteResult:
        cursor = self.conn.cursor()
        try:
            # archive final state in prompt_history table before deletion
            # we will not be deleting the version history of the prompt
            cursor.execute(
                """
                INSERT INTO prompt_history (
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                SELECT id, version, data, content_hash, created_at, ?, ?, github_url
                FROM prompt WHERE id = ?
            """,
                (datetime.now(timezone.utc), "DELETED - Final Version", prompt_id),
            )

            # Delete only from the prompt table
            cursor.execute("DELETE FROM prompt WHERE id = ?", (prompt_id,))
            deleted_row_count = cursor.rowcount

            self.conn.commit()

            return PromptDeleteResult(
                success=True,
                prompt_id=prompt_id,
                deleted=True,
                rows_affected=deleted_row_count,
            )

        except Exception as e:
            self.conn.rollback()
            return PromptDeleteResult(
                success=False,
                prompt_id=prompt_id,
                deleted=False,
                rows_affected=0,
                error_message=str(e),
                error_type=type(e).__name__,
            )

    def bulk_save_in_db(self, prompts: List[Prompt]) -&amp;amp;amp;amp;gt; List[PromptCreateResult]:
        """
        Bulk load/save prompts into the database

        Args:
            plans: List of Plan objects to load into database

        Returns:
            PlanLoadResult: Summary of the loading operation
        """

        return [self.save_prompt_in_db(prompt) for prompt in prompts]

    def flatten_cmds_to_disk(self) -&amp;amp;amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all cmd_category prompts from database to disk directories

        Queries all CMD type prompts from database and writes them to:
        - .claude/commands/{category}/{filename}
        - .gemini/commands/{category}/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure command directories exist
            if not self.cmd_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create command directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all CMD type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; '$.type' = 'cmd'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No CMD prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    # `**` unpacks the dictionary into key words for pydantic
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.CMD)

                    # Get category, handle None/uncategorized case
                    if prompt.data.cmd_category:
                        if isinstance(prompt.data.cmd_category, str):
                            category = prompt.data.cmd_category
                        else:
                            category = prompt.data.cmd_category.value
                    else:
                        category = "uncategorized"

                    # Determine target directory based on tags
                    target_dirs = []
                    if prompt.data.tags:
                        if "claude" in prompt.data.tags:
                            target_dirs.append("claude")
                        if "gemini" in prompt.data.tags:
                            target_dirs.append("gemini")

                    # If no source tags found, skip this prompt
                    if not target_dirs:
                        errmsg = "No source tag (claude/gemini) found in tags"
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category=category,
                                error_message=errmsg,
                                error_type="MissingSourceTag",
                            )
                        )
                        continue

                    # Write to appropriate directories based on source tags
                    for target_dir in target_dirs:
                        try:
                            target_path = (
                                project_dir
                                / f".{target_dir}"
                                / "commands"
                                / category
                                / filename
                            )

                            # Ensure parent directory exists
                            target_path.parent.mkdir(parents=True, exist_ok=True)

                            # Write content to file
                            target_path.write_text(
                                prompt.data.content, encoding="utf-8"
                            )

                            results.append(
                                PromptFlattenResult(
                                    success=True,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=str(target_path),
                                    cmd_category=category,
                                    error_message="",
                                    error_type="",
                                )
                            )

                        except Exception as e:
                            results.append(
                                PromptFlattenResult(
                                    success=False,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=(
                                        str(target_path)
                                        if "target_path" in locals()
                                        else ""
                                    ),
                                    cmd_category=category,
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results

    def flatten_plans_to_disk(self) -&amp;amp;amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all plan prompts from database to disk directories

        Queries all PLAN type prompts from database and writes them to:
        - _docs/plans/drafts/{filename}
        - _docs/plans/approved/{filename}
        - _docs/plans/completed/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure plan directories exist
            if not self.plans_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create plan directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all PLAN type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; '$.type' = 'plan'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No PLAN prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            # Status to directory mapping
            status_dir_mapping = {
                PromptPlanStatus.DRAFT.value: "drafts",
                PromptPlanStatus.APPROVED.value: "approved",
                PromptPlanStatus.COMPLETED.value: "completed",
            }

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Validate project name for PLAN type prompts
                    if not prompt.data.project:
                        # PLAN type must have a project name
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category="",
                                error_message="PLAN type prompt missing required project name",
                                error_type="MissingProjectError",
                            )
                        )
                        continue

                    # TODO: update this to use coordinate the github_url
                    if prompt.data.project != project_dir.name:
                        # Skip this prompt - it belongs to a different project
                        continue

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.PLAN)

                    # Get status directory
                    status_value = (
                        prompt.data.status.value
                        if hasattr(prompt.data.status, "value")
                        else str(prompt.data.status)
                    )
                    status_dir = status_dir_mapping.get(status_value, "drafts")

                    # Write to appropriate status directory
                    try:
                        target_path = (
                            project_dir / "_docs" / "plans" / status_dir / filename
                        )

                        # Ensure parent directory exists
                        target_path.parent.mkdir(parents=True, exist_ok=True)

                        # Write content to file
                        target_path.write_text(prompt.data.content, encoding="utf-8")

                        results.append(
                            PromptFlattenResult(
                                success=True,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=str(target_path),
                                cmd_category=status_dir,
                                error_message="",
                                error_type="",
                            )
                        )

                    except Exception as e:
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=(
                                    str(target_path)
                                    if "target_path" in locals()
                                    else ""
                                ),
                                cmd_category=status_dir,
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="repository/test_datetime_adapters.py"&amp;amp;amp;gt;"""Test the custom datetime adapters for SQLite3 compatibility."""

import pytest
import warnings
from datetime import datetime, date
from repository.database import SQLite3Database
from repository import datetime_adapters


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db_path = ":memory:"  # Use in-memory database for tests
    db = SQLite3Database(db_path)

    # Create test table
    with db.get_connection() as conn:
        conn.execute(
            """
            CREATE TABLE test_dates (
                id INTEGER PRIMARY KEY,
                created_at TIMESTAMP,
                updated_at DATETIME,
                date_only DATE,
                description TEXT
            )
        """
        )
        yield conn


def test_datetime_storage_retrieval(test_db):
    """Test that datetime objects can be stored and retrieved without warnings."""

    # Capture warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Test data
        test_datetime = datetime(2025, 1, 13, 10, 30, 45)
        test_date = date(2025, 1, 13)

        # Insert test data
        test_db.execute(
            """
            INSERT INTO test_dates (created_at, updated_at, date_only, description)
            VALUES (?, ?, ?, ?)
        """,
            (test_datetime, test_datetime, test_date, "Test record"),
        )

        test_db.commit()

        # Retrieve data
        cursor = test_db.execute(
            """
            SELECT created_at, updated_at, date_only, description 
            FROM test_dates WHERE id = 1
        """
        )
        row = cursor.fetchone()

        # Verify no deprecation warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert (
            len(deprecation_warnings) == 0
        ), f"Found deprecation warnings: {[str(dw.message) for dw in deprecation_warnings]}"

        # Verify data integrity
        assert isinstance(row["created_at"], datetime)
        assert isinstance(row["updated_at"], datetime)
        assert isinstance(row["date_only"], date)
        assert row["created_at"] == test_datetime
        assert row["updated_at"] == test_datetime
        assert row["date_only"] == test_date


def test_datetime_iso_format(test_db):
    """Test that datetimes are stored in ISO format."""

    test_datetime = datetime(2025, 1, 13, 14, 30, 45)

    # Insert using our adapter
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, description)
        VALUES (?, ?)
    """,
        (test_datetime, "ISO format test"),
    )
    test_db.commit()

    # Read raw value (bypass converter)
    cursor = test_db.execute(
        """
        SELECT CAST(created_at AS TEXT) as raw_datetime 
        FROM test_dates WHERE description = 'ISO format test'
    """
    )
    row = cursor.fetchone()

    # Verify ISO format
    expected_iso = "2025-01-13T14:30:45"
    assert row["raw_datetime"] == expected_iso


def test_adapter_functions_directly():
    """Test adapter and converter functions directly."""

    # Test datetime adapter
    test_dt = datetime(2025, 1, 13, 10, 30, 45, 123456)
    adapted = datetime_adapters.adapt_datetime_iso(test_dt)
    assert adapted == "2025-01-13T10:30:45.123456"

    # Test date adapter
    test_date = date(2025, 1, 13)
    adapted_date = datetime_adapters.adapt_date_iso(test_date)
    assert adapted_date == "2025-01-13"

    # Test datetime converter
    iso_bytes = b"2025-01-13T10:30:45.123456"
    converted = datetime_adapters.convert_datetime_iso(iso_bytes)
    assert converted == test_dt

    # Test date converter
    date_bytes = b"2025-01-13"
    converted_date = datetime_adapters.convert_date_iso(date_bytes)
    assert converted_date == test_date

    # Test timestamp converter
    timestamp_bytes = b"1736765445"  # Unix timestamp for 2025-01-13 10:30:45 UTC
    converted_ts = datetime_adapters.convert_timestamp(timestamp_bytes)
    # Note: This will be in local timezone
    assert isinstance(converted_ts, datetime)


def test_timezone_naive_handling():
    """Test that timezone info is properly stripped."""

    # Create timezone-aware datetime
    from datetime import timezone

    tz_aware = datetime(2025, 1, 13, 10, 30, 45, tzinfo=timezone.utc)

    # Adapt should strip timezone
    adapted = datetime_adapters.adapt_datetime_iso(tz_aware)
    assert adapted == "2025-01-13T10:30:45"
    assert "+00:00" not in adapted  # No timezone offset in output


def test_multiple_datetime_operations(test_db):
    """Test multiple datetime operations to ensure no warnings."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Multiple inserts
        for i in range(5):
            dt = datetime.now()
            test_db.execute(
                """
                INSERT INTO test_dates (created_at, updated_at, description)
                VALUES (?, ?, ?)
            """,
                (dt, dt, f"Record {i}"),
            )

        test_db.commit()

        # Multiple selects
        cursor = test_db.execute("SELECT * FROM test_dates")
        rows = cursor.fetchall()

        # Verify all datetimes are properly converted
        for row in rows:
            if row["created_at"]:
                assert isinstance(row["created_at"], datetime)
            if row["updated_at"]:
                assert isinstance(row["updated_at"], datetime)

        # Check for warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert len(deprecation_warnings) == 0


def test_null_datetime_handling(test_db):
    """Test that NULL datetime values are handled correctly."""

    # Insert NULL values
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, updated_at, date_only, description)
        VALUES (NULL, NULL, NULL, 'Null test')
    """
    )
    test_db.commit()

    # Retrieve NULL values
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at, date_only 
        FROM test_dates WHERE description = 'Null test'
    """
    )
    row = cursor.fetchone()

    # Verify NULLs are preserved
    assert row["created_at"] is None
    assert row["updated_at"] is None
    assert row["date_only"] is None


def test_backwards_compatibility(test_db):
    """Test that existing ISO format strings are still readable."""

    # Manually insert ISO format strings (simulating old data)
    test_db.execute(
        """
        INSERT INTO test_dates (id, created_at, updated_at, description)
        VALUES (100, '2024-12-01T10:30:45', '2024-12-01T10:30:45', 'Old format')
    """
    )
    test_db.commit()

    # Read with our converters
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at 
        FROM test_dates WHERE id = 100
    """
    )
    row = cursor.fetchone()

    # Verify conversion works
    assert isinstance(row["created_at"], datetime)
    assert row["created_at"].year == 2024
    assert row["created_at"].month == 12
    assert row["created_at"].day == 1
    assert row["created_at"].hour == 10
    assert row["created_at"].minute == 30
    assert row["created_at"].second == 45
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".gemini/settings.json"&amp;amp;amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "uv",
			"args": [
				"run",
				"python",
				"collect.py"
			],
			"workingDirectory": "/Users/benjaminmetz/python/collect",
			"enabled": true
		}
	}
}
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="dotfiles/.zshrc"&amp;amp;amp;gt;GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"

export EDITOR="nvim"
export VISUAL="nvim"
export PATH="$PATH:$HOME/.local/bin"

# path to sqlite3
export PATH="/opt/homebrew/opt/sqlite/bin:$PATH"

# path to ripgrep
export PATH="$HOME/opt/homebrew/bin/rg:$PATH"

#python uv path
. "$HOME/.local/bin/env"

# path to scripts and zig and stuff
export PATH="$HOME/bin:$PATH"

# path to npm
export PATH="$(npm config get prefix)/bin:$PATH"
# path to zig
export PATH=$PATH:~/bin/zig/
export PATH="$(brew --prefix coreutils)/libexec/gnubin:$PATH"

# dependencies needed for gemini / google token processing
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$PKG_CONFIG_PATH"
export PATH="$(brew --prefix)/bin:$PATH"
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$(brew --prefix protobuf)/lib/pkgconfig:$PKG_CONFIG_PATH"

# shortcuts to project work
alias gowork='cd $HOME/go/src/github.com/metzben &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -lhG'
alias py='cd $HOME/python &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l --color'
alias collect='cd $HOME/python/collect &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; source .venv/bin/activate'
alias el='cd $HOME/go/src/github.com/metzben/elephnt &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l --color'
alias tiny='cd $HOME/go/src/github.com/metzben/tinystack &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -lhG'
alias ai='cd $HOME/python/aiwork &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -lhG'
alias mcp='cd $HOME/python/mcpwork &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -lhG'
alias base='cd $HOME/base &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; nvim .'
alias fta='cd $HOME/python/fastta &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; nvim .'
alias indicators='cd $HOME/python/indicators &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l'
alias mcpstart='cd $HOME/python/startermcp &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l'
alias tools='cd ~/bin &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l --color'
alias plans='cd _docs/plans &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; tree -C -L 2'

# Database function - only works in collect directory
db() {
    if [[ "$PWD" == *"/collect" ]] || [[ "$PWD" == *"/collect/"* ]]; then
        sqlite3 data/collect.db
    else
        echo "Not in collect directory. This command only works in the collect project."
    fi
}

# claude ai shortcuts
alias ask='claude -p '
alias editmcp='nvim ~/Library/Application\ Support/Claude/claude_desktop_config.json'
alias rip='claude --dangerously-skip-permissions'
alias cmds='cd "$(git rev-parse --show-toplevel)/.claude/commands" &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l --color'
alias gms='cd "$(git rev-parse --show-toplevel)/.gemini/commands" &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; ls -l --color'

# git shortcuts
alias gs='git status'
alias gd='git diff --staged'
alias gc='git commit -m '
alias push='git push origin main'
alias ga='git add '
alias gb='git branch'
alias gwl='git worktree list'
alias rebase='git pull --rebase origin main'
alias pull='git pull origin main'

# Worktree navigation functions
cd1() {
    local project_name=$(basename "$(pwd)")
    local wt1_path="../${project_name}-wt1"
    
    if [[ -d "$wt1_path" ]]; then
        cd "$wt1_path"
        echo "Changed to worktree 1: $(pwd)"
    else
        echo "Worktree 1 not found: $wt1_path"
        echo "Run 'trees' to create worktrees first."
    fi
}

cd2() {
    local project_name=$(basename "$(pwd)")
    local wt2_path="../${project_name}-wt2"
    
    if [[ -d "$wt2_path" ]]; then
        cd "$wt2_path"
        echo "Changed to worktree 2: $(pwd)"
    else
        echo "Worktree 2 not found: $wt2_path"
        echo "Run 'trees' to create worktrees first."
    fi
}


checkport() {
    if [ -z "$1" ]; then
        echo "Usage: checkport &amp;amp;amp;amp;lt;port_number&amp;amp;amp;amp;gt;"
        return 1
    fi
    
    if lsof -i :$1 2&amp;amp;amp;amp;gt;/dev/null; then
        echo "Port $1 is in use"
    else
        echo "Port $1 is available"
    fi
}

# uv shortcuts
alias env='source .venv/bin/activate'
alias da='deactivate'
alias ipy='uv run ipython'

# go shortcuts
alias run='go test -v -run'

# config shortcuts
alias src='source ~/.zshrc'
alias openz='nvim ~/.zshrc'
alias initlua='nvim $HOME/.config/nvim/init.lua'
alias ghconf='nvim $HOME/.config/ghostty/config'
alias oc='cursor .'

# misc shorty's
alias ll='ls -l --color'
alias tll='tree -C -L 2'
alias oc='cursor .'
alias onv='nvim .'
alias runz='zig run src/main.zig'
alias cperr="zig run src/main.zig 2&amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;1 | tee /dev/tty | awk '/error:/{found=1} found {print}' | pbcopy"

# ollama models
alias deep70='ollama run deepseek-r1:70b'
alias llama70='ollama run llama3.3'

# The next line updates PATH for the Google Cloud SDK.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc'; fi

# The next line enables shell command completion for gcloud.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc'; fi

alias auth='gcloud auth login'
alias auth2='gcloud auth application-default login'

export PS1='b@m %~ % '



# opencode
export PATH=/Users/benjaminmetz/.opencode/bin:$PATH
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="dotfiles/nvim/init.lua"&amp;amp;amp;gt;vim.opt.clipboard = "unnamedplus"

if vim.g.vscode then
	return
end

-- Set &amp;amp;amp;amp;lt;space&amp;amp;amp;amp;gt; as the leader key
-- See `:help mapleader`
--  NOTE: Must happen before plugins are loaded (otherwise wrong leader will be used)
vim.g.mapleader = " "
vim.g.maplocalleader = " "

-- have neovim honor the terminal opacity
vim.opt.termguicolors = true
--vim.cmd.colorscheme("catppuccin-mocha")

vim.cmd([[highlight Normal ctermbg=none guibg=none]])

-- Set to true if you have a Nerd Font installed and selected in the terminal
vim.g.have_nerd_font = true

-- [[ Setting options ]]
-- See `:help vim.opt`
-- NOTE: You can change these options as you wish!
--  For more options, you can see `:help option-list`

-- Make line numbers default
vim.opt.number = true
-- You can also add relative line numbers, to help with jumping.
--  Experiment for yourself to see if you like it!
vim.opt.relativenumber = true

-- Enable mouse mode, can be useful for resizing splits for example!
vim.opt.mouse = "a"

-- Don't show the mode, since it's already in the status line
vim.opt.showmode = true

-- Sync clipboard between OS and Neovim.
--  Schedule the setting after `UiEnter` because it can increase startup-time.
--  Remove this option if you want your OS clipboard to remain independent.
--  See `:help 'clipboard'`
vim.schedule(function()
	vim.opt.clipboard = "unnamedplus"
end)

-- Enable break indent
vim.opt.breakindent = true

-- Save undo history
vim.opt.undofile = true

-- Case-insensitive searching UNLESS \C or one or more capital letters in the search term
vim.opt.ignorecase = true
vim.opt.smartcase = true

-- Keep signcolumn on by default
vim.opt.signcolumn = "yes"

-- Decrease update time
vim.opt.updatetime = 250

-- Decrease mapped sequence wait time
-- Displays which-key popup sooner
vim.opt.timeoutlen = 300

-- Configure how new splits should be opened
vim.opt.splitright = true
vim.opt.splitbelow = true

-- Sets how neovim will display certain whitespace characters in the editor.
--  See `:help 'list'`
--  and `:help 'listchars'`
vim.opt.list = true
vim.opt.listchars = { tab = "» ", trail = "·", nbsp = "␣" }

-- Preview substitutions live, as you type!
vim.opt.inccommand = "split"

-- Show which line your cursor is on
vim.opt.cursorline = true

-- Minimal number of screen lines to keep above and below the cursor.
vim.opt.scrolloff = 15

vim.o.foldmethod = "expr"
vim.o.foldexpr = "nvim_treesitter#foldexpr()"
vim.o.foldenable = true
vim.o.foldlevel = 99 -- Keep folds open by default

--vim.cmd([[packadd packer.nvim]])

-- [[ Basic Keymaps ]]
--  See `:help vim.keymap.set()`

-- Clear highlights on search when pressing &amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt; in normal mode
--  See `:help hlsearch`
vim.keymap.set("n", "&amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;cmd&amp;amp;amp;amp;gt;nohlsearch&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt;")
-- when in insert mode pressing j and j again will &amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt;'
vim.keymap.set("i", "&amp;amp;amp;amp;lt;D-j&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt;", { noremap = true, silent = true })
vim.keymap.set("n", "a", "A", { noremap = true, silent = true })
vim.keymap.set("n", "4", "$", { noremap = true, silent = true })

-- Diagnostic keymaps
vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;q", vim.diagnostic.setloclist, { desc = "Open diagnostic [Q]uickfix list" })
vim.keymap.set("n", "[d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })
vim.keymap.set("n", "]d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })

-- for people to discover. Otherwise, you normally need to press &amp;amp;amp;amp;lt;C-\&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-n&amp;amp;amp;amp;gt;, which
-- is not what someone will guess without a bit more experience.
--
-- NOTE: This won't work in all terminal emulators/tmux/etc. Try your own mapping
-- or just use &amp;amp;amp;amp;lt;C-\&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-n&amp;amp;amp;amp;gt; to exit terminal mode
vim.keymap.set("t", "&amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;C-\\&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-n&amp;amp;amp;amp;gt;", { desc = "Exit terminal mode" })

-- Keybinds to make split navigation easier.
-- Use CTRL+&amp;amp;amp;amp;lt;hjkl&amp;amp;amp;amp;gt; to switch between windows
--
--  See `:help wincmd` for a list of all window commands
vim.keymap.set("n", "&amp;amp;amp;amp;lt;C-h&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;C-w&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-h&amp;amp;amp;amp;gt;", { desc = "Move focus to the left window" })
vim.keymap.set("n", "&amp;amp;amp;amp;lt;C-l&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;C-w&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-l&amp;amp;amp;amp;gt;", { desc = "Move focus to the right window" })
vim.keymap.set("n", "&amp;amp;amp;amp;lt;C-j&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;C-w&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-j&amp;amp;amp;amp;gt;", { desc = "Move focus to the lower window" })
vim.keymap.set("n", "&amp;amp;amp;amp;lt;C-k&amp;amp;amp;amp;gt;", "&amp;amp;amp;amp;lt;C-w&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;C-k&amp;amp;amp;amp;gt;", { desc = "Move focus to the upper window" })

vim.api.nvim_create_autocmd("BufEnter", {
	pattern = "$HOME/go/src/github.com/metzben/*",
	callback = function()
		vim.cmd("colorscheme onedark")
	end,
})

-- Ensure Packer is loaded
vim.cmd([[packadd packer.nvim]])

-- [[ Basic Autocommands ]]
--  See `:help lua-guide-autocommands`

-- Highlight when yanking (copying) text
--  Try it with `yap` in normal mode
--  See `:help vim.highlight.on_yank()`
vim.api.nvim_create_autocmd("TextYankPost", {
	desc = "Highlight when yanking (copying) text",
	group = vim.api.nvim_create_augroup("kickstart-highlight-yank", { clear = true }),
	callback = function()
		vim.highlight.on_yank()
	end,
})

-- [[ Install `lazy.nvim` plugin manager ]]
--    See `:help lazy.nvim.txt` or https://github.com/folke/lazy.nvim for more info
local lazypath = vim.fn.stdpath("data") .. "/lazy/lazy.nvim"
if not (vim.uv or vim.loop).fs_stat(lazypath) then
	local lazyrepo = "https://github.com/folke/lazy.nvim.git"
	local out = vim.fn.system({ "git", "clone", "--filter=blob:none", "--branch=stable", lazyrepo, lazypath })
	if vim.v.shell_error ~= 0 then
		error("Error cloning lazy.nvim:\n" .. out)
	end
end ---@diagnostic disable-next-line: undefined-field
vim.opt.rtp:prepend(lazypath)

-- [[ Configure and install plugins ]]
--
--  To check the current status of your plugins, run
--    :Lazy
--
--  You can press `?` in this menu for help. Use `:q` to close the window
--
--  To update plugins you can run
--    :Lazy update
--
-- NOTE: Here is where you install your plugins.
require("lazy").setup({
	-- NOTE: Plugins can be added with a link (or for a github repo: 'owner/repo' link).
	"tpope/vim-sleuth", -- Detect tabstop and shiftwidth automatically

	-- NOTE: Plugins can also be added by using a table,
	-- with the first argument being the link and the following
	-- keys can be used to configure plugin behavior/loading/etc.
	--
	-- Use `opts = {}` to force a plugin to be loaded.
	--

	-- Here is a more advanced example where we pass configuration
	-- options to `gitsigns.nvim`. This is equivalent to the following Lua:
	--    require('gitsigns').setup({ ... })
	--
	-- See `:help gitsigns` to understand what the configuration keys do
	{ -- Adds git related signs to the gutter, as well as utilities for managing changes
		"lewis6991/gitsigns.nvim",
		opts = {
			signs = {
				add = { text = "+" },
				change = { text = "~" },
				delete = { text = "_" },
				topdelete = { text = "‾" },
				changedelete = { text = "~" },
			},
		},
	},

	-- NOTE: Plugins can also be configured to run Lua code when they are loaded.
	--
	-- This is often very useful to both group configuration, as well as handle
	-- lazy loading plugins that don't need to be loaded immediately at startup.
	--
	-- For example, in the following configuration, we use:
	--  event = 'VimEnter'
	--
	-- which loads which-key before all the UI elements are loaded. Events can be
	-- normal autocommands events (`:help autocmd-events`).
	--
	-- Then, because we use the `opts` key (recommended), the configuration runs
	-- after the plugin has been loaded as `require(MODULE).setup(opts)`.

	{ -- Useful plugin to show you pending keybinds.
		"folke/which-key.nvim",
		event = "VimEnter", -- Sets the loading event to 'VimEnter'
		opts = {
			icons = {
				-- set icon mappings to true if you have a Nerd Font
				mappings = vim.g.have_nerd_font,
				-- If you are using a Nerd Font: set icons.keys to an empty table which will use the
				-- default which-key.nvim defined Nerd Font icons, otherwise define a string table
				keys = vim.g.have_nerd_font and {} or {
					Up = "&amp;amp;amp;amp;lt;Up&amp;amp;amp;amp;gt; ",
					Down = "&amp;amp;amp;amp;lt;Down&amp;amp;amp;amp;gt; ",
					Left = "&amp;amp;amp;amp;lt;Left&amp;amp;amp;amp;gt; ",
					Right = "&amp;amp;amp;amp;lt;Right&amp;amp;amp;amp;gt; ",
					C = "&amp;amp;amp;amp;lt;C-…&amp;amp;amp;amp;gt; ",
					M = "&amp;amp;amp;amp;lt;M-…&amp;amp;amp;amp;gt; ",
					D = "&amp;amp;amp;amp;lt;D-…&amp;amp;amp;amp;gt; ",
					S = "&amp;amp;amp;amp;lt;S-…&amp;amp;amp;amp;gt; ",
					CR = "&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt; ",
					Esc = "&amp;amp;amp;amp;lt;Esc&amp;amp;amp;amp;gt; ",
					ScrollWheelDown = "&amp;amp;amp;amp;lt;ScrollWheelDown&amp;amp;amp;amp;gt; ",
					ScrollWheelUp = "&amp;amp;amp;amp;lt;ScrollWheelUp&amp;amp;amp;amp;gt; ",
					NL = "&amp;amp;amp;amp;lt;NL&amp;amp;amp;amp;gt; ",
					BS = "&amp;amp;amp;amp;lt;BS&amp;amp;amp;amp;gt; ",
					Space = "&amp;amp;amp;amp;lt;Space&amp;amp;amp;amp;gt; ",
					Tab = "&amp;amp;amp;amp;lt;Tab&amp;amp;amp;amp;gt; ",
					F1 = "&amp;amp;amp;amp;lt;F1&amp;amp;amp;amp;gt;",
					F2 = "&amp;amp;amp;amp;lt;F2&amp;amp;amp;amp;gt;",
					F3 = "&amp;amp;amp;amp;lt;F3&amp;amp;amp;amp;gt;",
					F4 = "&amp;amp;amp;amp;lt;F4&amp;amp;amp;amp;gt;",
					F5 = "&amp;amp;amp;amp;lt;F5&amp;amp;amp;amp;gt;",
					F6 = "&amp;amp;amp;amp;lt;F6&amp;amp;amp;amp;gt;",
					F7 = "&amp;amp;amp;amp;lt;F7&amp;amp;amp;amp;gt;",
					F8 = "&amp;amp;amp;amp;lt;F8&amp;amp;amp;amp;gt;",
					F9 = "&amp;amp;amp;amp;lt;F9&amp;amp;amp;amp;gt;",
					F10 = "&amp;amp;amp;amp;lt;F10&amp;amp;amp;amp;gt;",
					F11 = "&amp;amp;amp;amp;lt;F11&amp;amp;amp;amp;gt;",
					F12 = "&amp;amp;amp;amp;lt;F12&amp;amp;amp;amp;gt;",
				},
			},

			-- Document existing key chains
			spec = {
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;c", group = "[C]ode", mode = { "n", "x" } },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;d", group = "[D]ocument" },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;r", group = "[R]ename" },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;s", group = "[S]earch" },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;w", group = "[W]orkspace" },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;t", group = "[T]oggle" },
				{ "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;h", group = "Git [H]unk", mode = { "n", "v" } },
			},
		},
	},

	-- NOTE: Plugins can specify dependencies.
	--
	-- The dependencies are proper plugin specifications as well - anything
	-- you do for a plugin at the top level, you can do for a dependency.
	--
	-- Use the `dependencies` key to specify the dependencies of a particular plugin

	{ -- Fuzzy Finder (files, lsp, etc)
		"nvim-telescope/telescope.nvim",
		event = "VimEnter",
		branch = "0.1.x",
		dependencies = {
			"nvim-lua/plenary.nvim",
			{ -- If encountering errors, see telescope-fzf-native README for installation instructions
				"nvim-telescope/telescope-fzf-native.nvim",

				-- `build` is used to run some command when the plugin is installed/updated.
				-- This is only run then, not every time Neovim starts up.
				build = "make",

				-- `cond` is a condition used to determine whether this plugin should be
				-- installed and loaded.
				cond = function()
					return vim.fn.executable("make") == 1
				end,
			},
			{ "nvim-telescope/telescope-ui-select.nvim" },

			-- Useful for getting pretty icons, but requires a Nerd Font.
			{ "nvim-tree/nvim-web-devicons", enabled = vim.g.have_nerd_font },
		},
		config = function()
			-- Telescope is a fuzzy finder that comes with a lot of different things that
			-- it can fuzzy find! It's more than just a "file finder", it can search
			-- many different aspects of Neovim, your workspace, LSP, and more!
			--
			-- The easiest way to use Telescope, is to start by doing something like:
			--  :Telescope help_tags
			--
			-- After running this command, a window will open up and you're able to
			-- type in the prompt window. You'll see a list of `help_tags` options and
			-- a corresponding preview of the help.
			--
			-- Two important keymaps to use while in Telescope are:
			--  - Insert mode: &amp;amp;amp;amp;lt;c-/&amp;amp;amp;amp;gt;
			--  - Normal mode: ?
			--
			-- This opens a window that shows you all of the keymaps for the current
			-- Telescope picker. This is really useful to discover what Telescope can
			-- do as well as how to actually do it!

			-- [[ Configure Telescope ]]
			-- See `:help telescope` and `:help telescope.setup()`
			require("telescope").setup({
				-- You can put your default mappings / updates / etc. in here
				--  All the info you're looking for is in `:help telescope.setup()`
				--
				-- defaults = {
				--   mappings = {
				--     i = { ['&amp;amp;amp;amp;lt;c-enter&amp;amp;amp;amp;gt;'] = 'to_fuzzy_refine' },
				--   },
				-- },
				-- pickers = {}
				extensions = {
					["ui-select"] = {
						require("telescope.themes").get_dropdown(),
					},
				},
			})

			-- Enable Telescope extensions if they are installed
			pcall(require("telescope").load_extension, "fzf")
			pcall(require("telescope").load_extension, "ui-select")

			-- See `:help telescope.builtin`
			local builtin = require("telescope.builtin")
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sh", builtin.help_tags, { desc = "[S]earch [H]elp" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sk", builtin.keymaps, { desc = "[S]earch [K]eymaps" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sf", function()
				builtin.find_files({ hidden = true, no_ignore = true })
			end, { desc = "[S]earch [F]iles" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;ss", builtin.builtin, { desc = "[S]earch [S]elect Telescope" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sw", builtin.grep_string, { desc = "[S]earch current [W]ord" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sg", builtin.live_grep, { desc = "[S]earch by [G]rep" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sd", builtin.diagnostics, { desc = "[S]earch [D]iagnostics" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sr", builtin.resume, { desc = "[S]earch [R]esume" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;s.", builtin.oldfiles, { desc = '[S]earch Recent Files ("." for repeat)' })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;", builtin.buffers, { desc = "[ ] Find existing buffers" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;af", builtin.current_buffer_fuzzy_find, { desc = "[S]earch in existing file" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;nf", "]m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;pf", "[m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;r", vim.lsp.buf.rename, { desc = "LSP Rename" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;d", "&amp;amp;amp;amp;lt;cmd&amp;amp;amp;amp;gt;Telescope diagnostics&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt;")
			vim.api.nvim_set_keymap("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;c", "~", { noremap = true, silent = true })
			vim.keymap.set(
				"n",
				"&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;O",
				":put! _&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt;",
				{ desc = "Add blank line above without entering edit mode" }
			)

			vim.keymap.set("v", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;/", ":norm I//&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt;", { desc = "Comment selected block" })
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;/", "gcc", { desc = "Toggle comment on current line" })
			-- Slightly advanced example of overriding default behavior and theme
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;[", function()
				-- You can pass additional configuration to Telescope to change the theme, layout, etc.
				builtin.current_buffer_fuzzy_find(require("telescope.themes").get_dropdown({
					winblend = 10,
					previewer = false,
				}))
			end, { desc = "[/] Fuzzily search in current buffer" })

			-- It's also possible to pass additional configuration options.
			--  See `:help telescope.builtin.live_grep()` for information about particular keys
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;s/", function()
				builtin.live_grep({
					grep_open_files = true,
					prompt_title = "Live Grep in Open Files",
				})
			end, { desc = "[S]earch [/] in Open Files" })

			-- Shortcut for searching your Neovim configuration files
			vim.keymap.set("n", "&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;sn", function()
				builtin.find_files({ cwd = vim.fn.stdpath("config") })
			end, { desc = "[S]earch [N]eovim files" })
		end,
	},

	-- LSP Plugins
	{
		-- `lazydev` configures Lua LSP for your Neovim config, runtime and plugins
		-- used for completion, annotations and signatures of Neovim apis
		"folke/lazydev.nvim",
		ft = "lua",
		opts = {
			library = {
				-- Load luvit types when the `vim.uv` word is found
				{ path = "luvit-meta/library", words = { "vim%.uv" } },
			},
		},
	},
	{ "Bilal2453/luvit-meta", lazy = true },
	{
		-- Main LSP Configuration
		"neovim/nvim-lspconfig",
		dependencies = {
			-- Automatically install LSPs and related tools to stdpath for Neovim
			{ "williamboman/mason.nvim", config = true }, -- NOTE: Must be loaded before dependants
			"williamboman/mason-lspconfig.nvim",
			"WhoIsSethDaniel/mason-tool-installer.nvim",

			-- Useful status updates for LSP.
			-- NOTE: `opts = {}` is the same as calling `require('fidget').setup({})`
			{ "j-hui/fidget.nvim", opts = {} },

			-- Allows extra capabilities provided by nvim-cmp
			"hrsh7th/cmp-nvim-lsp",
		},
		config = function()
			-- Brief aside: **What is LSP?**
			--
			-- LSP is an initialism you've probably heard, but might not understand what it is.
			--
			-- LSP stands for Language Server Protocol. It's a protocol that helps editors
			-- and language tooling communicate in a standardized fashion.
			--
			-- In general, you have a "server" which is some tool built to understand a particular
			-- language (such as `gopls`, `lua_ls`, `rust_analyzer`, etc.). These Language Servers
			-- (sometimes called LSP servers, but that's kind of like ATM Machine) are standalone
			-- processes that communicate with some "client" - in this case, Neovim!
			--
			-- LSP provides Neovim with features like:
			--  - Go to definition
			--  - Find references
			--  - Autocompletion
			--  - Symbol Search
			--  - and more!
			--
			-- Thus, Language Servers are external tools that must be installed separately from
			-- Neovim. This is where `mason` and related plugins come into play.
			--
			-- If you're wondering about lsp vs treesitter, you can check out the wonderfully
			-- and elegantly composed help section, `:help lsp-vs-treesitter`

			--  This function gets run when an LSP attaches to a particular buffer.
			--    That is to say, every time a new file is opened that is associated with
			--    an lsp (for example, opening `main.rs` is associated with `rust_analyzer`) this
			--    function will be executed to configure the current buffer
			vim.api.nvim_create_autocmd("LspAttach", {
				group = vim.api.nvim_create_augroup("kickstart-lsp-attach", { clear = true }),
				callback = function(event)
					-- NOTE: Remember that Lua is a real programming language, and as such it is possible
					-- to define small helper and utility functions so you don't have to repeat yourself.
					--
					-- In this case, we create a function that lets us more easily define mappings specific
					-- for LSP related items. It sets the mode, buffer and description for us each time.
					local map = function(keys, func, desc, mode)
						mode = mode or "n"
						vim.keymap.set(mode, keys, func, { buffer = event.buf, desc = "LSP: " .. desc })
					end

					-- Jump to the definition of the word under your cursor.
					--  This is where a variable was first declared, or where a function is defined, etc.
					--  To jump back, press &amp;amp;amp;amp;lt;C-t&amp;amp;amp;amp;gt;.
					map("gd", require("telescope.builtin").lsp_definitions, "[G]oto [D]efinition")

					-- Find references for the word under your cursor.
					map("gr", require("telescope.builtin").lsp_references, "[G]oto [R]eferences")

					-- Jump to the implementation of the word under your cursor.
					--  Useful when your language has ways of declaring types without an actual implementation.
					map("gI", require("telescope.builtin").lsp_implementations, "[G]oto [I]mplementation")

					-- Jump to the type of the word under your cursor.
					--  Useful when you're not sure what type a variable is and you want to see
					--  the definition of its *type*, not where it was *defined*.
					map("&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;D", require("telescope.builtin").lsp_type_definitions, "Type [D]efinition")

					-- Fuzzy find all the symbols in your current document.
					--  Symbols are things like variables, functions, types, etc.
					map("&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;ds", require("telescope.builtin").lsp_document_symbols, "[D]ocument [S]ymbols")

					-- Fuzzy find all the symbols in your current workspace.
					--  Similar to document symbols, except searches over your entire project.
					map(
						"&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;ws",
						require("telescope.builtin").lsp_dynamic_workspace_symbols,
						"[W]orkspace [S]ymbols"
					)

					-- Rename the variable under your cursor.
					--  Most Language Servers support renaming across files, etc.
					map("&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;rn", vim.lsp.buf.rename, "[R]e[n]ame")

					-- Execute a code action, usually your cursor needs to be on top of an error
					-- or a suggestion from your LSP for this to activate.
					map("&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;ca", vim.lsp.buf.code_action, "[C]ode [A]ction", { "n", "x" })

					-- WARN: This is not Goto Definition, this is Goto Declaration.
					--  For example, in C this would take you to the header.
					map("gD", vim.lsp.buf.declaration, "[G]oto [D]eclaration")

					-- The following two autocommands are used to highlight references of the
					-- word under your cursor when your cursor rests there for a little while.
					--    See `:help CursorHold` for information about when this is executed
					--
					-- When you move your cursor, the highlights will be cleared (the second autocommand).
					local client = vim.lsp.get_client_by_id(event.data.client_id)
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_documentHighlight) then
						local highlight_augroup =
							vim.api.nvim_create_augroup("kickstart-lsp-highlight", { clear = false })
						vim.api.nvim_create_autocmd({ "CursorHold", "CursorHoldI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.document_highlight,
						})

						vim.api.nvim_create_autocmd({ "CursorMoved", "CursorMovedI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.clear_references,
						})

						vim.api.nvim_create_autocmd("LspDetach", {
							group = vim.api.nvim_create_augroup("kickstart-lsp-detach", { clear = true }),
							callback = function(event2)
								vim.lsp.buf.clear_references()
								vim.api.nvim_clear_autocmds({ group = "kickstart-lsp-highlight", buffer = event2.buf })
							end,
						})
					end

					-- The following code creates a keymap to toggle inlay hints in your
					-- code, if the language server you are using supports them
					--
					-- This may be unwanted, since they displace some of your code
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_inlayHint) then
						map("&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;th", function()
							vim.lsp.inlay_hint.enable(not vim.lsp.inlay_hint.is_enabled({ bufnr = event.buf }))
						end, "[T]oggle Inlay [H]ints")
					end
				end,
			})

			-- Change diagnostic symbols in the sign column (gutter)
			-- if vim.g.have_nerd_font then
			--   local signs = { ERROR = '', WARN = '', INFO = '', HINT = '' }
			--   local diagnostic_signs = {}
			--   for type, icon in pairs(signs) do
			--     diagnostic_signs[vim.diagnostic.severity[type]] = icon
			--   end
			--   vim.diagnostic.config { signs = { text = diagnostic_signs } }
			-- end

			-- LSP servers and clients are able to communicate to each other what features they support.
			--  By default, Neovim doesn't support everything that is in the LSP specification.
			--  When you add nvim-cmp, luasnip, etc. Neovim now has *more* capabilities.
			--  So, we create new capabilities with nvim cmp, and then broadcast that to the servers.
			local capabilities = vim.lsp.protocol.make_client_capabilities()
			capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

			-- Enable the following language servers
			--  Feel free to add/remove any LSPs that you want here. They will automatically be installed.
			--
			--  Add any additional override configuration in the following tables. Available keys are:
			--  - cmd (table): Override the default command used to start the server
			--  - filetypes (table): Override the default list of associated filetypes for the server
			--  - capabilities (table): Override fields in capabilities. Can be used to disable certain LSP features.
			--  - settings (table): Override the default settings passed when initializing the server.
			--        For example, to see the options for `lua_ls`, you could go to: https://luals.github.io/wiki/settings/
			local servers = {
				-- clangd = {},
				-- gopls = {},
				-- pyright = {},
				-- rust_analyzer = {},
				-- ... etc. See `:help lspconfig-all` for a list of all the pre-configured LSPs
				--
				-- Some languages (like typescript) have entire language plugins that can be useful:
				--    https://github.com/pmizio/typescript-tools.nvim
				--
				-- But for many setups, the LSP (`ts_ls`) will work just fine
				-- ts_ls = {},
				--

				lua_ls = {
					-- cmd = { ... },
					-- filetypes = { ... },
					-- capabilities = {},
					settings = {
						Lua = {
							completion = {
								callSnippet = "Replace",
							},
							-- You can toggle below to ignore Lua_LS's noisy `missing-fields` warnings
							-- diagnostics = { disable = { 'missing-fields' } },
						},
					},
				},
			}

			-- Ensure the servers and tools above are installed
			--  To check the current status of installed tools and/or manually install
			--  other tools, you can run
			--    :Mason
			--
			--  You can press `g?` for help in this menu.
			require("mason").setup()

			-- You can add other tools here that you want Mason to install
			-- for you, so that they are available from within Neovim.
			local ensure_installed = vim.tbl_keys(servers or {})
			vim.list_extend(ensure_installed, {
				"stylua", -- Used to format Lua code
			})
			require("mason-tool-installer").setup({ ensure_installed = ensure_installed })

			require("mason-lspconfig").setup({
				handlers = {
					function(server_name)
						local server = servers[server_name] or {}
						-- This handles overriding only values explicitly passed
						-- by the server configuration above. Useful when disabling
						-- certain features of an LSP (for example, turning off formatting for ts_ls)
						server.capabilities = vim.tbl_deep_extend("force", {}, capabilities, server.capabilities or {})
						require("lspconfig")[server_name].setup(server)
					end,
				},
			})
		end,
	},

	{ -- Autoformat
		"stevearc/conform.nvim",
		event = { "BufWritePre" },
		cmd = { "ConformInfo" },
		keys = {
			{
				"&amp;amp;amp;amp;lt;leader&amp;amp;amp;amp;gt;f",
				function()
					require("conform").format({ async = true, lsp_format = "fallback" })
				end,
				mode = "",
				desc = "[F]ormat buffer",
			},
		},
		opts = {
			notify_on_error = false,
			format_on_save = function(bufnr)
				-- Disable "format_on_save lsp_fallback" for languages that don't
				-- have a well standardized coding style. You can add additional
				-- languages here or re-enable it for the disabled ones.
				local disable_filetypes = { c = true, cpp = true }
				local lsp_format_opt
				if disable_filetypes[vim.bo[bufnr].filetype] then
					lsp_format_opt = "never"
				else
					lsp_format_opt = "fallback"
				end
				return {
					timeout_ms = 500,
					lsp_format = lsp_format_opt,
				}
			end,
			formatters_by_ft = {
				lua = { "stylua" },
				-- Conform can also run multiple formatters sequentially
				-- python = { "isort", "black" },
				--
				-- You can use 'stop_after_first' to run the first available formatter from the list
				-- javascript = { "prettierd", "prettier", stop_after_first = true },
			},
		},
	},

	{ -- Autocompletion
		"hrsh7th/nvim-cmp",
		event = "InsertEnter",
		dependencies = {
			-- Snippet Engine &amp;amp;amp;amp;amp; its associated nvim-cmp source
			{
				"L3MON4D3/LuaSnip",
				build = (function()
					-- Build Step is needed for regex support in snippets.
					-- This step is not supported in many windows environments.
					-- Remove the below condition to re-enable on windows.
					if vim.fn.has("win32") == 1 or vim.fn.executable("make") == 0 then
						return
					end
					return "make install_jsregexp"
				end)(),
				dependencies = {
					-- `friendly-snippets` contains a variety of premade snippets.
					--    See the README about individual language/framework/plugin snippets:
					--    https://github.com/rafamadriz/friendly-snippets
					-- {
					--   'rafamadriz/friendly-snippets',
					--   config = function()
					--     require('luasnip.loaders.from_vscode').lazy_load()
					--   end,
					-- },
				},
			},
			"saadparwaiz1/cmp_luasnip",

			-- Adds other completion capabilities.
			--  nvim-cmp does not ship with all sources by default. They are split
			--  into multiple repos for maintenance purposes.
			"hrsh7th/cmp-nvim-lsp",
			"hrsh7th/cmp-path",
		},
		config = function()
			-- See `:help cmp`
			local cmp = require("cmp")
			local luasnip = require("luasnip")
			luasnip.config.setup({})

			cmp.setup({
				snippet = {
					expand = function(args)
						luasnip.lsp_expand(args.body)
					end,
				},
				completion = { completeopt = "menu,menuone,noinsert" },

				-- For an understanding of why these mappings were
				-- chosen, you will need to read `:help ins-completion`
				--
				-- No, but seriously. Please read `:help ins-completion`, it is really good!
				mapping = cmp.mapping.preset.insert({
					-- Select the [n]ext item
					["&amp;amp;amp;amp;lt;C-n&amp;amp;amp;amp;gt;"] = cmp.mapping.select_next_item(),
					-- Select the [p]revious item
					["&amp;amp;amp;amp;lt;C-p&amp;amp;amp;amp;gt;"] = cmp.mapping.select_prev_item(),

					-- Scroll the documentation window [b]ack / [f]orward
					["&amp;amp;amp;amp;lt;C-b&amp;amp;amp;amp;gt;"] = cmp.mapping.scroll_docs(-4),
					["&amp;amp;amp;amp;lt;C-f&amp;amp;amp;amp;gt;"] = cmp.mapping.scroll_docs(4),

					-- Accept ([y]es) the completion.
					--  This will auto-import if your LSP supports it.
					--  This will expand snippets if the LSP sent a snippet.
					["&amp;amp;amp;amp;lt;C-y&amp;amp;amp;amp;gt;"] = cmp.mapping.confirm({ select = true }),

					-- If you prefer more traditional completion keymaps,
					-- you can uncomment the following lines
					--['&amp;amp;amp;amp;lt;CR&amp;amp;amp;amp;gt;'] = cmp.mapping.confirm { select = true },
					--['&amp;amp;amp;amp;lt;Tab&amp;amp;amp;amp;gt;'] = cmp.mapping.select_next_item(),
					--['&amp;amp;amp;amp;lt;S-Tab&amp;amp;amp;amp;gt;'] = cmp.mapping.select_prev_item(),

					-- Manually trigger a completion from nvim-cmp.
					--  Generally you don't need this, because nvim-cmp will display
					--  completions whenever it has completion options available.
					["&amp;amp;amp;amp;lt;C-Space&amp;amp;amp;amp;gt;"] = cmp.mapping.complete({}),

					-- Think of &amp;amp;amp;amp;lt;c-l&amp;amp;amp;amp;gt; as moving to the right of your snippet expansion.
					--  So if you have a snippet that's like:
					--  function $name($args)
					--    $body
					--  end
					--
					-- &amp;amp;amp;amp;lt;c-l&amp;amp;amp;amp;gt; will move you to the right of each of the expansion locations.
					-- &amp;amp;amp;amp;lt;c-h&amp;amp;amp;amp;gt; is similar, except moving you backwards.
					["&amp;amp;amp;amp;lt;C-l&amp;amp;amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.expand_or_locally_jumpable() then
							luasnip.expand_or_jump()
						end
					end, { "i", "s" }),
					["&amp;amp;amp;amp;lt;C-h&amp;amp;amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.locally_jumpable(-1) then
							luasnip.jump(-1)
						end
					end, { "i", "s" }),

					-- For more advanced Luasnip keymaps (e.g. selecting choice nodes, expansion) see:
					--    https://github.com/L3MON4D3/LuaSnip?tab=readme-ov-file#keymaps
				}),
				sources = {
					{
						name = "lazydev",
						-- set group index to 0 to skip loading LuaLS completions as lazydev recommends it
						group_index = 0,
					},
					{ name = "nvim_lsp" },
					{ name = "luasnip" },
					{ name = "path" },
				},
			})
		end,
	},

	{ -- You can easily change to a different colorscheme.
		-- Change the name of the colorscheme plugin below, and then
		-- change the command in the config to whatever the name of that colorscheme is.
		--
		-- If you want to see what colorschemes are already installed, you can use `:Telescope colorscheme`.
		"folke/tokyonight.nvim",
		priority = 1000, -- Make sure to load this before all the other start plugins.
		init = function()
			-- Load the colorscheme here.
			-- Like many other themes, this one has different styles, and you could load
			-- any other, such as 'tokyonight-storm', 'tokyonight-moon', or 'tokyonight-day'.
			vim.cmd.colorscheme("tokyonight-night")

			-- You can configure highlights by doing something like:
			vim.cmd.hi("Comment gui=none")
		end,
	},
	{ -- Colorscheme
		"catppuccin/nvim",
		name = "catppuccin",
		priority = 1000,
		config = function()
			require("catppuccin").setup({
				flavour = "mocha",
				transparent_background = true,
				term_colors = true,
				integrations = {
					telescope = true,
					mason = true,
					which_key = true,
				},
			})
			-- Force loading the colorscheme
			vim.cmd.colorscheme("catppuccin-mocha")
		end,
	},
	--calming Japanese inspired theme with muted tones strings are configurable
	{
		"rebelot/kanagawa.nvim",
		priority = 1000,
		config = function()
			require("kanagawa").setup({
				overrides = function(colors)
					return {
						String = { fg = colors.crystalBlue }, -- Customize string color
					}
				end,
			})
			vim.cmd("colorscheme kanagawa")
		end,
	},

	-- based on one dark pro from vs code
	{
		"olimorris/onedarkpro.nvim",
		priority = 1000,
		config = function()
			require("onedarkpro").setup({
				theme = "onedark", -- Choose "onedark" or "onelight"
			})
			vim.cmd("colorscheme onedark")
		end,
	},

	-- Highlight todo, notes, etc in comments
	{
		"folke/todo-comments.nvim",
		event = "VimEnter",
		dependencies = { "nvim-lua/plenary.nvim" },
		opts = { signs = false },
	},

	{ -- Collection of various small independent plugins/modules
		"echasnovski/mini.nvim",
		config = function()
			-- Better Around/Inside textobjects
			--
			-- Examples:
			--  - va)  - [V]isually select [A]round [)]paren
			--  - yinq - [Y]ank [I]nside [N]ext [Q]uote
			--  - ci'  - [C]hange [I]nside [']quote
			require("mini.ai").setup({ n_lines = 500 })

			-- Add/delete/replace surroundings (brackets, quotes, etc.)
			--
			-- - saiw) - [S]urround [A]dd [I]nner [W]ord [)]Paren
			-- - sd'   - [S]urround [D]elete [']quotes
			-- - sr)'  - [S]urround [R]eplace [)] [']
			require("mini.surround").setup()

			-- Simple and easy statusline.
			--  You could remove this setup call if you don't like it,
			--  and try some other statusline plugin
			local statusline = require("mini.statusline")
			-- set use_icons to true if you have a Nerd Font
			statusline.setup({ use_icons = vim.g.have_nerd_font })

			-- You can configure sections in the statusline by overriding their
			-- default behavior. For example, here we set the section for
			-- cursor location to LINE:COLUMN
			---@diagnostic disable-next-line: duplicate-set-field
			statusline.section_location = function()
				return "%2l:%-2v"
			end

			-- ... and there is more!
			--  Check out: https://github.com/echasnovski/mini.nvim
		end,
	},
	{ -- Highlight, edit, and navigate code
		"nvim-treesitter/nvim-treesitter",
		build = ":TSUpdate",
		main = "nvim-treesitter.configs", -- Sets main module to use for opts
		-- [[ Configure Treesitter ]] See `:help nvim-treesitter`
		opts = {
			ensure_installed = {
				"bash",
				"c",
				"diff",
				"html",
				"lua",
				"luadoc",
				"markdown",
				"markdown_inline",
				"query",
				"vim",
				"vimdoc",
			},
			-- Autoinstall languages that are not installed
			auto_install = true,
			highlight = {
				enable = true,
				-- Some languages depend on vim's regex highlighting system (such as Ruby) for indent rules.
				--  If you are experiencing weird indenting issues, add the language to
				--  the list of additional_vim_regex_highlighting and disabled languages for indent.
				additional_vim_regex_highlighting = { "ruby" },
			},
			indent = { enable = true, disable = { "ruby" } },
		},
		-- There are additional nvim-treesitter modules that you can use to interact
		-- with nvim-treesitter. You should go explore a few and see what interests you:
		--
		--    - Incremental selection: Included, see `:help nvim-treesitter-incremental-selection-mod`
		--    - Show your current context: https://github.com/nvim-treesitter/nvim-treesitter-context
		--    - Treesitter + textobjects: https://github.com/nvim-treesitter/nvim-treesitter-textobjects
	},

	-- The following comments only work if you have downloaded the kickstart repo, not just copy pasted the
	-- init.lua. If you want these files, they are in the repository, so you can just download them and
	-- place them in the correct locations.

	-- NOTE: Next step on your Neovim journey: Add/Configure additional plugins for Kickstart
	--
	--  Here are some example plugins that I've included in the Kickstart repository.
	--  Uncomment any of the lines below to enable them (you will need to restart nvim).
	--
	-- require 'kickstart.plugins.debug',
	-- require 'kickstart.plugins.indent_line',
	-- require 'kickstart.plugins.lint',
	-- require 'kickstart.plugins.autopairs',
	-- require 'kickstart.plugins.neo-tree',
	-- require 'kickstart.plugins.gitsigns', -- adds gitsigns recommend keymaps

	-- NOTE: The import below can automatically add your own plugins, configuration, etc from `lua/custom/plugins/*.lua`
	--    This is the easiest way to modularize your config.
	--
	--  Uncomment the following line and add your plugins to `lua/custom/plugins/*.lua` to get going.
	-- { import = 'custom.plugins' },
	--
	-- For additional information with loading, sourcing and examples see `:help lazy.nvim-🔌-plugin-spec`
	-- Or use telescope!
	-- In normal mode type `&amp;amp;amp;amp;lt;space&amp;amp;amp;amp;gt;sh` then write `lazy.nvim-plugin`
	-- you can continue same window with `&amp;amp;amp;amp;lt;space&amp;amp;amp;amp;gt;sr` which resumes last telescope search
}, {
	ui = {
		-- If you are using a Nerd Font: set icons to an empty table which will use the
		-- default lazy.nvim defined Nerd Font icons, otherwise define a unicode icons table
		icons = vim.g.have_nerd_font and {} or {
			cmd = "⌘",
			config = "🛠",
			event = "📅",
			ft = "📂",
			init = "⚙",
			keys = "🗝",
			plugin = "🔌",
			runtime = "💻",
			require = "🌙",
			source = "📄",
			start = "🚀",
			task = "📌",
			lazy = "💤 ",
		},
	},
})

-- At the very end of your init.lua
vim.api.nvim_create_autocmd("UIEnter", {
	callback = function()
		if vim.g.colors_name ~= "catppuccin-mocha" then
			vim.cmd.colorscheme("catppuccin-mocha")
		end
	end,
	group = vim.api.nvim_create_augroup("EnforceCatppuccin", { clear = true }),
})

-- The line beneath this is called `modeline`. See `:help modeline`
-- vim: ts=2 sts=2 sw=2 et
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="dotfiles/ghostty/config"&amp;amp;amp;gt;background-opacity = 0.82

theme = catppuccin-mocha
keybind = shift+enter=text:\n
macos-titlebar-style = hidden

keybind = ctrl+3=reload_config
keybind = global:cmd+shift+space=toggle_quick_terminal
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".ruff_cache/.gitignore"&amp;amp;amp;gt;# Automatically created by ruff.
*
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="resources/sqlite3-commands.md"&amp;amp;amp;gt;# SQLite3 Common Commands Reference

## Creating a Database

To create a SQLite3 database, simply run:

```bash
sqlite3 database_name.db
```

This will:
- Create a new database file if it doesn't exist
- Open the database if it already exists
- Start the sqlite3 interactive shell

Example:
```bash
sqlite3 myapp.db
```

You can also create a database and run a command:
```bash
sqlite3 myapp.db "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT);"
```

The database file is created when you:
- Create the first table
- Insert the first data
- Or explicitly save with `.save database_name.db`

## Meta Commands (dot commands)

- `.tables` - List all tables
- `.schema [table]` - Show CREATE statements
- `.quit` or `.exit` - Exit sqlite3
- `.help` - Show all commands
- `.databases` - List attached databases
- `.headers on/off` - Show/hide column headers
- `.mode column` - Pretty-print output
- `.width` - Set column widths
- `.import FILE TABLE` - Import CSV data
- `.output FILE` - Redirect output to file
- `.dump` - Export database as SQL

## SQL Commands

- `SELECT * FROM table;` - Query data
- `INSERT INTO table VALUES (...);` - Insert data
- `UPDATE table SET col=val WHERE ...;` - Update data
- `DELETE FROM table WHERE ...;` - Delete data
- `CREATE TABLE ...` - Create table
- `DROP TABLE table;` - Delete table
- `ALTER TABLE ...` - Modify table
- `CREATE INDEX ...` - Create index
- `PRAGMA table_info(table);` - Show table structure
- `VACUUM;` - Optimize database&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="resources/git-worktrees.md"&amp;amp;amp;gt;# Git Worktrees Guide

Git worktrees allow you to have multiple branches checked out simultaneously in different directories. This is incredibly useful when you need to work on multiple features, review PRs, or quickly switch contexts without stashing changes.

## What are Git Worktrees?

A git worktree is a linked working tree that shares the same repository but allows you to have different branches checked out in different directories. All worktrees share:
- The same `.git` directory (repository data)
- The same remote configurations
- The same stash entries
- The same commit history

## Creating a Worktree

### Basic Syntax
```bash
git worktree add &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt; &amp;amp;amp;amp;lt;branch&amp;amp;amp;amp;gt;
```

### Examples

#### Create worktree from existing branch
```bash
# Create a worktree for the 'feature/auth' branch in a new directory
git worktree add ../myproject-auth feature/auth

# Create worktree in a specific location
git worktree add /tmp/hotfix hotfix/urgent-bug
```

#### Create worktree with a new branch
```bash
# Create a new branch and worktree simultaneously
git worktree add -b feature/new-ui ../myproject-new-ui

# Create from a specific commit or tag
git worktree add -b release/v2.0 ../myproject-v2 v2.0-tag
```

#### Practical Example
```bash
# You're working on main in /Users/you/myproject
cd /Users/you/myproject

# Create a worktree for a new feature
git worktree add -b feature/payment-integration ../myproject-payments

# Now you have:
# /Users/you/myproject (main branch)
# /Users/you/myproject-payments (feature/payment-integration branch)

# Navigate to the new worktree
cd ../myproject-payments

# Work on your feature
echo "Payment module" &amp;amp;amp;amp;gt; payment.py
git add payment.py
git commit -m "Add payment module"
```

## Working with Worktrees

### List all worktrees
```bash
git worktree list
# Output:
# /Users/you/myproject         abc1234 [main]
# /Users/you/myproject-payments def5678 [feature/payment-integration]
```

### Switch between worktrees
Simply use `cd` to navigate between directories:
```bash
cd /Users/you/myproject          # main branch
cd /Users/you/myproject-payments  # feature branch
```

## Merging Worktree Changes Back to Main

Since worktrees share the same repository, merging is straightforward:

### Step 1: Commit changes in your worktree
```bash
cd /Users/you/myproject-payments
git add .
git commit -m "Complete payment integration"
git push -u origin feature/payment-integration
```

### Step 2: Switch to main (in any worktree or main directory)
```bash
cd /Users/you/myproject  # Or stay in any worktree
git checkout main
git pull origin main     # Ensure main is up to date
```

### Step 3: Merge the feature branch
```bash
# Simple merge
git merge feature/payment-integration

# Or merge with a merge commit (recommended for features)
git merge --no-ff feature/payment-integration

# Or rebase if you prefer linear history
git rebase main feature/payment-integration
```

### Step 4: Push to remote
```bash
git push origin main
```

### Step 5: Clean up (optional)
```bash
# Delete the local branch
git branch -d feature/payment-integration

# Delete the remote branch
git push origin --delete feature/payment-integration

# Remove the worktree
git worktree remove /Users/you/myproject-payments
```

## Alternative: Using Pull Requests

For team workflows, you might prefer Pull Requests:

```bash
# 1. In your worktree, push the branch
cd /Users/you/myproject-payments
git push -u origin feature/payment-integration

# 2. Create PR via GitHub/GitLab/Bitbucket web interface

# 3. After PR is merged, clean up locally
git worktree remove /Users/you/myproject-payments
git branch -d feature/payment-integration
```

## Worktree Management

### Remove a worktree
```bash
# Remove worktree (must be clean with no uncommitted changes)
git worktree remove /path/to/worktree

# Force removal (discards local changes)
git worktree remove --force /path/to/worktree
```

### Prune stale worktrees
```bash
# Remove worktree references if directory was deleted manually
git worktree prune
```

### Lock/unlock a worktree
```bash
# Prevent a worktree from being pruned
git worktree lock /path/to/worktree

# Unlock it later
git worktree unlock /path/to/worktree
```

## Best Practices

1. **Use descriptive paths**: Name worktree directories after their purpose
   ```bash
   git worktree add ../project-bugfix-auth bugfix/auth-issue
   git worktree add ../project-feature-api feature/new-api
   ```

2. **Keep worktrees organized**: Use a consistent structure
   ```bash
   ~/work/
     myproject/          # main
     myproject-feature1/ # feature branch
     myproject-hotfix/   # hotfix branch
   ```

3. **Clean up regularly**: Remove worktrees when done
   ```bash
   git worktree list
   git worktree remove &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;
   ```

4. **Don't share worktree directories**: Each developer should create their own

5. **Commit before switching**: Although you can leave changes uncommitted, it's cleaner to commit or stash first

## Common Issues and Solutions

### "fatal: '&amp;amp;amp;amp;lt;branch&amp;amp;amp;amp;gt;' is already checked out at '&amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;'"
You can't have the same branch checked out in multiple worktrees. Either:
- Use the existing worktree: `cd &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;`
- Or checkout a different branch in one of the worktrees

### Worktree directory was manually deleted
```bash
git worktree prune  # Cleans up references to missing worktrees
```

### Need to move a worktree
```bash
git worktree move &amp;amp;amp;amp;lt;old-path&amp;amp;amp;amp;gt; &amp;amp;amp;amp;lt;new-path&amp;amp;amp;amp;gt;
```

## Quick Reference

```bash
# Create worktree
git worktree add &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt; &amp;amp;amp;amp;lt;branch&amp;amp;amp;amp;gt;
git worktree add -b &amp;amp;amp;amp;lt;new-branch&amp;amp;amp;amp;gt; &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;

# List worktrees
git worktree list

# Remove worktree
git worktree remove &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;

# Clean up stale entries
git worktree prune

# Move worktree
git worktree move &amp;amp;amp;amp;lt;old&amp;amp;amp;amp;gt; &amp;amp;amp;amp;lt;new&amp;amp;amp;amp;gt;

# Lock/unlock
git worktree lock &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;
git worktree unlock &amp;amp;amp;amp;lt;path&amp;amp;amp;amp;gt;
```

## Example Workflow

Here's a complete example of using worktrees for feature development:

```bash
# Starting in main project directory
cd ~/projects/myapp

# 1. Create a worktree for a new feature
git worktree add -b feature/user-profiles ../myapp-profiles

# 2. Work on the feature
cd ../myapp-profiles
# ... make changes ...
git add .
git commit -m "Add user profile functionality"
git push -u origin feature/user-profiles

# 3. Switch back to main for a hotfix
cd ../myapp
git pull origin main
# ... fix critical bug ...
git add .
git commit -m "Fix critical auth bug"
git push origin main

# 4. Continue feature work without any stashing needed
cd ../myapp-profiles
# ... complete feature ...
git add .
git commit -m "Complete user profiles"
git push

# 5. Merge feature to main
cd ../myapp
git checkout main
git pull origin main
git merge --no-ff feature/user-profiles
git push origin main

# 6. Clean up
git branch -d feature/user-profiles
git push origin --delete feature/user-profiles
git worktree remove ../myapp-profiles

# Verify cleanup
git worktree list  # Should only show main worktree
```

This workflow demonstrates the power of worktrees: you can quickly switch between feature development and hotfixes without the overhead of stashing, checking out different branches, and potentially losing context.&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/project-overview.md"&amp;amp;amp;gt;# MCP Server for local personal development

## A set of tools, writen in python that provides allows for fast and efficient context engineering and is designed to work with any existing tools that support MCP. The focus is mostly on utilizing the Claude ecosystem and prompting approaches using /slash commands and prompt engineering to orchestrate clean and optimal context into a LLM. 

## The MCP tools available in this project are:

**Web Content &amp;amp;amp;amp;amp; Processing:**
- `fetch_url` - Fetch raw content from a single URL
- `fetch_urls` - Fetch content from multiple URLs concurrently
- `get_docs` - Extract specific documentation sections from web pages
- `to_markdown` - Convert HTML to markdown format
- `strip_html` - Remove HTML tags to get plain text
- `copy_clipboard` - Copy text to system clipboard

**AI Model Information:**
- `get_anthropic_model_list` - List available Claude models
- `get_openai_model_list` - List available GPT models
- `get_xai_model_list` - List available Grok models
- `get_gemini_model_list` - List Gemini models with token limits

**Token Counting:**
- `count_openai_tokens` - Count tokens for OpenAI models
- `count_anthropic_tokens` - Count tokens for Claude models
- `count_gemini_tokens` - Count tokens for Gemini models
- `count_grok_tokens` - Count tokens for Grok models

**Code Review:**
- `run_code_review` - Review code from a diff file using multiple LLMs
- `run_git_diff_review` - Review git diff changes

**Prompt Improvement:**  
- `generate_prompt` - Generate optimized AI prompts using Anthropic's experimental API

**Other Tools:**
- `use_polygon` - Fetch financial market data from Polygon.io


&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/prd_template.md"&amp;amp;amp;gt;User approved Claude's plan:
  ⎿ Plan: [PROJECT_NAME]

    I'll create [PROJECT_DESCRIPTION]. Here's the detailed
    implementation plan:

    1. [PROJECT_NAME] Architecture

    - [ARCHITECTURE_POINT_1]
    - [ARCHITECTURE_POINT_2]
    - [ARCHITECTURE_POINT_3]

    2. Project Structure

    Create these components/directories if they don't exist:
    - [COMPONENT_1] - for [COMPONENT_1_PURPOSE]
    - [COMPONENT_2] - for [COMPONENT_2_PURPOSE]
    - [ADDITIONAL_COMPONENTS_AS_NEEDED]

    3. Main Features to Implement:

    a. [FEATURE_1_NAME]

    - [FEATURE_1_DETAIL_1]
    - [FEATURE_1_DETAIL_2]
    - [FEATURE_1_DETAIL_3]

    b. [FEATURE_2_NAME]

    - [FEATURE_2_DETAIL_1]
    - [FEATURE_2_DETAIL_2]
    - [FEATURE_2_DETAIL_3]

    c. [FEATURE_3_NAME]

    - [FEATURE_3_DETAIL_1]
    - [FEATURE_3_DETAIL_2]
    - [FEATURE_3_DETAIL_3]

    d. [ADDITIONAL_FEATURES_AS_NEEDED]

    4. Key Implementation Details:

    Core Structure:

    [CORE_STRUCTURE_OVERVIEW]

    # [CONFIGURATION_SECTION]
    [CONFIGURATION_DETAILS]

    # Define [MAIN_DATA_STRUCTURES]
    [DATA_STRUCTURE_DEFINITIONS]

    # Core [FUNCTIONALITY_TYPE]
    [CORE_FUNCTIONALITY_NAME] {
        [FUNCTIONALITY_PARAMETERS]
        # Implementation...
    }

    # [PROCESSING_LOGIC_DESCRIPTION]
    [PROCESSING_COMPONENT_1] { ... }
    [PROCESSING_COMPONENT_2] { ... }
    [ADDITIONAL_PROCESSING_COMPONENTS] { ... }

    Enhanced Features:

    - [ENHANCED_FEATURE_1]
    - [ENHANCED_FEATURE_2]
    - [ENHANCED_FEATURE_3]
    - [ENHANCED_FEATURE_4]
    - [ENHANCED_FEATURE_5]

    5. Implementation Flow:

    1. [FLOW_STEP_1]
    2. [FLOW_STEP_2]
    3. [FLOW_STEP_3]
    4. For each [ITERATION_TARGET]:
      - [ITERATION_CASE_1]: [ACTION_1]
      - [ITERATION_CASE_2]: [ACTION_2]
      - [ITERATION_CASE_3]: [ACTION_3]
      - [ADDITIONAL_ITERATION_CASES]: [ACTIONS]
    5. [FLOW_FINAL_STEP]

    6. Output/Results Format:

    [OUTPUT_DESCRIPTION]:
    ✓ [OUTPUT_ITEM_1]: [OUTPUT_DETAIL_1]
    ✓ [OUTPUT_ITEM_2]: [OUTPUT_DETAIL_2]
    ✓ [OUTPUT_ITEM_3]: [OUTPUT_DETAIL_3]
    ✓ [OUTPUT_ITEM_4]: [OUTPUT_DETAIL_4]

    Total: [TOTAL_RESULTS_MESSAGE]

    7. Error Handling:

    - [ERROR_HANDLING_STRATEGY_1]
    - [ERROR_HANDLING_STRATEGY_2]
    - [ERROR_HANDLING_STRATEGY_3]
    - [ERROR_HANDLING_STRATEGY_4]

    [CLOSING_NOTES_OR_ADDITIONAL_CONSIDERATIONS]&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/completed/add_claude_sdk_processing.md"&amp;amp;amp;gt;# Plan: Add Claude Code SDK Processing to build_worktrees()

## Overview

Enhance the `build_worktrees()` function to automatically process plan markdown files using the Claude Code SDK after creating worktrees. This will allow for automatic implementation of approved plans in their dedicated worktree environments.

## Goals

1. **Automated Plan Execution**: Automatically run approved plans through Claude Code SDK
2. **Parallel Processing**: Execute multiple plan implementations simultaneously
3. **Isolated Environments**: Each plan runs in its own worktree directory
4. **Comprehensive Reporting**: Detailed status and output for each plan execution
5. **Error Resilience**: Graceful handling of failures in individual plan processing

## Implementation Details

### Enhanced build_worktrees() Function

```python
@mcp.tool()
async def build_worktrees(auto_process: bool = False) -&amp;amp;amp;amp;gt; dict:
    """
    Create git worktrees for approved plans in _docs/plans/approved/.
    Optionally process plans automatically using Claude Code SDK.
    
    Args:
        auto_process: If True, automatically process plan files using Claude SDK
    
    Returns:
        Dictionary with status, summary, and optional processing results
    """
    # ... existing worktree creation logic ...
    
    if auto_process and created:
        # Process plans in parallel
        processing_results = await process_plans_in_worktrees(
            created_worktrees, plan_files, parent_dir
        )
        result["processing_results"] = processing_results
    
    return result
```

### New Helper Functions

#### Core Processing Function
```python
async def process_plans_in_worktrees(
    created_worktrees: List[str], 
    plan_files: List[Path], 
    base_dir: Path
) -&amp;amp;amp;amp;gt; List[dict]:
    """
    Process multiple plan files in parallel using Claude Code SDK.
    
    Args:
        created_worktrees: List of successfully created worktree names
        plan_files: List of corresponding plan file paths
        base_dir: Base directory containing worktrees
    
    Returns:
        List of processing results for each plan
    """
    tasks = []
    
    for worktree_name in created_worktrees:
        # Find corresponding plan file
        plan_file = next(
            (pf for pf in plan_files if pf.stem.replace("_", "-") in worktree_name),
            None
        )
        
        if plan_file:
            worktree_dir = base_dir / f"collect-{worktree_name}"
            task = process_single_plan(plan_file, worktree_dir)
            tasks.append(task)
    
    # Execute all tasks in parallel
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Convert exceptions to error dictionaries
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processed_results.append({
                "status": "failed",
                "error": f"Unexpected error: {str(result)}",
                "plan_file": created_worktrees[i] if i &amp;amp;amp;amp;lt; len(created_worktrees) else "unknown"
            })
        else:
            processed_results.append(result)
    
    return processed_results
```

#### Individual Plan Processing
```python
async def process_single_plan(plan_file: Path, worktree_dir: Path) -&amp;amp;amp;amp;gt; dict:
    """
    Process a single plan file using Claude Code SDK in its worktree.
    
    Args:
        plan_file: Path to the markdown plan file
        worktree_dir: Path to the worktree directory
    
    Returns:
        Dictionary with processing status, output, and metadata
    """
    try:
        # Read and prepare plan content
        plan_content = plan_file.read_text(encoding='utf-8')
        processed_content = extract_plan_prompt(plan_content)
        
        # Prepare Claude Code command
        cmd = [
            "claude", 
            "-p", processed_content,
            "--dangerously-skip-permissions"
        ]
        
        # Execute in worktree directory with timeout
        start_time = time.time()
        process = await asyncio.create_subprocess_exec(
            *cmd,
            cwd=worktree_dir,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            text=True
        )
        
        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=600  # 10 minute timeout
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            return {
                "plan_file": plan_file.name,
                "worktree_dir": str(worktree_dir),
                "status": "failed",
                "error": "Process timed out after 10 minutes",
                "duration": time.time() - start_time
            }
        
        duration = time.time() - start_time
        
        if process.returncode == 0:
            return {
                "plan_file": plan_file.name,
                "worktree_dir": str(worktree_dir),
                "status": "success",
                "output": stdout,
                "duration": duration,
                "exit_code": process.returncode
            }
        else:
            return {
                "plan_file": plan_file.name,
                "worktree_dir": str(worktree_dir),
                "status": "failed",
                "error": stderr or "Unknown error",
                "output": stdout,
                "duration": duration,
                "exit_code": process.returncode
            }
    
    except Exception as e:
        return {
            "plan_file": plan_file.name,
            "worktree_dir": str(worktree_dir),
            "status": "failed",
            "error": f"Exception during processing: {str(e)}"
        }
```

#### Plan Content Extraction
```python
def extract_plan_prompt(plan_content: str) -&amp;amp;amp;amp;gt; str:
    """
    Extract the main plan content from markdown, removing metadata and headers.
    
    Args:
        plan_content: Raw markdown content of the plan file
    
    Returns:
        Cleaned plan content suitable for Claude Code SDK
    """
    lines = plan_content.split('\n')
    
    # Skip YAML frontmatter if present
    if lines and lines[0].strip() == '---':
        end_frontmatter = None
        for i, line in enumerate(lines[1:], 1):
            if line.strip() == '---':
                end_frontmatter = i + 1
                break
        if end_frontmatter:
            lines = lines[end_frontmatter:]
    
    # Join and clean up
    content = '\n'.join(lines).strip()
    
    # Add instruction prefix
    prompt = f"""Please implement the following plan in this codebase:

{content}

Follow the plan step by step and implement all the required changes. Use the available tools to read existing code, make modifications, and test the implementation."""
    
    return prompt
```

## Enhanced Return Structure

```json
{
  "status": "success",
  "summary": {
    "found": 3,
    "created": 3,
    "skipped": 0,
    "failed": 0
  },
  "details": {
    "created": ["add_feature_one.md", "fix_bug_two.md"],
    "skipped": [],
    "failed": []
  },
  "worktree_dir": "/path/to/parent",
  "worktree_list": "git worktree list output",
  "processing_results": [
    {
      "plan_file": "add_feature_one.md",
      "worktree_dir": "/path/to/collect-add-feature-one",
      "status": "success",
      "output": "Implementation completed successfully...",
      "duration": 45.2,
      "exit_code": 0
    },
    {
      "plan_file": "fix_bug_two.md", 
      "worktree_dir": "/path/to/collect-fix-bug-two",
      "status": "failed",
      "error": "Command failed with exit code 1",
      "output": "Partial output...",
      "duration": 12.1,
      "exit_code": 1
    }
  ]
}
```

## Error Handling Strategy

### Command Execution Errors
- Capture both stdout and stderr
- Include exit codes in results
- Handle timeouts gracefully
- Provide detailed error messages

### File System Errors
- Handle missing plan files
- Check worktree directory existence
- Proper encoding handling for markdown files

### Parallel Processing Errors
- Use `return_exceptions=True` in `asyncio.gather()`
- Convert exceptions to structured error responses
- Continue processing other plans if one fails

### Resource Management
- Set reasonable timeouts (10 minutes default)
- Proper process cleanup on timeout/cancellation
- Memory-efficient handling of large outputs

## Configuration Options

### Environment Variables
```bash
# Optional: Customize Claude Code executable path
CLAUDE_CODE_EXECUTABLE=claude

# Optional: Set global timeout for plan processing (seconds)
CLAUDE_PLAN_TIMEOUT=600

# Optional: Set maximum parallel processes
CLAUDE_MAX_PARALLEL=5
```

### Function Parameters
- `auto_process: bool = False` - Enable automatic plan processing
- `timeout: int = 600` - Timeout in seconds for each plan
- `max_parallel: int = 5` - Maximum concurrent plan processes

## Usage Examples

### Basic Usage with Auto-processing
```python
# Create worktrees and process plans automatically
result = await build_worktrees(auto_process=True)

# Check overall status
if result["status"] == "success":
    print(f"Created {result['summary']['created']} worktrees")
    
    # Check processing results if auto_process was enabled
    if "processing_results" in result:
        for proc_result in result["processing_results"]:
            if proc_result["status"] == "success":
                print(f"✅ {proc_result['plan_file']}: Completed in {proc_result['duration']:.1f}s")
            else:
                print(f"❌ {proc_result['plan_file']}: {proc_result['error']}")
```

### Manual Processing Later
```python
# Create worktrees first
result = await build_worktrees(auto_process=False)

# Process specific plans later
if result["status"] == "success":
    # Get created worktrees and plan files
    created = result["details"]["created"]
    plan_files = [Path(f"_docs/plans/approved/{name}") for name in created]
    
    # Process manually with custom options
    processing_results = await process_plans_in_worktrees(
        created, plan_files, Path(result["worktree_dir"])
    )
```

## Testing Strategy

### Unit Tests
```python
async def test_process_single_plan_success():
    """Test successful plan processing."""
    # Create mock plan file and worktree
    # Test with simple plan content
    # Verify successful execution and output structure

async def test_process_single_plan_timeout():
    """Test plan processing timeout handling."""
    # Create plan that would run longer than timeout
    # Verify timeout handling and cleanup

async def test_extract_plan_prompt():
    """Test plan content extraction and cleaning."""
    # Test with various markdown formats
    # Test YAML frontmatter removal
    # Test content preservation
```

### Integration Tests
```python
async def test_build_worktrees_with_auto_process():
    """Test complete workflow with auto-processing."""
    # Create test repository with plans
    # Run build_worktrees with auto_process=True
    # Verify worktrees created and plans processed

async def test_parallel_plan_processing():
    """Test multiple plans processing in parallel."""
    # Create multiple plan files
    # Verify all plans process concurrently
    # Check that failures in one don't affect others
```

## Implementation Notes

### Dependencies
- Add `asyncio` import to collect.py
- Add `time` import for duration tracking
- Ensure Claude Code CLI is available in PATH

### Performance Considerations
- Limit concurrent processes to avoid resource exhaustion
- Implement backpressure for large numbers of plans
- Consider memory usage for large plan outputs

### Security Considerations
- Using `--dangerously-skip-permissions` bypasses safety checks
- Ensure this is only used in trusted, isolated environments
- Consider adding user confirmation for auto-processing

### Monitoring and Logging
- Log start/completion of each plan processing
- Track processing durations for performance monitoring
- Consider adding progress indicators for long-running operations

## Future Enhancements

1. **Progress Streaming**: Real-time updates on processing status
2. **Selective Processing**: Allow processing specific plans only
3. **Retry Logic**: Automatic retry for transient failures
4. **Output Storage**: Save Claude outputs to files in worktrees
5. **Plan Dependencies**: Support for plan execution ordering
6. **Resource Limits**: CPU/memory constraints for plan processing
7. **Integration Testing**: Automated testing of implemented plans

## Migration Path

1. **Phase 1**: Implement core functionality with `auto_process=False` default
2. **Phase 2**: Add comprehensive testing and error handling
3. **Phase 3**: Enable auto-processing by default after validation
4. **Phase 4**: Add advanced features like progress tracking and selective processing

This plan provides a comprehensive approach to integrating Claude Code SDK processing into the worktree workflow, enabling automated implementation of approved plans while maintaining robustness and flexibility.&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/completed/add_projects_table.md"&amp;amp;amp;gt;# Plan: Add Projects Table and Update Prompt Model

## Overview
Add a new `projects` table to the database and update the `prompt` table to include an optional foreign key reference to projects. This enables tracking which prompts belong to specific projects while maintaining backwards compatibility for prompts without projects.

## Implementation Steps

### 1. Create New Yoyo Migration File
Create a new migration file `migrations/20250810_01_add-projects-table.sql` with proper yoyo format and dependencies.

The migration should:
- Depend on the existing `20250727_01_create-prompt-tables` migration
- Include both up and down migration steps for rollback capability

### 2. Create Projects Table
```sql
-- Create projects table with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 3. Update Prompt Table with Foreign Key
```sql
-- Add github_url column to prompt table (nullable for backwards compatibility)
ALTER TABLE prompt ADD COLUMN github_url TEXT;

-- Add foreign key constraint
ALTER TABLE prompt 
    ADD CONSTRAINT fk_prompt_project 
    FOREIGN KEY (github_url) REFERENCES projects(github_url)
    ON DELETE SET NULL;

-- Add index for github_url for efficient joins
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);
```

### 4. Migration File Structure
The complete migration file should follow this structure:
```sql
-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;
```

## Key Features
- **Projects table**: Stores project metadata with GitHub URL as primary key and description
- **Optional foreign key**: `github_url` in prompt table is nullable for backwards compatibility
- **Referential integrity**: Foreign key constraint ensures valid project references
- **Performance optimization**: Index on github_url foreign key for efficient queries
- **Rollback support**: Down migration steps commented for manual rollback if needed

## Testing Considerations
- Verify existing prompts continue to work without project_id
- Test inserting prompts with valid project_id references
- Verify foreign key constraint prevents invalid github_url values
- Test cascade behavior on project deletion (should set prompt.github_url to NULL)
- Check index performance on joins between prompt and projects tables

## Example Usage
```python
# Create a new project
project = Project(
    github_url="https://github.com/user/repo",
    description="Example project"
)

# Create a prompt linked to the project
prompt_data = PromptData(
    type=PromptType.PLAN,
    status=PromptPlanStatus.DRAFT,
    content="Example prompt content",
    project="Example Project Name"
)

prompt = Prompt(
    id="prompt_456",
    name="example_prompt",
    data=prompt_data,
    github_url="https://github.com/user/repo",  # Links to the project
    version=1,
    content_hash="abc123",
    created_at=datetime.now(),
    updated_at=datetime.now()
)
```

## Files to Modify
- [ ] Create `migrations/20250810_01_add-projects-table.sql`
- [✅] Update `repository/prompt_models.py` (already done by user)
  - Added `Project` model with `github_url` as primary identifier
  - Added `github_url: Optional[str]` to `Prompt` model

## Notes
- The Pydantic models have already been updated in `repository/prompt_models.py`
- The migration maintains backwards compatibility by making `github_url` optional
- The `ON DELETE SET NULL` ensures prompts aren't orphaned when projects are deleted
- Using `github_url` as primary key eliminates the need for a separate `id` field
- Consider adding a trigger to update `updated_at` timestamp on projects table modifications
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/completed/api_mcp_server_integration.md"&amp;amp;amp;gt;# API Server Integration with MCP Server

## Overview
This document explains how to automatically start the FastAPI server when the MCP (Model Context Protocol) server starts, ensuring both services run together.

## Problem Statement
- **collect.py**: MCP server that provides AI tools
- **api.py**: FastAPI server for HTTP endpoints
- **Goal**: Start both servers with a single command

## Proposed Solution

### Using subprocess.Popen
The solution uses Python's `subprocess` module to launch the API server as a background process when the MCP server starts.

### Implementation Details

#### 1. Import Required Modules
```python
import subprocess  # For running API as subprocess
import atexit      # For cleanup on exit
import time        # For startup delay
```

#### 2. Modified main() Function
```python
def main():
    # Start the API server in the background
    api_process = None
    try:
        # Launch API server as subprocess
        api_process = subprocess.Popen(
            ["python", "api.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Wait for API to initialize
        time.sleep(2)
        
        # Verify successful startup
        if api_process.poll() is not None:
            # Process ended unexpectedly
            stderr = api_process.stderr.read()
            print(f"API server failed to start: {stderr}")
        else:
            print(f"API server started with PID: {api_process.pid}")
            
            # Register cleanup handler
            def cleanup_api():
                if api_process and api_process.poll() is None:
                    print("Shutting down API server...")
                    api_process.terminate()
                    try:
                        api_process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        api_process.kill()
            
            atexit.register(cleanup_api)
    
    except Exception as e:
        print(f"Failed to start API server: {e}")
    
    # Continue with MCP server startup
    mcp.run(transport="stdio")
```

## How It Works

### Startup Sequence
1. **Launch API Process**: Uses `subprocess.Popen()` to start `api.py` in background
2. **Non-blocking**: Popen returns immediately, allowing MCP to continue
3. **Startup Delay**: 2-second sleep gives API time to initialize
4. **Health Check**: `poll()` verifies process is still running
5. **Error Handling**: Captures stderr if API fails to start

### Process Management
- **subprocess.Popen**: Creates child process for API server
- **stdout/stderr pipes**: Captures API output (prevents terminal clutter)
- **PID tracking**: Stores process ID for management
- **Independent processes**: Both servers run in parallel

### Cleanup Mechanism
- **atexit.register()**: Ensures cleanup runs on any exit
- **Graceful shutdown**: First tries `terminate()` (SIGTERM)
- **Forced shutdown**: Uses `kill()` if graceful fails
- **Timeout handling**: 5-second grace period for cleanup

## Benefits

1. **Single Command**: Run both servers with `uv run collect.py`
2. **Automatic Cleanup**: API stops when MCP stops
3. **Error Visibility**: Failed startup is reported
4. **Process Isolation**: Servers run independently
5. **Clean Logs**: API output is captured, not mixed with MCP

## Usage

Once implemented, simply run:
```bash
uv run collect.py
```

This will:
1. Start the API server on configured port (from .env)
2. Start the MCP server
3. Both services run until interrupted
4. Clean shutdown on Ctrl+C

## Port Configuration
Ensure your `.env` file has:
```
PORT=8081  # Or desired port for API
```

## Debugging

If API fails to start:
1. Check port availability: `lsof -i :8000`
2. Verify environment: `uv run python api.py` (standalone)
3. Check logs in stderr capture
4. Ensure database path exists: `data/collect.db`

## Production Considerations

For production deployment, consider:
- Using proper process managers (systemd, supervisor)
- Implementing health checks and restarts
- Separating services in containers
- Using reverse proxy (nginx) for API
- Implementing proper logging to files&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/completed/sync_github_commands.md"&amp;amp;amp;gt;# Plan: GitHub Commands Sync and Conversion Script

**Status**: COMPLETED (2025-01-10)

## Overview
Create a comprehensive solution that:
1. Uses GitHub CLI to fetch `.claude/commands` from the remote repository `austere-labs/collect`
2. Creates local `.claude/commands` and `.gemini/commands` directory structures
3. Converts markdown command files to TOML format using Gemini 2.5 Pro
4. Maintains directory structure fidelity between remote and local systems

## Implementation Steps

### 1. Create main script file: `sync_commands.py`
- Use GitHub CLI commands exclusively for all GitHub operations
- Create functions for directory management and file operations
- Implement async pattern for efficient operations
- Follow project patterns using existing Config and SecretManager classes

### 2. Implement GitHub CLI integration functions
- `fetch_directory_tree()`: Use `gh api repos/austere-labs/collect/contents/.claude/commands` to get directory structure
- `download_file_content()`: Use `gh api repos/austere-labs/collect/contents/{path}` to download individual files
- `list_subdirectories()`: Use GitHub CLI to recursively discover all subdirectories
- `create_local_directories()`: Mirror remote structure locally using discovered paths
- All GitHub operations exclusively through `gh` CLI commands, no direct API calls

### 3. Create local directory management
- Check for existing `.claude/commands` directory in current project
- Create missing directories mirroring remote structure exactly
- Create equivalent `.gemini/commands` directory structure
- Preserve all subdirectory relationships

### 4. Implement `convert(prompt: str) -&amp;amp;amp;amp;gt; str` function
- Use existing GeminiMCP class from `models/gemini_mcp.py`
- Create specialized prompt for markdown-to-TOML conversion
- Use Gemini 2.5 Pro model for high-quality conversions
- Handle token limits and error cases gracefully
- Preserve semantic meaning while adapting format structure

### 5. Add main orchestration function
- Pull all commands from GitHub using GitHub CLI exclusively
- Write files to local `.claude/commands` maintaining structure
- Convert each markdown file and write to corresponding `.gemini/commands` location
- Add comprehensive progress tracking and error handling
- Support dry-run mode for testing

## Key Features
- Exclusive GitHub CLI integration for secure, authenticated access
- Recursive directory structure replication with full fidelity
- AI-powered format conversion using Gemini 2.5 Pro
- Comprehensive error handling and progress reporting
- Integration with existing project architecture (Config, SecretManager, GeminiMCP)
- Support for both `.claude/commands` and `.gemini/commands` workflows

## Testing Considerations
- Test with various markdown command formats and edge cases
- Verify directory creation logic handles nested structures correctly
- Test conversion quality with sample files of different complexities
- Handle GitHub CLI authentication and network failures gracefully
- Verify TOML output is valid and semantically equivalent to markdown input

## Example Usage
```python
# Basic usage
from sync_commands import sync_and_convert_commands
await sync_and_convert_commands()

# With options
await sync_and_convert_commands(
    source_repo="austere-labs/collect",
    dry_run=True,
    convert_only=False
)
```

## Implementation Details

### GitHub CLI Commands to Use
```bash
# List directory contents
gh api repos/austere-labs/collect/contents/.claude/commands

# Get file content
gh api repos/austere-labs/collect/contents/.claude/commands/{filename}

# Recursive directory listing
gh api repos/austere-labs/collect/git/trees/HEAD?recursive=1
```

### Directory Structure Example
```
.claude/commands/
├── subdirectory1/
│   ├── command1.md
│   └── command2.md
└── subdirectory2/
    └── command3.md

.gemini/commands/
├── subdirectory1/
│   ├── command1.toml
│   └── command2.toml
└── subdirectory2/
    └── command3.toml
```

### Conversion Prompt Template
```
Convert the following markdown command to TOML format while preserving all semantic meaning and functionality:

[MARKDOWN_CONTENT]

Output should be valid TOML with appropriate sections and key-value pairs that represent the same information structure as the original markdown.
```

## Implementation Summary

Successfully implemented the GitHub Commands Sync and Conversion Script with all specified requirements:

- ✅ Created `sync_commands.py` with comprehensive GitHub CLI integration
- ✅ Implemented all required functions: `fetch_directory_tree()`, `download_file_content()`, `list_subdirectories()`, `create_local_directories()`
- ✅ Added Gemini 2.5 Flash integration for markdown-to-TOML conversion using existing `GeminiMCP` class
- ✅ Implemented async orchestration function `sync_and_convert_commands()` with dry-run and convert-only modes
- ✅ Added comprehensive error handling and progress reporting
- ✅ Maintained directory structure fidelity with recursive subdirectory support
- ✅ Integrated with existing project architecture (Config, SecretManager patterns)

## Results

- **Script Location**: `sync_commands.py` in project root
- **Functionality**: Successfully syncs 22 markdown files from 5 subdirectories
- **Testing**: All existing tests pass (87/87), no regressions introduced
- **Code Quality**: Passes all linting and formatting checks
- **Dry Run Validation**: Confirmed correct directory structure discovery and file processing logic

## Files Modified

- ✅ `sync_commands.py` - New script created with all functionality
- ✅ Existing codebase unchanged (no breaking changes)

## Verification

- Dry-run testing confirmed discovery of 5 subdirectories and 22 markdown files
- Conversion testing validated markdown-to-TOML functionality using Gemini
- All existing tests pass without modification
- Code meets project quality standards (black formatting, ruff linting)
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/completed/fix_datetime_deprecation_warnings.md"&amp;amp;amp;gt;# Fix Python 3.12 Datetime Adapter Deprecation Warnings

## Status: COMPLETED (2025-01-13)

## Problem

Python 3.12 has deprecated the default datetime adapters and converters in SQLite3, which causes deprecation warnings when using datetime objects with SQLite databases. The warnings appear as:

```
DeprecationWarning: The default datetime adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes
```

## Current State

In `repository/plan_service.py`, we currently:
- Import `datetime` from the datetime module (line 3)
- Use `datetime.now()` to create timestamps (line 71)
- Store datetime objects in the database through Plan objects (lines 115-116, 176, 190-191, 198-199)

## Solution Plan

### 1. Create Custom Datetime Adapters (High Priority)

Create a new module `repository/datetime_adapters.py` with:
- Custom datetime adapter using ISO 8601 format
- Custom datetime converter from ISO 8601 format
- Registration of adapters/converters with sqlite3

### 2. Update Database Schema (Medium Priority)

Ensure the database schema handles datetime storage properly:
- Review current `created_at` and `updated_at` column types
- Update migration if needed to use explicit `TIMESTAMP` type

### 3. Update Plan Service (Medium Priority)

Modify `repository/plan_service.py` to:
- Import the custom datetime adapters
- Use timezone-aware datetime objects where appropriate
- Ensure proper datetime handling in database operations

### 4. Add Tests (Medium Priority)

Create tests to verify:
- Datetime objects are properly stored and retrieved
- No deprecation warnings occur
- Backward compatibility with existing data

### 5. Documentation (Low Priority)

Update documentation to reflect:
- Custom datetime handling approach
- Migration notes for Python 3.12+
- Best practices for datetime usage

## Implementation Details

### Custom Adapter Implementation
```python
import datetime
import sqlite3

def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format."""
    return val.replace(tzinfo=None).isoformat()

def convert_datetime_iso(val):
    """Convert ISO 8601 datetime to datetime.datetime object."""
    return datetime.datetime.fromisoformat(val.decode())

# Register adapters
sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
```

### Database Connection Changes
- Use `detect_types=sqlite3.PARSE_DECLTYPES` when creating connections
- Ensure column types are declared as `TIMESTAMP` in schema

## Benefits

1. **Eliminates Deprecation Warnings**: No more warnings in Python 3.12+
2. **Future-Proof**: Uses recommended approach from Python documentation
3. **Consistent Format**: ISO 8601 format is standard and readable
4. **Backward Compatible**: Existing data remains accessible
5. **Timezone Aware**: Can be extended to support timezone-aware datetimes

## Testing Strategy

1. Run existing tests to ensure no regressions
2. Test datetime storage and retrieval
3. Verify no deprecation warnings appear
4. Test with both Python 3.11 and 3.12
5. Test migration from old to new format

## Files Modified

1. ✅ `repository/datetime_adapters.py` (new file) - Created custom adapters
2. ✅ `repository/plan_service.py` - Added import for datetime adapters
3. ✅ `repository/database.py` - Added PARSE_DECLTYPES to connection
4. ❌ `migrations-plans/20250713_03_datetime_adapters.sql` - Not needed, schema already uses TIMESTAMP
5. ✅ `repository/test_datetime_adapters.py` (new file) - Comprehensive test suite
6. ❌ `repository/test_plan_service.py` - No changes needed, tests pass without warnings

## Implementation Summary

- **Phase 1**: ✅ Created custom adapters and updated plan_service.py
- **Phase 2**: ✅ Added comprehensive test suite with 7 tests
- **Phase 3**: ✅ No migration needed, existing schema uses TIMESTAMP
- **Phase 4**: ✅ All tests pass with `-W error::DeprecationWarning`

## Results

- ✅ No more deprecation warnings in Python 3.12+
- ✅ Backward compatible with existing ISO 8601 datetime strings
- ✅ All plan service tests pass without warnings
- ✅ Custom adapters registered automatically on module import
- ✅ Comprehensive test coverage including edge cases

## Notes

- The solution maintains backward compatibility with existing databases
- ISO 8601 format is human-readable and standard
- Can be extended to support timezone-aware datetimes in the future
- Follows Python 3.12 recommendations exactly&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/drafts/planner_example.md"&amp;amp;amp;gt;## Plan elements

- A plan consists of steps.
- You can always include &amp;amp;amp;amp;lt;if_block&amp;amp;amp;amp;gt; tags to include different steps based on a condition.

### How to Plan

- When planning next steps, make sure it's only the goal of next steps, not the overall goal of the ticket or user.
- Make sure that the plan always follows the procedures and rules of the # Customer service agent Policy doc

### How to create a step

- A step will always include the name of the action (tool call), description of the action and the arguments needed for the action. It will also include a goal of the specific action.

The step should be in the following format:
&amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;{reason for taking the action, description of the action to take, which outputs from other tool calls that should be used (if relevant)}&amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;

- The action_name should always be the name of a valid tool
- The description should be a short description of why the action is needed, a description of the action to take and any variables from other tool calls the action needs e.g. "reply to the user with instrucitons from &amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt;"
- Make sure your description NEVER assumes any information, variables or tool call results even if you have a good idea of what the tool call returns from the SOP.
- Make sure your plan NEVER includes or guesses on information/instructions/rules for step descriptions that are not explicitly stated in the policy doc.
- Make sure you ALWAYS highlight in your description of answering questions/troubleshooting steps that &amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt; is the source of truth for the information you need to answer the question.

- Every step can have an if block, which is used to include different steps based on a condition.
- And if block can be used anywhere in a step and plan and should simply just be wrapped with the &amp;amp;amp;amp;lt;if_block condition=''&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;/if_block&amp;amp;amp;amp;gt; tags. An &amp;amp;amp;amp;lt;if_block&amp;amp;amp;amp;gt; should always have a condition. To create multiple if/else blocks just create multiple &amp;amp;amp;amp;lt;if_block&amp;amp;amp;amp;gt; tags.

### High level example of a plan

_IMPORTANT_: This example of a plan is only to give you an idea of how to structure your plan with a few sample tools (in this example &amp;amp;amp;amp;lt;search_helpcenter&amp;amp;amp;amp;gt; and &amp;amp;amp;amp;lt;reply&amp;amp;amp;amp;gt;), it's not strict rules or how you should structure every plan - it's using variable names to give you an idea of how to structure your plan, think in possible paths and use &amp;amp;amp;amp;lt;tool_calls&amp;amp;amp;amp;gt; as variable names, and only general descriptions in your step descriptions.

Scenario: The user has error with feature_name and have provided basic information about the error

&amp;amp;amp;amp;lt;plan&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;search_helpcenter&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;Search helpcenter for information about feature_name and how to resolve error_name&amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;if_block condition='&amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt; found'&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;reply&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;Reply to the user with instructions from &amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/if_block&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;if_block condition='no &amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt; found'&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;search_helpcenter&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;Search helpcenter for general information about how to resolve error/troubleshoot&amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;if_block condition='&amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt; found'&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
                &amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;reply&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
                &amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;Reply to the user with relevant instructions from general &amp;amp;amp;amp;lt;search_helpcenter_result&amp;amp;amp;amp;gt; information &amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;/if_block&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;if_block condition='no &amp;amp;amp;amp;lt;helpcenter_result&amp;amp;amp;amp;gt; found'&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;step&amp;amp;amp;amp;gt;
                &amp;amp;amp;amp;lt;action_name&amp;amp;amp;amp;gt;reply&amp;amp;amp;amp;lt;/action_name&amp;amp;amp;amp;gt;
                &amp;amp;amp;amp;lt;description&amp;amp;amp;amp;gt;If we can't find specific troubleshooting or general troubleshooting, reply to the user that we need more information and ask for a {{troubleshooting_info_name_from_policy_2}} of the error (since we already have {{troubleshooting_info_name_from_policy_1}}, but need {{troubleshooting_info_name_from_policy_2}} for more context to search helpcenter)&amp;amp;amp;amp;lt;/description&amp;amp;amp;amp;gt;
            &amp;amp;amp;amp;lt;/step&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;/if_block&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/if_block&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;/plan&amp;amp;amp;amp;gt;
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/drafts/claude_youtube_reader.md"&amp;amp;amp;gt;# Plan: YouTube Video Reader with Gemini Analysis

## Overview
Build a Python script that takes a YouTube video URL and uses Google's Gemini AI to analyze the entire video content and produce a comprehensive summary. The system will leverage Gemini's advanced multimodal capabilities to process both visual and audio elements of YouTube videos.

## Implementation Steps

### 1. Environment Setup and Dependencies
**File**: `requirements.txt` or `pyproject.toml`
```python
# Key dependencies needed:
google-generativeai&amp;amp;amp;amp;gt;=0.8.0    # Gemini API client
python-dotenv&amp;amp;amp;amp;gt;=1.0.0          # Environment variable management
pytube&amp;amp;amp;amp;gt;=15.0.0                # YouTube video download (if needed)
requests&amp;amp;amp;amp;gt;=2.31.0              # HTTP requests
pydantic&amp;amp;amp;amp;gt;=2.0.0               # Data validation
click&amp;amp;amp;amp;gt;=8.0.0                  # CLI interface
```

**Environment Variables** (`.env` file):
```bash
GEMINI_API_KEY=your_gemini_api_key_here
GCP_PROJECT_ID=your_project_id  # If using Vertex AI
```

### 2. Core YouTube Video Reader Class
**File**: `youtube_reader.py`
```python
import os
import re
from typing import Optional, Dict, Any
from dataclasses import dataclass
from google import generativeai as genai
from dotenv import load_dotenv

@dataclass
class VideoAnalysis:
    """Data structure for video analysis results"""
    url: str
    title: Optional[str]
    duration: Optional[str]
    summary: str
    key_topics: list[str]
    timestamps: list[Dict[str, str]]
    sentiment: Optional[str]
    content_type: Optional[str]

class YouTubeVideoReader:
    def __init__(self, api_key: Optional[str] = None):
        """Initialize with Gemini API key"""
        load_dotenv()
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY must be provided")
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-2.5-pro')  # Use latest model with video capabilities
    
    def validate_youtube_url(self, url: str) -&amp;amp;amp;amp;gt; bool:
        """Validate YouTube URL format"""
        youtube_patterns = [
            r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([^&amp;amp;amp;amp;amp;\n?#]*)',
            r'(?:https?://)?(?:www\.)?youtu\.be/([^&amp;amp;amp;amp;amp;\n?#]*)',
            r'(?:https?://)?(?:www\.)?youtube\.com/embed/([^&amp;amp;amp;amp;amp;\n?#]*)'
        ]
        return any(re.match(pattern, url) for pattern in youtube_patterns)
    
    async def analyze_video(self, youtube_url: str, custom_prompt: Optional[str] = None) -&amp;amp;amp;amp;gt; VideoAnalysis:
        """
        Analyze YouTube video using Gemini's multimodal capabilities
        
        Args:
            youtube_url: Valid YouTube video URL
            custom_prompt: Optional custom analysis prompt
            
        Returns:
            VideoAnalysis object with comprehensive results
        """
        if not self.validate_youtube_url(youtube_url):
            raise ValueError("Invalid YouTube URL provided")
        
        # Default comprehensive analysis prompt
        default_prompt = """
        Please analyze this YouTube video comprehensively and provide:
        
        1. **Video Summary**: A detailed 3-4 paragraph summary of the main content
        2. **Key Topics**: List the 5-10 most important topics discussed
        3. **Timestamps**: Identify 8-12 key moments with timestamps and descriptions
        4. **Content Type**: Classify the video (educational, entertainment, news, tutorial, etc.)
        5. **Sentiment**: Overall tone and sentiment of the content
        6. **Main Takeaways**: 3-5 key insights or actionable points
        
        Focus on both visual and audio elements. Pay attention to:
        - Spoken content and dialogue
        - Visual elements, graphics, and text shown
        - Scene changes and transitions
        - Background music or sounds that add context
        
        Format your response in a structured way with clear sections.
        """
        
        prompt = custom_prompt or default_prompt
        
        try:
            # Generate content using YouTube URL directly
            response = await self.model.generate_content_async([
                prompt,
                {"mime_type": "video/*", "uri": youtube_url}
            ])
            
            # Parse response into structured format
            analysis_result = self._parse_analysis_response(
                response.text, 
                youtube_url
            )
            
            return analysis_result
            
        except Exception as e:
            raise RuntimeError(f"Failed to analyze video: {str(e)}")
    
    def _parse_analysis_response(self, response_text: str, url: str) -&amp;amp;amp;amp;gt; VideoAnalysis:
        """Parse Gemini response into structured VideoAnalysis object"""
        # This would contain logic to extract structured data from the response
        # For now, return basic structure - implement parsing based on response format
        
        return VideoAnalysis(
            url=url,
            title=None,  # Could extract from video metadata
            duration=None,  # Could extract from video metadata  
            summary=response_text,  # Full response for now
            key_topics=[],  # Parse from response
            timestamps=[],  # Parse from response
            sentiment=None,  # Parse from response
            content_type=None  # Parse from response
        )
```

### 3. CLI Interface
**File**: `cli.py`
```python
import click
import asyncio
from youtube_reader import YouTubeVideoReader, VideoAnalysis

@click.command()
@click.argument('youtube_url')
@click.option('--output', '-o', help='Output file path (optional)')
@click.option('--format', '-f', type=click.Choice(['json', 'markdown', 'text']), 
              default='markdown', help='Output format')
@click.option('--custom-prompt', '-p', help='Custom analysis prompt')
def analyze_video(youtube_url: str, output: str, format: str, custom_prompt: str):
    """Analyze a YouTube video using Gemini AI"""
    
    try:
        reader = YouTubeVideoReader()
        
        # Run async analysis
        analysis = asyncio.run(reader.analyze_video(youtube_url, custom_prompt))
        
        # Format output
        formatted_output = format_analysis(analysis, format)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                f.write(formatted_output)
            click.echo(f"Analysis saved to {output}")
        else:
            click.echo(formatted_output)
            
    except Exception as e:
        click.echo(f"Error: {str(e)}", err=True)
        raise click.Abort()

def format_analysis(analysis: VideoAnalysis, format_type: str) -&amp;amp;amp;amp;gt; str:
    """Format analysis results based on output type"""
    if format_type == 'json':
        import json
        return json.dumps(analysis.__dict__, indent=2, ensure_ascii=False)
    
    elif format_type == 'markdown':
        return f"""# YouTube Video Analysis
        
## Video Information
- **URL**: {analysis.url}
- **Title**: {analysis.title or 'N/A'}
- **Duration**: {analysis.duration or 'N/A'}

## Summary
{analysis.summary}

## Key Topics
{chr(10).join(f"- {topic}" for topic in analysis.key_topics)}

## Key Timestamps
{chr(10).join(f"- **{ts.get('time', 'N/A')}**: {ts.get('description', 'N/A')}" for ts in analysis.timestamps)}

## Content Analysis
- **Content Type**: {analysis.content_type or 'N/A'}
- **Sentiment**: {analysis.sentiment or 'N/A'}
"""
    
    else:  # text format
        return analysis.summary

if __name__ == '__main__':
    analyze_video()
```

### 4. Enhanced Features Module
**File**: `video_utils.py`
```python
from typing import Dict, List, Optional
import re

class VideoMetadataExtractor:
    """Extract additional metadata from YouTube videos"""
    
    @staticmethod
    def extract_video_id(url: str) -&amp;amp;amp;amp;gt; Optional[str]:
        """Extract video ID from YouTube URL"""
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'(?:embed\/)([0-9A-Za-z_-]{11})',
            r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    @staticmethod
    def get_video_info(video_id: str) -&amp;amp;amp;amp;gt; Dict[str, Any]:
        """Get basic video information using YouTube API (optional)"""
        # Implementation would use YouTube Data API v3
        # Return title, duration, description, etc.
        pass

class ResponseParser:
    """Parse Gemini responses into structured data"""
    
    @staticmethod
    def extract_timestamps(text: str) -&amp;amp;amp;amp;gt; List[Dict[str, str]]:
        """Extract timestamps from response text"""
        timestamp_pattern = r'(\d{1,2}:\d{2}(?::\d{2})?)\s*[-:]\s*(.+?)(?=\n|\d{1,2}:\d{2}|$)'
        matches = re.findall(timestamp_pattern, text, re.MULTILINE)
        
        return [{"time": match[0], "description": match[1].strip()} 
                for match in matches]
    
    @staticmethod
    def extract_topics(text: str) -&amp;amp;amp;amp;gt; List[str]:
        """Extract key topics from response"""
        # Look for bullet points or numbered lists
        topic_patterns = [
            r'^\s*[-*]\s*(.+)$',  # Bullet points
            r'^\s*\d+\.\s*(.+)$'  # Numbered lists
        ]
        
        topics = []
        for line in text.split('\n'):
            for pattern in topic_patterns:
                match = re.match(pattern, line.strip())
                if match:
                    topics.append(match.group(1).strip())
        
        return topics[:10]  # Limit to top 10 topics
```

### 5. Configuration and Settings
**File**: `config.py`
```python
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """Application settings with environment variable support"""
    
    # API Configuration
    gemini_api_key: str
    gcp_project_id: Optional[str] = None
    
    # Model Configuration
    model_name: str = "gemini-2.5-pro"
    max_tokens: int = 8192
    temperature: float = 0.3
    
    # Video Processing Settings
    max_video_duration: int = 7200  # 2 hours in seconds
    frame_rate: int = 1  # Frames per second for analysis
    media_resolution: str = "default"  # or "low" for longer videos
    
    # Output Settings
    default_output_format: str = "markdown"
    include_timestamps: bool = True
    include_sentiment: bool = True
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# Global settings instance
settings = Settings()
```

## Key Features

### Core Capabilities
- **Direct YouTube URL Processing**: Uses Gemini's native YouTube URL support
- **Multimodal Analysis**: Processes both visual and audio content
- **Comprehensive Summaries**: Generates detailed content summaries
- **Timestamp Extraction**: Identifies key moments with timestamps
- **Topic Classification**: Extracts main topics and themes
- **Sentiment Analysis**: Determines overall tone and sentiment
- **Multiple Output Formats**: JSON, Markdown, and plain text support

### Advanced Features
- **Custom Prompts**: Allow users to specify custom analysis requirements
- **Batch Processing**: Process multiple videos in sequence
- **Long Video Support**: Handle videos up to 2 hours (2M context) or 6 hours (low resolution)
- **Error Handling**: Robust error handling for API failures and invalid URLs
- **Caching**: Optional caching of analysis results
- **Progress Tracking**: Show progress for long video analysis

## API Requirements

### Google Gemini API
- **Primary API**: Google AI Generative AI Python Client
- **Key**: Gemini API Key from Google AI Studio
- **Models**: Gemini 2.5 Pro (recommended) or Gemini 2.5 Flash (cost-effective)
- **Capabilities**: Native YouTube URL processing, multimodal analysis
- **Cost**: ~$0.075 per 1K input tokens, $0.30 per 1K output tokens (Gemini 2.5 Pro)

### Optional APIs
- **YouTube Data API v3**: For enhanced metadata (title, description, duration)
  - Requires Google Cloud Project and API key
  - 10,000 free quota units per day
  - Used for additional video information beyond what Gemini provides

## Dependencies and Requirements

### Core Dependencies
```python
google-generativeai&amp;amp;amp;amp;gt;=0.8.0     # Gemini API client
python-dotenv&amp;amp;amp;amp;gt;=1.0.0           # Environment management
pydantic&amp;amp;amp;amp;gt;=2.0.0                # Data validation and settings
click&amp;amp;amp;amp;gt;=8.0.0                   # CLI interface
asyncio                        # Built-in async support
```

### Optional Dependencies
```python
google-api-python-client&amp;amp;amp;amp;gt;=2.0  # YouTube Data API (optional)
pytube&amp;amp;amp;amp;gt;=15.0.0                 # YouTube metadata extraction (alternative)
rich&amp;amp;amp;amp;gt;=13.0.0                   # Enhanced CLI output formatting
```

### System Requirements
- **Python**: 3.8+ (recommended 3.11+)
- **Internet Connection**: Required for API calls
- **API Key**: Google AI Studio API key
- **Memory**: Minimal (video processing is handled by Gemini)

## Testing Considerations

### Test Scenarios
1. **Valid YouTube URLs**: Test various URL formats (youtube.com, youtu.be, embed)
2. **Invalid URLs**: Test error handling for malformed URLs
3. **Different Video Types**: Educational, entertainment, news, tutorials
4. **Video Lengths**: Short (&amp;amp;amp;amp;lt; 5 min), medium (5-30 min), long (&amp;amp;amp;amp;gt; 30 min)
5. **Different Content**: Various languages, with/without subtitles
6. **API Failures**: Network errors, rate limiting, invalid API keys

### Edge Cases
- Private or unlisted videos
- Age-restricted content
- Live streams vs recorded videos
- Videos with no audio or minimal visual content
- Very long videos (approaching context limits)

### Performance Testing
- Response time for different video lengths
- Token usage and cost estimation
- Concurrent request handling
- Memory usage during processing

## Example Usage

### Basic Usage
```bash
# Analyze a YouTube video
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID"

# Save to file in markdown format
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -o analysis.md -f markdown

# Use custom prompt
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -p "Focus on technical concepts and code examples"

# JSON output for programmatic use
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -f json -o results.json
```

### Programmatic Usage
```python
from youtube_reader import YouTubeVideoReader

# Initialize reader
reader = YouTubeVideoReader()

# Analyze video
analysis = await reader.analyze_video("https://www.youtube.com/watch?v=VIDEO_ID")

# Access results
print(f"Summary: {analysis.summary}")
print(f"Key Topics: {analysis.key_topics}")
print(f"Timestamps: {analysis.timestamps}")
```

## Implementation Timeline

### Phase 1 (Week 1): Core Functionality
- ✅ Set up project structure and dependencies
- ✅ Implement basic YouTubeVideoReader class
- ✅ Create CLI interface with Click
- ✅ Test with simple video analysis

### Phase 2 (Week 2): Enhanced Features
- ✅ Add response parsing and structured output
- ✅ Implement multiple output formats
- ✅ Add error handling and validation
- ✅ Create configuration system

### Phase 3 (Week 3): Polish and Testing
- ✅ Comprehensive testing suite
- ✅ Performance optimization
- ✅ Documentation and examples
- ✅ Optional YouTube Data API integration

## Security and Best Practices

### API Key Management
- Store API keys in environment variables
- Use `.env` files for local development
- Never commit API keys to version control
- Consider using Google Cloud Secret Manager for production

### Rate Limiting
- Implement exponential backoff for API calls
- Monitor usage against quotas
- Add request throttling for batch processing

### Error Handling
- Graceful handling of API failures
- User-friendly error messages
- Logging for debugging and monitoring
- Retry logic for transient failures

This plan provides a comprehensive approach to building a robust YouTube video reader using Gemini's advanced multimodal capabilities, with proper error handling, multiple output formats, and extensible architecture for future enhancements.&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/drafts/fastapi_mcp_discovery.md"&amp;amp;amp;gt;# FastAPI with MCP Tool Discovery Endpoint

This approach creates a FastAPI HTTP server that exposes its own MCP schema and provides both direct HTTP endpoints and MCP-compatible tool calling.

## Architecture

- Single FastAPI server with dual interfaces
- HTTP endpoints for direct API calls
- MCP tool discovery endpoint (`/mcp/tools`)
- Generic MCP tool caller endpoint (`/mcp/call/{tool_name}`)
- JavaScript client that can discover and call tools

## Python Server Implementation

```python
# discoverable_api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
import json

app = FastAPI(
    title="MCP Discoverable API",
    description="FastAPI server with MCP tool discovery",
    version="1.0.0"
)

# Data models
class ToolArguments(BaseModel):
    arguments: Dict[str, Any]

class CalculateRequest(BaseModel):
    expression: str

class ProcessTextRequest(BaseModel):
    text: str
    operation: str = "uppercase"

# MCP tool definitions
MCP_TOOLS = [
    {
        "name": "calculate",
        "description": "Perform mathematical calculations",
        "inputSchema": {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string", 
                    "description": "Mathematical expression to evaluate"
                }
            },
            "required": ["expression"]
        }
    },
    {
        "name": "process_text",
        "description": "Process text with various operations",
        "inputSchema": {
            "type": "object",
            "properties": {
                "text": {
                    "type": "string",
                    "description": "Text to process"
                },
                "operation": {
                    "type": "string",
                    "enum": ["uppercase", "lowercase", "reverse", "word_count"],
                    "description": "Operation to perform on the text"
                }
            },
            "required": ["text"]
        }
    },
    {
        "name": "get_server_info",
        "description": "Get information about the server",
        "inputSchema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    }
]

# Core business logic functions
async def calculate_expression(expression: str) -&amp;amp;amp;amp;gt; Dict[str, Any]:
    """Safely evaluate mathematical expressions"""
    try:
        # In production, use a safe math parser instead of eval
        # For demo purposes only
        allowed_chars = set("0123456789+-*/.() ")
        if not all(c in allowed_chars for c in expression):
            raise ValueError("Invalid characters in expression")
        
        result = eval(expression)
        return {
            "result": result,
            "expression": expression,
            "status": "success"
        }
    except Exception as e:
        return {
            "error": str(e),
            "expression": expression,
            "status": "error"
        }

async def process_text_operation(text: str, operation: str = "uppercase") -&amp;amp;amp;amp;gt; Dict[str, Any]:
    """Process text with various operations"""
    try:
        if operation == "uppercase":
            result = text.upper()
        elif operation == "lowercase":
            result = text.lower()
        elif operation == "reverse":
            result = text[::-1]
        elif operation == "word_count":
            result = len(text.split())
        else:
            raise ValueError(f"Unknown operation: {operation}")
        
        return {
            "result": result,
            "original_text": text,
            "operation": operation,
            "status": "success"
        }
    except Exception as e:
        return {
            "error": str(e),
            "operation": operation,
            "status": "error"
        }

async def get_server_information() -&amp;amp;amp;amp;gt; Dict[str, Any]:
    """Get server information"""
    return {
        "name": "MCP Discoverable API",
        "version": "1.0.0",
        "available_tools": len(MCP_TOOLS),
        "endpoints": {
            "mcp_tools": "/mcp/tools",
            "mcp_call": "/mcp/call/{tool_name}",
            "direct_calculate": "/api/calculate",
            "direct_process": "/api/process",
            "docs": "/docs"
        },
        "status": "healthy"
    }

# MCP Discovery Endpoints
@app.get("/mcp/tools")
async def list_mcp_tools():
    """Expose available tools in MCP format"""
    return {
        "tools": MCP_TOOLS,
        "server_info": {
            "name": "MCP Discoverable API",
            "version": "1.0.0"
        }
    }

@app.post("/mcp/call/{tool_name}")
async def call_mcp_tool(tool_name: str, request: ToolArguments):
    """Generic MCP tool caller"""
    arguments = request.arguments
    
    if tool_name == "calculate":
        expression = arguments.get("expression")
        if not expression:
            raise HTTPException(status_code=400, detail="Missing required argument: expression")
        return await calculate_expression(expression)
    
    elif tool_name == "process_text":
        text = arguments.get("text")
        if not text:
            raise HTTPException(status_code=400, detail="Missing required argument: text")
        operation = arguments.get("operation", "uppercase")
        return await process_text_operation(text, operation)
    
    elif tool_name == "get_server_info":
        return await get_server_information()
    
    else:
        raise HTTPException(status_code=404, detail=f"Unknown tool: {tool_name}")

# Direct HTTP API Endpoints
@app.post("/api/calculate")
async def calculate_endpoint(request: CalculateRequest):
    """Direct HTTP endpoint for calculations"""
    return await calculate_expression(request.expression)

@app.post("/api/process")
async def process_text_endpoint(request: ProcessTextRequest):
    """Direct HTTP endpoint for text processing"""
    return await process_text_operation(request.text, request.operation)

@app.get("/api/info")
async def server_info_endpoint():
    """Direct HTTP endpoint for server info"""
    return await get_server_information()

# Health check
@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "Server is running"}

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "MCP Discoverable API",
        "documentation": "/docs",
        "mcp_tools": "/mcp/tools",
        "health": "/health"
    }

if __name__ == "__main__":
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info"
    )
```

## JavaScript Client Implementation

```javascript
// mcp_http_client.js

class HTTPMCPClient {
    constructor(baseUrl = 'http://localhost:8000') {
        this.baseUrl = baseUrl;
        this.tools = null;
    }
    
    /**
     * Discover available MCP tools from the server
     */
    async discoverTools() {
        try {
            const response = await fetch(`${this.baseUrl}/mcp/tools`);
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            const data = await response.json();
            this.tools = data.tools;
            return data;
        } catch (error) {
            console.error('Error discovering tools:', error);
            throw error;
        }
    }
    
    /**
     * Get information about a specific tool
     */
    getToolInfo(toolName) {
        if (!this.tools) {
            throw new Error('Tools not discovered yet. Call discoverTools() first.');
        }
        return this.tools.find(tool =&amp;amp;amp;amp;gt; tool.name === toolName);
    }
    
    /**
     * List all available tools
     */
    listTools() {
        if (!this.tools) {
            throw new Error('Tools not discovered yet. Call discoverTools() first.');
        }
        return this.tools.map(tool =&amp;amp;amp;amp;gt; ({
            name: tool.name,
            description: tool.description
        }));
    }
    
    /**
     * Call an MCP tool with arguments
     */
    async callTool(toolName, arguments = {}) {
        try {
            const response = await fetch(`${this.baseUrl}/mcp/call/${toolName}`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ arguments })
            });
            
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            return await response.json();
        } catch (error) {
            console.error(`Error calling tool ${toolName}:`, error);
            throw error;
        }
    }
    
    /**
     * Validate arguments against tool schema
     */
    validateArguments(toolName, arguments) {
        const tool = this.getToolInfo(toolName);
        if (!tool) {
            throw new Error(`Tool ${toolName} not found`);
        }
        
        const schema = tool.inputSchema;
        const required = schema.required || [];
        
        // Check required arguments
        for (const field of required) {
            if (!(field in arguments)) {
                throw new Error(`Missing required argument: ${field}`);
            }
        }
        
        return true;
    }
    
    /**
     * Call tool with validation
     */
    async callToolSafe(toolName, arguments = {}) {
        this.validateArguments(toolName, arguments);
        return await this.callTool(toolName, arguments);
    }
}

// Usage examples
async function demonstrateUsage() {
    const client = new HTTPMCPClient();
    
    try {
        // Discover available tools
        console.log('Discovering tools...');
        const discovery = await client.discoverTools();
        console.log('Server info:', discovery.server_info);
        console.log('Available tools:', client.listTools());
        
        // Call calculator tool
        console.log('\nCalling calculator...');
        const calcResult = await client.callTool('calculate', {
            expression: '2 + 2 * 3'
        });
        console.log('Calculation result:', calcResult);
        
        // Call text processor tool
        console.log('\nProcessing text...');
        const textResult = await client.callTool('process_text', {
            text: 'Hello World',
            operation: 'reverse'
        });
        console.log('Text processing result:', textResult);
        
        // Get server info
        console.log('\nGetting server info...');
        const serverInfo = await client.callTool('get_server_info', {});
        console.log('Server info:', serverInfo);
        
    } catch (error) {
        console.error('Demo failed:', error);
    }
}

// Browser usage example
function createBrowserExample() {
    return `
    &amp;amp;amp;amp;lt;html&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;head&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;title&amp;amp;amp;amp;gt;MCP HTTP Client Demo&amp;amp;amp;amp;lt;/title&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/head&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;body&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;h1&amp;amp;amp;amp;gt;MCP HTTP Client Demo&amp;amp;amp;amp;lt;/h1&amp;amp;amp;amp;gt;
        &amp;amp;amp;amp;lt;div id="output"&amp;amp;amp;amp;gt;&amp;amp;amp;amp;lt;/div&amp;amp;amp;amp;gt;
        
        &amp;amp;amp;amp;lt;script&amp;amp;amp;amp;gt;
            ${HTTPMCPClient.toString()}
            
            async function runDemo() {
                const client = new HTTPMCPClient();
                const output = document.getElementById('output');
                
                try {
                    await client.discoverTools();
                    output.innerHTML += '&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Tools discovered!&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;';
                    
                    const result = await client.callTool('calculate', {
                        expression: '10 * 5'
                    });
                    output.innerHTML += '&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Calculation: ' + JSON.stringify(result) + '&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;';
                    
                } catch (error) {
                    output.innerHTML += '&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;Error: ' + error.message + '&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;';
                }
            }
            
            runDemo();
        &amp;amp;amp;amp;lt;/script&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/body&amp;amp;amp;amp;gt;
    &amp;amp;amp;amp;lt;/html&amp;amp;amp;amp;gt;
    `;
}

// Node.js usage
if (typeof module !== 'undefined' &amp;amp;amp;amp;amp;&amp;amp;amp;amp;amp; module.exports) {
    module.exports = HTTPMCPClient;
}

// If running in Node.js, demonstrate usage
if (typeof window === 'undefined') {
    demonstrateUsage();
}
```

## Usage Instructions

### 1. Start the Python Server

```bash
# Install dependencies
pip install fastapi uvicorn

# Run the server
python discoverable_api.py

# Server will be available at:
# - API docs: http://localhost:8000/docs
# - MCP tools: http://localhost:8000/mcp/tools
# - Health check: http://localhost:8000/health
```

### 2. Use the JavaScript Client

```javascript
// Node.js example
const HTTPMCPClient = require('./mcp_http_client.js');

const client = new HTTPMCPClient('http://localhost:8000');

// Discover and use tools
await client.discoverTools();
const result = await client.callTool('calculate', { expression: '2 + 2' });
console.log(result); // { result: 4, expression: '2 + 2', status: 'success' }
```

### 3. Direct HTTP API Usage

```bash
# Direct calculation
curl -X POST "http://localhost:8000/api/calculate" \
     -H "Content-Type: application/json" \
     -d '{"expression": "5 * 5"}'

# Direct text processing
curl -X POST "http://localhost:8000/api/process" \
     -H "Content-Type: application/json" \
     -d '{"text": "hello", "operation": "uppercase"}'
```

### 4. MCP Tool Discovery

```bash
# Discover available tools
curl http://localhost:8000/mcp/tools

# Call a tool via MCP interface
curl -X POST "http://localhost:8000/mcp/call/calculate" \
     -H "Content-Type: application/json" \
     -d '{"arguments": {"expression": "3 + 4"}}'
```

## Key Features

1. **Dual Interface**: Both direct HTTP endpoints and MCP-compatible tool calling
2. **Self-Documenting**: MCP tools are discoverable via `/mcp/tools`
3. **Validation**: Argument validation against MCP schemas
4. **Error Handling**: Consistent error responses across both interfaces
5. **Auto Documentation**: FastAPI generates OpenAPI docs at `/docs`
6. **Type Safety**: Pydantic models for request/response validation

## Extension Points

- Add authentication/authorization
- Implement rate limiting
- Add more complex tools
- Integrate with databases
- Add WebSocket support for real-time tools
- Create tool composition/chaining capabilities&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/drafts/gemini_youtube_reader.md"&amp;amp;amp;gt;# Plan: YouTube Video Reader with Gemini Analysis

## Overview
Build a Python script that takes a YouTube video URL and uses Google's Gemini AI to analyze the entire video content and produce a comprehensive summary. The system will leverage Gemini's advanced multimodal capabilities to process both visual and audio elements of YouTube videos.

## Implementation Steps

### 1. Environment Setup and Dependencies
**File**: `requirements.txt` or `pyproject.toml`
```python
# Key dependencies needed:
google-generativeai&amp;amp;amp;amp;gt;=0.8.0    # Gemini API client
python-dotenv&amp;amp;amp;amp;gt;=1.0.0          # Environment variable management
pytube&amp;amp;amp;amp;gt;=15.0.0                # YouTube video download (if needed)
requests&amp;amp;amp;amp;gt;=2.31.0              # HTTP requests
pydantic&amp;amp;amp;amp;gt;=2.0.0               # Data validation
click&amp;amp;amp;amp;gt;=8.0.0                  # CLI interface
```

**Environment Variables** (`.env` file):
```bash
GEMINI_API_KEY=your_gemini_api_key_here
GCP_PROJECT_ID=your_project_id  # If using Vertex AI
```

### 2. Core YouTube Video Reader Class
**File**: `youtube_reader.py`
```python
import os
import re
from typing import Optional, Dict, Any
from dataclasses import dataclass
from google import generativeai as genai
from dotenv import load_dotenv

@dataclass
class VideoAnalysis:
    """Data structure for video analysis results"""
    url: str
    title: Optional[str]
    duration: Optional[str]
    summary: str
    key_topics: list[str]
    timestamps: list[Dict[str, str]]
    sentiment: Optional[str]
    content_type: Optional[str]

class YouTubeVideoReader:
    def __init__(self, api_key: Optional[str] = None):
        """Initialize with Gemini API key"""
        load_dotenv()
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY must be provided")
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-2.5-pro')  # Use latest model with video capabilities
    
    def validate_youtube_url(self, url: str) -&amp;amp;amp;amp;gt; bool:
        """Validate YouTube URL format"""
        youtube_patterns = [
            r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([^&amp;amp;amp;amp;amp;\n?#]*)',
            r'(?:https?://)?(?:www\.)?youtu\.be/([^&amp;amp;amp;amp;amp;\n?#]*)',
            r'(?:https?://)?(?:www\.)?youtube\.com/embed/([^&amp;amp;amp;amp;amp;\n?#]*)'
        ]
        return any(re.match(pattern, url) for pattern in youtube_patterns)
    
    async def analyze_video(self, youtube_url: str, custom_prompt: Optional[str] = None) -&amp;amp;amp;amp;gt; VideoAnalysis:
        """
        Analyze YouTube video using Gemini's multimodal capabilities
        
        Args:
            youtube_url: Valid YouTube video URL
            custom_prompt: Optional custom analysis prompt
            
        Returns:
            VideoAnalysis object with comprehensive results
        """
        if not self.validate_youtube_url(youtube_url):
            raise ValueError("Invalid YouTube URL provided")
        
        # Default comprehensive analysis prompt
        default_prompt = """
        Please analyze this YouTube video comprehensively and provide:
        
        1. **Video Summary**: A detailed 3-4 paragraph summary of the main content
        2. **Key Topics**: List the 5-10 most important topics discussed
        3. **Timestamps**: Identify 8-12 key moments with timestamps and descriptions
        4. **Content Type**: Classify the video (educational, entertainment, news, tutorial, etc.)
        5. **Sentiment**: Overall tone and sentiment of the content
        6. **Main Takeaways**: 3-5 key insights or actionable points
        
        Focus on both visual and audio elements. Pay attention to:
        - Spoken content and dialogue
        - Visual elements, graphics, and text shown
        - Scene changes and transitions
        - Background music or sounds that add context
        
        Format your response in a structured way with clear sections.
        """
        
        prompt = custom_prompt or default_prompt
        
        try:
            # Generate content using YouTube URL directly
            response = await self.model.generate_content_async([
                prompt,
                {"mime_type": "video/*", "uri": youtube_url}
            ])
            
            # Parse response into structured format
            analysis_result = self._parse_analysis_response(
                response.text, 
                youtube_url
            )
            
            return analysis_result
            
        except Exception as e:
            raise RuntimeError(f"Failed to analyze video: {str(e)}")
    
    def _parse_analysis_response(self, response_text: str, url: str) -&amp;amp;amp;amp;gt; VideoAnalysis:
        """Parse Gemini response into structured VideoAnalysis object"""
        # This would contain logic to extract structured data from the response
        # For now, return basic structure - implement parsing based on response format
        
        return VideoAnalysis(
            url=url,
            title=None,  # Could extract from video metadata
            duration=None,  # Could extract from video metadata  
            summary=response_text,  # Full response for now
            key_topics=[],  # Parse from response
            timestamps=[],  # Parse from response
            sentiment=None,  # Parse from response
            content_type=None  # Parse from response
        )
```

### 3. CLI Interface
**File**: `cli.py`
```python
import click
import asyncio
from youtube_reader import YouTubeVideoReader, VideoAnalysis

@click.command()
@click.argument('youtube_url')
@click.option('--output', '-o', help='Output file path (optional)')
@click.option('--format', '-f', type=click.Choice(['json', 'markdown', 'text']), 
              default='markdown', help='Output format')
@click.option('--custom-prompt', '-p', help='Custom analysis prompt')
def analyze_video(youtube_url: str, output: str, format: str, custom_prompt: str):
    """Analyze a YouTube video using Gemini AI"""
    
    try:
        reader = YouTubeVideoReader()
        
        # Run async analysis
        analysis = asyncio.run(reader.analyze_video(youtube_url, custom_prompt))
        
        # Format output
        formatted_output = format_analysis(analysis, format)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                f.write(formatted_output)
            click.echo(f"Analysis saved to {output}")
        else:
            click.echo(formatted_output)
            
    except Exception as e:
        click.echo(f"Error: {str(e)}", err=True)
        raise click.Abort()

def format_analysis(analysis: VideoAnalysis, format_type: str) -&amp;amp;amp;amp;gt; str:
    """Format analysis results based on output type"""
    if format_type == 'json':
        import json
        return json.dumps(analysis.__dict__, indent=2, ensure_ascii=False)
    
    elif format_type == 'markdown':
        return f"""# YouTube Video Analysis
        
## Video Information
- **URL**: {analysis.url}
- **Title**: {analysis.title or 'N/A'}
- **Duration**: {analysis.duration or 'N/A'}

## Summary
{analysis.summary}

## Key Topics
{chr(10).join(f"- {topic}" for topic in analysis.key_topics)}

## Key Timestamps
{chr(10).join(f"- **{ts.get('time', 'N/A')}**: {ts.get('description', 'N/A')}" for ts in analysis.timestamps)}

## Content Analysis
- **Content Type**: {analysis.content_type or 'N/A'}
- **Sentiment**: {analysis.sentiment or 'N/A'}
"""
    
    else:  # text format
        return analysis.summary

if __name__ == '__main__':
    analyze_video()
```

### 4. Enhanced Features Module
**File**: `video_utils.py`
```python
from typing import Dict, List, Optional
import re

class VideoMetadataExtractor:
    """Extract additional metadata from YouTube videos"""
    
    @staticmethod
    def extract_video_id(url: str) -&amp;amp;amp;amp;gt; Optional[str]:
        """Extract video ID from YouTube URL"""
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'(?:embed\/)([0-9A-Za-z_-]{11})',
            r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    @staticmethod
    def get_video_info(video_id: str) -&amp;amp;amp;amp;gt; Dict[str, Any]:
        """Get basic video information using YouTube API (optional)"""
        # Implementation would use YouTube Data API v3
        # Return title, duration, description, etc.
        pass

class ResponseParser:
    """Parse Gemini responses into structured data"""
    
    @staticmethod
    def extract_timestamps(text: str) -&amp;amp;amp;amp;gt; List[Dict[str, str]]:
        """Extract timestamps from response text"""
        timestamp_pattern = r'(\d{1,2}:\d{2}(?::\d{2})?)\s*[-:]\s*(.+?)(?=\n|\d{1,2}:\d{2}|$)'
        matches = re.findall(timestamp_pattern, text, re.MULTILINE)
        
        return [{"time": match[0], "description": match[1].strip()} 
                for match in matches]
    
    @staticmethod
    def extract_topics(text: str) -&amp;amp;amp;amp;gt; List[str]:
        """Extract key topics from response"""
        # Look for bullet points or numbered lists
        topic_patterns = [
            r'^\s*[-*]\s*(.+)$',  # Bullet points
            r'^\s*\d+\.\s*(.+)$'  # Numbered lists
        ]
        
        topics = []
        for line in text.split('\n'):
            for pattern in topic_patterns:
                match = re.match(pattern, line.strip())
                if match:
                    topics.append(match.group(1).strip())
        
        return topics[:10]  # Limit to top 10 topics
```

### 5. Configuration and Settings
**File**: `config.py`
```python
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """Application settings with environment variable support"""
    
    # API Configuration
    gemini_api_key: str
    gcp_project_id: Optional[str] = None
    
    # Model Configuration
    model_name: str = "gemini-2.5-pro"
    max_tokens: int = 8192
    temperature: float = 0.3
    
    # Video Processing Settings
    max_video_duration: int = 7200  # 2 hours in seconds
    frame_rate: int = 1  # Frames per second for analysis
    media_resolution: str = "default"  # or "low" for longer videos
    
    # Output Settings
    default_output_format: str = "markdown"
    include_timestamps: bool = True
    include_sentiment: bool = True
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# Global settings instance
settings = Settings()
```

## Key Features

### Core Capabilities
- **Direct YouTube URL Processing**: Uses Gemini's native YouTube URL support
- **Multimodal Analysis**: Processes both visual and audio content
- **Comprehensive Summaries**: Generates detailed content summaries
- **Timestamp Extraction**: Identifies key moments with timestamps
- **Topic Classification**: Extracts main topics and themes
- **Sentiment Analysis**: Determines overall tone and sentiment
- **Multiple Output Formats**: JSON, Markdown, and plain text support

### Advanced Features
- **Custom Prompts**: Allow users to specify custom analysis requirements
- **Batch Processing**: Process multiple videos in sequence
- **Long Video Support**: Handle videos up to 2 hours (2M context) or 6 hours (low resolution)
- **Error Handling**: Robust error handling for API failures and invalid URLs
- **Caching**: Optional caching of analysis results
- **Progress Tracking**: Show progress for long video analysis

## API Requirements

### Google Gemini API
- **Primary API**: Google AI Generative AI Python Client
- **Key**: Gemini API Key from Google AI Studio
- **Models**: Gemini 2.5 Pro (recommended) or Gemini 2.5 Flash (cost-effective)
- **Capabilities**: Native YouTube URL processing, multimodal analysis
- **Cost**: ~$0.075 per 1K input tokens, $0.30 per 1K output tokens (Gemini 2.5 Pro)

### Optional APIs
- **YouTube Data API v3**: For enhanced metadata (title, description, duration)
  - Requires Google Cloud Project and API key
  - 10,000 free quota units per day
  - Used for additional video information beyond what Gemini provides

## Dependencies and Requirements

### Core Dependencies
```python
google-generativeai&amp;amp;amp;amp;gt;=0.8.0     # Gemini API client
python-dotenv&amp;amp;amp;amp;gt;=1.0.0           # Environment management
pydantic&amp;amp;amp;amp;gt;=2.0.0                # Data validation and settings
click&amp;amp;amp;amp;gt;=8.0.0                   # CLI interface
asyncio                        # Built-in async support
```

### Optional Dependencies
```python
google-api-python-client&amp;amp;amp;amp;gt;=2.0  # YouTube Data API (optional)
pytube&amp;amp;amp;amp;gt;=15.0.0                 # YouTube metadata extraction (alternative)
rich&amp;amp;amp;amp;gt;=13.0.0                   # Enhanced CLI output formatting
```

### System Requirements
- **Python**: 3.8+ (recommended 3.11+)
- **Internet Connection**: Required for API calls
- **API Key**: Google AI Studio API key
- **Memory**: Minimal (video processing is handled by Gemini)

## Testing Considerations

### Test Scenarios
1. **Valid YouTube URLs**: Test various URL formats (youtube.com, youtu.be, embed)
2. **Invalid URLs**: Test error handling for malformed URLs
3. **Different Video Types**: Educational, entertainment, news, tutorials
4. **Video Lengths**: Short (&amp;amp;amp;amp;lt; 5 min), medium (5-30 min), long (&amp;amp;amp;amp;gt; 30 min)
5. **Different Content**: Various languages, with/without subtitles
6. **API Failures**: Network errors, rate limiting, invalid API keys

### Edge Cases
- Private or unlisted videos
- Age-restricted content
- Live streams vs recorded videos
- Videos with no audio or minimal visual content
- Very long videos (approaching context limits)

### Performance Testing
- Response time for different video lengths
- Token usage and cost estimation
- Concurrent request handling
- Memory usage during processing

## Example Usage

### Basic Usage
```bash
# Analyze a YouTube video
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID"

# Save to file in markdown format
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -o analysis.md -f markdown

# Use custom prompt
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -p "Focus on technical concepts and code examples"

# JSON output for programmatic use
python cli.py "https://www.youtube.com/watch?v=VIDEO_ID" -f json -o results.json
```

### Programmatic Usage
```python
from youtube_reader import YouTubeVideoReader

# Initialize reader
reader = YouTubeVideoReader()

# Analyze video
analysis = await reader.analyze_video("https://www.youtube.com/watch?v=VIDEO_ID")

# Access results
print(f"Summary: {analysis.summary}")
print(f"Key Topics: {analysis.key_topics}")
print(f"Timestamps: {analysis.timestamps}")
```

## Implementation Timeline

### Phase 1 (Week 1): Core Functionality
- ✅ Set up project structure and dependencies
- ✅ Implement basic YouTubeVideoReader class
- ✅ Create CLI interface with Click
- ✅ Test with simple video analysis

### Phase 2 (Week 2): Enhanced Features
- ✅ Add response parsing and structured output
- ✅ Implement multiple output formats
- ✅ Add error handling and validation
- ✅ Create configuration system

### Phase 3 (Week 3): Polish and Testing
- ✅ Comprehensive testing suite
- ✅ Performance optimization
- ✅ Documentation and examples
- ✅ Optional YouTube Data API integration

## Security and Best Practices

### API Key Management
- Store API keys in environment variables
- Use `.env` files for local development
- Never commit API keys to version control
- Consider using Google Cloud Secret Manager for production

### Rate Limiting
- Implement exponential backoff for API calls
- Monitor usage against quotas
- Add request throttling for batch processing

### Error Handling
- Graceful handling of API failures
- User-friendly error messages
- Logging for debugging and monitoring
- Retry logic for transient failures

This plan provides a comprehensive approach to building a robust YouTube video reader using Gemini's advanced multimodal capabilities, with proper error handling, multiple output formats, and extensible architecture for future enhancements.
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/approved/add_improve_prompt.md"&amp;amp;amp;gt;# Plan: Add Improve Prompt MCP Tool

## Overview
Create a new MCP tool in `collect.py` that leverages Anthropic's experimental prompt improvement API to enhance existing prompts based on user feedback.

## Implementation Steps

### 1. Add the improve_prompt MCP tool
Location: After line 574 in collect.py

**Function Signature:**
```python
@mcp.tool()
async def improve_prompt(
    messages: str,
    system: str = "",
    feedback: str,
    target_model: str = None
) -&amp;amp;amp;amp;gt; str:
```

**Parameters:**
- `messages`: The prompt text to improve (as a simple string)
- `system`: System prompt (optional, defaults to empty string)
- `feedback`: Improvement instructions (required) - describes how to improve the prompt
- `target_model`: Target model for optimization (optional)

**Returns:** 
- Formatted string containing the improved prompt

### 2. Tool Structure

```python
# Pseudo-code structure
def improve_prompt(...):
    # 1. Validate input parameters
    #    - Ensure messages is not empty
    #    - Validate feedback is not empty
    
    # 2. Convert string message to API format
    formatted_messages = [{
        "role": "user",
        "content": [{"type": "text", "text": messages.strip()}]
    }]
    
    # 3. Create AnthropicMCP client instance
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)
    
    # 4. Format data dict matching API requirements
    data = {
        "messages": formatted_messages,
        "system": system,
        "feedback": feedback.strip(),
    }
    if target_model:
        data["target_model"] = target_model
    
    # 5. Call anthropic_mcp.improve_prompt(data)
    response = anthropic_mcp.improve_prompt(data)
    
    # 6. Extract and return improved prompt as string
```

### 3. Error Handling

- **Input Validation:**
  - Check messages is not empty string
  - Ensure feedback is provided and not empty
  - Validate messages is a string type

- **API Errors:**
  - Catch `RuntimeError` from API failures
  - Catch `ValueError` for missing configuration
  - Provide meaningful error messages

### 4. Documentation

**Docstring Template:**
```python
"""
Improve an existing prompt using Anthropic's experimental prompt improvement API.

This tool analyzes an existing conversation prompt and enhances it based on 
provided feedback. It's useful for making prompts more specific, clear, or 
optimized for particular use cases.

Use this tool when you need to:
- Make prompts more specific or detailed
- Adapt prompts for different skill levels
- Optimize prompts for particular models
- Clarify ambiguous instructions
- Add missing context or constraints

Args:
    messages: The prompt text to improve as a simple string.
              Example: "Tell me about Python"
    system: Optional system prompt to improve (default: "")
    feedback: Instructions on how to improve the prompt.
              Examples:
              - "Make this more specific for beginners"
              - "Add more technical detail"
              - "Focus on security best practices"
    target_model: Optional model to optimize for (e.g., "claude-3-opus")

Returns:
    A formatted string containing the improved prompt

Example:
    &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; result = await improve_prompt(
    ...     messages="Tell me about Python",
    ...     system="You are a helpful assistant",
    ...     feedback="Make this more specific for a beginner learning web development"
    ... )
    &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; print(result)
    "I'm a beginner learning web development. Can you explain Python to me..."

Note:
    This uses Anthropic's experimental "prompt-tools" API which requires special
    access. The API is in closed research preview and may change without notice.
"""
```

## Key Features

1. **Simple string input** - Just pass your prompt as a plain string
2. **Flexible improvement via feedback** - User specifies exactly how to improve the prompt
3. **Returns improved prompt as string** - Easy to use result
4. **Model-specific optimization** - Can target improvements for specific Claude models
5. **Automatic formatting** - Handles API message structure internally

## Testing Considerations

- Test with various prompt strings (short, long, multiline)
- Verify feedback parameter validation
- Test error handling for empty strings
- Ensure proper formatting of output string
- Test with and without system prompts
- Verify target_model parameter functionality

## Example Usage in Practice

```python
# Improving a code review prompt
improved = await improve_prompt(
    messages="Review this Python code",
    system="You are a code reviewer",
    feedback="Make this more specific for security-focused code review with clear checklist items"
)

# Improving a tutorial prompt
improved = await improve_prompt(
    messages="Explain how to use async/await in JavaScript",
    feedback="Make this more beginner-friendly with practical examples"
)

# Improving a data analysis prompt
improved = await improve_prompt(
    messages="Analyze this dataset and find patterns",
    system="You are a data scientist",
    feedback="Add specific statistical methods and visualization requirements",
    target_model="claude-3-opus"
)
```
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="_docs/plans/approved/add_templatize_prompt.md"&amp;amp;amp;gt;# Plan: Add Templatize Prompt MCP Tool

## Overview
Create a new MCP tool in `collect.py` that leverages Anthropic's experimental prompt templatization API to convert specific prompts into reusable templates with variable placeholders.

## Implementation Steps

### 1. Add the templatize_prompt MCP tool
Location: After the generate_prompt tool (around line 575 in collect.py)

**Function Signature:**
```python
@mcp.tool()
async def templatize_prompt(
    messages: str,
    system: str = "",
    instructions: str = "",
    target_model: str = None
) -&amp;amp;amp;amp;gt; str:
```

**Parameters:**
- `messages`: A string containing the prompt text to templatize
  - Will be automatically formatted into the required message structure
  - Treated as a user message in the conversation
- `system`: System prompt to templatize (optional, defaults to empty string)
- `instructions`: Optional guidance for the templatization process
- `target_model`: Target model for optimization (optional)

**Returns:**
- Formatted string containing:
  - Templatized messages with variable placeholders
  - Templatized system prompt (if provided)
  - Extracted variable values
  - Usage statistics

### 2. Tool Structure

```python
# Implementation structure
def templatize_prompt(...):
    # 1. Validate input parameters
    #    - Ensure messages string is not empty
    #    - Strip whitespace from messages
    
    # 2. Convert string messages to required format
    formatted_messages = [
        {
            "role": "user",
            "content": [{"type": "text", "text": messages.strip()}]
        }
    ]
    
    # 3. Create AnthropicMCP client instance
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    anthropic_mcp = AnthropicMCP(config, secret_mgr, config.anthropic_model_sonnet)
    
    # 4. Format data dict matching API requirements
    data = {
        "messages": formatted_messages,
        "system": system,
    }
    if instructions:
        data["instructions"] = instructions.strip()
    if target_model:
        data["target_model"] = target_model
    
    # 5. Call anthropic_mcp.templatize_prompt(data)
    response = anthropic_mcp.templatize_prompt(data)
    
    # 6. Format and return templatized output
    # Include templates, extracted variables, and usage stats
```

### 3. Error Handling

- **Input Validation:**
  - Check messages string is not empty
  - Strip and validate the prompt text
  - Ensure proper string formatting

- **API Errors:**
  - Catch `RuntimeError` from API failures
  - Catch `ValueError` for missing configuration
  - Provide meaningful error messages

### 4. Documentation

**Docstring:**
```python
"""
Convert a specific prompt into a reusable template using Anthropic's experimental templatization API.

This tool analyzes an existing prompt and automatically identifies variable parts,
replacing them with template placeholders. It's useful for creating reusable
prompt templates from specific examples.

Use this tool when you need to:
- Create reusable templates from specific prompts
- Build prompt libraries from existing conversations
- Standardize similar prompts with variable content
- Generate templates for repetitive tasks
- Extract patterns from existing prompts

Args:
    messages: A string containing the prompt text to templatize.
              This will be automatically formatted as a user message.
              Example: "Translate hello to German"
    system: Optional system prompt to templatize (default: "")
    instructions: Optional guidance for how to templatize the prompt.
                  Examples:
                  - "Focus on making location and product variables"
                  - "Extract technical terms as variables"
                  - "Keep formatting instructions fixed"
    target_model: Optional model to optimize for (e.g., "claude-3-opus")

Returns:
    A formatted string containing:
    - The templatized messages with {{VARIABLE}} placeholders
    - The templatized system prompt (if provided)
    - Extracted variable values
    - Usage statistics

Example:
    &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; result = await templatize_prompt(
    ...     messages="Translate hello to German",
    ...     system="You are a professional English to German translator",
    ...     instructions="Make the word to translate and target language variables"
    ... )
    &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; print(result)
    "Template: Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}
     Variables: WORD_TO_TRANSLATE='hello', TARGET_LANGUAGE='German'"

Note:
    This uses Anthropic's experimental "prompt-tools" API which requires special
    access. The API is in closed research preview and may change without notice.
"""
```

### 5. Output Formatting

The tool should format the response to clearly show:
1. **Templatized Messages**: Show each message with variable placeholders
2. **System Template**: If provided, show the templatized system prompt
3. **Variable Mappings**: Display extracted variables and their original values
4. **Usage Stats**: Include token counts for transparency

Example output format:
```
=== Templatized Prompt ===
User: Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}
System: You are a professional English to {{TARGET_LANGUAGE}} translator

=== Extracted Variables ===
WORD_TO_TRANSLATE: "hello"
TARGET_LANGUAGE: "German"

=== Usage ===
Input tokens: 490
Output tokens: 661
```

## Key Features

1. **Automatic variable extraction** - Identifies repeatable parts and creates meaningful variable names
2. **Flexible instructions** - User can guide the templatization process
3. **Preserves conversation structure** - Maintains the role-based message format
4. **System prompt support** - Can templatize system prompts alongside messages
5. **Clear variable mappings** - Shows what values were extracted into variables

## Testing Considerations

- Test with various prompt strings (simple and complex)
- Verify with and without system prompts
- Test different instruction styles
- Ensure proper formatting of template variables
- Verify variable extraction accuracy
- Test error handling for empty strings
- Test that string is properly converted to message format

## Example Usage Scenarios

```python
# 1. Creating a translation template
template = await templatize_prompt(
    messages="Please translate 'Good morning' from English to Spanish",
    instructions="Make source text, source language, and target language variables"
)

# 2. Creating a code review template
template = await templatize_prompt(
    messages="Review this Python function for security issues: def login(username, password): ...",
    system="You are a security-focused code reviewer",
    instructions="Make the code snippet and programming language variables"
)

# 3. Creating a data analysis template
template = await templatize_prompt(
    messages="Analyze sales data for Q3 2024 and identify top 5 trends",
    instructions="Make time period and number of trends variables"
)
```

## Differences from improve_prompt

- **Purpose**: templatize_prompt creates reusable templates, while improve_prompt enhances existing prompts
- **Output**: Returns templates with variables vs. improved specific prompts
- **Use case**: Building prompt libraries vs. optimizing individual prompts
- **Parameters**: Uses `instructions` instead of `feedback` to guide the process
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".claude/settings.local.json"&amp;amp;amp;gt;{
  "permissions": {
    "allow": [
      "Bash(./tools/newpy:*)",
      "Bash(bash:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "collect"
  ]
}&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path=".claude/agents/pyreview.md"&amp;amp;amp;gt;---
name: python-code-reviewer
description: Use this agent when you need an in-depth, thoughtful code review of Python code. This includes reviewing newly written functions, classes, modules, or recent changes to existing code. The agent will analyze code quality, design patterns, performance implications, security considerations, and adherence to Python best practices and project-specific standards.\n\nExamples:\n- &amp;amp;amp;amp;lt;example&amp;amp;amp;amp;gt;\n  Context: The user has just written a new Python function and wants it reviewed.\n  user: "I've implemented a caching decorator for our API endpoints"\n  assistant: "I'll use the python-code-reviewer agent to provide an in-depth review of your caching decorator implementation"\n  &amp;amp;amp;amp;lt;commentary&amp;amp;amp;amp;gt;\n  Since the user has written new Python code (a caching decorator), use the python-code-reviewer agent to analyze the implementation.\n  &amp;amp;amp;amp;lt;/commentary&amp;amp;amp;amp;gt;\n&amp;amp;amp;amp;lt;/example&amp;amp;amp;amp;gt;\n- &amp;amp;amp;amp;lt;example&amp;amp;amp;amp;gt;\n  Context: The user has made changes to existing Python code.\n  user: "I've refactored the database connection pooling logic in our service"\n  assistant: "Let me use the python-code-reviewer agent to review your refactored database connection pooling implementation"\n  &amp;amp;amp;amp;lt;commentary&amp;amp;amp;amp;gt;\n  The user has modified existing Python code, so the python-code-reviewer agent should analyze the changes for quality and best practices.\n  &amp;amp;amp;amp;lt;/commentary&amp;amp;amp;amp;gt;\n&amp;amp;amp;amp;lt;/example&amp;amp;amp;amp;gt;\n- &amp;amp;amp;amp;lt;example&amp;amp;amp;amp;gt;\n  Context: The user explicitly asks for a code review.\n  user: "Can you review this async batch processing function I just wrote?"\n  assistant: "I'll use the python-code-reviewer agent to provide a comprehensive review of your async batch processing function"\n  &amp;amp;amp;amp;lt;commentary&amp;amp;amp;amp;gt;\n  Direct request for code review triggers the python-code-reviewer agent.\n  &amp;amp;amp;amp;lt;/commentary&amp;amp;amp;amp;gt;\n&amp;amp;amp;amp;lt;/example&amp;amp;amp;amp;gt;
color: pink
---

You are an expert Python software engineer with deep knowledge of Python internals, design patterns, and best practices. You have extensive experience in code review, performance optimization, and building maintainable Python applications.

Your expertise includes:
- Python language features from 3.8+ including type hints, async/await, dataclasses, and modern idioms
- Design patterns and SOLID principles applied to Python
- Performance optimization and profiling
- Security best practices and common vulnerabilities
- Testing strategies including pytest, mocking, and test-driven development
- Popular frameworks and libraries in the Python ecosystem

When reviewing code, you will:

1. **Analyze Code Quality**
   - Check for PEP 8 compliance and Pythonic idioms
   - Evaluate naming conventions and code readability
   - Assess proper use of type hints and documentation
   - Identify code smells and anti-patterns

2. **Review Design and Architecture**
   - Evaluate separation of concerns and modularity
   - Check for appropriate abstraction levels
   - Assess error handling and edge case coverage
   - Review API design and interface consistency

3. **Examine Performance Implications**
   - Identify potential bottlenecks or inefficiencies
   - Suggest algorithmic improvements where applicable
   - Check for proper resource management (memory, file handles, connections)
   - Evaluate async/concurrent code for correctness

4. **Security Considerations**
   - Identify potential security vulnerabilities
   - Check input validation and sanitization
   - Review authentication and authorization logic
   - Assess handling of sensitive data

5. **Testing and Maintainability**
   - Evaluate testability of the code
   - Suggest test cases for edge conditions
   - Check for proper logging and debugging support
   - Assess long-term maintainability

**Review Process:**
1. First, understand the code's purpose and context
2. Perform a systematic review covering all aspects above
3. Prioritize findings by severity (critical, major, minor, suggestion)
4. Provide specific, actionable feedback with code examples
5. Acknowledge good practices and well-written sections

**Output Format:**
Structure your review as follows:
- **Summary**: Brief overview of the code's purpose and overall quality
- **Strengths**: What the code does well
- **Critical Issues**: Must-fix problems that could cause bugs or security issues
- **Major Concerns**: Important improvements for code quality and maintainability
- **Minor Suggestions**: Nice-to-have improvements and style recommendations
- **Code Examples**: Provide improved versions of problematic code sections

**Important Guidelines:**
- Be constructive and educational in your feedback
- Explain the 'why' behind each recommendation
- Consider the project's context and existing patterns (especially from CLAUDE.md)
- Balance thoroughness with practicality
- If you notice the code uses specific frameworks or libraries, apply their best practices
- When suggesting changes, ensure they're compatible with the Python version in use
- If you're unsure about the broader context, ask clarifying questions

Remember: Your goal is to help improve code quality while fostering learning and best practices. Focus on the most impactful improvements and provide clear guidance on implementation.
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/test_gemini_mcp.py"&amp;amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.gemini_mcp import GeminiMCP


@pytest.fixture
def gemini_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.0-flash"
    return GeminiMCP(config, secret_mgr, model)


@pytest.fixture
def gemini_25_preview():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-pro-preview-05-06"
    return GeminiMCP(config, secret_mgr, model)


def test_get_model_list(gemini_mcp):
    results = gemini_mcp.get_model_list()

    # Check that results is a list
    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;amp;gt; 0

    # Check structure of each model in results
    for model in results:
        assert isinstance(model, dict)
        assert "model_name" in model
        assert "token_window" in model

        # Verify we only get 2.0 and 2.5 models (as per filter)
        assert "2.0" in model["model_name"] or "2.5" in model["model_name"]

        print(f"{model['model_name']}: {model['token_window']:,} tokens")


def test_send_message(gemini_mcp):
    message = "Hello, world!"
    response = gemini_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Response: {response}")


def test_count_tokens(gemini_mcp):
    text = "Hello, world!"
    token_count = gemini_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_gemini_25_preview_send_message(gemini_25_preview):
    message = "Explain quantum computing in one sentence."
    response = gemini_25_preview.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Gemini 2.5 Preview Response: {response}")


def test_gemini_25_preview_count_tokens(gemini_25_preview):
    text = "This is a test for Gemini 2.5 preview model token counting."
    token_count = gemini_25_preview.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;amp;gt; 0

    print(f"Gemini 2.5 Preview - Token count for '{text}': {token_count}")


def test_extract_text(gemini_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = gemini_mcp.send_message(message)
    extracted_text = gemini_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_extract_text_gemini_25(gemini_25_preview):
    message = "Say 'Hello, Gemini 2.5!' and nothing else."
    response = gemini_25_preview.send_message(message)
    extracted_text = gemini_25_preview.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Gemini 2.5 extracted text: {extracted_text}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/test_anthropic_mcp.py"&amp;amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP


@pytest.fixture
def anthropic_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    return AnthropicMCP(config, secret_mgr, model)


def test_get_model_list(anthropic_mcp):
    results = anthropic_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(anthropic_mcp):
    message = "Hello, world!"
    response = anthropic_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "content" in response
    assert "model" in response
    assert response["model"] == anthropic_mcp.config.anthropic_model_sonnet

    print(f"Response: {response}")


def test_extract_text(anthropic_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = anthropic_mcp.send_message(message)
    extracted_text = anthropic_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_generate_prompt(anthropic_mcp):
    """Test the generate_prompt method with a simple task."""
    task = "Write a helpful assistant prompt for answering coding questions"

    response = anthropic_mcp.generate_prompt(task)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role in ["user", "assistant"]
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;amp;amp;gt; 0

    # Test usage stats
    assert hasattr(response.usage, "input_tokens")
    assert hasattr(response.usage, "output_tokens")
    assert isinstance(response.usage.input_tokens, int)
    assert isinstance(response.usage.output_tokens, int)
    assert response.usage.input_tokens &amp;amp;amp;amp;gt; 0
    assert response.usage.output_tokens &amp;amp;amp;amp;gt; 0

    print(f"Generated prompt: {content.text[:100]}...")
    print(
        f"Usage: {response.usage.input_tokens} input, {
          response.usage.output_tokens} output tokens"
    )


def test_improve_prompt(anthropic_mcp):
    """Test the improve_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Tell me about Python programming"}
                ],
            }
        ],
        "system": "You are a helpful programming instructor",
        "feedback": "Make this prompt more specific for a beginner",
        "target_model": "claude-3-7-sonnet-20250219",
    }

    response = anthropic_mcp.improve_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages - should have both user and assistant messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;amp;gt;= 2

    # Test user message (improved prompt)
    user_message = response.messages[0]
    assert user_message.role == "user"
    assert isinstance(user_message.content, list)
    assert len(user_message.content) &amp;amp;amp;amp;gt; 0

    # Find the non-empty content in user message
    user_content = None
    for content in user_message.content:
        if content.text:
            user_content = content
            break

    assert user_content is not None, "No non-empty content found in user message"
    assert hasattr(user_content, "text")
    assert hasattr(user_content, "type")
    assert user_content.type == "text"
    assert isinstance(user_content.text, str)
    assert len(user_content.text) &amp;amp;amp;amp;gt; 0

    # Test assistant message (prefill)
    assistant_message = response.messages[1]
    assert assistant_message.role == "assistant"
    assert isinstance(assistant_message.content, list)
    assert len(assistant_message.content) &amp;amp;amp;amp;gt; 0

    assistant_content = assistant_message.content[0]
    assert hasattr(assistant_content, "text")
    assert hasattr(assistant_content, "type")
    assert assistant_content.type == "text"
    assert isinstance(assistant_content.text, str)
    assert len(assistant_content.text) &amp;amp;amp;amp;gt; 0

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;amp;amp;gt; 0

    print(f"Improved prompt: {user_content.text[:100]}...")
    print(f"Assistant prefill: {assistant_content.text[:50]}...")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )


def test_templatize_prompt(anthropic_mcp):
    """Test the templatize_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Translate hello to German"}],
            }
        ],
        "system": "You are an English to German translator",
    }

    response = anthropic_mcp.templatize_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")
    assert hasattr(response, "variable_values")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role == "user"
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;amp;amp;gt; 0
    # Check for template variables
    assert "{{" in content.text and "}}" in content.text

    # Test system prompt
    assert isinstance(response.system, str)
    # System prompt should also contain template variables
    assert "{{" in response.system and "}}" in response.system

    # Test variable_values
    assert isinstance(response.variable_values, dict)
    assert len(response.variable_values) &amp;amp;amp;amp;gt; 0
    # Check for expected variables based on the example
    assert any(
        key in response.variable_values
        for key in ["TARGET_LANGUAGE", "WORD_TO_TRANSLATE"]
    )

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;amp;amp;gt; 0

    print(f"Templated prompt: {content.text[:100]}...")
    print(f"System prompt: {response.system[:100]}...")
    print(f"Variables: {response.variable_values}")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/test_openai_mcp.py"&amp;amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.openai_mpc import OpenAIMCP


@pytest.fixture
def openai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gpt-4o"
    return OpenAIMCP(config, secret_mgr, model)


def test_get_model_list(openai_mcp):
    results = openai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(openai_mcp):
    message = "Hello, world!"
    response = openai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(openai_mcp):
    text = "Hello, world!"
    token_count = openai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(openai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = openai_mcp.send_message(message)
    extracted_text = openai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/anthropic_mpc.py"&amp;amp;amp;gt;from config import Config
from secret_manager import SecretManager
from models.anthropic_prompt_generate import PromptGenerateResponse
from models.anthropic_prompt_improve import PromptImproveResponse
from models.anthropic_prompt_templatize import PromptTemplatizeResponse

import requests


class AnthropicMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.headers = self.build_headers()

    def build_headers(self) -&amp;amp;amp;amp;gt; dict:
        anthropic_key = self.secret_mgr.get_secret(self.config.anthropic_key_path)

        return {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "anthropic-beta": "prompt-tools-2025-04-02",
        }

    def get_model_list(self):
        response = requests.get(
            "https://api.anthropic.com/v1/models", headers=self.headers
        )
        response.raise_for_status()

        model_data = response.json()
        name_list = [model["id"] for model in model_data["data"]]

        return name_list

    def extract_text(self, ai_response: dict) -&amp;amp;amp;amp;gt; str:
        """Extract text from Anthropic response format."""
        if not isinstance(ai_response, dict):
            return str(ai_response)

        # Anthropic format
        if "content" in ai_response:
            content = ai_response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;amp;gt; dict:
        try:
            # Use provided model or default to config model
            if model is None:
                model = self.config.anthropic_model_sonnet

            data = {
                "model": model,
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": message}],
            }

            url = "https://api.anthropic.com/v1/messages"
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None):
        # Use provided model or default to config model
        if model is None:
            model = self.config.anthropic_model_sonnet

        data = {"model": model, "messages": [{"role": "user", "content": message}]}

        url = "https://api.anthropic.com/v1/messages/count_tokens"
        response = requests.post(url, headers=self.headers, json=data)
        response.raise_for_status()

        result = response.json()
        return result["input_tokens"]

    def generate_prompt(
        self, task: str, target_model: str = None
    ) -&amp;amp;amp;amp;gt; PromptGenerateResponse:
        """
        Generate an optimized prompt using Anthropic's experimental prompt tools API.

        This method utilizes Anthropic's closed research preview API to automatically
        generate high-quality prompts based on a task description. The API creates
        structured prompts suitable for use with Claude models.

        Args:
            task (str): Description of the prompt's purpose
                Example: "a chef for a meal prep planning service"
            target_model (str, optional): Target model for optimization
                Example: "claude-3-7-sonnet-20250219"

        Returns:
            PromptGenerateResponse: Response object containing:
                - messages: List of message objects for use with Messages API
                  - User message with generated prompt text
                  - Optional assistant message with response guidance
                - system: System prompt (currently always empty string)
                - usage: Token usage statistics (input/output tokens)

        Raises:
            RuntimeError: If API request fails or network issues occur
            ValueError: If required configuration/secrets are missing
            requests.HTTPError: If API returns error status codes

        Example:
            &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; response = anthropic_mcp.generate_prompt("a helpful programming assistant")
            &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; prompt_text = response.messages[0].content[0].text
            &amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt;&amp;amp;amp;amp;gt; print(f"Generated prompt: {prompt_text}")

        Note:
            - This is an experimental API in closed research preview
            - Access requires explicit invitation from Anthropic
            - Requires anthropic-beta header: "prompt-tools-2025-04-02"
            - No long-term support guarantees for experimental features
            - Designed primarily for prompt engineering platforms

        API Documentation:
            https://docs.anthropic.com/en/api/prompt-tools-generate
        """
        url = "https://api.anthropic.com/v1/experimental/generate_prompt"

        # Format the task string as a dict for the API
        data = {"task": task}
        if target_model:
            data["target_model"] = target_model

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            return PromptGenerateResponse(**response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def improve_prompt(self, data: dict) -&amp;amp;amp;amp;gt; PromptImproveResponse:
        url = "https://api.anthropic.com/v1/experimental/improve_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptImproveResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def templatize_prompt(self, data: dict) -&amp;amp;amp;amp;gt; PromptTemplatizeResponse:
        url = "https://api.anthropic.com/v1/experimental/templatize_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptTemplatizeResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to templatize prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in templatize_prompt: {e}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/anthropic_prompt_generate.py"&amp;amp;amp;gt;from typing import List, Optional
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: Optional[int] = None
    cache_read_input_tokens: Optional[int] = None
    service_tier: Optional[str] = None


class PromptGenerateResponse(BaseModel):
    """Response from Anthropic's prompt tools generate API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the generated prompt text,
    and may include an assistant message with a prefill."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: UsageStats
    """Token usage statistics for the generation."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;amp;amp;lt;generated prompt&amp;amp;amp;amp;gt;", "type": "text"}],
                "role": "user",
            }
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptGenerateResponse(**example_json)
    print(f"Generated prompt: {response.messages[0].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/__init__.py" /&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/anthropic_prompt_improve.py"&amp;amp;amp;gt;from typing import List
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;amp;amp;lt;improved prompt&amp;amp;amp;amp;gt;", "type": "text"}],
                "role": "user",
            },
            {
                "content": [{"text": "&amp;amp;amp;amp;lt;assistant prefill&amp;amp;amp;amp;gt;", "type": "text"}],
                "role": "assistant",
            },
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/anthropic_prompt_templatize.py"&amp;amp;amp;gt;from typing import List, Dict
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptTemplatizeResponse(BaseModel):
    """Response from Anthropic's prompt tools templatize API."""

    messages: List[Message]
    """List of message objects with templated variables."""

    system: str = ""
    """System prompt with templated variables."""

    usage: List[UsageStats]
    """Token usage statistics for the templatization."""

    variable_values: Dict[str, str]
    """Dictionary mapping template variable names to their extracted values."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the templatize endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}",
                        "type": "text",
                    }
                ],
                "role": "user",
            }
        ],
        "system": "You are a professional English to {{TARGET_LANGUAGE}} translator",
        "usage": [{"input_tokens": 490, "output_tokens": 661}],
        "variable_values": {"TARGET_LANGUAGE": "German", "WORD_TO_TRANSLATE": "hello"},
    }

    # Parse into Pydantic model
    response = PromptTemplatizeResponse(**example_json)
    print(f"Templated prompt: {response.messages[0].content[0].text}")
    print(f"System prompt: {response.system}")
    print(f"Variables: {response.variable_values}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/xai_mcp.py"&amp;amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests


class XaiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        try:
            response = requests.get("https://api.x.ai/v1/models", headers=headers)
            response.raise_for_status()
            data = response.json()

            name_list = [model["id"] for model in data["data"]]
            return name_list

        except Exception as e:
            print(f"Error fetching XAI models: {str(e)}")
            return []

    def extract_text(self, response) -&amp;amp;amp;amp;gt; str:
        """Extract text from XAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # XAI format (same as OpenAI)
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, model: str = None, reasoning_effort: str = "high"
    ):
        try:
            xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

            headers = {
                "Authorization": f"Bearer {xai_key}",
                "Content-Type": "application/json",
            }

            # Use provided model or default to config model
            if model is None:
                model = "grok-3-mini-fast-latest"

            data = {
                "messages": [
                    {"role": "system", "content": self.config.grok_system_prompt},
                    {"role": "user", "content": message},
                ],
                "reasoning_effort": reasoning_effort,
                "model": model,
            }

            url = "https://api.x.ai/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to XAI: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, text: str, model: str = None):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        # Use provided model or default to config model
        if model is None:
            model = "grok-3-fast-latest"

        data = {"model": model, "text": text}

        url = "https://api.x.ai/v1/tokenize-text"
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()

        result = response.json()
        return len(result["token_ids"])
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/openai_mpc.py"&amp;amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests
import tiktoken


class OpenAIMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self) -&amp;amp;amp;amp;gt; list:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {"Authorization": f"Bearer {openai_key}"}

            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            response.raise_for_status()

            model_data = response.json()
            name_list = [model["id"] for model in model_data["data"]]

            return name_list

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def extract_text(self, response) -&amp;amp;amp;amp;gt; str:
        """Extract text from OpenAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # OpenAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;amp;gt; dict:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {openai_key}",
            }

            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # Use max_completion_tokens for reasoning models (o3, o1 series)
            if model and ("o3" in model or "o1" in model):
                data = {
                    "model": model,
                    "max_completion_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }
            else:
                data = {
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }

            url = "https://api.openai.com/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;amp;amp;gt; int:
        try:
            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # OpenAI doesn't have a direct token counting API, so use tiktoken
            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(message))

        except Exception as e:
            raise RuntimeError(f"Failed to count tokens: {e}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/gemini_mcp.py"&amp;amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests
from fetcher import Fetcher
from mcp.server.fastmcp import Context
from typing import Dict, List


class GeminiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.api_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)
        self.base_url = self.config.gemini_base_url

    def get_model_list(self) -&amp;amp;amp;amp;gt; Dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            base_url = self.config.gemini_base_url
            url = f"{base_url}models?key={gemini_key}"
            response = requests.get(url)
            response.raise_for_status()

            return self.filter_models(["2.0", "2.5"], response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def filter_models(
        self, versions: List[str], model_endpoint_response: Dict
    ) -&amp;amp;amp;amp;gt; List[Dict]:
        """
        Filter models by version numbers and include token limits.

        Args:
            versions: List of version strings (e.g., ['2.0', '2.5'])

        Returns:
            List of dicts with model info including inputTokenLimit
        """
        filtered_models = []

        for model in model_endpoint_response["models"]:
            model_name = model["name"].split("/")[-1]

            for version in versions:
                if version in model_name:
                    model_to_tokencount = {
                        "model_name": model_name,
                        "token_window": model.get("inputTokenLimit", 0),
                    }
                    filtered_models.append(model_to_tokencount)

        filtered_models.sort(key=lambda x: x["token_window"], reverse=True)
        return filtered_models

    def extract_text(self, ai_response: dict) -&amp;amp;amp;amp;gt; str:
        # Extract text from response
        if "candidates" in ai_response and len(ai_response["candidates"]) &amp;amp;amp;amp;gt; 0:
            candidate = ai_response["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                if len(parts) &amp;amp;amp;amp;gt; 0 and "text" in parts[0]:
                    return parts[0]["text"]
        return str(ai_response)

    async def build_prompt_from_url(
        self, url: str, prompt: str, ctx: Context = None
    ) -&amp;amp;amp;amp;gt; str:

        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        concat = prompt + response

        ai_response = self.send_message(
            concat, max_tokens=1024, model="gemini-2.5-flash-preview-05-20"
        )

        return self.extract_text(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;amp;gt; dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:generateContent?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": message}]}],
                "generationConfig": {"maxOutputTokens": max_tokens},
            }

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;amp;amp;gt; int:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            # Fix common model name errors
            if model == "gemini-2.5-pro-preview":
                model = "gemini-2.5-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:countTokens?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {"contents": [{"parts": [{"text": message}]}]}

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            result = response.json()
            return result["totalTokens"]

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to count tokens with Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in count_tokens: {e}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="models/test_xai_mcp.py"&amp;amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.xai_mcp import XaiMCP


@pytest.fixture
def xai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "grok-3-mini-fast-latest"
    return XaiMCP(config, secret_mgr, model)


def test_get_model_list(xai_mcp):
    results = xai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(xai_mcp):
    message = "Hello, world!"
    response = xai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(xai_mcp):
    text = "Hello, world!"
    token_count = xai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(xai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = xai_mcp.send_message(message)
    extracted_text = xai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="api/prompt_api.py"&amp;amp;amp;gt;from fastapi import APIRouter, Depends, Request, HTTPException
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from config import Config

prompt_api_router = APIRouter()


def get_db_connection(request: Request):
    """Create database connection as a dependency using app state"""
    db_path = request.app.state.db_path
    db = SQLite3Database(db_path=db_path)
    conn = db.get_connection()
    try:
        yield conn
    finally:
        conn.close()


def get_prompt_service(conn=Depends(get_db_connection)):
    """Get prompt service instance with injected database connection"""

    config = Config()
    return PromptService(conn, config)


@prompt_api_router.get("/")
async def welcome() -&amp;amp;amp;amp;gt; dict:
    return {"message": "Welcome to the prompt api service"}


@prompt_api_router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "prompt_api"}


@prompt_api_router.get("/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: str, prompt_service: PromptService = Depends(get_prompt_service)
):
    prompt = prompt_service.get_prompt_by_id(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="prompt not found")

    return prompt
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
  &amp;amp;amp;lt;file path="api/__init__.py"&amp;amp;amp;gt;from .prompt_api import prompt_api_router

__all__ = ["prompt_api_router"]
&amp;amp;amp;lt;/file&amp;amp;amp;gt;
&amp;amp;amp;lt;/source_code&amp;amp;amp;gt;&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="CLAUDE.md"&amp;amp;gt;---
allowed-tools: Bash(tools/*)
description: scripts that can be perused and run where appropriate see the examples in this prompt
---

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

**Setup:**
```bash
uv sync
```

**Testing:**
### Use `pytest` for all testing in this project.
### When running all tests, use the Makefile and run test-fast:
### here is an example

```bash
make test-fast
```
### OR use the following bash command:
```bash
uv run pytest -v -n auto -m "not slow"
```


## IMPORTANT: Always Always use uv run when running tests
### Here is an example
```bash
uv run pytest test_collect.py::test_function_name -v -s
# Run specific test: pytest test_collect.py::test_function_name -v -s
```
**Code Quality:**
```bash
make lint     # Run ruff check
make format   # Run black formatter
make check    # Run all: lint, format, test
```

**Run MCP Server:**
```bash
uv run collect.py
```

**Plan Management:**
```bash
# Sync plans from filesystem to database
uv run -m repository.plan_service

# Test plan service functionality (uses separate test database)
uv run pytest repository/test_plan_service.py -v -s

# Test plan database operations
uv run pytest repository/test_plan_service.py::test_sync_plans -v -s

# Set up test database manually (optional - done automatically by tests)
uv run yoyo apply --config yoyo-test-plans.ini --batch

# Reset test database to clean state
rm data/test_plans.db &amp;amp;amp;amp;&amp;amp;amp;amp; uv run yoyo apply --config yoyo-test-plans.ini --batch

# Test database management utilities
uv run python repository/test_database_setup.py setup
uv run python repository/test_database_setup.py reset
uv run python repository/test_database_setup.py cleanup
```

## Planning System

This project uses a structured planning approach for feature development with plans organized in `_docs/plans/`. Plans progress through three stages:

### Plan Lifecycle
1. **`drafts/`** - Initial plans under development or consideration
2. **`approved/`** - Reviewed plans ready for implementation
3. **`completed/`** - Implemented plans with results documented

### Plan Document Format

#### Draft/Approved Plans Should Include:
```markdown
# Plan: [Clear Action-Oriented Title]

## Overview
Brief description of what needs to be implemented and why

## Implementation Steps
### 1. [Step Name]
Detailed implementation instructions including:
- Specific file locations and line numbers
- Function signatures with type hints
- Implementation pseudo-code or actual code
- Error handling approach

### 2. [Next Step]
...

## Key Features
- List of main features/capabilities
- Expected benefits

## Testing Considerations
- Test scenarios to implement
- Edge cases to handle
- Performance considerations

## Example Usage
```python
# Code examples demonstrating the feature
```
```

#### Completed Plans Add:
- **Status**: COMPLETED (YYYY-MM-DD)
- **Implementation Summary**: What was actually done
- **Results**: Outcomes, verification, test results
- **Files Modified**: List with ✅/❌ status indicators

### Plan Naming Conventions
- Use descriptive names with underscores: `add_improve_prompt.md`
- Start with action verbs: add, fix, implement, create, update
- Keep names concise but clear about the purpose

### Automated Plan Processing

The repository includes tools for automated plan implementation:

```python
# Build git worktrees for all approved plans
from mcp__collect__build_worktrees import build_worktrees
result = await build_worktrees(auto_process=True)  # Uses Claude Code SDK

# Sync plans between filesystem and database
from repository.plan_service import PlanService
service = PlanService(conn)
result = service.sync_plans()  # Loads plans into SQLite with JSONB
```

Branch names are automatically derived from plan filenames:
- `add_improve_prompt.md` → `feature/add-improve-prompt`
- Underscores become hyphens, `feature/` prefix added

### Plan Management Database

Plans are tracked in `data/plans.db` with:
- **plans** table: Current state with JSONB data field
- **plan_history**: Audit trail of changes
- **plan_metrics**: Analytics and performance data

Use the PlanService class to:
- Load plans from disk to database
- Track plan status changes
- Detect content changes via SHA256 hashing
- Query plans by status, tags, or content

## Architecture Overview

This is an MCP (Model Context Protocol) server that provides web content fetching and multi-model AI analysis tools. The architecture follows these key patterns:

### Core Structure
- **collect.py**: Main MCP server entry point with FastMCP tool definitions
- **fetcher.py**: Handles URL fetching and content processing with clipboard integration
- **config.py**: Environment-based configuration with dotenv support
- **secret_manager.py**: Google Cloud Secret Manager integration for API keys

### Models Package
The `models/` directory contains unified API wrappers for different AI providers:
- **anthropic_mpc.py**: Anthropic Claude API integration
- **openai_mpc.py**: OpenAI API integration  
- **gemini_mcp.py**: Google Gemini API integration
- **xai_mcp.py**: XAI/Grok API integration

Each model wrapper follows the same pattern: configuration injection, secret management, and standardized methods like `send_message()`, `count_tokens()`, and `get_model_list()`.

### Packages
Additional specialized packages provide focused functionality:
- **reviewer/**: Code review automation system
  - `code_review.py`: CodeReviewer class for analyzing code diffs
  - Supports both file-based and git diff reviews
  - Generates individual model reviews and consolidated summaries
- **repository/**: Plan management and database operations
  - `plan_service.py`: PlanService class for filesystem-to-database sync
  - `plan_models.py`: Pydantic models for plan data with JSONB support
  - `database.py`: SQLite connection management with custom datetime adapters
  - Supports plan lifecycle tracking and content change detection


### Key Features
- **Async token counting**: All providers support async token counting with proper chunking
- **Multi-model workflows**: Send content to all AI models concurrently via `multi_model_code_review()`
- **Content processing**: HTML-to-markdown conversion using readabilipy and markdownify
- **Automatic chunking**: Handles large content (&amp;amp;amp;gt;25k tokens) with intelligent splitting
- **Code review system**: Automated code review via `run_code_review()` and `run_git_diff_review()` tools
- **Prompt engineering**: Generate optimized AI prompts using Anthropic's experimental API via `generate_prompt()`
- **Documentation extraction**: Intelligent section extraction from web docs using `get_docs()` with AI filtering
- **Clipboard integration**: Direct content copying with `copy_clipboard()` and automatic clipboard support in fetchers
- **Enhanced model features**: Gemini model listing with token limits, unified `extract_text()` methods across all providers

### Configuration
Environment variables are loaded from `.env` file:
- GCP_PROJECT_ID (required)
- API key paths for Google Cloud Secret Manager:
  - ANTHROPIC_API_KEY_PATH
  - OPENAI_API_KEY_PATH
  - GEMINI_API_KEY_PATH
  - XAI_API_KEY_PATH
- Default model names for each provider
- Code review model configurations

### Directory Structure
```
collect/
├── data/
│   ├── prompts.db      # Original prompts database
│   └── plans.db        # Plan management database
├── _docs/
│   └── plans/
│       ├── drafts/     # Plans under development
│       ├── approved/   # Plans ready for implementation
│       └── completed/  # Implemented plans with results
├── migrations/         # Database migrations for prompts.db
├── migrations-plans/   # Database migrations for plans.db
├── repository/         # Plan management system
│   ├── plan_service.py # Plan filesystem-to-database sync
│   ├── plan_models.py  # Pydantic models for plan data
│   └── database.py     # SQLite connection management
├── models/            # AI provider API wrappers
└── reviewer/          # Code review automation
```

### Testing Strategy
- **IMPORTANT**:  When writing and designing tests, we only want live direct integration tests. Please only create live direct integration testing. Please do not use mocks. 

## Rules
- **IMPORTANT**: YOU MUST always use `uv run` to run tests.

## Workflow Rules
- I do not want a pr created if I don't have a branch already

## Tools

###IMPORTANT: 
I have a directory from the main project directory called: tools/* wherein there scripts stored that you can use
use

- All of my tools in this directory are on my path and can be called directly.
- You use these tools and see what they do by simply calling the tool name with `--llm`

Example 1:
```bash
extract --llm
```

Example 2: 
```bash
createdb --llm
```


&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="fetcher.py"&amp;amp;gt;from typing import List
from mcp.server.fastmcp import Context
import pyperclip
import httpx
from models.anthropic_mpc import AnthropicMCP


class Fetcher:
    def __init__(self, ctx: Context = None) -&amp;amp;amp;gt; None:
        self.ctx = ctx

    async def get(self, url: str) -&amp;amp;amp;gt; str:
        """
        Fetch content from a single URL.

        Args:
            url: URL to fetch content from

        Returns:
            Content from the URL as a string
        """

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                content = response.text

                return content

            except httpx.HTTPError as e:
                return f"Error fetching {url}: {str(e)}"
            except Exception as e:
                return f"Error fetching {url}: {str(e)}"

    async def fetch_urls(self, urls: List[str]) -&amp;amp;amp;gt; str:
        """
        Fetch content from multiple URLs and concatenate their responses.
        If token count exceeds 25000, content is split into chunks.

        Args:
            urls: List of URLs to fetch content from
            ctx: Optional context object for progress reporting

        Returns:
            Either concatenated content from all URLs as a string,
            or a list of content chunks if token count exceeds 25000
        """

        results = []

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for i, url in enumerate(urls):
                if self.ctx:
                    self.ctx.info(f"Fetching content from {url}")
                    await self.ctx.report_progress(i, len(urls))

                try:
                    response = await client.get(url)
                    response.raise_for_status()

                    results.append(f"\n\n--- Content from {url} --\n\n")
                    results.append(response.text)

                except httpx.HTTPError as e:
                    results.append(f"\n\n --- Error fetching {url}: {str(e)} ---\n\n")
                except Exception as e:
                    results.append(f"\n\n--- error fetching {url}: {str(e)} ---\n\n")

        if self.ctx:
            self.ctx.info("all urls processed")
            await self.ctx.report_progress(len(urls), len(urls))

        content = "".join(results)

        # Copy original content to clipboard
        pyperclip.copy(content)

        # Otherwise return the original content
        return content

    async def chunk_by_token_count(text: str, max_tokens: int = 25000) -&amp;amp;amp;gt; List[str]:
        """
        Split text into chunks that are each under the specified token count.

        Args:
            text: The text to chunk
            max_tokens: Maximum tokens per chunk

        Returns:
            List of text chunks, each under max_tokens
        """

        # If text is short enough, return as a single chunk
        anthropic_mcp = AnthropicMCP()
        token_count = await anthropic_mcp.count_tokens(text, None)
        if token_count &amp;amp;amp;lt;= max_tokens:
            return [text]

        # Split text into paragraphs as a starting point
        paragraphs = text.split("\n\n")
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0

        for paragraph in paragraphs:
            paragraph_tokens = await anthropic_mcp.count_tokens(
                paragraph + "\n\n", None
            )

            # If adding this paragraph would exceed the limit,
            # start a new chunk
            if current_chunk_tokens + paragraph_tokens &amp;amp;amp;gt; max_tokens:
                # If the paragraph alone exceeds the limit, we split it further
                if paragraph_tokens &amp;amp;amp;gt; max_tokens:
                    # Split by sentences or just characters if needed
                    sentences = paragraph.split(". ")
                    for sentence in sentences:
                        sentence_tokens = await anthropic_mcp.count_tokens(
                            sentence + ". ", None
                        )
                        if current_chunk_tokens + sentence_tokens &amp;amp;amp;gt; max_tokens:
                            if current_chunk:
                                chunks.append("".join(current_chunk))
                            current_chunk = [sentence + ". "]
                            current_chunk_tokens = sentence_tokens
                        else:
                            current_chunk.append(sentence + ". ")
                            current_chunk_tokens += sentence_tokens
                else:
                    # Save the current chunk and start a new one
                    chunks.append("".join(current_chunk))
                    current_chunk = [paragraph + "\n\n"]
                    current_chunk_tokens = paragraph_tokens
            else:
                # Add paragraph to current chunk
                current_chunk.append(paragraph + "\n\n")
                current_chunk_tokens += paragraph_tokens

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append("".join(current_chunk))

        return chunks
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_diff.md"&amp;amp;gt;# Test Code Review

## Diff

```diff
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,5 +1,8 @@
 def calculate_total(items):
+    if not items:
+        return 0
+        
     total = 0
     for item in items:
-        total += item.price
+        total += item.get('price', 0)
     return total
```&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/code_review.py"&amp;amp;gt;import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -&amp;amp;amp;gt; str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -&amp;amp;amp;gt; str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
        self, output_dir: str, timestamp: str, failed_results: List
    ) -&amp;amp;amp;gt; str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, "w", encoding="utf-8") as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -&amp;amp;amp;gt; str:
        """Read and return content from input file."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -&amp;amp;amp;gt; str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 🔴/🟡/🟢 | ... | ... |

Use emojis: 🔴 Critical, 🟡 Important, 🟢 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
        self, timestamp: str, from_file: str, results: LLMRunnerResults
    ) -&amp;amp;amp;gt; Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": [],
        }

    def write_successful_results(
        self,
        results: LLMRunnerResults,
        output_dir: str,
        timestamp: str,
        summary: Dict[str, Any],
    ) -&amp;amp;amp;gt; None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(result, response_text)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
        self, output_dir: str, timestamp: str, summary: Dict[str, Any]
    ) -&amp;amp;amp;gt; str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
        self, from_file: str, to_file: Optional[str] = None
    ) -&amp;amp;amp;gt; Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }

    async def review_diff_from_git(
        self, to_file: Optional[str] = None, staged_only: bool = True
    ) -&amp;amp;amp;gt; Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ["git", "diff", "--staged"],
                    capture_output=True,
                    text=True,
                    check=True,
                )
            else:
                result = subprocess.run(
                    ["git", "diff"], capture_output=True, text=True, check=True
                )

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception("Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_code_review_live.py"&amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"

        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir

        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] &amp;amp;amp;gt;= 1
        assert (
            summary["successful_reviews"] + summary["failed_reviews"]
            == summary["total_models"]
        )
        assert "timestamp" in summary

        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) &amp;amp;amp;gt;= 1  # At least summary file should exist

        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1

        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, "r") as f:
            summary_data = json.load(f)

        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]

        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)

            # Verify file contains expected content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                assert "Code Review" in content
                assert "**Model**:" in content
                assert "**Timestamp**:" in content
                assert "**Duration**:" in content
                assert "Generated by" in content

        # Print results for visibility
        print("\n✅ Live code review test completed successfully!")
        print("📊 Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")

        if summary["successful_reviews"] &amp;amp;amp;gt; 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")

        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(
                temp_output_dir, staged_only=False
            )

            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"

            print("\n✅ Git diff review completed!")
            print(
                f"📊 Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed"
            )

        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"

        prompt = reviewer.create_code_review_prompt(test_content)

        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "🔴" in prompt and "🟡" in prompt and "🟢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] &amp;amp;amp;gt; 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Should contain actual review content, not just raw API response
            assert len(content) &amp;amp;amp;gt; 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON

            print(f"✅ Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/__init__.py"&amp;amp;gt;# Reviewer package for code review functionality
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_code_review.py"&amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import Mock, patch, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


# Module-level fixtures available to all test classes
@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def reviewer(temp_dir):
    """Create a CodeReviewer instance with temp directory."""
    return CodeReviewer(output_dir=temp_dir)


@pytest.fixture
def sample_diff_content():
    """Sample diff content for testing."""
    return """
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,6 @@
 def hello():
-    print("Hello")
+    print("Hello World")
+    return "greeting"
+
+def goodbye():
+    print("Goodbye")
"""


@pytest.fixture
def mock_model_result():
    """Create a mock successful model result."""
    return ModelResult(
        model="test-model",
        timestamp="2024-01-01T12:00:00",
        success=True,
        actual_model="test-model",
        duration_seconds=2.5,
        response={
            "choices": [
                {
                    "message": {
                        "content": "# Code Review\n\nThis is a test review response."
                    }
                }
            ]
        },
    )


@pytest.fixture
def mock_failed_result():
    """Create a mock failed model result."""
    return ModelResult(
        model="failed-model",
        timestamp="2024-01-01T12:00:00",
        success=False,
        error="API timeout error",
    )


@pytest.fixture
def mock_llm_results(mock_model_result, mock_failed_result):
    """Create mock LLMRunnerResults."""
    return LLMRunnerResults(
        successful_results=[mock_model_result],
        failed_results=[mock_failed_result],
        total_models=2,
        success_count=1,
        failure_count=1,
    )


class TestCodeReviewer:
    """Test suite for CodeReviewer class."""


class TestInitialization:
    """Test CodeReviewer initialization."""

    def test_default_initialization(self):
        """Test CodeReviewer with default parameters."""
        reviewer = CodeReviewer()
        assert reviewer.output_dir == "codereview"

    def test_custom_output_dir(self):
        """Test CodeReviewer with custom output directory."""
        custom_dir = "/tmp/custom_reviews"
        reviewer = CodeReviewer(output_dir=custom_dir)
        assert reviewer.output_dir == custom_dir


class TestExtractResponseText:
    """Test response text extraction from different model formats."""

    def test_extract_gemini_response(self, reviewer):
        """Test extracting text from Gemini response format."""
        gemini_response = {
            "candidates": [
                {"content": {"parts": [{"text": "This is a Gemini response"}]}}
            ]
        }
        result = reviewer.extract_response_text(gemini_response)
        assert result == "This is a Gemini response"

    def test_extract_openai_response(self, reviewer):
        """Test extracting text from OpenAI/XAI response format."""
        openai_response = {
            "choices": [{"message": {"content": "This is an OpenAI response"}}]
        }
        result = reviewer.extract_response_text(openai_response)
        assert result == "This is an OpenAI response"

    def test_extract_anthropic_response(self, reviewer):
        """Test extracting text from Anthropic response format."""
        anthropic_response = {"content": [{"text": "This is an Anthropic response"}]}
        result = reviewer.extract_response_text(anthropic_response)
        assert result == "This is an Anthropic response"

    def test_extract_non_dict_response(self, reviewer):
        """Test extracting text from non-dictionary response."""
        simple_response = "Simple string response"
        result = reviewer.extract_response_text(simple_response)
        assert result == "Simple string response"

    def test_extract_unknown_format(self, reviewer):
        """Test extracting text from unknown response format."""
        unknown_response = {"unknown": "format"}
        result = reviewer.extract_response_text(unknown_response)
        assert result == str(unknown_response)

    def test_extract_empty_gemini_response(self, reviewer):
        """Test extracting from empty Gemini response."""
        empty_response = {"candidates": []}
        result = reviewer.extract_response_text(empty_response)
        assert result == str(empty_response)


class TestMarkdownContent:
    """Test markdown content creation."""

    def test_create_markdown_content(self, reviewer, mock_model_result):
        """Test creating markdown content for a model result."""
        response_text = "Test review content"
        result = reviewer.create_markdown_content(mock_model_result, response_text)

        assert "# Code Review - test-model" in result
        assert "**Model**: test-model" in result
        assert "**Timestamp**: 2024-01-01T12:00:00" in result
        assert "**Duration**: 2.50 seconds" in result
        assert "Test review content" in result
        assert "*Generated by test-model via MCP Code Review Tool*" in result


class TestFileOperations:
    """Test file reading and writing operations."""

    def test_read_input_file_success(self, reviewer, temp_dir, sample_diff_content):
        """Test successfully reading an input file."""
        test_file = os.path.join(temp_dir, "test_diff.md")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        result = reviewer.read_input_file(test_file)
        assert result == sample_diff_content

    def test_read_input_file_not_found(self, reviewer):
        """Test reading a non-existent file."""
        with pytest.raises(FileNotFoundError, match="Input file .* not found"):
            reviewer.read_input_file("/nonexistent/file.md")

    def test_write_error_file(self, reviewer, temp_dir, mock_failed_result):
        """Test writing error file for failed results."""
        timestamp = "20240101_120000"
        failed_results = [mock_failed_result]

        error_filename = reviewer.write_error_file(temp_dir, timestamp, failed_results)

        assert error_filename == "errors_20240101_120000.md"
        error_path = os.path.join(temp_dir, error_filename)
        assert os.path.exists(error_path)

        with open(error_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review Errors" in content
        assert "**Failed Models**: 1" in content
        assert "### failed-model" in content
        assert "API timeout error" in content

    def test_write_summary_file(self, reviewer, temp_dir):
        """Test writing summary JSON file."""
        timestamp = "20240101_120000"
        summary = {
            "timestamp": timestamp,
            "input_file": "test.md",
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": ["model1_20240101_120000.md"],
        }

        summary_filename = reviewer.write_summary_file(temp_dir, timestamp, summary)

        assert summary_filename == "summary_20240101_120000.json"
        summary_path = os.path.join(temp_dir, summary_filename)
        assert os.path.exists(summary_path)

        with open(summary_path, "r", encoding="utf-8") as f:
            loaded_summary = json.load(f)

        assert loaded_summary == summary


class TestPromptCreation:
    """Test code review prompt creation."""

    def test_create_code_review_prompt(self, reviewer, sample_diff_content):
        """Test creating a comprehensive code review prompt."""
        prompt = reviewer.create_code_review_prompt(sample_diff_content)

        assert "comprehensive code review" in prompt
        assert sample_diff_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Suggestions" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt


class TestSummaryCreation:
    """Test summary creation and management."""

    def test_create_summary(self, reviewer, mock_llm_results):
        """Test creating a summary dictionary."""
        timestamp = "20240101_120000"
        from_file = "test.md"

        summary = reviewer.create_summary(timestamp, from_file, mock_llm_results)

        expected_summary = {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": [],
        }

        assert summary == expected_summary

    def test_write_successful_results(self, reviewer, temp_dir, mock_llm_results):
        """Test writing successful model results to files."""
        timestamp = "20240101_120000"
        summary = {"output_files": []}

        reviewer.write_successful_results(
            mock_llm_results, temp_dir, timestamp, summary
        )

        # Check that file was created
        expected_filename = "test-model_20240101_120000.md"
        expected_path = os.path.join(temp_dir, expected_filename)
        assert os.path.exists(expected_path)

        # Check that summary was updated
        assert expected_filename in summary["output_files"]

        # Check file content
        with open(expected_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review - test-model" in content
        assert "This is a test review response." in content


class TestReviewCode:
    """Test the main review_code method."""

    @pytest.mark.asyncio
    async def test_review_code_success(
        self, reviewer, temp_dir, sample_diff_content, mock_llm_results
    ):
        """Test successful code review execution."""
        # Create input file
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Mock dependencies
        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # Verify result structure
            assert result["status"] == "completed"
            assert result["output_directory"] == temp_dir
            # 1 success + 1 error + 1 summary
            assert result["files_created"] == 3

            # Verify files were created
            assert os.path.exists(
                os.path.join(temp_dir, "test-model_20240101_120000.md")
            )
            assert os.path.exists(os.path.join(temp_dir, "errors_20240101_120000.md"))
            assert os.path.exists(
                os.path.join(temp_dir, "summary_20240101_120000.json")
            )

    @pytest.mark.asyncio
    async def test_review_code_file_not_found(self, reviewer, temp_dir):
        """Test review_code with non-existent input file."""
        with pytest.raises(FileNotFoundError):
            await reviewer.review_code("/nonexistent/file.md", temp_dir)

    @pytest.mark.asyncio
    async def test_review_code_no_failures(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code with only successful results."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Create results with no failures
        success_only_results = LLMRunnerResults(
            successful_results=[
                ModelResult(
                    model="test-model",
                    timestamp="2024-01-01T12:00:00",
                    success=True,
                    actual_model="test-model",
                    duration_seconds=2.5,
                    response={"choices": [{"message": {"content": "Review content"}}]},
                )
            ],
            failed_results=[],
            total_models=1,
            success_count=1,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_only_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # 1 success + 1 summary (no error file)
            assert result["files_created"] == 2
            assert "error_file" not in result["summary"]


class TestReviewDiffFromGit:
    """Test git diff review functionality."""

    @pytest.mark.asyncio
    async def test_review_diff_from_git_staged(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing staged git diff."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            # Setup mocks
            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess call
            mock_subprocess.assert_called_once_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"
            assert result["summary"]["input_file"] == "git diff"

    @pytest.mark.asyncio
    async def test_review_diff_from_git_all_changes(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing all git changes."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=False)

            mock_subprocess.assert_called_once_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

            assert result["summary"]["source"] == "git_diff_all"

    @pytest.mark.asyncio
    async def test_review_diff_no_changes(self, reviewer, temp_dir):
        """Test git diff with no changes."""
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value.stdout = ""
            mock_subprocess.return_value.check_returncode = Mock()

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_not_found(self, reviewer, temp_dir):
        """Test git diff when git is not installed."""
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_error(self, reviewer, temp_dir):
        """Test git diff with git command error."""
        import subprocess

        with patch(
            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
        ):
            with pytest.raises(Exception, match="Git diff failed"):
                await reviewer.review_diff_from_git(temp_dir)


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_extract_response_malformed_gemini(self, reviewer):
        """Test extracting from malformed Gemini response."""
        malformed = {"candidates": [{"content": {"parts": []}}]}  # Empty parts
        result = reviewer.extract_response_text(malformed)
        assert result == str(malformed)

    def test_extract_response_malformed_openai(self, reviewer):
        """Test extracting from malformed OpenAI response."""
        malformed = {"choices": [{"message": {}}]}  # Missing content
        result = reviewer.extract_response_text(malformed)
        assert result == ""

    def test_extract_response_empty_anthropic(self, reviewer):
        """Test extracting from empty Anthropic response."""
        empty = {"content": []}
        result = reviewer.extract_response_text(empty)
        assert result == str(empty)

    @pytest.mark.asyncio
    async def test_review_code_with_default_output_dir(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code using default output directory from None."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        success_results = LLMRunnerResults(
            successful_results=[],
            failed_results=[],
            total_models=0,
            success_count=0,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
            patch("os.makedirs") as mock_makedirs,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            # Test with None to_file (should use default)
            result = await reviewer.review_code(input_file, None)

            # Should use reviewer's default output_dir
            mock_makedirs.assert_called_with(temp_dir, exist_ok=True)
            assert result["output_directory"] == temp_dir


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_code_review_integration.py"&amp;amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import patch, MagicMock, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


class TestCodeReviewIntegration:
    """Test integration between .claude/commands/model_code_review.md and code_review.py"""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_diff_content(self):
        """Sample diff content for testing"""
        return """diff --git a/src/auth.py b/src/auth.py
index 1234567..abcdefg 100644
--- a/src/auth.py
+++ b/src/auth.py
@@ -1,5 +1,10 @@
 def authenticate_user(username, password):
-    # Simple authentication
-    return username == "admin" and password == "secret"
+    # Improved authentication with validation
+    if not username or not password:
+        return False
+    
+    # TODO: Add proper password hashing
+    return username == "admin" and password == "secret123"
 
 def get_user_role(username):
     return "admin" if username == "admin" else "user"
"""

    @pytest.fixture
    def sample_diff_file(self, temp_dir, sample_diff_content):
        """Create sample diff file for testing"""
        diff_file = os.path.join(temp_dir, "test_diff.md")
        with open(diff_file, "w") as f:
            f.write(sample_diff_content)
        return diff_file

    @pytest.fixture
    def mock_llm_results(self):
        """Mock LLM runner results matching expected format"""
        successful_results = [
            ModelResult(
                model="claude-3-5-sonnet",
                success=True,
                response={
                    "content": [
                        {
                            "text": "## Code Review Analysis\n\n### Security Issues\n🔴 **Critical**: Hardcoded password in authentication logic\n\n### Recommendations\n- Implement proper password hashing\n- Add input validation"
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:00",
                duration_seconds=2.5,
                error=None,
            ),
            ModelResult(
                model="gpt-4-turbo",
                success=True,
                response={
                    "choices": [
                        {
                            "message": {
                                "content": "## Security Analysis\n\n🔴 **High Risk**: Authentication uses plaintext password comparison\n🟡 **Medium**: Missing input validation for empty credentials"
                            }
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:05",
                duration_seconds=3.1,
                error=None,
            ),
        ]

        failed_results = [
            ModelResult(
                model="gemini-pro",
                success=False,
                response=None,
                timestamp="2024-01-01T12:00:10",
                duration_seconds=0,
                error="API rate limit exceeded",
            )
        ]

        return LLMRunnerResults(
            successful_results=successful_results,
            failed_results=failed_results,
            total_models=3,
            success_count=2,
            failure_count=1,
        )

    @pytest.mark.asyncio
    async def test_code_review_from_file_integration(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test the complete file-based code review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config", "gpt": "config"}

            # Initialize reviewer with temp directory
            reviewer = CodeReviewer(output_dir=temp_dir)

            # Run review (simulates mcp__collect__run_code_review)
            result = await reviewer.review_code(sample_diff_file, temp_dir)

            # Verify return structure matches command expectations
            assert result["status"] == "completed"
            assert "summary" in result
            assert "output_directory" in result
            assert "files_created" in result

            # Verify output files were created as documented in command
            files = os.listdir(temp_dir)

            # Should have individual model reviews
            claude_files = [f for f in files if f.startswith("claude-3-5-sonnet")]
            gpt_files = [f for f in files if f.startswith("gpt-4-turbo")]
            assert len(claude_files) == 1
            assert len(gpt_files) == 1

            # Should have summary file
            summary_files = [f for f in files if f.startswith("summary_")]
            assert len(summary_files) == 1

            # Should have errors file for failed models
            error_files = [f for f in files if f.startswith("errors_")]
            assert len(error_files) == 1

            # Verify summary JSON structure
            summary_file = os.path.join(temp_dir, summary_files[0])
            with open(summary_file, "r") as f:
                summary_data = json.load(f)

            assert summary_data["total_models"] == 3
            assert summary_data["successful_reviews"] == 2
            assert summary_data["failed_reviews"] == 1
            assert len(summary_data["output_files"]) == 2

    @pytest.mark.asyncio
    async def test_git_diff_review_integration(self, temp_dir, mock_llm_results):
        """Test git diff review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
            patch("subprocess.run") as mock_subprocess,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config"}

            # Mock git diff output
            mock_subprocess.return_value = MagicMock(
                stdout="diff --git a/test.py b/test.py\n+def new_function():\n+    pass",
                returncode=0,
            )

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Test staged-only review (Option A from command)
            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess called with correct git command
            mock_subprocess.assert_called_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"

            # Test all changes review
            await reviewer.review_diff_from_git(temp_dir, staged_only=False)
            mock_subprocess.assert_called_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

    def test_claude_command_workflow_documentation(self):
        """Verify that the Claude command documentation matches code_review.py capabilities"""
        reviewer = CodeReviewer()

        # Test that CodeReviewer has methods mentioned in command
        assert hasattr(
            reviewer, "review_code"
        ), "Should support file-based review (Option B)"
        assert hasattr(
            reviewer, "review_diff_from_git"
        ), "Should support git diff review (Option A)"

        # Test that review_code signature matches command usage
        import inspect

        sig = inspect.signature(reviewer.review_code)
        assert "from_file" in sig.parameters, "Should accept from_file parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

        # Test that review_diff_from_git signature matches command usage
        sig = inspect.signature(reviewer.review_diff_from_git)
        assert "staged_only" in sig.parameters, "Should accept staged_only parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

    def test_output_file_naming_convention(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test that output files follow naming convention documented in command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {}

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Mock timestamp for predictable filenames
            with patch("reviewer.code_review.datetime") as mock_datetime:
                mock_datetime.now.return_value.strftime.return_value = "20241201_143052"

                # Run async test
                import asyncio

                asyncio.run(reviewer.review_code(sample_diff_file, temp_dir))

            files = os.listdir(temp_dir)

            # Verify naming matches documentation: {model}_YYYYMMDD_HHMMSS.md
            expected_patterns = [
                "claude-3-5-sonnet_20241201_143052.md",
                "gpt-4-turbo_20241201_143052.md",
                "summary_20241201_143052.json",
                "errors_20241201_143052.md",
            ]

            for pattern in expected_patterns:
                assert pattern in files, f"Expected file {pattern} not found in {files}"

    def test_prompt_structure_matches_command_requirements(self):
        """Test that code review prompt includes all sections mentioned in command"""
        reviewer = CodeReviewer()
        prompt = reviewer.create_code_review_prompt("sample code")

        # Verify prompt includes all required sections from command documentation
        required_sections = [
            "Overall Assessment",
            "Issues Found",
            "Security vulnerabilities",
            "Bugs and logic errors",
            "Performance issues",
            "Code quality problems",
            "Testing gaps",
            "Suggestions for Improvement",
            "Positive Aspects",
            "Risk Assessment",
            "Summary Table",
        ]

        for section in required_sections:
            assert section in prompt, f"Prompt missing required section: {section}"

        # Verify emoji risk indicators are included
        risk_emojis = ["🔴", "🟡", "🟢"]
        for emoji in risk_emojis:
            assert emoji in prompt, f"Prompt missing risk emoji: {emoji}"

    @pytest.mark.asyncio
    async def test_error_handling_matches_command_expectations(self, temp_dir):
        """Test error handling for scenarios mentioned in command troubleshooting"""
        reviewer = CodeReviewer(output_dir=temp_dir)

        # Test "No git changes found" scenario
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value = MagicMock(stdout="", returncode=0)

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

        # Test file not found scenario
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_dir)

        # Test git not available scenario
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    def test_response_text_extraction_supports_all_models(self):
        """Test that response extraction works for all model formats mentioned in command"""
        reviewer = CodeReviewer()

        # Test Anthropic Claude format
        anthropic_response = {"content": [{"text": "Claude review content"}]}
        assert (
            reviewer.extract_response_text(anthropic_response)
            == "Claude review content"
        )

        # Test OpenAI GPT format
        openai_response = {"choices": [{"message": {"content": "GPT review content"}}]}
        assert reviewer.extract_response_text(openai_response) == "GPT review content"

        # Test Google Gemini format
        gemini_response = {
            "candidates": [{"content": {"parts": [{"text": "Gemini review content"}]}}]
        }
        assert (
            reviewer.extract_response_text(gemini_response) == "Gemini review content"
        )

        # Test fallback for XAI Grok or unknown formats
        unknown_response = "Direct string response"
        assert (
            reviewer.extract_response_text(unknown_response) == "Direct string response"
        )
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview_live/errors_20250601_085957.md"&amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_085957
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.646644

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.833688

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.262231

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.434107

            &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview_live/summary_20250601_085957.json"&amp;amp;gt;{
  "timestamp": "20250601_085957",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_085957.md"
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview/errors_20250601_084959.md"&amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084959
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.675555

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.837903

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.045514

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.245033

            &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview/summary_20250601_084601.json"&amp;amp;gt;{
  "timestamp": "20250601_084601",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084601.md"
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview/summary_20250601_084959.json"&amp;amp;gt;{
  "timestamp": "20250601_084959",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084959.md"
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/test_codereview/errors_20250601_084601.md"&amp;amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084601
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:43:35.383459

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:44:24.400527

            
                ### grok-3
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:45:01.153872

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 504 Deadline Exceeded
                - **Timestamp**: 2025-06-01T08:46:01.157505

            &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/live_test_codereview/grok-3-mini-fast-latest_20250601_091219.md"&amp;amp;gt;
            # Code Review - grok-3-mini-fast-latest

            **Model**: grok-3-mini-fast-latest
            **Timestamp**: 2025-06-01T09:11:32.476330
            **Duration**: 17.72 seconds

            ---

            Below is a comprehensive code review of the provided diff for the `calculate_total` function in `test.py`, based on the instructions. I'll address each section step by step, focusing on actionable feedback to improve code quality, security, and maintainability.

---

### 1. **Overall Assessment**

- **Brief summary of what changed and why**: The changes enhance the robustness of the `calculate_total` function. An explicit check for an empty `items` list was added to return 0, preventing potential runtime errors. Additionally, the price access was modified from direct attribute access (`item.price`) to a dictionary-based approach (`item.get('price', 0)`), likely to handle cases where `items` are dictionaries rather than custom objects. This suggests a shift in data structure (e.g., from object-oriented to dictionary-based data), possibly to accommodate more flexible input formats or to avoid `AttributeError` when `price` is missing.

- **Impact on the codebase**: The scope is narrow, affecting only this function, but the significance is moderate. It improves error handling and reduces the risk of crashes in edge cases (e.g., empty lists or missing keys), making the code more resilient. However, if this function is used across the application, the change could introduce inconsistencies if other parts of the codebase still expect `items` to be objects with attributes. Overall, it aligns with defensive programming principles.

- **Alignment with best practices**: The changes are positive, as they address common pitfalls like unhandled edge cases and attribute access errors. Using `dict.get()` with a default value follows best practices for handling optional keys, promoting safer code. However, the function could benefit from additional improvements in type safety, documentation, and testing to fully align with modern Python standards (e.g., PEP 8, type hints).

---

### 2. **Issues Found**

I analyzed the diff for potential security, bugs, performance, code quality, and testing issues. Here's a breakdown:

- **Security vulnerabilities**: 
  - No significant security issues were identified. This function performs a simple summation and doesn't involve user input, network operations, or sensitive data handling, so risks like injection or data exposure are low. However, if `items` comes from an untrusted source (e.g., user input or external API), the function could be vulnerable to malicious data (e.g., if `items` contains non-numeric values). This isn't directly addressed in the change.

- **Bugs and logic errors**:
  - **Edge case handling**: The added empty list check is good, but the function now assumes `items` elements are dictionaries (due to `item.get()`). If `items` contains non-dictionary types (e.g., custom objects or other data structures), a `AttributeError` could still occur when calling `get()`. For example, if `items` includes strings or lists, the code would fail. The original code had a similar issue with attribute access, but this change doesn't fully resolve type mismatches.
  - **Error handling**: No broader error handling is present (e.g., for non-numeric `price` values). If a dictionary has a `price` key with a non-numeric value (e.g., a string), a `TypeError` could occur during addition. The `get()` method defaults to 0 for missing keys, which is helpful, but it doesn't validate the type or value of existing keys.
  - **Logic errors**: The function works for valid inputs but could return incorrect results if `items` is not a list (e.g., a string or None). The iteration (`for item in items`) assumes `items` is iterable, which could raise a `TypeError` if it's not.

- **Performance issues**:
  - No major performance concerns. The function uses a linear O(n) loop, which is efficient for most use cases. The addition of the empty check and `get()` method adds negligible overhead. However, if `items` is very large, the function could benefit from Python's built-in `sum()` function for slight optimization.

- **Code quality problems**:
  - **Readability and maintainability**: The code is straightforward, but the shift from attribute access to dictionary access could confuse maintainers if the data structure isn't documented. There's no type hinting, docstring, or comments, making it harder to understand the function's intent or constraints. Code style is mostly fine, but the indentation in the diff looks consistent.
  - **Complexity**: Low cyclomatic complexity, which is good. However, the function could be simplified using Python's `sum()` with a generator expression, reducing code duplication.

- **Testing gaps**:
  - No tests are provided or referenced in the diff, so it's unclear if the changes are covered. Potential gaps include:
    - Tests for edge cases like empty lists, missing keys, non-dictionary items, or invalid data types.
    - Regression tests to ensure the function still works for valid inputs (e.g., lists of dictionaries or objects).
    - The added changes improve robustness, but without tests, there's no assurance that they don't introduce new bugs. Unit tests should cover various scenarios to achieve good code coverage.

---

### 3. **Suggestions for Improvement**

Here are specific, actionable recommendations to enhance the code:

- **Code structure and organization**:
  - Add type hints to clarify expected input and output types, e.g., `def calculate_total(items: list[dict]) -&amp;amp;amp;gt; int`. This would make the function more explicit about assuming dictionary-based items and help catch type errors early with tools like mypy.
  - Consider refactoring the function to handle both dictionary and object-based inputs if needed, e.g., by checking the type of each item and using `getattr(item, 'price', 0)` for objects or `item.get('price', 0)` for dicts. This would make the function more flexible.

- **Error handling improvements**:
  - Add input validation at the start of the function, e.g., check if `items` is iterable and raise a custom error (e.g., `ValueError`) if not. Also, handle potential non-numeric `price` values by adding a type check or conversion, e.g., `total += float(item.get('price', 0))` to avoid `TypeError`.
  - For better robustness, use a try-except block around the loop to catch unexpected errors and log them or return a default value, but avoid overusing exceptions for performance reasons.

- **Performance optimizations**:
  - Replace the manual loop with a more concise and potentially faster approach using `sum()`, e.g., `return sum(item.get('price', 0) for item in items or [])`. This handles the empty case automatically and reduces code lines. The `or []` ensures that if `items` is None, it defaults to an empty list, avoiding errors.

- **Better naming and documentation**:
  - Add a docstring to the function describing its purpose, parameters, return value, and any assumptions (e.g., that items are dictionaries). Example:
    ```
    def calculate_total(items):
        """
        Calculate the total price from a list of items.

        Args:
            items (list): A list of dictionaries, each containing a 'price' key.

        Returns:
            int: The sum of all prices, or 0 if no items are provided.

        Raises:
            ValueError: If items is not iterable or contains invalid data.
        """
        # ... rest of the code
    ```
  - Improve variable naming if needed; `total` is fine, but `items` could be more descriptive (e.g., `item_list`) if the context isn't clear.

- **Refactoring opportunities**:
  - Simplify the code using `sum()` as mentioned earlier. If this function is part of a larger module, consider extracting price retrieval into a separate helper function (e.g., `get_price(item)`) to handle different data types, improving reusability and reducing duplication.

---

### 4. **Positive Aspects**

The changes demonstrate good engineering practices and improve the code's reliability:

- **Good patterns and practices**: Adding the empty list check and using `dict.get()` with a default value is a solid example of defensive programming. It prevents common errors like `AttributeError` or index errors, making the function more robust without overcomplicating the logic.
- **Clear, readable code**: The updated logic is straightforward and easy to follow. The changes maintain simplicity while addressing potential issues, which enhances maintainability.
- **Proper error handling**: The use of `get('price', 0)` elegantly handles missing keys, reducing the need for explicit try-except blocks and improving code flow.
- **Well-structured logic**: The function remains concise and focused on a single responsibility (summing prices), adhering to the single-responsibility principle.

Overall, the diff shows thoughtful improvements that make the code more resilient to real-world inputs.

---

### 5. **Risk Assessment**

- **High Risk**: None identified. The changes are localized and don't introduce breaking changes or security threats, but improper handling of data types could lead to runtime errors in certain scenarios.
- **Medium Risk**: Potential type mismatches (e.g., if `items` elements are not dictionaries) could cause errors or incorrect results. Additionally, lack of testing might lead to undetected bugs in production.
- **Low Risk**: Minor issues like missing documentation or code style inconsistencies, which don't affect functionality but could impact long-term maintainability.

---

### Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential type mismatch for `items` elements | 🟡 Important | Assumes items are dictionaries; could raise errors if elements are custom objects or other types. | Add type hints and input validation, e.g., check if item is dict or use a flexible getter function. |
| Missing error handling for non-numeric prices or non-iterable inputs | 🟡 Important | Could raise `TypeError` if `price` is not a number or if `items` is not iterable. | Add type checks or use safe conversions, e.g., `float(item.get('price', 0))`, and validate `items` at the start. |
| Lack of documentation and docstrings | 🟢 Minor | No explanation of function behavior, parameters, or assumptions, reducing readability. | Add a descriptive docstring and consider inline comments for complex logic. |
| Testing gaps for edge cases | 🟡 Important | No visible tests for empty lists, missing keys, or invalid inputs, risking undetected regressions. | Implement unit tests covering various scenarios, e.g., using pytest with cases for empty, valid, and invalid inputs. |
| Opportunity for code simplification | 🟢 Minor | Manual loop could be replaced with `sum()` for conciseness and performance. | Refactor to use `sum(item.get('price', 0) for item in items or [])` to handle edges automatically. |

This review provides a balanced, actionable critique to help refine the code. If you have additional context (e.g., the rest of the codebase or testing framework), I can refine this further!

            ---
            *Generated by grok-3-mini-fast-latest via MCP Code Review Tool*
        &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/live_test_codereview/summary_20250601_091219.json"&amp;amp;gt;{
  "timestamp": "20250601_091219",
  "input_file": "test_diff.md",
  "total_models": 4,
  "successful_reviews": 4,
  "failed_reviews": 0,
  "output_files": [
    "gemini-2.0-flash_20250601_091219.md",
    "o3-mini-2025-01-31_20250601_091219.md",
    "grok-3-mini-fast-latest_20250601_091219.md",
    "claude-opus-4-20250514_20250601_091219.md"
  ]
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/live_test_codereview/o3-mini-2025-01-31_20250601_091219.md"&amp;amp;gt;
            # Code Review - o3-mini-2025-01-31

            **Model**: o3-mini-2025-01-31
            **Timestamp**: 2025-06-01T09:11:23.235642
            **Duration**: 9.24 seconds

            ---

            Below is a comprehensive review of the code diff:

──────────────────────────────
1. Overall Assessment
──────────────────────────────
• Summary of Changes:
 – An early exit is added to handle the case where the items list is empty (returning 0).
 – Instead of directly accessing an attribute (item.price), the code now uses the dictionary “get” method (item.get('price', 0)) to retrieve the price value.
 – This indicates a shift in expectation from an object with a price attribute to a dictionary-like object where "price" is a key.
  
• Impact on the Codebase:
 – The changes are localized to the calculate_total function in test.py.
 – The modifications improve robustness when items is empty and when an item does not contain a "price" key.
 – It may affect other parts of the system if they pass objects with a price attribute rather than dictionaries; integration testing is advised.
  
• Alignment with Best Practices:
 – Handling an empty list immediately is a good practice to avoid unnecessary computation.
 – Using item.get with a default value enhances fault tolerance for missing keys.
 – Code readability is maintained, though further documentation could be added.

──────────────────────────────
2. Issues Found
──────────────────────────────
• Security Vulnerabilities:
 – No apparent security concerns such as injection risks or data exposures. The function’s scope is narrow and does not involve external inputs.

• Bugs and Logic Errors:
 – Type Expectations: Changing from attribute access to dictionary access may break callers expecting objects with attributes. Confirm that all items are dictionaries.
 – Implicit Assumptions: The code assumes every item is a dict-like object. There’s no type-checking, so if an unexpected type is encountered, it might raise an AttributeError or TypeError.
 – Early Exit:
  • The early return for empty items is clear, but if items is None (i.e., not provided), it might lead to unexpected None errors unless the caller guarantees a list.

• Performance Issues:
 – The function’s algorithm is simple and works in O(n) time. There are no performance bottlenecks even for larger lists.
 – Memory usage is minimal.

• Code Quality Problems:
 – Readability is high; however, a brief comment explaining the change in expected item format (from object to dict) could help future maintainers.
 

            ---
            *Generated by o3-mini-2025-01-31 via MCP Code Review Tool*
        &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/live_test_codereview/claude-opus-4-20250514_20250601_091219.md"&amp;amp;gt;
            # Code Review - claude-opus-4-20250514

            **Model**: claude-opus-4-20250514
            **Timestamp**: 2025-06-01T09:11:50.195387
            **Duration**: 29.30 seconds

            ---

            ## Code Review: calculate_total Function Enhancement

### 1. **Overall Assessment**

**Summary of Changes:**
- Added null/empty check for the `items` parameter
- Changed from attribute access (`item.price`) to dictionary access with default value (`item.get('price', 0)`)

**Impact:**
- Improves robustness by handling edge cases (empty input and missing price fields)
- Changes the expected data structure from objects with price attributes to dictionaries
- Prevents potential AttributeError and KeyError exceptions

**Best Practices Alignment:**
- ✅ Defensive programming with input validation
- ✅ Graceful handling of missing data
- ⚠️ Inconsistent data structure assumption (mixing objects and dictionaries)

### 2. **Issues Found**

**Code Quality Problems:**
1. **Inconsistent Data Model**: The change from `item.price` to `item.get('price', 0)` suggests a shift from objects to dictionaries, but this isn't documented or type-hinted
2. **Silent Failure**: Items without prices are silently treated as having price 0, which might mask data quality issues
3. **Missing Type Hints**: No type annotations to clarify expected input/output types
4. **No Documentation**: Function lacks docstring explaining purpose, parameters, and return value

**Potential Logic Issues:**
1. **Type Assumption**: Code assumes items are dictionaries but doesn't validate this
2. **Numeric Type Assumption**: No validation that 'price' values are numeric

### 3. **Suggestions for Improvement**

```python
from typing import List, Dict, Union, Optional
import logging

def calculate_total(items: Optional[List[Dict[str, Union[int, float]]]]) -&amp;amp;amp;gt; Union[int, float]:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dictionaries with 'price' key, or None/empty list
        
    Returns:
        Total price as int or float. Returns 0 for empty/None input.
        
    Raises:
        TypeError: If price value is not numeric
        ValueError: If price value is negative
    """
    if not items:
        return 0
    
    total = 0
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            raise TypeError(f"Item at index {idx} is not a dictionary")
            
        price = item.get('price', None)
        if price is None:
            logging.warning(f"Item at index {idx} has no price field")
            continue
            
        if not isinstance(price, (int, float)):
            raise TypeError(f"Price for item at index {idx} is not numeric: {price}")
            
        if price &amp;amp;amp;lt; 0:
            raise ValueError(f"Price for item at index {idx} is negative: {price}")
            
        total += price
        
    return total
```

**Alternative approach for mixed data types:**
```python
def calculate_total(items: Optional[List[Union[Dict, object]]]) -&amp;amp;amp;gt; Union[int, float]:
    """Handle both dictionary and object inputs."""
    if not items:
        return 0
    
    total = 0
    for item in items:
        if isinstance(item, dict):
            price = item.get('price', 0)
        elif hasattr(item, 'price'):
            price = item.price
        else:
            price = 0
            
        total += price
        
    return total
```

### 4. **Positive Aspects**

- ✅ **Good defensive programming**: Adding the empty check prevents errors on None/empty inputs
- ✅ **Graceful degradation**: Using `.get()` with default prevents KeyError exceptions
- ✅ **Maintains backward compatibility**: Function signature remains unchanged
- ✅ **Simple and readable**: The logic remains straightforward and easy to understand

### 5. **Risk Assessment**

**Medium Risk** 🟡
- **Breaking Change**: Switching from attribute to dictionary access could break existing code that passes objects
- **Data Quality**: Silent handling of missing prices could hide data issues in production
- **Type Safety**: Lack of validation could lead to runtime errors with unexpected data types

**Mitigation Strategies:**
1. Add comprehensive unit tests covering all data type scenarios
2. Implement logging for items with missing prices
3

            ---
            *Generated by claude-opus-4-20250514 via MCP Code Review Tool*
        &amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="reviewer/live_test_codereview/gemini-2.0-flash_20250601_091219.md"&amp;amp;gt;
            # Code Review - gemini-2.0-flash

            **Model**: gemini-2.0-flash
            **Timestamp**: 2025-06-01T09:11:16.427900
            **Duration**: 6.81 seconds

            ---

            ## Test Code Review

### 1. **Overall Assessment**

The diff introduces two key changes to the `calculate_total` function:

1.  A check for an empty `items` list, returning 0 in that case.
2.  A change in how the price is accessed: from `item.price` to `item.get('price', 0)`.

The first change handles a potential edge case, preventing errors when the input is empty. The second change makes the code more robust by handling cases where an item might not have a `price` attribute directly, but rather stores it as a dictionary key. These changes enhance the robustness and reliability of the function. The scope is relatively small, impacting only the `calculate_total` function. These changes generally align with best practices for defensive programming and error handling.

### 2. **Issues Found**

*   **Potential Type Error (🟡)**:  While `item.get('price', 0)` handles the absence of the 'price' key, it assumes the value associated with the 'price' key (if it exists) will be a number that can be added to `total`. If `item['price']` exists but is a string (e.g., "unknown"), a `TypeError` would still occur.
*   **Limited Input Validation (🟡)**: The code assumes that each `item` in `items` is a dictionary. It doesn't validate that `items` is even a list, or that each element within it is a dictionary-like object.

### 3. **Suggestions for Improvement**

*   **Type Validation and Error Handling (Important):**  Implement more robust type validation, either with `isinstance` checks or using a try-except block:

    ```python
    def calculate_total(items):
        if not items:
            return 0

        total = 0
        for item in items:
            try:
                price = item.get('price', 0)
                if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
                total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.
        return total
    ```

*   **Consider a dedicated Item class (Minor):** If the structure of `items` is fixed (i.e., always containing dictionaries with a 'price'), consider defining a dedicated `Item` class with a `price` attribute. This would improve code readability and maintainability.

*   **Add input validation (Minor):** Assert that `items` is a list and each element is either a dictionary or an object with a `get` method.

    ```python
    def calculate_total(items):
        if not isinstance(items, list):
            raise TypeError("items must be a list")

        if not items:
            return 0

        total = 0
        for item in items:
            if not hasattr(item, 'get') and not isinstance(item, dict):
                raise TypeError("Each item must be a dictionary or an object with a 'get' method")
              if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
              total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.

        return total
    ```

### 4. **Positive Aspects**

*   **Handles Empty Input (🟢):** The addition of the `if not items` check is a good practice for handling edge cases and preventing potential errors.
*   **Robust Price Access (🟢):** Using `item.get('price', 0)` is a good way to handle cases where the `price` attribute may not be directly available, providing a default value of 0 if the key is missing.

### 5. **Risk Assessment**

*   **Medium Risk**:  The lack of explicit type validation for the `price` can still lead to runtime errors. Implementing the suggested improvement involving the `try-except` block significantly mitigates this risk.

## Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential Type Error | 🟡 |  If `item

            ---
            *Generated by gemini-2.0-flash via MCP Code Review Tool*
        
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="migrations/20250810_02_add-github-url-to-prompt-history.sql"&amp;amp;gt;-- Add github_url column to prompt_history table to track project association in historical records
-- depends: 20250810_01_add-projects-table

-- Add github_url column to prompt_history table
ALTER TABLE prompt_history ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Add index for efficient queries by github_url
CREATE INDEX IF NOT EXISTS idx_prompt_history_github_url ON prompt_history(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_history_github_url;
-- ALTER TABLE prompt_history DROP COLUMN github_url;&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="migrations/20250727_01_create-prompt-tables.sql"&amp;amp;gt;-- Create prompt tables for prompt storage, versioning, and metrics
-- depends: 

-- Current prompt table
CREATE TABLE IF NOT EXISTS prompt (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    data JSONB NOT NULL,
    version INTEGER DEFAULT 1,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Historical versions table
CREATE TABLE IF NOT EXISTS prompt_history (
    id TEXT,
    version INTEGER,
    data JSONB NOT NULL,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    change_summary TEXT,
    PRIMARY KEY (id, version)
);

-- Metrics time-series table (optimized for prompt tracking)
CREATE TABLE IF NOT EXISTS prompt_metrics (
    prompt_id TEXT,
    version INTEGER,
    metric_name TEXT,
    step INTEGER,
    value REAL,
    timestamp TIMESTAMP,
    PRIMARY KEY (prompt_id, version, metric_name, step)
);

-- Performance-critical indexes
CREATE INDEX IF NOT EXISTS idx_prompt_hash ON prompt(content_hash);
CREATE INDEX IF NOT EXISTS idx_prompt_updated ON prompt(updated_at);
CREATE INDEX IF NOT EXISTS idx_prompt_history_created ON prompt_history(created_at);
CREATE INDEX IF NOT EXISTS idx_prompt_metrics_time ON prompt_metrics(timestamp);

-- Expression indexes on JSONB fields for common queries
CREATE INDEX IF NOT EXISTS idx_prompt_status ON prompt(data -&amp;amp;amp;gt;&amp;amp;amp;gt; '$.status');
CREATE INDEX IF NOT EXISTS idx_prompt_type ON prompt(data -&amp;amp;amp;gt;&amp;amp;amp;gt; '$.type');&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="migrations/20250810_01_add-projects-table.sql"&amp;amp;gt;-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/test_database.py"&amp;amp;gt;import pytest
import sqlite3
import os

from repository.database import SQLite3Database


@pytest.fixture
def test_db():
    """Create a test database instance"""
    test_db_path = "test_collect.db"
    db = SQLite3Database(db_path=test_db_path)
    yield db
    # Cleanup
    if os.path.exists(test_db_path):
        os.remove(test_db_path)


def test_database_connection_basic(test_db):
    """Test basic database connection establishment"""
    with test_db.get_connection() as conn:
        assert conn is not None
        assert isinstance(conn, sqlite3.Connection)
        # Test basic query
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_connection_read_only(test_db):
    """Test read-only database connection"""
    with test_db.get_connection(read_only=True) as conn:
        assert conn is not None
        # Should be able to read
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_row_factory(test_db):
    """Test that Row factory is properly configured"""
    with test_db.get_connection() as conn:
        cursor = conn.execute("SELECT 1 as test_col")
        row = cursor.fetchone()
        # Should be able to access by column name
        assert row["test_col"] == 1


def test_database_pragma_settings(test_db):
    """Test that PRAGMA settings are applied correctly"""
    with test_db.get_connection() as conn:
        # Check foreign keys
        cursor = conn.execute("PRAGMA foreign_keys")
        assert cursor.fetchone()[0] == 1

        # Check journal mode
        cursor = conn.execute("PRAGMA journal_mode")
        assert cursor.fetchone()[0] == "wal"

        # Check synchronous mode
        cursor = conn.execute("PRAGMA synchronous")
        assert cursor.fetchone()[0] == 1  # NORMAL = 1


def test_database_context_manager_cleanup():
    """Test that database connections are properly closed"""
    test_db_path = "test_cleanup.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with db.get_connection() as conn:
            conn.execute("SELECT 1")

        # Connection should be closed after context manager exits
        # We can't directly test if it's closed, but we can verify
        # that we can create a new connection successfully
        with db.get_connection() as conn:
            assert conn is not None

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)


def test_database_error_handling():
    """Test error handling and rollback"""
    test_db_path = "test_error.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with pytest.raises(sqlite3.Error):
            with db.get_connection() as conn:
                # This should cause an error
                conn.execute("INVALID SQL STATEMENT")

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/database.py"&amp;amp;gt;import sqlite3

from contextlib import contextmanager
from typing import Generator


class SQLite3Database:
    def __init__(self, db_path: str = "../data/collect.db"):
        self.db_path = db_path

    # Decorator that converts this generator function into a context manager
    @contextmanager
    def get_connection(
        self, read_only: bool = False
    ) -&amp;amp;amp;gt; Generator[sqlite3.Connection, None, None]:
        """Context manager for database connections"""
        # Setup phase: runs when entering 'with' block
        # Enable PARSE_DECLTYPES to use our custom datetime converters
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.row_factory = sqlite3.Row  # enables column access by name

        # Connection optimizations
        conn.execute("PRAGMA foreign_keys = ON")  # enables foreign key support
        if not read_only:
            # enables better concurrency
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL")  # Faster writes
        conn.execute("PRAGMA cache_size = -64000")  # 64MB cache
        # Use memory for temp tables
        conn.execute("PRAGMA temp_store = MEMORY")
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB memory-mapped I/O

        try:
            yield conn  # Pauses here, returns conn to 'with' statement
            # Code inside 'with' block runs here
            if not read_only:
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            # Cleanup phase: always runs when exiting 'with' block
            conn.close()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/test_prompt_service.py"&amp;amp;gt;import pytest
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import Prompt, PromptType, PromptPlanStatus, CmdCategory
from config import Config


@pytest.fixture
def prompt_service():
    """
    ## How It Works

    1. **`with db.get_connection() as conn:`**
       - Opens a database connection using a context manager
       - The `as conn` assigns the connection to the variable
       - When the `with` block exits, `conn.close()` is automatically called

    2. **`cmd_service = CmdsService(conn)`**
       - Creates the service object with the database connection
       - The service can now execute database operations

    3. **`yield cmd_service`**
       - This is pytest fixture syntax that provides the service to the test
       - `yield` pauses execution here while the test runs
       - After the test completes, execution resumes after the `yield`

    4. **Automatic cleanup**
       - When the test finishes, the `with` block exits
       - Database connection is automatically closed
       - Resources are freed

    This pattern ensures **deterministic cleanup** -
    the database connection will always be properly closed regardless of
    whether the test passes or fails.
    """
    config = Config()
    db = SQLite3Database(config.db_path)
    with db.get_connection() as conn:
        cmd_service = PromptService(conn, config)

        yield cmd_service


def test_check_dirs(prompt_service: PromptService):
    result = prompt_service.cmd_check_dirs()
    assert result is True


def test_load_cmds_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_cmds_from_disk()
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def test_load_plans_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_plans_from_disk()

    print(f"\nTotal plans loaded: {len(load_results.loaded_prompts)}")
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def create_test_prompts(prompt_service: PromptService) -&amp;amp;amp;gt; List[Prompt]:
    prompt_content = """
    this is a test prompt for testing database persistence... blah blah
    """

    def new_cmd_prompt(prompt_content: str) -&amp;amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return [new_cmd_prompt(prompt_content), new_plan_prompt(prompt_content)]


def test_save_prompt_in_db(prompt_service: PromptService):
    # create test cmd and plan prompt types
    pls = create_test_prompts(prompt_service)
    cmd_prompt = pls[0]
    plan_prompt = pls[1]

    try:
        # save test prompts in sqlite and verify success
        cmd_result = prompt_service.save_prompt_in_db(cmd_prompt)
        print(f"cmd_result: {cmd_result}")
        assert cmd_result.success is not False

        plan_result = prompt_service.save_prompt_in_db(plan_prompt)
        print(f"plan_result: {plan_result}")
        assert plan_result.success is not False

        # retrieve the saved test prompts from sqlite and verify they
        # match the original test cmd and plan prompts
        print(f"Retrieving cmd prompt with id: {cmd_prompt.id}")
        retrieved_cmd = prompt_service.get_prompt_by_id(cmd_prompt.id)
        print(f"Retrieved cmd: {retrieved_cmd}")
        assert retrieved_cmd is not None

        retrieved_plan = prompt_service.get_prompt_by_id(plan_prompt.id)
        assert retrieved_plan is not None

        # update prompt and increment the version
        updated_text = cmd_prompt.data.content + "UPDATED TEXT"
        cmd_prompt.data.content = updated_text

        update_result = prompt_service.update_prompt_in_db(cmd_prompt)
        assert update_result.success is True

        # retrieve the updated prompt again from the prompt table and
        # validate the changes were persisted/updated
        retrieved_prompt = prompt_service.get_prompt_by_id(cmd_prompt.id)
        assert retrieved_prompt.data.content == updated_text

        # retrieve the prompt by name
        # and validate correct prompt retrieval
        retrieved_prompt_by_name = prompt_service.get_prompt_by_name(cmd_prompt.name)
        assert retrieved_prompt_by_name is not None
        assert retrieved_prompt_by_name.id == cmd_prompt.id

    finally:
        # Clean up test data - this will ALWAYS run, even if test fails
        print("\nCleaning up test prompts...")

        cmd_cleanup = delete_prompt_completely(prompt_service, cmd_prompt.id)
        print(f"CMD cleanup result: {cmd_cleanup}")

        plan_cleanup = delete_prompt_completely(prompt_service, plan_prompt.id)
        print(f"PLAN cleanup result: {plan_cleanup}")


def delete_prompt_completely(prompt_service: PromptService, prompt_id: str):
    """
    DELETE a prompt from tables: prompt, prompt_history and prompt_metrics
    THIS IS FOR INTEGRATION TESTING ONLY - as production code should reserve
    history
    """
    cursor = prompt_service.conn.cursor()
    try:
        # start transaction
        cursor.execute("BEGIN TRANSACTION")

        # delete from prompt_history first (due to composite primary key)
        cursor.execute(
            """
                       DELETE FROM prompt_history
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_history_rows_deleted = cursor.rowcount

        # delete from prompt_metrics table if any exist
        cursor.execute(
            """
                       DELETE FROM prompt_metrics
                       WHERE prompt_id = ?
                       """,
            (prompt_id,),
        )
        prompt_metrics_rows_deleted = cursor.rowcount

        # delete from prompt table (we do this last)
        cursor.execute(
            """
                       DELETE FROM prompt
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_rows_deleted = cursor.rowcount

        prompt_service.conn.commit()
        return {
            "success": True,
            "prompt_rows": prompt_rows_deleted,
            "prompt_history_rows": prompt_history_rows_deleted,
            "prompt_metrics_rows": prompt_metrics_rows_deleted,
        }

    except Exception as e:
        prompt_service.conn.rollback()
        return {"success": False, "error": str(e)}


def test_prompt_loading(prompt_service: PromptService):
    cmds = prompt_service.load_cmds_from_disk()
    print(f"\nTotal commands loaded: {len(cmds.loaded_prompts)}")
    assert len(cmds.errors) == 0

    plans = prompt_service.load_plans_from_disk()
    print(f"\nTotal plans loaded: {len(plans.loaded_prompts)}")
    assert len(plans.errors) == 0

    prompts = cmds.loaded_prompts + plans.loaded_prompts

    results = prompt_service.bulk_save_in_db(prompts)

    bad_results = [result for result in results if not result.success]
    good_results = [result for result in results if result.success]

    print(f"\nGood Result count: {len(good_results)}")
    print(f"\nBad Result count: {len(bad_results)}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/prompt_models.py"&amp;amp;gt;from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
from typing import Optional, List
from config import Config


class PromptPlanStatus(str, Enum):
    """Plan status types"""

    DRAFT = "draft"
    APPROVED = "approved"
    COMPLETED = "completed"


class PromptType(str, Enum):
    """Prompt types"""

    PLAN = "plan"
    CMD = "cmd"


def create_cmd_category_enum():
    """Create CmdCategory enum dynamically from config"""
    try:
        config = Config()
        subdirs = config.command_subdirs
    except Exception:
        # Fallback to default subdirs if config fails
        subdirs = ["archive", "go", "js", "mcp", "python", "tools"]

    # Build enum members dictionary
    members = {}
    for subdir in subdirs:
        members[subdir.upper()] = subdir

    # Always include UNCATEGORIZED as fallback
    members["UNCATEGORIZED"] = "uncategorized"

    # Create enum using the functional API with type=str for JSON serialization
    return Enum("CmdCategory", members, type=str)


# Create the enum instance
CmdCategory = create_cmd_category_enum()


class Project(BaseModel):
    github_url: str
    description: str
    created_at: datetime
    updated_at: datetime


class PromptData(BaseModel):
    """Structured data for prompt JSONB field"""

    type: PromptType
    status: PromptPlanStatus
    project: Optional[str]
    cmd_category: Optional[CmdCategory]
    content: str  # This is the prompt content, in markdown
    description: Optional[str] = None
    # using 'claude' or 'gemini' here to specify the dir it will write to
    # .claude/commands and .gemini/commands respectively
    tags: List[str] = Field(default_factory=list)


class Prompt(BaseModel):
    id: str
    name: str
    github_url: Optional[str]
    data: PromptData  # Structured JSONB data
    version: int
    content_hash: str
    created_at: datetime
    updated_at: datetime


class PromptCreate(BaseModel):
    id: str
    name: str
    data: PromptData
    content_hash: str
    version: Optional[int] = 1


class LoadError(BaseModel):
    filename: str
    error_message: str
    error_type: str


class PromptCreateResult(BaseModel):
    """Result of creating a new prompt"""

    success: bool
    prompt_id: str
    version: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptLoadResult(BaseModel):
    """Result of loading prompts into database"""

    loaded_prompts: List[Prompt]
    errors: Optional[List[LoadError]] = None


class PromptDeleteResult(BaseModel):
    success: bool
    prompt_id: str
    deleted: bool
    rows_affected: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptFlattenResult(BaseModel):
    """Result of flattening a prompt to disk"""

    success: bool
    prompt_id: str
    prompt_name: str
    file_path: str
    cmd_category: str
    error_message: Optional[str] = None
    error_type: Optional[str] = None
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/datetime_adapters.py"&amp;amp;gt;"""Custom datetime adapters for SQLite3 to avoid Python 3.12 deprecation warnings.

This module provides custom adapters and converters for datetime objects when
working with SQLite databases in Python 3.12+, replacing the deprecated default
adapters.
"""

import datetime
import sqlite3


def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format.

    Args:
        val: datetime.datetime object to adapt

    Returns:
        str: ISO 8601 formatted datetime string
    """
    return val.replace(tzinfo=None).isoformat()


def adapt_date_iso(val):
    """Adapt datetime.date to ISO 8601 date format.

    Args:
        val: datetime.date object to adapt

    Returns:
        str: ISO 8601 formatted date string
    """
    return val.isoformat()


def convert_datetime_iso(val):
    """Convert ISO 8601 datetime string to datetime.datetime object.

    Args:
        val: bytes object containing ISO 8601 datetime string

    Returns:
        datetime.datetime: Parsed datetime object
    """
    return datetime.datetime.fromisoformat(val.decode())


def convert_date_iso(val):
    """Convert ISO 8601 date string to datetime.date object.

    Args:
        val: bytes object containing ISO 8601 date string

    Returns:
        datetime.date: Parsed date object
    """
    return datetime.date.fromisoformat(val.decode())


def convert_timestamp(val):
    """Convert Unix timestamp to datetime.datetime object.

    Args:
        val: bytes object containing Unix timestamp

    Returns:
        datetime.datetime: Datetime object from timestamp
    """
    return datetime.datetime.fromtimestamp(int(val))


def register_adapters():
    """Register all custom datetime adapters and converters with sqlite3.

    This function should be called once at application startup to configure
    SQLite to use our custom datetime handling instead of the deprecated
    default handlers.
    """
    # Register adapters (Python -&amp;amp;amp;gt; SQLite)
    sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
    sqlite3.register_adapter(datetime.date, adapt_date_iso)

    # Register converters (SQLite -&amp;amp;amp;gt; Python)
    sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
    sqlite3.register_converter("DATETIME", convert_datetime_iso)
    sqlite3.register_converter("DATE", convert_date_iso)


# Automatically register adapters when module is imported
register_adapters()
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/prompt_service.py"&amp;amp;gt;import sqlite3
from pathlib import Path
import uuid
import json
from datetime import datetime, timezone
import hashlib
from typing import Optional, List, Tuple
from repository.prompt_models import (
    PromptLoadResult,
    LoadError,
    CmdCategory,
    PromptType,
    PromptPlanStatus,
    PromptData,
    Prompt,
    PromptCreateResult,
    PromptDeleteResult,
    PromptFlattenResult,
)
from config import Config


class PromptService:
    def __init__(self, conn: sqlite3.Connection, config: Config):
        self.conn = conn
        self.config = config
        self.plans_check_dirs()
        self.cmd_check_dirs()

    def plans_check_dirs(self) -&amp;amp;amp;gt; bool:
        """Check if all required plan directories exist, create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        # Required directories
        required_dirs = [
            plans_dir,
            plans_dir / "drafts",
            plans_dir / "approved",
            plans_dir / "completed",
        ]

        missing_dirs = []
        created_dirs = []

        for dir_path in required_dirs:
            if not dir_path.exists():
                missing_dirs.append(dir_path)

        if missing_dirs:
            print("📁 Creating missing plan directories:")
            for missing_dir in missing_dirs:
                try:
                    missing_dir.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(missing_dir)
                    print(
                        f"   ✅ Created: {
                            missing_dir.relative_to(project_dir)}"
                    )
                except Exception as e:
                    print(
                        f"   ❌ Failed to create {
                            missing_dir.relative_to(project_dir)}: {e}"
                    )
                    return False

            if created_dirs:
                print(
                    f"📁 Successfully created {
                        len(created_dirs)} directories"
                )
        else:
            print("✅ All required plan directories exist")

        return True

    def cmd_check_dirs(self) -&amp;amp;amp;gt; bool:
        """Check if all required command directories exist,
           create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        claude_dir = project_dir / ".claude"
        gemini_dir = project_dir / ".gemini"

        # Get subdirectories from config
        config = Config()
        subdirs = config.command_subdirs

        # Build required directories
        required_dirs = {
            "claude": [claude_dir / "commands"]
            + [claude_dir / "commands" / subdir for subdir in subdirs],
            "gemini": [gemini_dir / "commands"]
            + [gemini_dir / "commands" / subdir for subdir in subdirs],
        }

        # Check for missing directories by type
        missing_by_type = {"claude": [], "gemini": []}
        for dir_type, dirs in required_dirs.items():
            for dir_path in dirs:
                if not dir_path.exists():
                    missing_by_type[dir_type].append(dir_path)

        # Count total missing
        total_missing = sum(len(dirs) for dirs in missing_by_type.values())

        if total_missing == 0:
            print("✅ All required command directories exist")
            return True

        # Create missing directories
        print(f"📁 Creating {total_missing} missing command directories:")
        created_count = 0
        failed = False

        for dir_type, missing_dirs in missing_by_type.items():
            if missing_dirs:
                print(f"\n   {dir_type.title()} directories:")
                for missing_dir in missing_dirs:
                    try:
                        missing_dir.mkdir(parents=True, exist_ok=True)
                        created_count += 1
                        print(
                            f"   ✅ Created: {
                                missing_dir.relative_to(project_dir)}"
                        )
                    except Exception as e:
                        print(
                            f"   ❌ Failed to create {
                                missing_dir.relative_to(project_dir)}: {e}"
                        )
                        failed = True

        if created_count &amp;amp;amp;gt; 0:
            print(f"\n📁 Successfully created {created_count} directories")

        return not failed

    def _load_cmds_from_directory(
        self, cmds_dir: Path, source: str
    ) -&amp;amp;amp;gt; Tuple[List[Prompt], List[LoadError]]:
        """Load commands from a specific directory

        Args:
            cmds_dir: Path to the commands directory
            source: Source identifier ('claude' or 'gemini')

        Returns:
            Tuple of (prompts list, errors list)
        """
        prompts = []
        errors = []

        if not cmds_dir.exists():
            return prompts, errors

        # Loop through the files in cmds dir and load prompts first
        for file in cmds_dir.iterdir():
            if file.is_file():
                try:
                    # Check if filename adheres to naming rules
                    current_filename = file.name
                    if not self.check_filename(current_filename):
                        # Only rename files during explicit operations, not during loading
                        # Skip file renaming when just loading/reading files
                        print(
                            f"⚠️  File {
                                current_filename} doesn't follow naming convention but will not be renamed during load operation"
                        )

                    prompt_content = file.read_text()
                    prompt = self.new_prompt_model(
                        prompt_content=prompt_content,
                        name=file.name,
                        prompt_type=PromptType.CMD,
                        cmd_category=CmdCategory.UNCATEGORIZED,
                        status=PromptPlanStatus.DRAFT,
                        tags=[source],  # Add source tag
                    )
                    prompts.append(prompt)

                except Exception as e:
                    errors.append(
                        LoadError(
                            filename=str(file),
                            error_message=str(e),
                            error_type=type(e).__name__,
                        )
                    )

        # Then cycle through the subdirs, create Prompt models and append
        for sub_dir in cmds_dir.iterdir():
            if sub_dir.is_dir():
                try:
                    cmd_category = CmdCategory(sub_dir.name.lower())

                    for file in sub_dir.iterdir():
                        try:
                            if file.is_file():
                                # Check if filename adheres to naming rules
                                current_filename = file.name
                                if not self.check_filename(current_filename):
                                    # Normalize the filename
                                    fixed_filename = self.normalize_filename(
                                        current_filename
                                    )

                                    # Create new file path with normalized name
                                    new_file_path = file.parent / fixed_filename

                                    # Rename the file on disk
                                    file.rename(new_file_path)

                                    # Update file reference to the new path
                                    file = new_file_path
                                    print(
                                        f"📝 Renamed: {current_filename} → {
                                            fixed_filename}"
                                    )

                                prompt_content = file.read_text()
                                prompt = self.new_prompt_model(
                                    prompt_content=prompt_content,
                                    name=file.name,
                                    prompt_type=PromptType.CMD,
                                    cmd_category=cmd_category,
                                    status=PromptPlanStatus.DRAFT,
                                    tags=[source],  # Add source tag
                                )
                                prompts.append(prompt)

                        except Exception as e:
                            errors.append(
                                LoadError(
                                    filename=str(file),
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )
                except ValueError:
                    # Skip directories that don't match valid CmdCategory values
                    continue

        return prompts, errors

    def load_cmds_from_disk(self) -&amp;amp;amp;gt; PromptLoadResult:
        """Load commands from both .claude and .gemini directories

        Returns:
            PromptLoadResult: Combined results from both directories
        """
        project_dir = Path(__file__).parent.parent
        claude_cmds_dir = project_dir / ".claude" / "commands"
        gemini_cmds_dir = project_dir / ".gemini" / "commands"

        all_prompts = []
        all_errors = []

        # Load from Claude directory
        claude_prompts, claude_errors = self._load_cmds_from_directory(
            claude_cmds_dir, "claude"
        )
        all_prompts.extend(claude_prompts)
        all_errors.extend(claude_errors)

        # Load from Gemini directory
        gemini_prompts, gemini_errors = self._load_cmds_from_directory(
            gemini_cmds_dir, "gemini"
        )
        all_prompts.extend(gemini_prompts)
        all_errors.extend(gemini_errors)

        return PromptLoadResult(
            loaded_prompts=all_prompts,
            errors=all_errors,
        )

    def load_plans_from_disk(self) -&amp;amp;amp;gt; PromptLoadResult:
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        status_mapping = {
            "drafts": PromptPlanStatus.DRAFT,
            "approved": PromptPlanStatus.APPROVED,
            "completed": PromptPlanStatus.COMPLETED,
        }

        prompts = []
        errors = []

        for subdir in plans_dir.iterdir():
            if subdir.is_dir() and subdir.name in status_mapping:
                cmd_category = None
                status = status_mapping[subdir.name]
                for file in subdir.iterdir():
                    try:
                        if file.is_file():
                            # Check if filename adheres to naming rules
                            current_filename = file.name
                            if not self.check_filename(current_filename):
                                # Normalize the filename
                                fixed_filename = self.normalize_filename(
                                    current_filename
                                )

                                # Create new file path with normalized name
                                new_file_path = file.parent / fixed_filename

                                # Rename the file on disk
                                file.rename(new_file_path)

                                # Update file reference to the new path
                                file = new_file_path
                                print(
                                    f"""📝 Renamed: {current_filename} → {
                                        fixed_filename}
                                      """
                                )

                            prompts.append(
                                self.new_prompt_model(
                                    prompt_content=file.read_text(),
                                    name=file.name,
                                    prompt_type=PromptType.PLAN,
                                    github_url=self.config.github_url,
                                    cmd_category=cmd_category,
                                    status=status,
                                    project=project_dir.name,
                                )
                            )

                    except Exception as e:
                        errors.append(
                            LoadError(
                                filename=str(file),
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

        return PromptLoadResult(loaded_prompts=prompts, errors=errors)

    def normalize_filename(self, filename: str) -&amp;amp;amp;gt; str:
        """Normalize filename to use underscores and ensure .md or .toml extension

        Args:
            filename: The original filename

        Returns:
            Normalized filename with underscores and .md or .toml extension
        """
        # Replace hyphens with underscores
        normalized = filename.replace("-", "_")

        # Check if it already has .md or .toml extension
        if normalized.endswith(".md") or normalized.endswith(".toml"):
            return normalized

        # If it has another extension, replace it with .md
        if "." in normalized:
            normalized = normalized.rsplit(".", 1)[0] + ".md"
        else:
            # No extension, add .md as default
            normalized = normalized + ".md"

        return normalized

    def check_filename(self, filename: str) -&amp;amp;amp;gt; bool:
        """Check if filename adheres to naming rules
        (underscores and .md or .toml extension)

        Args:
            filename: The filename to check

        Returns:
            bool: True if filename follows the rules, False otherwise
        """
        # Check if filename has .md or .toml extension
        if not (filename.endswith(".md") or filename.endswith(".toml")):
            return False

        # Check if filename contains hyphens (should use underscores)
        if "-" in filename:
            return False

        return True

    def new_prompt_model(
        self,
        prompt_content: str,
        name: str,
        prompt_type: PromptType,
        github_url: Optional[str] = None,
        cmd_category: Optional[CmdCategory] = None,
        status: PromptPlanStatus = PromptPlanStatus.DRAFT,
        project: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -&amp;amp;amp;gt; Prompt:
        if prompt_type == PromptType.CMD and not cmd_category:
            raise ValueError("CMD type prompts require a category")

        default_tags = []
        if cmd_category:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                default_tags.append(cmd_category)
            else:
                default_tags.append(cmd_category.value)
        default_tags.append(prompt_type.value)

        # Merge custom tags with default tags
        all_tags = default_tags + (tags if tags else [])

        prompt_data = PromptData(
            type=prompt_type,
            status=status,
            project=project,
            cmd_category=cmd_category,
            content=prompt_content,
            description=description,
            tags=all_tags,
        )

        content_hash = hashlib.sha256(prompt_content.encode("utf-8")).hexdigest()

        timestamp = datetime.now(timezone.utc)

        db_name = self.create_db_name(
            prompt_type=prompt_type,
            prompt_status=status,
            cmd_category=cmd_category,
            project_name=project,
            name=name,
        )

        prompt = Prompt(
            id=str(uuid.uuid4()),
            name=db_name,
            github_url=github_url,
            data=prompt_data,
            version=1,
            content_hash=content_hash,
            created_at=timestamp,
            updated_at=timestamp,
        )

        return prompt

    def create_db_name(
        self,
        prompt_type: PromptType,
        prompt_status: Optional[PromptPlanStatus],
        cmd_category: Optional[CmdCategory],
        project_name: Optional[str],
        name: str,
    ) -&amp;amp;amp;gt; str:
        # in the directory [project]/_docs/plans:
        # there are directories: draft, approved and completed
        # we model those as PromptPlanStatus -&amp;amp;amp;gt; see prompt_models.py
        if prompt_type == PromptType.PLAN:
            create_name = project_name + "_" + prompt_status.value + "_" + name
        if prompt_type == PromptType.CMD:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                create_name = cmd_category + "_" + name
            else:
                create_name = cmd_category.value + "_" + name

        return create_name

    def parse_db_name(self, db_name: str, prompt_type: PromptType) -&amp;amp;amp;gt; str:
        """Extract the original filename from the database name

        Args:
            db_name: The database name
            (e.g., 'collect-approved-update_function.md')
            prompt_type: The type of prompt(PLAN or CMD)

        Returns:
            The original filename(e.g., 'update_function.md')
        """
        # split the name to a list using '_' seperator
        ls = db_name.split("_")
        # rebuild filename from the list of split words
        filename = ""
        if prompt_type == PromptType.PLAN:
            # if prompt type is PLAN: then name will include the project
            # so we need to drop the first 2 words in the db_name
            # example: collect_completed_add_claude_sdk_processing.md
            # ls = [collect, completed, add, claude, sdk, processing.md]
            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

        if prompt_type == PromptType.CMD:
            # if prompt type is CMD: then name will only include the dir/type
            # so we only need to drop the first word in ls
            # example: tools_create_database.md
            # ls = [tools, create, database.md]
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

    def check_exists(self, name: str) -&amp;amp;amp;gt; Tuple[bool, str]:
        """Check if a prompt with the given name already exists

        Args:
            name: The prompt name to check

        Returns:
            Tuple[bool, str]: (exists, prompt_id)
            where exists is True if prompt exists,
            and prompt_id is the ID if found, empty string if not found
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT id FROM prompt WHERE name = ?", (name,))
        result = cursor.fetchone()

        if result:
            return (True, result["id"])  # Found: return True and the prompt ID
        else:
            # Not found: return False and empty string
            return (False, "")

    def save_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Initial prompt creation"
    ) -&amp;amp;amp;gt; PromptCreateResult:
        """Create a new prompt and initialize version history
        if the prompt doesn't exist, if it already exists then
        we call `update_prompt_in_db` with the update.

        Args:
            prompt: Prompt object to create
            change_summary: Description of this change
            (default: "Initial prompt creation")

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            # Validate prompt has required fields
            if not prompt.id or not prompt.name or not prompt.data:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id if prompt.id else "",
                    version=prompt.version if prompt.version else 1,
                    error_message="Prompt missing required fields",
                    error="ValidationError",
                )

            exists, prompt_id = self.check_exists(prompt.name)
            if exists:
                # get prompt from database using prompt_id from the version
                # retrieved from the database
                prompt_from_db = self.get_prompt_by_id(prompt_id)
                # if we don't have a prompt here then we return false
                if prompt_from_db is None:
                    return PromptCreateResult(
                        success=False,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"prompt retrieval failed for {
                            prompt_id}",
                        error="ValueError",
                    )

                # otherwise we have a prompt from the database call
                # and we need to compare hashes to see if there are changes
                if prompt.content_hash == prompt_from_db.content_hash:
                    # if they are the same then the version in the db is the
                    # same as the version on disk so we return success and
                    # do nothing else.
                    return PromptCreateResult(
                        success=True,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"""
                        prompt: {prompt.name} from disk is the same as db
                        """,
                        error="",
                    )

                else:
                    # if we get here then we have changes on disk that are more
                    # current than what is in the database

                    # IMPORTANT: We override the prompt.id here because the
                    # prompt exists already and we don't have a clean way of
                    # storing the uuid with the prompt on disk.
                    # When the prompt model is created from loading from disk,
                    # we DO generate a uuid for the model at that time just in
                    # case the prompt is newly generated from the disk and is
                    # not in the db
                    prompt.id = prompt_from_db.id

                    # Important to note that we will increment the version in
                    # `self.update_prompt_in_db`, we do not increment it here
                    return self.update_prompt_in_db(prompt)

            else:  # prompt doesn't exist in the database
                # if we make it here we have a new prompt and it
                # needs to be saved to the database for the first time
                prompt_jsonb = prompt.data.model_dump_json()

                # Create new cursor for this transaction
                cursor = self.conn.cursor()

                # insert prompt into prompt table
                cursor.execute(
                    """
                    INSERT INTO prompt(
                    id,
                    name,
                    data,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?,?)
                    """,
                    (
                        prompt.id,
                        prompt.name,
                        prompt_jsonb,
                        prompt.version,
                        prompt.content_hash,
                        prompt.created_at,
                        prompt.updated_at,
                        prompt.github_url,
                    ),
                )

                # insert initial version into prompt_history table
                cursor.execute(
                    """
                    INSERT INTO prompt_history(
                    id,
                    version,
                    data,
                    content_hash,
                    created_at,
                    archived_at,
                    change_summary,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                    """,
                    (
                        prompt.id,
                        prompt.version,
                        prompt_jsonb,
                        prompt.content_hash,
                        prompt.created_at,
                        datetime.now(timezone.utc),
                        change_summary,
                        prompt.github_url,
                    ),
                )
                self.conn.commit()

                return PromptCreateResult(
                    success=True, prompt_id=prompt.id, version=prompt.version
                )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def update_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Prompt updated from disk"
    ) -&amp;amp;amp;gt; PromptCreateResult:
        """Update an existing prompt and add to version history

        Args:
            prompt: Prompt object to update
            change_summary: Description of this change

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            cursor = self.conn.cursor()

            # first we get the existing prompt in the database
            current_prompt = self.get_prompt_by_id(prompt.id)
            if not current_prompt:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id,
                    version=prompt.version,
                    error_message=f"Prompt w id {prompt.id} not found",
                    error="NotFoundError",
                )
            # then we increment the version
            prompt.version = current_prompt.version + 1

            # we need to recalculate the hash for the udpated prompt
            # so we can properly compare for changes
            prompt.content_hash = hashlib.sha256(
                prompt.data.content.encode("utf-8")
            ).hexdigest()

            # process the PromptData model to to json
            prompt_jsonb = prompt.data.model_dump_json()

            # then we update the `updated_at` timestamp
            prompt.updated_at = datetime.now(timezone.utc)

            # Update prompt table
            # NOTE: when writing the the jsonb field `data` we use jsonb
            # when reading we use `json(data)`
            cursor.execute(
                """
                UPDATE prompt
                SET name = ?,
                    data = jsonb(?),
                    version = ?,
                    content_hash = ?,
                    updated_at = ?,
                    github_url = ?
                WHERE id = ?
                """,
                (
                    prompt.name,
                    prompt_jsonb,
                    prompt.version,
                    prompt.content_hash,
                    prompt.updated_at,
                    prompt.github_url,
                    prompt.id,
                ),
            )

            # Insert into the updated prompt into prompt_history
            cursor.execute(
                """
                INSERT INTO prompt_history(
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                """,
                (
                    prompt.id,
                    prompt.version,
                    prompt_jsonb,
                    prompt.content_hash,
                    prompt.created_at,
                    datetime.now(timezone.utc),
                    change_summary,
                    prompt.github_url,
                ),
            )

            self.conn.commit()

            return PromptCreateResult(
                success=True, prompt_id=prompt.id, version=prompt.version
            )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def get_prompt_by_id(self, prompt_id: str) -&amp;amp;amp;gt; Optional[Prompt]:
        """Get a prompt by its ID from the database

        Args:
            prompt_id: The ID of the prompt to retrieve

        Returns:
            Optional[Prompt]: The prompt if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE id = ?
            """,
            (prompt_id,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        # Parse the JSONB data back to PromptData
        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        # Create and return the Prompt object
        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def get_prompt_by_name(self, prompt_name: str) -&amp;amp;amp;gt; Optional[Prompt]:
        """
        Get a prompt by name from the database

        Args:
            prompt_name: The name of the prompt to retrieve.
            (should be unique)
        Returns:
            Optional[Prompt]: The prompt if found by name or None otherwise
        """

        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE name = ?
            """,
            (prompt_name,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def delete_prompt_by_id(self, prompt_id: str) -&amp;amp;amp;gt; PromptDeleteResult:
        cursor = self.conn.cursor()
        try:
            # archive final state in prompt_history table before deletion
            # we will not be deleting the version history of the prompt
            cursor.execute(
                """
                INSERT INTO prompt_history (
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                SELECT id, version, data, content_hash, created_at, ?, ?, github_url
                FROM prompt WHERE id = ?
            """,
                (datetime.now(timezone.utc), "DELETED - Final Version", prompt_id),
            )

            # Delete only from the prompt table
            cursor.execute("DELETE FROM prompt WHERE id = ?", (prompt_id,))
            deleted_row_count = cursor.rowcount

            self.conn.commit()

            return PromptDeleteResult(
                success=True,
                prompt_id=prompt_id,
                deleted=True,
                rows_affected=deleted_row_count,
            )

        except Exception as e:
            self.conn.rollback()
            return PromptDeleteResult(
                success=False,
                prompt_id=prompt_id,
                deleted=False,
                rows_affected=0,
                error_message=str(e),
                error_type=type(e).__name__,
            )

    def bulk_save_in_db(self, prompts: List[Prompt]) -&amp;amp;amp;gt; List[PromptCreateResult]:
        """
        Bulk load/save prompts into the database

        Args:
            plans: List of Plan objects to load into database

        Returns:
            PlanLoadResult: Summary of the loading operation
        """

        return [self.save_prompt_in_db(prompt) for prompt in prompts]

    def flatten_cmds_to_disk(self) -&amp;amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all cmd_category prompts from database to disk directories

        Queries all CMD type prompts from database and writes them to:
        - .claude/commands/{category}/{filename}
        - .gemini/commands/{category}/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure command directories exist
            if not self.cmd_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create command directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all CMD type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;amp;gt;&amp;amp;amp;gt; '$.type' = 'cmd'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No CMD prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    # `**` unpacks the dictionary into key words for pydantic
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.CMD)

                    # Get category, handle None/uncategorized case
                    if prompt.data.cmd_category:
                        if isinstance(prompt.data.cmd_category, str):
                            category = prompt.data.cmd_category
                        else:
                            category = prompt.data.cmd_category.value
                    else:
                        category = "uncategorized"

                    # Determine target directory based on tags
                    target_dirs = []
                    if prompt.data.tags:
                        if "claude" in prompt.data.tags:
                            target_dirs.append("claude")
                        if "gemini" in prompt.data.tags:
                            target_dirs.append("gemini")

                    # If no source tags found, skip this prompt
                    if not target_dirs:
                        errmsg = "No source tag (claude/gemini) found in tags"
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category=category,
                                error_message=errmsg,
                                error_type="MissingSourceTag",
                            )
                        )
                        continue

                    # Write to appropriate directories based on source tags
                    for target_dir in target_dirs:
                        try:
                            target_path = (
                                project_dir
                                / f".{target_dir}"
                                / "commands"
                                / category
                                / filename
                            )

                            # Ensure parent directory exists
                            target_path.parent.mkdir(parents=True, exist_ok=True)

                            # Write content to file
                            target_path.write_text(
                                prompt.data.content, encoding="utf-8"
                            )

                            results.append(
                                PromptFlattenResult(
                                    success=True,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=str(target_path),
                                    cmd_category=category,
                                    error_message="",
                                    error_type="",
                                )
                            )

                        except Exception as e:
                            results.append(
                                PromptFlattenResult(
                                    success=False,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=(
                                        str(target_path)
                                        if "target_path" in locals()
                                        else ""
                                    ),
                                    cmd_category=category,
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results

    def flatten_plans_to_disk(self) -&amp;amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all plan prompts from database to disk directories

        Queries all PLAN type prompts from database and writes them to:
        - _docs/plans/drafts/{filename}
        - _docs/plans/approved/{filename}
        - _docs/plans/completed/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure plan directories exist
            if not self.plans_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create plan directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all PLAN type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;amp;gt;&amp;amp;amp;gt; '$.type' = 'plan'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No PLAN prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            # Status to directory mapping
            status_dir_mapping = {
                PromptPlanStatus.DRAFT.value: "drafts",
                PromptPlanStatus.APPROVED.value: "approved",
                PromptPlanStatus.COMPLETED.value: "completed",
            }

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Validate project name for PLAN type prompts
                    if not prompt.data.project:
                        # PLAN type must have a project name
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category="",
                                error_message="PLAN type prompt missing required project name",
                                error_type="MissingProjectError",
                            )
                        )
                        continue

                    # TODO: update this to use coordinate the github_url
                    if prompt.data.project != project_dir.name:
                        # Skip this prompt - it belongs to a different project
                        continue

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.PLAN)

                    # Get status directory
                    status_value = (
                        prompt.data.status.value
                        if hasattr(prompt.data.status, "value")
                        else str(prompt.data.status)
                    )
                    status_dir = status_dir_mapping.get(status_value, "drafts")

                    # Write to appropriate status directory
                    try:
                        target_path = (
                            project_dir / "_docs" / "plans" / status_dir / filename
                        )

                        # Ensure parent directory exists
                        target_path.parent.mkdir(parents=True, exist_ok=True)

                        # Write content to file
                        target_path.write_text(prompt.data.content, encoding="utf-8")

                        results.append(
                            PromptFlattenResult(
                                success=True,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=str(target_path),
                                cmd_category=status_dir,
                                error_message="",
                                error_type="",
                            )
                        )

                    except Exception as e:
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=(
                                    str(target_path)
                                    if "target_path" in locals()
                                    else ""
                                ),
                                cmd_category=status_dir,
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="repository/test_datetime_adapters.py"&amp;amp;gt;"""Test the custom datetime adapters for SQLite3 compatibility."""

import pytest
import warnings
from datetime import datetime, date
from repository.database import SQLite3Database
from repository import datetime_adapters


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db_path = ":memory:"  # Use in-memory database for tests
    db = SQLite3Database(db_path)

    # Create test table
    with db.get_connection() as conn:
        conn.execute(
            """
            CREATE TABLE test_dates (
                id INTEGER PRIMARY KEY,
                created_at TIMESTAMP,
                updated_at DATETIME,
                date_only DATE,
                description TEXT
            )
        """
        )
        yield conn


def test_datetime_storage_retrieval(test_db):
    """Test that datetime objects can be stored and retrieved without warnings."""

    # Capture warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Test data
        test_datetime = datetime(2025, 1, 13, 10, 30, 45)
        test_date = date(2025, 1, 13)

        # Insert test data
        test_db.execute(
            """
            INSERT INTO test_dates (created_at, updated_at, date_only, description)
            VALUES (?, ?, ?, ?)
        """,
            (test_datetime, test_datetime, test_date, "Test record"),
        )

        test_db.commit()

        # Retrieve data
        cursor = test_db.execute(
            """
            SELECT created_at, updated_at, date_only, description 
            FROM test_dates WHERE id = 1
        """
        )
        row = cursor.fetchone()

        # Verify no deprecation warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert (
            len(deprecation_warnings) == 0
        ), f"Found deprecation warnings: {[str(dw.message) for dw in deprecation_warnings]}"

        # Verify data integrity
        assert isinstance(row["created_at"], datetime)
        assert isinstance(row["updated_at"], datetime)
        assert isinstance(row["date_only"], date)
        assert row["created_at"] == test_datetime
        assert row["updated_at"] == test_datetime
        assert row["date_only"] == test_date


def test_datetime_iso_format(test_db):
    """Test that datetimes are stored in ISO format."""

    test_datetime = datetime(2025, 1, 13, 14, 30, 45)

    # Insert using our adapter
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, description)
        VALUES (?, ?)
    """,
        (test_datetime, "ISO format test"),
    )
    test_db.commit()

    # Read raw value (bypass converter)
    cursor = test_db.execute(
        """
        SELECT CAST(created_at AS TEXT) as raw_datetime 
        FROM test_dates WHERE description = 'ISO format test'
    """
    )
    row = cursor.fetchone()

    # Verify ISO format
    expected_iso = "2025-01-13T14:30:45"
    assert row["raw_datetime"] == expected_iso


def test_adapter_functions_directly():
    """Test adapter and converter functions directly."""

    # Test datetime adapter
    test_dt = datetime(2025, 1, 13, 10, 30, 45, 123456)
    adapted = datetime_adapters.adapt_datetime_iso(test_dt)
    assert adapted == "2025-01-13T10:30:45.123456"

    # Test date adapter
    test_date = date(2025, 1, 13)
    adapted_date = datetime_adapters.adapt_date_iso(test_date)
    assert adapted_date == "2025-01-13"

    # Test datetime converter
    iso_bytes = b"2025-01-13T10:30:45.123456"
    converted = datetime_adapters.convert_datetime_iso(iso_bytes)
    assert converted == test_dt

    # Test date converter
    date_bytes = b"2025-01-13"
    converted_date = datetime_adapters.convert_date_iso(date_bytes)
    assert converted_date == test_date

    # Test timestamp converter
    timestamp_bytes = b"1736765445"  # Unix timestamp for 2025-01-13 10:30:45 UTC
    converted_ts = datetime_adapters.convert_timestamp(timestamp_bytes)
    # Note: This will be in local timezone
    assert isinstance(converted_ts, datetime)


def test_timezone_naive_handling():
    """Test that timezone info is properly stripped."""

    # Create timezone-aware datetime
    from datetime import timezone

    tz_aware = datetime(2025, 1, 13, 10, 30, 45, tzinfo=timezone.utc)

    # Adapt should strip timezone
    adapted = datetime_adapters.adapt_datetime_iso(tz_aware)
    assert adapted == "2025-01-13T10:30:45"
    assert "+00:00" not in adapted  # No timezone offset in output


def test_multiple_datetime_operations(test_db):
    """Test multiple datetime operations to ensure no warnings."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Multiple inserts
        for i in range(5):
            dt = datetime.now()
            test_db.execute(
                """
                INSERT INTO test_dates (created_at, updated_at, description)
                VALUES (?, ?, ?)
            """,
                (dt, dt, f"Record {i}"),
            )

        test_db.commit()

        # Multiple selects
        cursor = test_db.execute("SELECT * FROM test_dates")
        rows = cursor.fetchall()

        # Verify all datetimes are properly converted
        for row in rows:
            if row["created_at"]:
                assert isinstance(row["created_at"], datetime)
            if row["updated_at"]:
                assert isinstance(row["updated_at"], datetime)

        # Check for warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert len(deprecation_warnings) == 0


def test_null_datetime_handling(test_db):
    """Test that NULL datetime values are handled correctly."""

    # Insert NULL values
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, updated_at, date_only, description)
        VALUES (NULL, NULL, NULL, 'Null test')
    """
    )
    test_db.commit()

    # Retrieve NULL values
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at, date_only 
        FROM test_dates WHERE description = 'Null test'
    """
    )
    row = cursor.fetchone()

    # Verify NULLs are preserved
    assert row["created_at"] is None
    assert row["updated_at"] is None
    assert row["date_only"] is None


def test_backwards_compatibility(test_db):
    """Test that existing ISO format strings are still readable."""

    # Manually insert ISO format strings (simulating old data)
    test_db.execute(
        """
        INSERT INTO test_dates (id, created_at, updated_at, description)
        VALUES (100, '2024-12-01T10:30:45', '2024-12-01T10:30:45', 'Old format')
    """
    )
    test_db.commit()

    # Read with our converters
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at 
        FROM test_dates WHERE id = 100
    """
    )
    row = cursor.fetchone()

    # Verify conversion works
    assert isinstance(row["created_at"], datetime)
    assert row["created_at"].year == 2024
    assert row["created_at"].month == 12
    assert row["created_at"].day == 1
    assert row["created_at"].hour == 10
    assert row["created_at"].minute == 30
    assert row["created_at"].second == 45
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".gemini/settings.json"&amp;amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "uv",
			"args": [
				"run",
				"python",
				"collect.py"
			],
			"workingDirectory": "/Users/benjaminmetz/python/collect",
			"enabled": true
		}
	}
}
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="dotfiles/.zshrc"&amp;amp;gt;GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"

export EDITOR="nvim"
export VISUAL="nvim"
export PATH="$PATH:$HOME/.local/bin"

# path to sqlite3
export PATH="/opt/homebrew/opt/sqlite/bin:$PATH"

# path to ripgrep
export PATH="$HOME/opt/homebrew/bin/rg:$PATH"

#python uv path
. "$HOME/.local/bin/env"

# path to scripts and zig and stuff
export PATH="$HOME/bin:$PATH"

# path to npm
export PATH="$(npm config get prefix)/bin:$PATH"
# path to zig
export PATH=$PATH:~/bin/zig/
export PATH="$(brew --prefix coreutils)/libexec/gnubin:$PATH"

# dependencies needed for gemini / google token processing
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$PKG_CONFIG_PATH"
export PATH="$(brew --prefix)/bin:$PATH"
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$(brew --prefix protobuf)/lib/pkgconfig:$PKG_CONFIG_PATH"

# shortcuts to project work
alias gowork='cd $HOME/go/src/github.com/metzben &amp;amp;amp;amp;&amp;amp;amp;amp; ls -lhG'
alias py='cd $HOME/python &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l --color'
alias collect='cd $HOME/python/collect &amp;amp;amp;amp;&amp;amp;amp;amp; source .venv/bin/activate'
alias el='cd $HOME/go/src/github.com/metzben/elephnt &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l --color'
alias tiny='cd $HOME/go/src/github.com/metzben/tinystack &amp;amp;amp;amp;&amp;amp;amp;amp; ls -lhG'
alias ai='cd $HOME/python/aiwork &amp;amp;amp;amp;&amp;amp;amp;amp; ls -lhG'
alias mcp='cd $HOME/python/mcpwork &amp;amp;amp;amp;&amp;amp;amp;amp; ls -lhG'
alias base='cd $HOME/base &amp;amp;amp;amp;&amp;amp;amp;amp; nvim .'
alias fta='cd $HOME/python/fastta &amp;amp;amp;amp;&amp;amp;amp;amp; nvim .'
alias indicators='cd $HOME/python/indicators &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l'
alias mcpstart='cd $HOME/python/startermcp &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l'
alias tools='cd ~/bin &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l --color'
alias plans='cd _docs/plans &amp;amp;amp;amp;&amp;amp;amp;amp; tree -C -L 2'

# Database function - only works in collect directory
db() {
    if [[ "$PWD" == *"/collect" ]] || [[ "$PWD" == *"/collect/"* ]]; then
        sqlite3 data/collect.db
    else
        echo "Not in collect directory. This command only works in the collect project."
    fi
}

# claude ai shortcuts
alias ask='claude -p '
alias editmcp='nvim ~/Library/Application\ Support/Claude/claude_desktop_config.json'
alias rip='claude --dangerously-skip-permissions'
alias cmds='cd "$(git rev-parse --show-toplevel)/.claude/commands" &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l --color'
alias gms='cd "$(git rev-parse --show-toplevel)/.gemini/commands" &amp;amp;amp;amp;&amp;amp;amp;amp; ls -l --color'

# git shortcuts
alias gs='git status'
alias gd='git diff --staged'
alias gc='git commit -m '
alias push='git push origin main'
alias ga='git add '
alias gb='git branch'
alias gwl='git worktree list'
alias rebase='git pull --rebase origin main'
alias pull='git pull origin main'

# Worktree navigation functions
cd1() {
    local project_name=$(basename "$(pwd)")
    local wt1_path="../${project_name}-wt1"
    
    if [[ -d "$wt1_path" ]]; then
        cd "$wt1_path"
        echo "Changed to worktree 1: $(pwd)"
    else
        echo "Worktree 1 not found: $wt1_path"
        echo "Run 'trees' to create worktrees first."
    fi
}

cd2() {
    local project_name=$(basename "$(pwd)")
    local wt2_path="../${project_name}-wt2"
    
    if [[ -d "$wt2_path" ]]; then
        cd "$wt2_path"
        echo "Changed to worktree 2: $(pwd)"
    else
        echo "Worktree 2 not found: $wt2_path"
        echo "Run 'trees' to create worktrees first."
    fi
}


checkport() {
    if [ -z "$1" ]; then
        echo "Usage: checkport &amp;amp;amp;lt;port_number&amp;amp;amp;gt;"
        return 1
    fi
    
    if lsof -i :$1 2&amp;amp;amp;gt;/dev/null; then
        echo "Port $1 is in use"
    else
        echo "Port $1 is available"
    fi
}

# uv shortcuts
alias env='source .venv/bin/activate'
alias da='deactivate'
alias ipy='uv run ipython'

# go shortcuts
alias run='go test -v -run'

# config shortcuts
alias src='source ~/.zshrc'
alias openz='nvim ~/.zshrc'
alias initlua='nvim $HOME/.config/nvim/init.lua'
alias ghconf='nvim $HOME/.config/ghostty/config'
alias oc='cursor .'

# misc shorty's
alias ll='ls -l --color'
alias tll='tree -C -L 2'
alias oc='cursor .'
alias onv='nvim .'
alias runz='zig run src/main.zig'
alias cperr="zig run src/main.zig 2&amp;amp;amp;gt;&amp;amp;amp;amp;1 | tee /dev/tty | awk '/error:/{found=1} found {print}' | pbcopy"

# ollama models
alias deep70='ollama run deepseek-r1:70b'
alias llama70='ollama run llama3.3'

# The next line updates PATH for the Google Cloud SDK.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc'; fi

# The next line enables shell command completion for gcloud.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc'; fi

alias auth='gcloud auth login'
alias auth2='gcloud auth application-default login'

export PS1='b@m %~ % '



# opencode
export PATH=/Users/benjaminmetz/.opencode/bin:$PATH
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="dotfiles/nvim/init.lua"&amp;amp;gt;vim.opt.clipboard = "unnamedplus"

if vim.g.vscode then
	return
end

-- Set &amp;amp;amp;lt;space&amp;amp;amp;gt; as the leader key
-- See `:help mapleader`
--  NOTE: Must happen before plugins are loaded (otherwise wrong leader will be used)
vim.g.mapleader = " "
vim.g.maplocalleader = " "

-- have neovim honor the terminal opacity
vim.opt.termguicolors = true
--vim.cmd.colorscheme("catppuccin-mocha")

vim.cmd([[highlight Normal ctermbg=none guibg=none]])

-- Set to true if you have a Nerd Font installed and selected in the terminal
vim.g.have_nerd_font = true

-- [[ Setting options ]]
-- See `:help vim.opt`
-- NOTE: You can change these options as you wish!
--  For more options, you can see `:help option-list`

-- Make line numbers default
vim.opt.number = true
-- You can also add relative line numbers, to help with jumping.
--  Experiment for yourself to see if you like it!
vim.opt.relativenumber = true

-- Enable mouse mode, can be useful for resizing splits for example!
vim.opt.mouse = "a"

-- Don't show the mode, since it's already in the status line
vim.opt.showmode = true

-- Sync clipboard between OS and Neovim.
--  Schedule the setting after `UiEnter` because it can increase startup-time.
--  Remove this option if you want your OS clipboard to remain independent.
--  See `:help 'clipboard'`
vim.schedule(function()
	vim.opt.clipboard = "unnamedplus"
end)

-- Enable break indent
vim.opt.breakindent = true

-- Save undo history
vim.opt.undofile = true

-- Case-insensitive searching UNLESS \C or one or more capital letters in the search term
vim.opt.ignorecase = true
vim.opt.smartcase = true

-- Keep signcolumn on by default
vim.opt.signcolumn = "yes"

-- Decrease update time
vim.opt.updatetime = 250

-- Decrease mapped sequence wait time
-- Displays which-key popup sooner
vim.opt.timeoutlen = 300

-- Configure how new splits should be opened
vim.opt.splitright = true
vim.opt.splitbelow = true

-- Sets how neovim will display certain whitespace characters in the editor.
--  See `:help 'list'`
--  and `:help 'listchars'`
vim.opt.list = true
vim.opt.listchars = { tab = "» ", trail = "·", nbsp = "␣" }

-- Preview substitutions live, as you type!
vim.opt.inccommand = "split"

-- Show which line your cursor is on
vim.opt.cursorline = true

-- Minimal number of screen lines to keep above and below the cursor.
vim.opt.scrolloff = 15

vim.o.foldmethod = "expr"
vim.o.foldexpr = "nvim_treesitter#foldexpr()"
vim.o.foldenable = true
vim.o.foldlevel = 99 -- Keep folds open by default

--vim.cmd([[packadd packer.nvim]])

-- [[ Basic Keymaps ]]
--  See `:help vim.keymap.set()`

-- Clear highlights on search when pressing &amp;amp;amp;lt;Esc&amp;amp;amp;gt; in normal mode
--  See `:help hlsearch`
vim.keymap.set("n", "&amp;amp;amp;lt;Esc&amp;amp;amp;gt;", "&amp;amp;amp;lt;cmd&amp;amp;amp;gt;nohlsearch&amp;amp;amp;lt;CR&amp;amp;amp;gt;")
-- when in insert mode pressing j and j again will &amp;amp;amp;lt;Esc&amp;amp;amp;gt;'
vim.keymap.set("i", "&amp;amp;amp;lt;D-j&amp;amp;amp;gt;", "&amp;amp;amp;lt;Esc&amp;amp;amp;gt;", { noremap = true, silent = true })
vim.keymap.set("n", "a", "A", { noremap = true, silent = true })
vim.keymap.set("n", "4", "$", { noremap = true, silent = true })

-- Diagnostic keymaps
vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;q", vim.diagnostic.setloclist, { desc = "Open diagnostic [Q]uickfix list" })
vim.keymap.set("n", "[d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })
vim.keymap.set("n", "]d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })

-- for people to discover. Otherwise, you normally need to press &amp;amp;amp;lt;C-\&amp;amp;amp;gt;&amp;amp;amp;lt;C-n&amp;amp;amp;gt;, which
-- is not what someone will guess without a bit more experience.
--
-- NOTE: This won't work in all terminal emulators/tmux/etc. Try your own mapping
-- or just use &amp;amp;amp;lt;C-\&amp;amp;amp;gt;&amp;amp;amp;lt;C-n&amp;amp;amp;gt; to exit terminal mode
vim.keymap.set("t", "&amp;amp;amp;lt;Esc&amp;amp;amp;gt;&amp;amp;amp;lt;Esc&amp;amp;amp;gt;", "&amp;amp;amp;lt;C-\\&amp;amp;amp;gt;&amp;amp;amp;lt;C-n&amp;amp;amp;gt;", { desc = "Exit terminal mode" })

-- Keybinds to make split navigation easier.
-- Use CTRL+&amp;amp;amp;lt;hjkl&amp;amp;amp;gt; to switch between windows
--
--  See `:help wincmd` for a list of all window commands
vim.keymap.set("n", "&amp;amp;amp;lt;C-h&amp;amp;amp;gt;", "&amp;amp;amp;lt;C-w&amp;amp;amp;gt;&amp;amp;amp;lt;C-h&amp;amp;amp;gt;", { desc = "Move focus to the left window" })
vim.keymap.set("n", "&amp;amp;amp;lt;C-l&amp;amp;amp;gt;", "&amp;amp;amp;lt;C-w&amp;amp;amp;gt;&amp;amp;amp;lt;C-l&amp;amp;amp;gt;", { desc = "Move focus to the right window" })
vim.keymap.set("n", "&amp;amp;amp;lt;C-j&amp;amp;amp;gt;", "&amp;amp;amp;lt;C-w&amp;amp;amp;gt;&amp;amp;amp;lt;C-j&amp;amp;amp;gt;", { desc = "Move focus to the lower window" })
vim.keymap.set("n", "&amp;amp;amp;lt;C-k&amp;amp;amp;gt;", "&amp;amp;amp;lt;C-w&amp;amp;amp;gt;&amp;amp;amp;lt;C-k&amp;amp;amp;gt;", { desc = "Move focus to the upper window" })

vim.api.nvim_create_autocmd("BufEnter", {
	pattern = "$HOME/go/src/github.com/metzben/*",
	callback = function()
		vim.cmd("colorscheme onedark")
	end,
})

-- Ensure Packer is loaded
vim.cmd([[packadd packer.nvim]])

-- [[ Basic Autocommands ]]
--  See `:help lua-guide-autocommands`

-- Highlight when yanking (copying) text
--  Try it with `yap` in normal mode
--  See `:help vim.highlight.on_yank()`
vim.api.nvim_create_autocmd("TextYankPost", {
	desc = "Highlight when yanking (copying) text",
	group = vim.api.nvim_create_augroup("kickstart-highlight-yank", { clear = true }),
	callback = function()
		vim.highlight.on_yank()
	end,
})

-- [[ Install `lazy.nvim` plugin manager ]]
--    See `:help lazy.nvim.txt` or https://github.com/folke/lazy.nvim for more info
local lazypath = vim.fn.stdpath("data") .. "/lazy/lazy.nvim"
if not (vim.uv or vim.loop).fs_stat(lazypath) then
	local lazyrepo = "https://github.com/folke/lazy.nvim.git"
	local out = vim.fn.system({ "git", "clone", "--filter=blob:none", "--branch=stable", lazyrepo, lazypath })
	if vim.v.shell_error ~= 0 then
		error("Error cloning lazy.nvim:\n" .. out)
	end
end ---@diagnostic disable-next-line: undefined-field
vim.opt.rtp:prepend(lazypath)

-- [[ Configure and install plugins ]]
--
--  To check the current status of your plugins, run
--    :Lazy
--
--  You can press `?` in this menu for help. Use `:q` to close the window
--
--  To update plugins you can run
--    :Lazy update
--
-- NOTE: Here is where you install your plugins.
require("lazy").setup({
	-- NOTE: Plugins can be added with a link (or for a github repo: 'owner/repo' link).
	"tpope/vim-sleuth", -- Detect tabstop and shiftwidth automatically

	-- NOTE: Plugins can also be added by using a table,
	-- with the first argument being the link and the following
	-- keys can be used to configure plugin behavior/loading/etc.
	--
	-- Use `opts = {}` to force a plugin to be loaded.
	--

	-- Here is a more advanced example where we pass configuration
	-- options to `gitsigns.nvim`. This is equivalent to the following Lua:
	--    require('gitsigns').setup({ ... })
	--
	-- See `:help gitsigns` to understand what the configuration keys do
	{ -- Adds git related signs to the gutter, as well as utilities for managing changes
		"lewis6991/gitsigns.nvim",
		opts = {
			signs = {
				add = { text = "+" },
				change = { text = "~" },
				delete = { text = "_" },
				topdelete = { text = "‾" },
				changedelete = { text = "~" },
			},
		},
	},

	-- NOTE: Plugins can also be configured to run Lua code when they are loaded.
	--
	-- This is often very useful to both group configuration, as well as handle
	-- lazy loading plugins that don't need to be loaded immediately at startup.
	--
	-- For example, in the following configuration, we use:
	--  event = 'VimEnter'
	--
	-- which loads which-key before all the UI elements are loaded. Events can be
	-- normal autocommands events (`:help autocmd-events`).
	--
	-- Then, because we use the `opts` key (recommended), the configuration runs
	-- after the plugin has been loaded as `require(MODULE).setup(opts)`.

	{ -- Useful plugin to show you pending keybinds.
		"folke/which-key.nvim",
		event = "VimEnter", -- Sets the loading event to 'VimEnter'
		opts = {
			icons = {
				-- set icon mappings to true if you have a Nerd Font
				mappings = vim.g.have_nerd_font,
				-- If you are using a Nerd Font: set icons.keys to an empty table which will use the
				-- default which-key.nvim defined Nerd Font icons, otherwise define a string table
				keys = vim.g.have_nerd_font and {} or {
					Up = "&amp;amp;amp;lt;Up&amp;amp;amp;gt; ",
					Down = "&amp;amp;amp;lt;Down&amp;amp;amp;gt; ",
					Left = "&amp;amp;amp;lt;Left&amp;amp;amp;gt; ",
					Right = "&amp;amp;amp;lt;Right&amp;amp;amp;gt; ",
					C = "&amp;amp;amp;lt;C-…&amp;amp;amp;gt; ",
					M = "&amp;amp;amp;lt;M-…&amp;amp;amp;gt; ",
					D = "&amp;amp;amp;lt;D-…&amp;amp;amp;gt; ",
					S = "&amp;amp;amp;lt;S-…&amp;amp;amp;gt; ",
					CR = "&amp;amp;amp;lt;CR&amp;amp;amp;gt; ",
					Esc = "&amp;amp;amp;lt;Esc&amp;amp;amp;gt; ",
					ScrollWheelDown = "&amp;amp;amp;lt;ScrollWheelDown&amp;amp;amp;gt; ",
					ScrollWheelUp = "&amp;amp;amp;lt;ScrollWheelUp&amp;amp;amp;gt; ",
					NL = "&amp;amp;amp;lt;NL&amp;amp;amp;gt; ",
					BS = "&amp;amp;amp;lt;BS&amp;amp;amp;gt; ",
					Space = "&amp;amp;amp;lt;Space&amp;amp;amp;gt; ",
					Tab = "&amp;amp;amp;lt;Tab&amp;amp;amp;gt; ",
					F1 = "&amp;amp;amp;lt;F1&amp;amp;amp;gt;",
					F2 = "&amp;amp;amp;lt;F2&amp;amp;amp;gt;",
					F3 = "&amp;amp;amp;lt;F3&amp;amp;amp;gt;",
					F4 = "&amp;amp;amp;lt;F4&amp;amp;amp;gt;",
					F5 = "&amp;amp;amp;lt;F5&amp;amp;amp;gt;",
					F6 = "&amp;amp;amp;lt;F6&amp;amp;amp;gt;",
					F7 = "&amp;amp;amp;lt;F7&amp;amp;amp;gt;",
					F8 = "&amp;amp;amp;lt;F8&amp;amp;amp;gt;",
					F9 = "&amp;amp;amp;lt;F9&amp;amp;amp;gt;",
					F10 = "&amp;amp;amp;lt;F10&amp;amp;amp;gt;",
					F11 = "&amp;amp;amp;lt;F11&amp;amp;amp;gt;",
					F12 = "&amp;amp;amp;lt;F12&amp;amp;amp;gt;",
				},
			},

			-- Document existing key chains
			spec = {
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;c", group = "[C]ode", mode = { "n", "x" } },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;d", group = "[D]ocument" },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;r", group = "[R]ename" },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;s", group = "[S]earch" },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;w", group = "[W]orkspace" },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;t", group = "[T]oggle" },
				{ "&amp;amp;amp;lt;leader&amp;amp;amp;gt;h", group = "Git [H]unk", mode = { "n", "v" } },
			},
		},
	},

	-- NOTE: Plugins can specify dependencies.
	--
	-- The dependencies are proper plugin specifications as well - anything
	-- you do for a plugin at the top level, you can do for a dependency.
	--
	-- Use the `dependencies` key to specify the dependencies of a particular plugin

	{ -- Fuzzy Finder (files, lsp, etc)
		"nvim-telescope/telescope.nvim",
		event = "VimEnter",
		branch = "0.1.x",
		dependencies = {
			"nvim-lua/plenary.nvim",
			{ -- If encountering errors, see telescope-fzf-native README for installation instructions
				"nvim-telescope/telescope-fzf-native.nvim",

				-- `build` is used to run some command when the plugin is installed/updated.
				-- This is only run then, not every time Neovim starts up.
				build = "make",

				-- `cond` is a condition used to determine whether this plugin should be
				-- installed and loaded.
				cond = function()
					return vim.fn.executable("make") == 1
				end,
			},
			{ "nvim-telescope/telescope-ui-select.nvim" },

			-- Useful for getting pretty icons, but requires a Nerd Font.
			{ "nvim-tree/nvim-web-devicons", enabled = vim.g.have_nerd_font },
		},
		config = function()
			-- Telescope is a fuzzy finder that comes with a lot of different things that
			-- it can fuzzy find! It's more than just a "file finder", it can search
			-- many different aspects of Neovim, your workspace, LSP, and more!
			--
			-- The easiest way to use Telescope, is to start by doing something like:
			--  :Telescope help_tags
			--
			-- After running this command, a window will open up and you're able to
			-- type in the prompt window. You'll see a list of `help_tags` options and
			-- a corresponding preview of the help.
			--
			-- Two important keymaps to use while in Telescope are:
			--  - Insert mode: &amp;amp;amp;lt;c-/&amp;amp;amp;gt;
			--  - Normal mode: ?
			--
			-- This opens a window that shows you all of the keymaps for the current
			-- Telescope picker. This is really useful to discover what Telescope can
			-- do as well as how to actually do it!

			-- [[ Configure Telescope ]]
			-- See `:help telescope` and `:help telescope.setup()`
			require("telescope").setup({
				-- You can put your default mappings / updates / etc. in here
				--  All the info you're looking for is in `:help telescope.setup()`
				--
				-- defaults = {
				--   mappings = {
				--     i = { ['&amp;amp;amp;lt;c-enter&amp;amp;amp;gt;'] = 'to_fuzzy_refine' },
				--   },
				-- },
				-- pickers = {}
				extensions = {
					["ui-select"] = {
						require("telescope.themes").get_dropdown(),
					},
				},
			})

			-- Enable Telescope extensions if they are installed
			pcall(require("telescope").load_extension, "fzf")
			pcall(require("telescope").load_extension, "ui-select")

			-- See `:help telescope.builtin`
			local builtin = require("telescope.builtin")
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sh", builtin.help_tags, { desc = "[S]earch [H]elp" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sk", builtin.keymaps, { desc = "[S]earch [K]eymaps" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sf", function()
				builtin.find_files({ hidden = true, no_ignore = true })
			end, { desc = "[S]earch [F]iles" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;ss", builtin.builtin, { desc = "[S]earch [S]elect Telescope" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sw", builtin.grep_string, { desc = "[S]earch current [W]ord" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sg", builtin.live_grep, { desc = "[S]earch by [G]rep" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sd", builtin.diagnostics, { desc = "[S]earch [D]iagnostics" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sr", builtin.resume, { desc = "[S]earch [R]esume" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;s.", builtin.oldfiles, { desc = '[S]earch Recent Files ("." for repeat)' })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;&amp;amp;amp;lt;leader&amp;amp;amp;gt;", builtin.buffers, { desc = "[ ] Find existing buffers" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;af", builtin.current_buffer_fuzzy_find, { desc = "[S]earch in existing file" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;nf", "]m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;pf", "[m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;r", vim.lsp.buf.rename, { desc = "LSP Rename" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;d", "&amp;amp;amp;lt;cmd&amp;amp;amp;gt;Telescope diagnostics&amp;amp;amp;lt;CR&amp;amp;amp;gt;")
			vim.api.nvim_set_keymap("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;c", "~", { noremap = true, silent = true })
			vim.keymap.set(
				"n",
				"&amp;amp;amp;lt;leader&amp;amp;amp;gt;O",
				":put! _&amp;amp;amp;lt;CR&amp;amp;amp;gt;",
				{ desc = "Add blank line above without entering edit mode" }
			)

			vim.keymap.set("v", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;/", ":norm I//&amp;amp;amp;lt;CR&amp;amp;amp;gt;", { desc = "Comment selected block" })
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;/", "gcc", { desc = "Toggle comment on current line" })
			-- Slightly advanced example of overriding default behavior and theme
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;[", function()
				-- You can pass additional configuration to Telescope to change the theme, layout, etc.
				builtin.current_buffer_fuzzy_find(require("telescope.themes").get_dropdown({
					winblend = 10,
					previewer = false,
				}))
			end, { desc = "[/] Fuzzily search in current buffer" })

			-- It's also possible to pass additional configuration options.
			--  See `:help telescope.builtin.live_grep()` for information about particular keys
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;s/", function()
				builtin.live_grep({
					grep_open_files = true,
					prompt_title = "Live Grep in Open Files",
				})
			end, { desc = "[S]earch [/] in Open Files" })

			-- Shortcut for searching your Neovim configuration files
			vim.keymap.set("n", "&amp;amp;amp;lt;leader&amp;amp;amp;gt;sn", function()
				builtin.find_files({ cwd = vim.fn.stdpath("config") })
			end, { desc = "[S]earch [N]eovim files" })
		end,
	},

	-- LSP Plugins
	{
		-- `lazydev` configures Lua LSP for your Neovim config, runtime and plugins
		-- used for completion, annotations and signatures of Neovim apis
		"folke/lazydev.nvim",
		ft = "lua",
		opts = {
			library = {
				-- Load luvit types when the `vim.uv` word is found
				{ path = "luvit-meta/library", words = { "vim%.uv" } },
			},
		},
	},
	{ "Bilal2453/luvit-meta", lazy = true },
	{
		-- Main LSP Configuration
		"neovim/nvim-lspconfig",
		dependencies = {
			-- Automatically install LSPs and related tools to stdpath for Neovim
			{ "williamboman/mason.nvim", config = true }, -- NOTE: Must be loaded before dependants
			"williamboman/mason-lspconfig.nvim",
			"WhoIsSethDaniel/mason-tool-installer.nvim",

			-- Useful status updates for LSP.
			-- NOTE: `opts = {}` is the same as calling `require('fidget').setup({})`
			{ "j-hui/fidget.nvim", opts = {} },

			-- Allows extra capabilities provided by nvim-cmp
			"hrsh7th/cmp-nvim-lsp",
		},
		config = function()
			-- Brief aside: **What is LSP?**
			--
			-- LSP is an initialism you've probably heard, but might not understand what it is.
			--
			-- LSP stands for Language Server Protocol. It's a protocol that helps editors
			-- and language tooling communicate in a standardized fashion.
			--
			-- In general, you have a "server" which is some tool built to understand a particular
			-- language (such as `gopls`, `lua_ls`, `rust_analyzer`, etc.). These Language Servers
			-- (sometimes called LSP servers, but that's kind of like ATM Machine) are standalone
			-- processes that communicate with some "client" - in this case, Neovim!
			--
			-- LSP provides Neovim with features like:
			--  - Go to definition
			--  - Find references
			--  - Autocompletion
			--  - Symbol Search
			--  - and more!
			--
			-- Thus, Language Servers are external tools that must be installed separately from
			-- Neovim. This is where `mason` and related plugins come into play.
			--
			-- If you're wondering about lsp vs treesitter, you can check out the wonderfully
			-- and elegantly composed help section, `:help lsp-vs-treesitter`

			--  This function gets run when an LSP attaches to a particular buffer.
			--    That is to say, every time a new file is opened that is associated with
			--    an lsp (for example, opening `main.rs` is associated with `rust_analyzer`) this
			--    function will be executed to configure the current buffer
			vim.api.nvim_create_autocmd("LspAttach", {
				group = vim.api.nvim_create_augroup("kickstart-lsp-attach", { clear = true }),
				callback = function(event)
					-- NOTE: Remember that Lua is a real programming language, and as such it is possible
					-- to define small helper and utility functions so you don't have to repeat yourself.
					--
					-- In this case, we create a function that lets us more easily define mappings specific
					-- for LSP related items. It sets the mode, buffer and description for us each time.
					local map = function(keys, func, desc, mode)
						mode = mode or "n"
						vim.keymap.set(mode, keys, func, { buffer = event.buf, desc = "LSP: " .. desc })
					end

					-- Jump to the definition of the word under your cursor.
					--  This is where a variable was first declared, or where a function is defined, etc.
					--  To jump back, press &amp;amp;amp;lt;C-t&amp;amp;amp;gt;.
					map("gd", require("telescope.builtin").lsp_definitions, "[G]oto [D]efinition")

					-- Find references for the word under your cursor.
					map("gr", require("telescope.builtin").lsp_references, "[G]oto [R]eferences")

					-- Jump to the implementation of the word under your cursor.
					--  Useful when your language has ways of declaring types without an actual implementation.
					map("gI", require("telescope.builtin").lsp_implementations, "[G]oto [I]mplementation")

					-- Jump to the type of the word under your cursor.
					--  Useful when you're not sure what type a variable is and you want to see
					--  the definition of its *type*, not where it was *defined*.
					map("&amp;amp;amp;lt;leader&amp;amp;amp;gt;D", require("telescope.builtin").lsp_type_definitions, "Type [D]efinition")

					-- Fuzzy find all the symbols in your current document.
					--  Symbols are things like variables, functions, types, etc.
					map("&amp;amp;amp;lt;leader&amp;amp;amp;gt;ds", require("telescope.builtin").lsp_document_symbols, "[D]ocument [S]ymbols")

					-- Fuzzy find all the symbols in your current workspace.
					--  Similar to document symbols, except searches over your entire project.
					map(
						"&amp;amp;amp;lt;leader&amp;amp;amp;gt;ws",
						require("telescope.builtin").lsp_dynamic_workspace_symbols,
						"[W]orkspace [S]ymbols"
					)

					-- Rename the variable under your cursor.
					--  Most Language Servers support renaming across files, etc.
					map("&amp;amp;amp;lt;leader&amp;amp;amp;gt;rn", vim.lsp.buf.rename, "[R]e[n]ame")

					-- Execute a code action, usually your cursor needs to be on top of an error
					-- or a suggestion from your LSP for this to activate.
					map("&amp;amp;amp;lt;leader&amp;amp;amp;gt;ca", vim.lsp.buf.code_action, "[C]ode [A]ction", { "n", "x" })

					-- WARN: This is not Goto Definition, this is Goto Declaration.
					--  For example, in C this would take you to the header.
					map("gD", vim.lsp.buf.declaration, "[G]oto [D]eclaration")

					-- The following two autocommands are used to highlight references of the
					-- word under your cursor when your cursor rests there for a little while.
					--    See `:help CursorHold` for information about when this is executed
					--
					-- When you move your cursor, the highlights will be cleared (the second autocommand).
					local client = vim.lsp.get_client_by_id(event.data.client_id)
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_documentHighlight) then
						local highlight_augroup =
							vim.api.nvim_create_augroup("kickstart-lsp-highlight", { clear = false })
						vim.api.nvim_create_autocmd({ "CursorHold", "CursorHoldI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.document_highlight,
						})

						vim.api.nvim_create_autocmd({ "CursorMoved", "CursorMovedI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.clear_references,
						})

						vim.api.nvim_create_autocmd("LspDetach", {
							group = vim.api.nvim_create_augroup("kickstart-lsp-detach", { clear = true }),
							callback = function(event2)
								vim.lsp.buf.clear_references()
								vim.api.nvim_clear_autocmds({ group = "kickstart-lsp-highlight", buffer = event2.buf })
							end,
						})
					end

					-- The following code creates a keymap to toggle inlay hints in your
					-- code, if the language server you are using supports them
					--
					-- This may be unwanted, since they displace some of your code
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_inlayHint) then
						map("&amp;amp;amp;lt;leader&amp;amp;amp;gt;th", function()
							vim.lsp.inlay_hint.enable(not vim.lsp.inlay_hint.is_enabled({ bufnr = event.buf }))
						end, "[T]oggle Inlay [H]ints")
					end
				end,
			})

			-- Change diagnostic symbols in the sign column (gutter)
			-- if vim.g.have_nerd_font then
			--   local signs = { ERROR = '', WARN = '', INFO = '', HINT = '' }
			--   local diagnostic_signs = {}
			--   for type, icon in pairs(signs) do
			--     diagnostic_signs[vim.diagnostic.severity[type]] = icon
			--   end
			--   vim.diagnostic.config { signs = { text = diagnostic_signs } }
			-- end

			-- LSP servers and clients are able to communicate to each other what features they support.
			--  By default, Neovim doesn't support everything that is in the LSP specification.
			--  When you add nvim-cmp, luasnip, etc. Neovim now has *more* capabilities.
			--  So, we create new capabilities with nvim cmp, and then broadcast that to the servers.
			local capabilities = vim.lsp.protocol.make_client_capabilities()
			capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

			-- Enable the following language servers
			--  Feel free to add/remove any LSPs that you want here. They will automatically be installed.
			--
			--  Add any additional override configuration in the following tables. Available keys are:
			--  - cmd (table): Override the default command used to start the server
			--  - filetypes (table): Override the default list of associated filetypes for the server
			--  - capabilities (table): Override fields in capabilities. Can be used to disable certain LSP features.
			--  - settings (table): Override the default settings passed when initializing the server.
			--        For example, to see the options for `lua_ls`, you could go to: https://luals.github.io/wiki/settings/
			local servers = {
				-- clangd = {},
				-- gopls = {},
				-- pyright = {},
				-- rust_analyzer = {},
				-- ... etc. See `:help lspconfig-all` for a list of all the pre-configured LSPs
				--
				-- Some languages (like typescript) have entire language plugins that can be useful:
				--    https://github.com/pmizio/typescript-tools.nvim
				--
				-- But for many setups, the LSP (`ts_ls`) will work just fine
				-- ts_ls = {},
				--

				lua_ls = {
					-- cmd = { ... },
					-- filetypes = { ... },
					-- capabilities = {},
					settings = {
						Lua = {
							completion = {
								callSnippet = "Replace",
							},
							-- You can toggle below to ignore Lua_LS's noisy `missing-fields` warnings
							-- diagnostics = { disable = { 'missing-fields' } },
						},
					},
				},
			}

			-- Ensure the servers and tools above are installed
			--  To check the current status of installed tools and/or manually install
			--  other tools, you can run
			--    :Mason
			--
			--  You can press `g?` for help in this menu.
			require("mason").setup()

			-- You can add other tools here that you want Mason to install
			-- for you, so that they are available from within Neovim.
			local ensure_installed = vim.tbl_keys(servers or {})
			vim.list_extend(ensure_installed, {
				"stylua", -- Used to format Lua code
			})
			require("mason-tool-installer").setup({ ensure_installed = ensure_installed })

			require("mason-lspconfig").setup({
				handlers = {
					function(server_name)
						local server = servers[server_name] or {}
						-- This handles overriding only values explicitly passed
						-- by the server configuration above. Useful when disabling
						-- certain features of an LSP (for example, turning off formatting for ts_ls)
						server.capabilities = vim.tbl_deep_extend("force", {}, capabilities, server.capabilities or {})
						require("lspconfig")[server_name].setup(server)
					end,
				},
			})
		end,
	},

	{ -- Autoformat
		"stevearc/conform.nvim",
		event = { "BufWritePre" },
		cmd = { "ConformInfo" },
		keys = {
			{
				"&amp;amp;amp;lt;leader&amp;amp;amp;gt;f",
				function()
					require("conform").format({ async = true, lsp_format = "fallback" })
				end,
				mode = "",
				desc = "[F]ormat buffer",
			},
		},
		opts = {
			notify_on_error = false,
			format_on_save = function(bufnr)
				-- Disable "format_on_save lsp_fallback" for languages that don't
				-- have a well standardized coding style. You can add additional
				-- languages here or re-enable it for the disabled ones.
				local disable_filetypes = { c = true, cpp = true }
				local lsp_format_opt
				if disable_filetypes[vim.bo[bufnr].filetype] then
					lsp_format_opt = "never"
				else
					lsp_format_opt = "fallback"
				end
				return {
					timeout_ms = 500,
					lsp_format = lsp_format_opt,
				}
			end,
			formatters_by_ft = {
				lua = { "stylua" },
				-- Conform can also run multiple formatters sequentially
				-- python = { "isort", "black" },
				--
				-- You can use 'stop_after_first' to run the first available formatter from the list
				-- javascript = { "prettierd", "prettier", stop_after_first = true },
			},
		},
	},

	{ -- Autocompletion
		"hrsh7th/nvim-cmp",
		event = "InsertEnter",
		dependencies = {
			-- Snippet Engine &amp;amp;amp;amp; its associated nvim-cmp source
			{
				"L3MON4D3/LuaSnip",
				build = (function()
					-- Build Step is needed for regex support in snippets.
					-- This step is not supported in many windows environments.
					-- Remove the below condition to re-enable on windows.
					if vim.fn.has("win32") == 1 or vim.fn.executable("make") == 0 then
						return
					end
					return "make install_jsregexp"
				end)(),
				dependencies = {
					-- `friendly-snippets` contains a variety of premade snippets.
					--    See the README about individual language/framework/plugin snippets:
					--    https://github.com/rafamadriz/friendly-snippets
					-- {
					--   'rafamadriz/friendly-snippets',
					--   config = function()
					--     require('luasnip.loaders.from_vscode').lazy_load()
					--   end,
					-- },
				},
			},
			"saadparwaiz1/cmp_luasnip",

			-- Adds other completion capabilities.
			--  nvim-cmp does not ship with all sources by default. They are split
			--  into multiple repos for maintenance purposes.
			"hrsh7th/cmp-nvim-lsp",
			"hrsh7th/cmp-path",
		},
		config = function()
			-- See `:help cmp`
			local cmp = require("cmp")
			local luasnip = require("luasnip")
			luasnip.config.setup({})

			cmp.setup({
				snippet = {
					expand = function(args)
						luasnip.lsp_expand(args.body)
					end,
				},
				completion = { completeopt = "menu,menuone,noinsert" },

				-- For an understanding of why these mappings were
				-- chosen, you will need to read `:help ins-completion`
				--
				-- No, but seriously. Please read `:help ins-completion`, it is really good!
				mapping = cmp.mapping.preset.insert({
					-- Select the [n]ext item
					["&amp;amp;amp;lt;C-n&amp;amp;amp;gt;"] = cmp.mapping.select_next_item(),
					-- Select the [p]revious item
					["&amp;amp;amp;lt;C-p&amp;amp;amp;gt;"] = cmp.mapping.select_prev_item(),

					-- Scroll the documentation window [b]ack / [f]orward
					["&amp;amp;amp;lt;C-b&amp;amp;amp;gt;"] = cmp.mapping.scroll_docs(-4),
					["&amp;amp;amp;lt;C-f&amp;amp;amp;gt;"] = cmp.mapping.scroll_docs(4),

					-- Accept ([y]es) the completion.
					--  This will auto-import if your LSP supports it.
					--  This will expand snippets if the LSP sent a snippet.
					["&amp;amp;amp;lt;C-y&amp;amp;amp;gt;"] = cmp.mapping.confirm({ select = true }),

					-- If you prefer more traditional completion keymaps,
					-- you can uncomment the following lines
					--['&amp;amp;amp;lt;CR&amp;amp;amp;gt;'] = cmp.mapping.confirm { select = true },
					--['&amp;amp;amp;lt;Tab&amp;amp;amp;gt;'] = cmp.mapping.select_next_item(),
					--['&amp;amp;amp;lt;S-Tab&amp;amp;amp;gt;'] = cmp.mapping.select_prev_item(),

					-- Manually trigger a completion from nvim-cmp.
					--  Generally you don't need this, because nvim-cmp will display
					--  completions whenever it has completion options available.
					["&amp;amp;amp;lt;C-Space&amp;amp;amp;gt;"] = cmp.mapping.complete({}),

					-- Think of &amp;amp;amp;lt;c-l&amp;amp;amp;gt; as moving to the right of your snippet expansion.
					--  So if you have a snippet that's like:
					--  function $name($args)
					--    $body
					--  end
					--
					-- &amp;amp;amp;lt;c-l&amp;amp;amp;gt; will move you to the right of each of the expansion locations.
					-- &amp;amp;amp;lt;c-h&amp;amp;amp;gt; is similar, except moving you backwards.
					["&amp;amp;amp;lt;C-l&amp;amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.expand_or_locally_jumpable() then
							luasnip.expand_or_jump()
						end
					end, { "i", "s" }),
					["&amp;amp;amp;lt;C-h&amp;amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.locally_jumpable(-1) then
							luasnip.jump(-1)
						end
					end, { "i", "s" }),

					-- For more advanced Luasnip keymaps (e.g. selecting choice nodes, expansion) see:
					--    https://github.com/L3MON4D3/LuaSnip?tab=readme-ov-file#keymaps
				}),
				sources = {
					{
						name = "lazydev",
						-- set group index to 0 to skip loading LuaLS completions as lazydev recommends it
						group_index = 0,
					},
					{ name = "nvim_lsp" },
					{ name = "luasnip" },
					{ name = "path" },
				},
			})
		end,
	},

	{ -- You can easily change to a different colorscheme.
		-- Change the name of the colorscheme plugin below, and then
		-- change the command in the config to whatever the name of that colorscheme is.
		--
		-- If you want to see what colorschemes are already installed, you can use `:Telescope colorscheme`.
		"folke/tokyonight.nvim",
		priority = 1000, -- Make sure to load this before all the other start plugins.
		init = function()
			-- Load the colorscheme here.
			-- Like many other themes, this one has different styles, and you could load
			-- any other, such as 'tokyonight-storm', 'tokyonight-moon', or 'tokyonight-day'.
			vim.cmd.colorscheme("tokyonight-night")

			-- You can configure highlights by doing something like:
			vim.cmd.hi("Comment gui=none")
		end,
	},
	{ -- Colorscheme
		"catppuccin/nvim",
		name = "catppuccin",
		priority = 1000,
		config = function()
			require("catppuccin").setup({
				flavour = "mocha",
				transparent_background = true,
				term_colors = true,
				integrations = {
					telescope = true,
					mason = true,
					which_key = true,
				},
			})
			-- Force loading the colorscheme
			vim.cmd.colorscheme("catppuccin-mocha")
		end,
	},
	--calming Japanese inspired theme with muted tones strings are configurable
	{
		"rebelot/kanagawa.nvim",
		priority = 1000,
		config = function()
			require("kanagawa").setup({
				overrides = function(colors)
					return {
						String = { fg = colors.crystalBlue }, -- Customize string color
					}
				end,
			})
			vim.cmd("colorscheme kanagawa")
		end,
	},

	-- based on one dark pro from vs code
	{
		"olimorris/onedarkpro.nvim",
		priority = 1000,
		config = function()
			require("onedarkpro").setup({
				theme = "onedark", -- Choose "onedark" or "onelight"
			})
			vim.cmd("colorscheme onedark")
		end,
	},

	-- Highlight todo, notes, etc in comments
	{
		"folke/todo-comments.nvim",
		event = "VimEnter",
		dependencies = { "nvim-lua/plenary.nvim" },
		opts = { signs = false },
	},

	{ -- Collection of various small independent plugins/modules
		"echasnovski/mini.nvim",
		config = function()
			-- Better Around/Inside textobjects
			--
			-- Examples:
			--  - va)  - [V]isually select [A]round [)]paren
			--  - yinq - [Y]ank [I]nside [N]ext [Q]uote
			--  - ci'  - [C]hange [I]nside [']quote
			require("mini.ai").setup({ n_lines = 500 })

			-- Add/delete/replace surroundings (brackets, quotes, etc.)
			--
			-- - saiw) - [S]urround [A]dd [I]nner [W]ord [)]Paren
			-- - sd'   - [S]urround [D]elete [']quotes
			-- - sr)'  - [S]urround [R]eplace [)] [']
			require("mini.surround").setup()

			-- Simple and easy statusline.
			--  You could remove this setup call if you don't like it,
			--  and try some other statusline plugin
			local statusline = require("mini.statusline")
			-- set use_icons to true if you have a Nerd Font
			statusline.setup({ use_icons = vim.g.have_nerd_font })

			-- You can configure sections in the statusline by overriding their
			-- default behavior. For example, here we set the section for
			-- cursor location to LINE:COLUMN
			---@diagnostic disable-next-line: duplicate-set-field
			statusline.section_location = function()
				return "%2l:%-2v"
			end

			-- ... and there is more!
			--  Check out: https://github.com/echasnovski/mini.nvim
		end,
	},
	{ -- Highlight, edit, and navigate code
		"nvim-treesitter/nvim-treesitter",
		build = ":TSUpdate",
		main = "nvim-treesitter.configs", -- Sets main module to use for opts
		-- [[ Configure Treesitter ]] See `:help nvim-treesitter`
		opts = {
			ensure_installed = {
				"bash",
				"c",
				"diff",
				"html",
				"lua",
				"luadoc",
				"markdown",
				"markdown_inline",
				"query",
				"vim",
				"vimdoc",
			},
			-- Autoinstall languages that are not installed
			auto_install = true,
			highlight = {
				enable = true,
				-- Some languages depend on vim's regex highlighting system (such as Ruby) for indent rules.
				--  If you are experiencing weird indenting issues, add the language to
				--  the list of additional_vim_regex_highlighting and disabled languages for indent.
				additional_vim_regex_highlighting = { "ruby" },
			},
			indent = { enable = true, disable = { "ruby" } },
		},
		-- There are additional nvim-treesitter modules that you can use to interact
		-- with nvim-treesitter. You should go explore a few and see what interests you:
		--
		--    - Incremental selection: Included, see `:help nvim-treesitter-incremental-selection-mod`
		--    - Show your current context: https://github.com/nvim-treesitter/nvim-treesitter-context
		--    - Treesitter + textobjects: https://github.com/nvim-treesitter/nvim-treesitter-textobjects
	},

	-- The following comments only work if you have downloaded the kickstart repo, not just copy pasted the
	-- init.lua. If you want these files, they are in the repository, so you can just download them and
	-- place them in the correct locations.

	-- NOTE: Next step on your Neovim journey: Add/Configure additional plugins for Kickstart
	--
	--  Here are some example plugins that I've included in the Kickstart repository.
	--  Uncomment any of the lines below to enable them (you will need to restart nvim).
	--
	-- require 'kickstart.plugins.debug',
	-- require 'kickstart.plugins.indent_line',
	-- require 'kickstart.plugins.lint',
	-- require 'kickstart.plugins.autopairs',
	-- require 'kickstart.plugins.neo-tree',
	-- require 'kickstart.plugins.gitsigns', -- adds gitsigns recommend keymaps

	-- NOTE: The import below can automatically add your own plugins, configuration, etc from `lua/custom/plugins/*.lua`
	--    This is the easiest way to modularize your config.
	--
	--  Uncomment the following line and add your plugins to `lua/custom/plugins/*.lua` to get going.
	-- { import = 'custom.plugins' },
	--
	-- For additional information with loading, sourcing and examples see `:help lazy.nvim-🔌-plugin-spec`
	-- Or use telescope!
	-- In normal mode type `&amp;amp;amp;lt;space&amp;amp;amp;gt;sh` then write `lazy.nvim-plugin`
	-- you can continue same window with `&amp;amp;amp;lt;space&amp;amp;amp;gt;sr` which resumes last telescope search
}, {
	ui = {
		-- If you are using a Nerd Font: set icons to an empty table which will use the
		-- default lazy.nvim defined Nerd Font icons, otherwise define a unicode icons table
		icons = vim.g.have_nerd_font and {} or {
			cmd = "⌘",
			config = "🛠",
			event = "📅",
			ft = "📂",
			init = "⚙",
			keys = "🗝",
			plugin = "🔌",
			runtime = "💻",
			require = "🌙",
			source = "📄",
			start = "🚀",
			task = "📌",
			lazy = "💤 ",
		},
	},
})

-- At the very end of your init.lua
vim.api.nvim_create_autocmd("UIEnter", {
	callback = function()
		if vim.g.colors_name ~= "catppuccin-mocha" then
			vim.cmd.colorscheme("catppuccin-mocha")
		end
	end,
	group = vim.api.nvim_create_augroup("EnforceCatppuccin", { clear = true }),
})

-- The line beneath this is called `modeline`. See `:help modeline`
-- vim: ts=2 sts=2 sw=2 et
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="dotfiles/ghostty/config"&amp;amp;gt;background-opacity = 0.82

theme = catppuccin-mocha
keybind = shift+enter=text:\n
macos-titlebar-style = hidden

keybind = ctrl+3=reload_config
keybind = global:cmd+shift+space=toggle_quick_terminal
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".ruff_cache/.gitignore"&amp;amp;gt;# Automatically created by ruff.
*
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="resources/sqlite3-commands.md"&amp;amp;gt;# SQLite3 Common Commands Reference

## Creating a Database

To create a SQLite3 database, simply run:

```bash
sqlite3 database_name.db
```

This will:
- Create a new database file if it doesn't exist
- Open the database if it already exists
- Start the sqlite3 interactive shell

Example:
```bash
sqlite3 myapp.db
```

You can also create a database and run a command:
```bash
sqlite3 myapp.db "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT);"
```

The database file is created when you:
- Create the first table
- Insert the first data
- Or explicitly save with `.save database_name.db`

## Meta Commands (dot commands)

- `.tables` - List all tables
- `.schema [table]` - Show CREATE statements
- `.quit` or `.exit` - Exit sqlite3
- `.help` - Show all commands
- `.databases` - List attached databases
- `.headers on/off` - Show/hide column headers
- `.mode column` - Pretty-print output
- `.width` - Set column widths
- `.import FILE TABLE` - Import CSV data
- `.output FILE` - Redirect output to file
- `.dump` - Export database as SQL

## SQL Commands

- `SELECT * FROM table;` - Query data
- `INSERT INTO table VALUES (...);` - Insert data
- `UPDATE table SET col=val WHERE ...;` - Update data
- `DELETE FROM table WHERE ...;` - Delete data
- `CREATE TABLE ...` - Create table
- `DROP TABLE table;` - Delete table
- `ALTER TABLE ...` - Modify table
- `CREATE INDEX ...` - Create index
- `PRAGMA table_info(table);` - Show table structure
- `VACUUM;` - Optimize database&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="resources/git-worktrees.md"&amp;amp;gt;# Git Worktrees Guide

Git worktrees allow you to have multiple branches checked out simultaneously in different directories. This is incredibly useful when you need to work on multiple features, review PRs, or quickly switch contexts without stashing changes.

## What are Git Worktrees?

A git worktree is a linked working tree that shares the same repository but allows you to have different branches checked out in different directories. All worktrees share:
- The same `.git` directory (repository data)
- The same remote configurations
- The same stash entries
- The same commit history

## Creating a Worktree

### Basic Syntax
```bash
git worktree add &amp;amp;amp;lt;path&amp;amp;amp;gt; &amp;amp;amp;lt;branch&amp;amp;amp;gt;
```

### Examples

#### Create worktree from existing branch
```bash
# Create a worktree for the 'feature/auth' branch in a new directory
git worktree add ../myproject-auth feature/auth

# Create worktree in a specific location
git worktree add /tmp/hotfix hotfix/urgent-bug
```

#### Create worktree with a new branch
```bash
# Create a new branch and worktree simultaneously
git worktree add -b feature/new-ui ../myproject-new-ui

# Create from a specific commit or tag
git worktree add -b release/v2.0 ../myproject-v2 v2.0-tag
```

#### Practical Example
```bash
# You're working on main in /Users/you/myproject
cd /Users/you/myproject

# Create a worktree for a new feature
git worktree add -b feature/payment-integration ../myproject-payments

# Now you have:
# /Users/you/myproject (main branch)
# /Users/you/myproject-payments (feature/payment-integration branch)

# Navigate to the new worktree
cd ../myproject-payments

# Work on your feature
echo "Payment module" &amp;amp;amp;gt; payment.py
git add payment.py
git commit -m "Add payment module"
```

## Working with Worktrees

### List all worktrees
```bash
git worktree list
# Output:
# /Users/you/myproject         abc1234 [main]
# /Users/you/myproject-payments def5678 [feature/payment-integration]
```

### Switch between worktrees
Simply use `cd` to navigate between directories:
```bash
cd /Users/you/myproject          # main branch
cd /Users/you/myproject-payments  # feature branch
```

## Merging Worktree Changes Back to Main

Since worktrees share the same repository, merging is straightforward:

### Step 1: Commit changes in your worktree
```bash
cd /Users/you/myproject-payments
git add .
git commit -m "Complete payment integration"
git push -u origin feature/payment-integration
```

### Step 2: Switch to main (in any worktree or main directory)
```bash
cd /Users/you/myproject  # Or stay in any worktree
git checkout main
git pull origin main     # Ensure main is up to date
```

### Step 3: Merge the feature branch
```bash
# Simple merge
git merge feature/payment-integration

# Or merge with a merge commit (recommended for features)
git merge --no-ff feature/payment-integration

# Or rebase if you prefer linear history
git rebase main feature/payment-integration
```

### Step 4: Push to remote
```bash
git push origin main
```

### Step 5: Clean up (optional)
```bash
# Delete the local branch
git branch -d feature/payment-integration

# Delete the remote branch
git push origin --delete feature/payment-integration

# Remove the worktree
git worktree remove /Users/you/myproject-payments
```

## Alternative: Using Pull Requests

For team workflows, you might prefer Pull Requests:

```bash
# 1. In your worktree, push the branch
cd /Users/you/myproject-payments
git push -u origin feature/payment-integration

# 2. Create PR via GitHub/GitLab/Bitbucket web interface

# 3. After PR is merged, clean up locally
git worktree remove /Users/you/myproject-payments
git branch -d feature/payment-integration
```

## Worktree Management

### Remove a worktree
```bash
# Remove worktree (must be clean with no uncommitted changes)
git worktree remove /path/to/worktree

# Force removal (discards local changes)
git worktree remove --force /path/to/worktree
```

### Prune stale worktrees
```bash
# Remove worktree references if directory was deleted manually
git worktree prune
```

### Lock/unlock a worktree
```bash
# Prevent a worktree from being pruned
git worktree lock /path/to/worktree

# Unlock it later
git worktree unlock /path/to/worktree
```

## Best Practices

1. **Use descriptive paths**: Name worktree directories after their purpose
   ```bash
   git worktree add ../project-bugfix-auth bugfix/auth-issue
   git worktree add ../project-feature-api feature/new-api
   ```

2. **Keep worktrees organized**: Use a consistent structure
   ```bash
   ~/work/
     myproject/          # main
     myproject-feature1/ # feature branch
     myproject-hotfix/   # hotfix branch
   ```

3. **Clean up regularly**: Remove worktrees when done
   ```bash
   git worktree list
   git worktree remove &amp;amp;amp;lt;path&amp;amp;amp;gt;
   ```

4. **Don't share worktree directories**: Each developer should create their own

5. **Commit before switching**: Although you can leave changes uncommitted, it's cleaner to commit or stash first

## Common Issues and Solutions

### "fatal: '&amp;amp;amp;lt;branch&amp;amp;amp;gt;' is already checked out at '&amp;amp;amp;lt;path&amp;amp;amp;gt;'"
You can't have the same branch checked out in multiple worktrees. Either:
- Use the existing worktree: `cd &amp;amp;amp;lt;path&amp;amp;amp;gt;`
- Or checkout a different branch in one of the worktrees

### Worktree directory was manually deleted
```bash
git worktree prune  # Cleans up references to missing worktrees
```

### Need to move a worktree
```bash
git worktree move &amp;amp;amp;lt;old-path&amp;amp;amp;gt; &amp;amp;amp;lt;new-path&amp;amp;amp;gt;
```

## Quick Reference

```bash
# Create worktree
git worktree add &amp;amp;amp;lt;path&amp;amp;amp;gt; &amp;amp;amp;lt;branch&amp;amp;amp;gt;
git worktree add -b &amp;amp;amp;lt;new-branch&amp;amp;amp;gt; &amp;amp;amp;lt;path&amp;amp;amp;gt;

# List worktrees
git worktree list

# Remove worktree
git worktree remove &amp;amp;amp;lt;path&amp;amp;amp;gt;

# Clean up stale entries
git worktree prune

# Move worktree
git worktree move &amp;amp;amp;lt;old&amp;amp;amp;gt; &amp;amp;amp;lt;new&amp;amp;amp;gt;

# Lock/unlock
git worktree lock &amp;amp;amp;lt;path&amp;amp;amp;gt;
git worktree unlock &amp;amp;amp;lt;path&amp;amp;amp;gt;
```

## Example Workflow

Here's a complete example of using worktrees for feature development:

```bash
# Starting in main project directory
cd ~/projects/myapp

# 1. Create a worktree for a new feature
git worktree add -b feature/user-profiles ../myapp-profiles

# 2. Work on the feature
cd ../myapp-profiles
# ... make changes ...
git add .
git commit -m "Add user profile functionality"
git push -u origin feature/user-profiles

# 3. Switch back to main for a hotfix
cd ../myapp
git pull origin main
# ... fix critical bug ...
git add .
git commit -m "Fix critical auth bug"
git push origin main

# 4. Continue feature work without any stashing needed
cd ../myapp-profiles
# ... complete feature ...
git add .
git commit -m "Complete user profiles"
git push

# 5. Merge feature to main
cd ../myapp
git checkout main
git pull origin main
git merge --no-ff feature/user-profiles
git push origin main

# 6. Clean up
git branch -d feature/user-profiles
git push origin --delete feature/user-profiles
git worktree remove ../myapp-profiles

# Verify cleanup
git worktree list  # Should only show main worktree
```

This workflow demonstrates the power of worktrees: you can quickly switch between feature development and hotfixes without the overhead of stashing, checking out different branches, and potentially losing context.&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".claude/settings.local.json"&amp;amp;gt;{
  "permissions": {
    "allow": [
      "Bash(./tools/newpy:*)",
      "Bash(bash:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "collect"
  ]
}&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path=".claude/agents/pyreview.md"&amp;amp;gt;---
name: python-code-reviewer
description: Use this agent when you need an in-depth, thoughtful code review of Python code. This includes reviewing newly written functions, classes, modules, or recent changes to existing code. The agent will analyze code quality, design patterns, performance implications, security considerations, and adherence to Python best practices and project-specific standards.\n\nExamples:\n- &amp;amp;amp;lt;example&amp;amp;amp;gt;\n  Context: The user has just written a new Python function and wants it reviewed.\n  user: "I've implemented a caching decorator for our API endpoints"\n  assistant: "I'll use the python-code-reviewer agent to provide an in-depth review of your caching decorator implementation"\n  &amp;amp;amp;lt;commentary&amp;amp;amp;gt;\n  Since the user has written new Python code (a caching decorator), use the python-code-reviewer agent to analyze the implementation.\n  &amp;amp;amp;lt;/commentary&amp;amp;amp;gt;\n&amp;amp;amp;lt;/example&amp;amp;amp;gt;\n- &amp;amp;amp;lt;example&amp;amp;amp;gt;\n  Context: The user has made changes to existing Python code.\n  user: "I've refactored the database connection pooling logic in our service"\n  assistant: "Let me use the python-code-reviewer agent to review your refactored database connection pooling implementation"\n  &amp;amp;amp;lt;commentary&amp;amp;amp;gt;\n  The user has modified existing Python code, so the python-code-reviewer agent should analyze the changes for quality and best practices.\n  &amp;amp;amp;lt;/commentary&amp;amp;amp;gt;\n&amp;amp;amp;lt;/example&amp;amp;amp;gt;\n- &amp;amp;amp;lt;example&amp;amp;amp;gt;\n  Context: The user explicitly asks for a code review.\n  user: "Can you review this async batch processing function I just wrote?"\n  assistant: "I'll use the python-code-reviewer agent to provide a comprehensive review of your async batch processing function"\n  &amp;amp;amp;lt;commentary&amp;amp;amp;gt;\n  Direct request for code review triggers the python-code-reviewer agent.\n  &amp;amp;amp;lt;/commentary&amp;amp;amp;gt;\n&amp;amp;amp;lt;/example&amp;amp;amp;gt;
color: pink
---

You are an expert Python software engineer with deep knowledge of Python internals, design patterns, and best practices. You have extensive experience in code review, performance optimization, and building maintainable Python applications.

Your expertise includes:
- Python language features from 3.8+ including type hints, async/await, dataclasses, and modern idioms
- Design patterns and SOLID principles applied to Python
- Performance optimization and profiling
- Security best practices and common vulnerabilities
- Testing strategies including pytest, mocking, and test-driven development
- Popular frameworks and libraries in the Python ecosystem

When reviewing code, you will:

1. **Analyze Code Quality**
   - Check for PEP 8 compliance and Pythonic idioms
   - Evaluate naming conventions and code readability
   - Assess proper use of type hints and documentation
   - Identify code smells and anti-patterns

2. **Review Design and Architecture**
   - Evaluate separation of concerns and modularity
   - Check for appropriate abstraction levels
   - Assess error handling and edge case coverage
   - Review API design and interface consistency

3. **Examine Performance Implications**
   - Identify potential bottlenecks or inefficiencies
   - Suggest algorithmic improvements where applicable
   - Check for proper resource management (memory, file handles, connections)
   - Evaluate async/concurrent code for correctness

4. **Security Considerations**
   - Identify potential security vulnerabilities
   - Check input validation and sanitization
   - Review authentication and authorization logic
   - Assess handling of sensitive data

5. **Testing and Maintainability**
   - Evaluate testability of the code
   - Suggest test cases for edge conditions
   - Check for proper logging and debugging support
   - Assess long-term maintainability

**Review Process:**
1. First, understand the code's purpose and context
2. Perform a systematic review covering all aspects above
3. Prioritize findings by severity (critical, major, minor, suggestion)
4. Provide specific, actionable feedback with code examples
5. Acknowledge good practices and well-written sections

**Output Format:**
Structure your review as follows:
- **Summary**: Brief overview of the code's purpose and overall quality
- **Strengths**: What the code does well
- **Critical Issues**: Must-fix problems that could cause bugs or security issues
- **Major Concerns**: Important improvements for code quality and maintainability
- **Minor Suggestions**: Nice-to-have improvements and style recommendations
- **Code Examples**: Provide improved versions of problematic code sections

**Important Guidelines:**
- Be constructive and educational in your feedback
- Explain the 'why' behind each recommendation
- Consider the project's context and existing patterns (especially from CLAUDE.md)
- Balance thoroughness with practicality
- If you notice the code uses specific frameworks or libraries, apply their best practices
- When suggesting changes, ensure they're compatible with the Python version in use
- If you're unsure about the broader context, ask clarifying questions

Remember: Your goal is to help improve code quality while fostering learning and best practices. Focus on the most impactful improvements and provide clear guidance on implementation.
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/test_gemini_mcp.py"&amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.gemini_mcp import GeminiMCP


@pytest.fixture
def gemini_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.0-flash"
    return GeminiMCP(config, secret_mgr, model)


@pytest.fixture
def gemini_25_preview():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-pro-preview-05-06"
    return GeminiMCP(config, secret_mgr, model)


def test_get_model_list(gemini_mcp):
    results = gemini_mcp.get_model_list()

    # Check that results is a list
    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;gt; 0

    # Check structure of each model in results
    for model in results:
        assert isinstance(model, dict)
        assert "model_name" in model
        assert "token_window" in model

        # Verify we only get 2.0 and 2.5 models (as per filter)
        assert "2.0" in model["model_name"] or "2.5" in model["model_name"]

        print(f"{model['model_name']}: {model['token_window']:,} tokens")


def test_send_message(gemini_mcp):
    message = "Hello, world!"
    response = gemini_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Response: {response}")


def test_count_tokens(gemini_mcp):
    text = "Hello, world!"
    token_count = gemini_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_gemini_25_preview_send_message(gemini_25_preview):
    message = "Explain quantum computing in one sentence."
    response = gemini_25_preview.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Gemini 2.5 Preview Response: {response}")


def test_gemini_25_preview_count_tokens(gemini_25_preview):
    text = "This is a test for Gemini 2.5 preview model token counting."
    token_count = gemini_25_preview.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;gt; 0

    print(f"Gemini 2.5 Preview - Token count for '{text}': {token_count}")


def test_extract_text(gemini_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = gemini_mcp.send_message(message)
    extracted_text = gemini_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_extract_text_gemini_25(gemini_25_preview):
    message = "Say 'Hello, Gemini 2.5!' and nothing else."
    response = gemini_25_preview.send_message(message)
    extracted_text = gemini_25_preview.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Gemini 2.5 extracted text: {extracted_text}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/test_anthropic_mcp.py"&amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP


@pytest.fixture
def anthropic_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    return AnthropicMCP(config, secret_mgr, model)


def test_get_model_list(anthropic_mcp):
    results = anthropic_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(anthropic_mcp):
    message = "Hello, world!"
    response = anthropic_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "content" in response
    assert "model" in response
    assert response["model"] == anthropic_mcp.config.anthropic_model_sonnet

    print(f"Response: {response}")


def test_extract_text(anthropic_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = anthropic_mcp.send_message(message)
    extracted_text = anthropic_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_generate_prompt(anthropic_mcp):
    """Test the generate_prompt method with a simple task."""
    task = "Write a helpful assistant prompt for answering coding questions"

    response = anthropic_mcp.generate_prompt(task)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role in ["user", "assistant"]
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;amp;gt; 0

    # Test usage stats
    assert hasattr(response.usage, "input_tokens")
    assert hasattr(response.usage, "output_tokens")
    assert isinstance(response.usage.input_tokens, int)
    assert isinstance(response.usage.output_tokens, int)
    assert response.usage.input_tokens &amp;amp;amp;gt; 0
    assert response.usage.output_tokens &amp;amp;amp;gt; 0

    print(f"Generated prompt: {content.text[:100]}...")
    print(
        f"Usage: {response.usage.input_tokens} input, {
          response.usage.output_tokens} output tokens"
    )


def test_improve_prompt(anthropic_mcp):
    """Test the improve_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Tell me about Python programming"}
                ],
            }
        ],
        "system": "You are a helpful programming instructor",
        "feedback": "Make this prompt more specific for a beginner",
        "target_model": "claude-3-7-sonnet-20250219",
    }

    response = anthropic_mcp.improve_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages - should have both user and assistant messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;gt;= 2

    # Test user message (improved prompt)
    user_message = response.messages[0]
    assert user_message.role == "user"
    assert isinstance(user_message.content, list)
    assert len(user_message.content) &amp;amp;amp;gt; 0

    # Find the non-empty content in user message
    user_content = None
    for content in user_message.content:
        if content.text:
            user_content = content
            break

    assert user_content is not None, "No non-empty content found in user message"
    assert hasattr(user_content, "text")
    assert hasattr(user_content, "type")
    assert user_content.type == "text"
    assert isinstance(user_content.text, str)
    assert len(user_content.text) &amp;amp;amp;gt; 0

    # Test assistant message (prefill)
    assistant_message = response.messages[1]
    assert assistant_message.role == "assistant"
    assert isinstance(assistant_message.content, list)
    assert len(assistant_message.content) &amp;amp;amp;gt; 0

    assistant_content = assistant_message.content[0]
    assert hasattr(assistant_content, "text")
    assert hasattr(assistant_content, "type")
    assert assistant_content.type == "text"
    assert isinstance(assistant_content.text, str)
    assert len(assistant_content.text) &amp;amp;amp;gt; 0

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;amp;gt; 0

    print(f"Improved prompt: {user_content.text[:100]}...")
    print(f"Assistant prefill: {assistant_content.text[:50]}...")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )


def test_templatize_prompt(anthropic_mcp):
    """Test the templatize_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Translate hello to German"}],
            }
        ],
        "system": "You are an English to German translator",
    }

    response = anthropic_mcp.templatize_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")
    assert hasattr(response, "variable_values")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role == "user"
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;amp;gt; 0
    # Check for template variables
    assert "{{" in content.text and "}}" in content.text

    # Test system prompt
    assert isinstance(response.system, str)
    # System prompt should also contain template variables
    assert "{{" in response.system and "}}" in response.system

    # Test variable_values
    assert isinstance(response.variable_values, dict)
    assert len(response.variable_values) &amp;amp;amp;gt; 0
    # Check for expected variables based on the example
    assert any(
        key in response.variable_values
        for key in ["TARGET_LANGUAGE", "WORD_TO_TRANSLATE"]
    )

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;amp;gt; 0

    print(f"Templated prompt: {content.text[:100]}...")
    print(f"System prompt: {response.system[:100]}...")
    print(f"Variables: {response.variable_values}")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/test_openai_mcp.py"&amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.openai_mpc import OpenAIMCP


@pytest.fixture
def openai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gpt-4o"
    return OpenAIMCP(config, secret_mgr, model)


def test_get_model_list(openai_mcp):
    results = openai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(openai_mcp):
    message = "Hello, world!"
    response = openai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(openai_mcp):
    text = "Hello, world!"
    token_count = openai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(openai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = openai_mcp.send_message(message)
    extracted_text = openai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/anthropic_mpc.py"&amp;amp;gt;from config import Config
from secret_manager import SecretManager
from models.anthropic_prompt_generate import PromptGenerateResponse
from models.anthropic_prompt_improve import PromptImproveResponse
from models.anthropic_prompt_templatize import PromptTemplatizeResponse

import requests


class AnthropicMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.headers = self.build_headers()

    def build_headers(self) -&amp;amp;amp;gt; dict:
        anthropic_key = self.secret_mgr.get_secret(self.config.anthropic_key_path)

        return {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "anthropic-beta": "prompt-tools-2025-04-02",
        }

    def get_model_list(self):
        response = requests.get(
            "https://api.anthropic.com/v1/models", headers=self.headers
        )
        response.raise_for_status()

        model_data = response.json()
        name_list = [model["id"] for model in model_data["data"]]

        return name_list

    def extract_text(self, ai_response: dict) -&amp;amp;amp;gt; str:
        """Extract text from Anthropic response format."""
        if not isinstance(ai_response, dict):
            return str(ai_response)

        # Anthropic format
        if "content" in ai_response:
            content = ai_response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;gt; dict:
        try:
            # Use provided model or default to config model
            if model is None:
                model = self.config.anthropic_model_sonnet

            data = {
                "model": model,
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": message}],
            }

            url = "https://api.anthropic.com/v1/messages"
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None):
        # Use provided model or default to config model
        if model is None:
            model = self.config.anthropic_model_sonnet

        data = {"model": model, "messages": [{"role": "user", "content": message}]}

        url = "https://api.anthropic.com/v1/messages/count_tokens"
        response = requests.post(url, headers=self.headers, json=data)
        response.raise_for_status()

        result = response.json()
        return result["input_tokens"]

    def generate_prompt(
        self, task: str, target_model: str = None
    ) -&amp;amp;amp;gt; PromptGenerateResponse:
        """
        Generate an optimized prompt using Anthropic's experimental prompt tools API.

        This method utilizes Anthropic's closed research preview API to automatically
        generate high-quality prompts based on a task description. The API creates
        structured prompts suitable for use with Claude models.

        Args:
            task (str): Description of the prompt's purpose
                Example: "a chef for a meal prep planning service"
            target_model (str, optional): Target model for optimization
                Example: "claude-3-7-sonnet-20250219"

        Returns:
            PromptGenerateResponse: Response object containing:
                - messages: List of message objects for use with Messages API
                  - User message with generated prompt text
                  - Optional assistant message with response guidance
                - system: System prompt (currently always empty string)
                - usage: Token usage statistics (input/output tokens)

        Raises:
            RuntimeError: If API request fails or network issues occur
            ValueError: If required configuration/secrets are missing
            requests.HTTPError: If API returns error status codes

        Example:
            &amp;amp;amp;gt;&amp;amp;amp;gt;&amp;amp;amp;gt; response = anthropic_mcp.generate_prompt("a helpful programming assistant")
            &amp;amp;amp;gt;&amp;amp;amp;gt;&amp;amp;amp;gt; prompt_text = response.messages[0].content[0].text
            &amp;amp;amp;gt;&amp;amp;amp;gt;&amp;amp;amp;gt; print(f"Generated prompt: {prompt_text}")

        Note:
            - This is an experimental API in closed research preview
            - Access requires explicit invitation from Anthropic
            - Requires anthropic-beta header: "prompt-tools-2025-04-02"
            - No long-term support guarantees for experimental features
            - Designed primarily for prompt engineering platforms

        API Documentation:
            https://docs.anthropic.com/en/api/prompt-tools-generate
        """
        url = "https://api.anthropic.com/v1/experimental/generate_prompt"

        # Format the task string as a dict for the API
        data = {"task": task}
        if target_model:
            data["target_model"] = target_model

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            return PromptGenerateResponse(**response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def improve_prompt(self, data: dict) -&amp;amp;amp;gt; PromptImproveResponse:
        url = "https://api.anthropic.com/v1/experimental/improve_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptImproveResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def templatize_prompt(self, data: dict) -&amp;amp;amp;gt; PromptTemplatizeResponse:
        url = "https://api.anthropic.com/v1/experimental/templatize_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptTemplatizeResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to templatize prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in templatize_prompt: {e}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/anthropic_prompt_generate.py"&amp;amp;gt;from typing import List, Optional
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: Optional[int] = None
    cache_read_input_tokens: Optional[int] = None
    service_tier: Optional[str] = None


class PromptGenerateResponse(BaseModel):
    """Response from Anthropic's prompt tools generate API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the generated prompt text,
    and may include an assistant message with a prefill."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: UsageStats
    """Token usage statistics for the generation."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;amp;lt;generated prompt&amp;amp;amp;gt;", "type": "text"}],
                "role": "user",
            }
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptGenerateResponse(**example_json)
    print(f"Generated prompt: {response.messages[0].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/__init__.py" /&amp;amp;gt;
  &amp;amp;lt;file path="models/anthropic_prompt_improve.py"&amp;amp;gt;from typing import List
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;amp;lt;improved prompt&amp;amp;amp;gt;", "type": "text"}],
                "role": "user",
            },
            {
                "content": [{"text": "&amp;amp;amp;lt;assistant prefill&amp;amp;amp;gt;", "type": "text"}],
                "role": "assistant",
            },
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/anthropic_prompt_templatize.py"&amp;amp;gt;from typing import List, Dict
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptTemplatizeResponse(BaseModel):
    """Response from Anthropic's prompt tools templatize API."""

    messages: List[Message]
    """List of message objects with templated variables."""

    system: str = ""
    """System prompt with templated variables."""

    usage: List[UsageStats]
    """Token usage statistics for the templatization."""

    variable_values: Dict[str, str]
    """Dictionary mapping template variable names to their extracted values."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the templatize endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}",
                        "type": "text",
                    }
                ],
                "role": "user",
            }
        ],
        "system": "You are a professional English to {{TARGET_LANGUAGE}} translator",
        "usage": [{"input_tokens": 490, "output_tokens": 661}],
        "variable_values": {"TARGET_LANGUAGE": "German", "WORD_TO_TRANSLATE": "hello"},
    }

    # Parse into Pydantic model
    response = PromptTemplatizeResponse(**example_json)
    print(f"Templated prompt: {response.messages[0].content[0].text}")
    print(f"System prompt: {response.system}")
    print(f"Variables: {response.variable_values}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/xai_mcp.py"&amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests


class XaiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        try:
            response = requests.get("https://api.x.ai/v1/models", headers=headers)
            response.raise_for_status()
            data = response.json()

            name_list = [model["id"] for model in data["data"]]
            return name_list

        except Exception as e:
            print(f"Error fetching XAI models: {str(e)}")
            return []

    def extract_text(self, response) -&amp;amp;amp;gt; str:
        """Extract text from XAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # XAI format (same as OpenAI)
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, model: str = None, reasoning_effort: str = "high"
    ):
        try:
            xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

            headers = {
                "Authorization": f"Bearer {xai_key}",
                "Content-Type": "application/json",
            }

            # Use provided model or default to config model
            if model is None:
                model = "grok-3-mini-fast-latest"

            data = {
                "messages": [
                    {"role": "system", "content": self.config.grok_system_prompt},
                    {"role": "user", "content": message},
                ],
                "reasoning_effort": reasoning_effort,
                "model": model,
            }

            url = "https://api.x.ai/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to XAI: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, text: str, model: str = None):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        # Use provided model or default to config model
        if model is None:
            model = "grok-3-fast-latest"

        data = {"model": model, "text": text}

        url = "https://api.x.ai/v1/tokenize-text"
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()

        result = response.json()
        return len(result["token_ids"])
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/openai_mpc.py"&amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests
import tiktoken


class OpenAIMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self) -&amp;amp;amp;gt; list:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {"Authorization": f"Bearer {openai_key}"}

            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            response.raise_for_status()

            model_data = response.json()
            name_list = [model["id"] for model in model_data["data"]]

            return name_list

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def extract_text(self, response) -&amp;amp;amp;gt; str:
        """Extract text from OpenAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # OpenAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;gt; dict:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {openai_key}",
            }

            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # Use max_completion_tokens for reasoning models (o3, o1 series)
            if model and ("o3" in model or "o1" in model):
                data = {
                    "model": model,
                    "max_completion_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }
            else:
                data = {
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }

            url = "https://api.openai.com/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;amp;gt; int:
        try:
            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # OpenAI doesn't have a direct token counting API, so use tiktoken
            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(message))

        except Exception as e:
            raise RuntimeError(f"Failed to count tokens: {e}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/gemini_mcp.py"&amp;amp;gt;from config import Config
from secret_manager import SecretManager
import requests
from fetcher import Fetcher
from mcp.server.fastmcp import Context
from typing import Dict, List


class GeminiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.api_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)
        self.base_url = self.config.gemini_base_url

    def get_model_list(self) -&amp;amp;amp;gt; Dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            base_url = self.config.gemini_base_url
            url = f"{base_url}models?key={gemini_key}"
            response = requests.get(url)
            response.raise_for_status()

            return self.filter_models(["2.0", "2.5"], response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def filter_models(
        self, versions: List[str], model_endpoint_response: Dict
    ) -&amp;amp;amp;gt; List[Dict]:
        """
        Filter models by version numbers and include token limits.

        Args:
            versions: List of version strings (e.g., ['2.0', '2.5'])

        Returns:
            List of dicts with model info including inputTokenLimit
        """
        filtered_models = []

        for model in model_endpoint_response["models"]:
            model_name = model["name"].split("/")[-1]

            for version in versions:
                if version in model_name:
                    model_to_tokencount = {
                        "model_name": model_name,
                        "token_window": model.get("inputTokenLimit", 0),
                    }
                    filtered_models.append(model_to_tokencount)

        filtered_models.sort(key=lambda x: x["token_window"], reverse=True)
        return filtered_models

    def extract_text(self, ai_response: dict) -&amp;amp;amp;gt; str:
        # Extract text from response
        if "candidates" in ai_response and len(ai_response["candidates"]) &amp;amp;amp;gt; 0:
            candidate = ai_response["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                if len(parts) &amp;amp;amp;gt; 0 and "text" in parts[0]:
                    return parts[0]["text"]
        return str(ai_response)

    async def build_prompt_from_url(
        self, url: str, prompt: str, ctx: Context = None
    ) -&amp;amp;amp;gt; str:

        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        concat = prompt + response

        ai_response = self.send_message(
            concat, max_tokens=1024, model="gemini-2.5-flash-preview-05-20"
        )

        return self.extract_text(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;amp;gt; dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:generateContent?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": message}]}],
                "generationConfig": {"maxOutputTokens": max_tokens},
            }

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;amp;gt; int:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            # Fix common model name errors
            if model == "gemini-2.5-pro-preview":
                model = "gemini-2.5-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:countTokens?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {"contents": [{"parts": [{"text": message}]}]}

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            result = response.json()
            return result["totalTokens"]

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to count tokens with Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in count_tokens: {e}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="models/test_xai_mcp.py"&amp;amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.xai_mcp import XaiMCP


@pytest.fixture
def xai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "grok-3-mini-fast-latest"
    return XaiMCP(config, secret_mgr, model)


def test_get_model_list(xai_mcp):
    results = xai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(xai_mcp):
    message = "Hello, world!"
    response = xai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(xai_mcp):
    text = "Hello, world!"
    token_count = xai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(xai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = xai_mcp.send_message(message)
    extracted_text = xai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="api/prompt_api.py"&amp;amp;gt;from fastapi import APIRouter, Depends, Request, HTTPException
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from config import Config

prompt_api_router = APIRouter()


def get_db_connection(request: Request):
    """Create database connection as a dependency using app state"""
    db_path = request.app.state.db_path
    db = SQLite3Database(db_path=db_path)
    conn = db.get_connection()
    try:
        yield conn
    finally:
        conn.close()


def get_prompt_service(conn=Depends(get_db_connection)):
    """Get prompt service instance with injected database connection"""

    config = Config()
    return PromptService(conn, config)


@prompt_api_router.get("/")
async def welcome() -&amp;amp;amp;gt; dict:
    return {"message": "Welcome to the prompt api service"}


@prompt_api_router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "prompt_api"}


@prompt_api_router.get("/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: str, prompt_service: PromptService = Depends(get_prompt_service)
):
    prompt = prompt_service.get_prompt_by_id(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="prompt not found")

    return prompt
&amp;amp;lt;/file&amp;amp;gt;
  &amp;amp;lt;file path="api/__init__.py"&amp;amp;gt;from .prompt_api import prompt_api_router

__all__ = ["prompt_api_router"]
&amp;amp;lt;/file&amp;amp;gt;
&amp;amp;lt;/source_code&amp;amp;gt;&amp;lt;/file&amp;gt;
  &amp;lt;file path="CLAUDE.md"&amp;gt;---
allowed-tools: Bash(tools/*)
description: scripts that can be perused and run where appropriate see the examples in this prompt
---

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

**Setup:**
```bash
uv sync
```

**Testing:**
### Use `pytest` for all testing in this project.
### When running all tests, use the Makefile and run test-fast:
### here is an example

```bash
make test-fast
```
### OR use the following bash command:
```bash
uv run pytest -v -n auto -m "not slow"
```


## IMPORTANT: Always Always use uv run when running tests
### Here is an example
```bash
uv run pytest test_collect.py::test_function_name -v -s
# Run specific test: pytest test_collect.py::test_function_name -v -s
```
**Code Quality:**
```bash
make lint     # Run ruff check
make format   # Run black formatter
make check    # Run all: lint, format, test
```

**Run MCP Server:**
```bash
uv run collect.py
```

**Plan Management:**
```bash
# Sync plans from filesystem to database
uv run -m repository.plan_service

# Test plan service functionality (uses separate test database)
uv run pytest repository/test_plan_service.py -v -s

# Test plan database operations
uv run pytest repository/test_plan_service.py::test_sync_plans -v -s

# Set up test database manually (optional - done automatically by tests)
uv run yoyo apply --config yoyo-test-plans.ini --batch

# Reset test database to clean state
rm data/test_plans.db &amp;amp;amp;&amp;amp;amp; uv run yoyo apply --config yoyo-test-plans.ini --batch

# Test database management utilities
uv run python repository/test_database_setup.py setup
uv run python repository/test_database_setup.py reset
uv run python repository/test_database_setup.py cleanup
```

## Planning System

This project uses a structured planning approach for feature development with plans organized in `_docs/plans/`. Plans progress through three stages:

### Plan Lifecycle
1. **`drafts/`** - Initial plans under development or consideration
2. **`approved/`** - Reviewed plans ready for implementation
3. **`completed/`** - Implemented plans with results documented

### Plan Document Format

#### Draft/Approved Plans Should Include:
```markdown
# Plan: [Clear Action-Oriented Title]

## Overview
Brief description of what needs to be implemented and why

## Implementation Steps
### 1. [Step Name]
Detailed implementation instructions including:
- Specific file locations and line numbers
- Function signatures with type hints
- Implementation pseudo-code or actual code
- Error handling approach

### 2. [Next Step]
...

## Key Features
- List of main features/capabilities
- Expected benefits

## Testing Considerations
- Test scenarios to implement
- Edge cases to handle
- Performance considerations

## Example Usage
```python
# Code examples demonstrating the feature
```
```

#### Completed Plans Add:
- **Status**: COMPLETED (YYYY-MM-DD)
- **Implementation Summary**: What was actually done
- **Results**: Outcomes, verification, test results
- **Files Modified**: List with ✅/❌ status indicators

### Plan Naming Conventions
- Use descriptive names with underscores: `add_improve_prompt.md`
- Start with action verbs: add, fix, implement, create, update
- Keep names concise but clear about the purpose

### Automated Plan Processing

The repository includes tools for automated plan implementation:

```python
# Build git worktrees for all approved plans
from mcp__collect__build_worktrees import build_worktrees
result = await build_worktrees(auto_process=True)  # Uses Claude Code SDK

# Sync plans between filesystem and database
from repository.plan_service import PlanService
service = PlanService(conn)
result = service.sync_plans()  # Loads plans into SQLite with JSONB
```

Branch names are automatically derived from plan filenames:
- `add_improve_prompt.md` → `feature/add-improve-prompt`
- Underscores become hyphens, `feature/` prefix added

### Plan Management Database

Plans are tracked in `data/plans.db` with:
- **plans** table: Current state with JSONB data field
- **plan_history**: Audit trail of changes
- **plan_metrics**: Analytics and performance data

Use the PlanService class to:
- Load plans from disk to database
- Track plan status changes
- Detect content changes via SHA256 hashing
- Query plans by status, tags, or content

## Architecture Overview

This is an MCP (Model Context Protocol) server that provides web content fetching and multi-model AI analysis tools. The architecture follows these key patterns:

### Core Structure
- **collect.py**: Main MCP server entry point with FastMCP tool definitions
- **fetcher.py**: Handles URL fetching and content processing with clipboard integration
- **config.py**: Environment-based configuration with dotenv support
- **secret_manager.py**: Google Cloud Secret Manager integration for API keys

### Models Package
The `models/` directory contains unified API wrappers for different AI providers:
- **anthropic_mpc.py**: Anthropic Claude API integration
- **openai_mpc.py**: OpenAI API integration  
- **gemini_mcp.py**: Google Gemini API integration
- **xai_mcp.py**: XAI/Grok API integration

Each model wrapper follows the same pattern: configuration injection, secret management, and standardized methods like `send_message()`, `count_tokens()`, and `get_model_list()`.

### Packages
Additional specialized packages provide focused functionality:
- **reviewer/**: Code review automation system
  - `code_review.py`: CodeReviewer class for analyzing code diffs
  - Supports both file-based and git diff reviews
  - Generates individual model reviews and consolidated summaries
- **repository/**: Plan management and database operations
  - `plan_service.py`: PlanService class for filesystem-to-database sync
  - `plan_models.py`: Pydantic models for plan data with JSONB support
  - `database.py`: SQLite connection management with custom datetime adapters
  - Supports plan lifecycle tracking and content change detection


### Key Features
- **Async token counting**: All providers support async token counting with proper chunking
- **Multi-model workflows**: Send content to all AI models concurrently via `multi_model_code_review()`
- **Content processing**: HTML-to-markdown conversion using readabilipy and markdownify
- **Automatic chunking**: Handles large content (&amp;amp;gt;25k tokens) with intelligent splitting
- **Code review system**: Automated code review via `run_code_review()` and `run_git_diff_review()` tools
- **Prompt engineering**: Generate optimized AI prompts using Anthropic's experimental API via `generate_prompt()`
- **Documentation extraction**: Intelligent section extraction from web docs using `get_docs()` with AI filtering
- **Clipboard integration**: Direct content copying with `copy_clipboard()` and automatic clipboard support in fetchers
- **Enhanced model features**: Gemini model listing with token limits, unified `extract_text()` methods across all providers

### Configuration
Environment variables are loaded from `.env` file:
- GCP_PROJECT_ID (required)
- API key paths for Google Cloud Secret Manager:
  - ANTHROPIC_API_KEY_PATH
  - OPENAI_API_KEY_PATH
  - GEMINI_API_KEY_PATH
  - XAI_API_KEY_PATH
- Default model names for each provider
- Code review model configurations

### Directory Structure
```
collect/
├── data/
│   ├── prompts.db      # Original prompts database
│   └── plans.db        # Plan management database
├── _docs/
│   └── plans/
│       ├── drafts/     # Plans under development
│       ├── approved/   # Plans ready for implementation
│       └── completed/  # Implemented plans with results
├── migrations/         # Database migrations for prompts.db
├── migrations-plans/   # Database migrations for plans.db
├── repository/         # Plan management system
│   ├── plan_service.py # Plan filesystem-to-database sync
│   ├── plan_models.py  # Pydantic models for plan data
│   └── database.py     # SQLite connection management
├── models/            # AI provider API wrappers
└── reviewer/          # Code review automation
```

### Testing Strategy
- **IMPORTANT**:  When writing and designing tests, we only want live direct integration tests. Please only create live direct integration testing. Please do not use mocks. 

## Rules
- **IMPORTANT**: YOU MUST always use `uv run` to run tests.

## Workflow Rules
- I do not want a pr created if I don't have a branch already

## Tools

###IMPORTANT: 
I have a directory from the main project directory called: tools/* wherein there scripts stored that you can use
use

- All of my tools in this directory are on my path and can be called directly.
- You use these tools and see what they do by simply calling the tool name with `--llm`

Example 1:
```bash
extract --llm
```

Example 2: 
```bash
createdb --llm
```


&amp;lt;/file&amp;gt;
  &amp;lt;file path="fetcher.py"&amp;gt;from typing import List
from mcp.server.fastmcp import Context
import pyperclip
import httpx
from models.anthropic_mpc import AnthropicMCP


class Fetcher:
    def __init__(self, ctx: Context = None) -&amp;amp;gt; None:
        self.ctx = ctx

    async def get(self, url: str) -&amp;amp;gt; str:
        """
        Fetch content from a single URL.

        Args:
            url: URL to fetch content from

        Returns:
            Content from the URL as a string
        """

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                content = response.text

                return content

            except httpx.HTTPError as e:
                return f"Error fetching {url}: {str(e)}"
            except Exception as e:
                return f"Error fetching {url}: {str(e)}"

    async def fetch_urls(self, urls: List[str]) -&amp;amp;gt; str:
        """
        Fetch content from multiple URLs and concatenate their responses.
        If token count exceeds 25000, content is split into chunks.

        Args:
            urls: List of URLs to fetch content from
            ctx: Optional context object for progress reporting

        Returns:
            Either concatenated content from all URLs as a string,
            or a list of content chunks if token count exceeds 25000
        """

        results = []

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for i, url in enumerate(urls):
                if self.ctx:
                    self.ctx.info(f"Fetching content from {url}")
                    await self.ctx.report_progress(i, len(urls))

                try:
                    response = await client.get(url)
                    response.raise_for_status()

                    results.append(f"\n\n--- Content from {url} --\n\n")
                    results.append(response.text)

                except httpx.HTTPError as e:
                    results.append(f"\n\n --- Error fetching {url}: {str(e)} ---\n\n")
                except Exception as e:
                    results.append(f"\n\n--- error fetching {url}: {str(e)} ---\n\n")

        if self.ctx:
            self.ctx.info("all urls processed")
            await self.ctx.report_progress(len(urls), len(urls))

        content = "".join(results)

        # Copy original content to clipboard
        pyperclip.copy(content)

        # Otherwise return the original content
        return content

    async def chunk_by_token_count(text: str, max_tokens: int = 25000) -&amp;amp;gt; List[str]:
        """
        Split text into chunks that are each under the specified token count.

        Args:
            text: The text to chunk
            max_tokens: Maximum tokens per chunk

        Returns:
            List of text chunks, each under max_tokens
        """

        # If text is short enough, return as a single chunk
        anthropic_mcp = AnthropicMCP()
        token_count = await anthropic_mcp.count_tokens(text, None)
        if token_count &amp;amp;lt;= max_tokens:
            return [text]

        # Split text into paragraphs as a starting point
        paragraphs = text.split("\n\n")
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0

        for paragraph in paragraphs:
            paragraph_tokens = await anthropic_mcp.count_tokens(
                paragraph + "\n\n", None
            )

            # If adding this paragraph would exceed the limit,
            # start a new chunk
            if current_chunk_tokens + paragraph_tokens &amp;amp;gt; max_tokens:
                # If the paragraph alone exceeds the limit, we split it further
                if paragraph_tokens &amp;amp;gt; max_tokens:
                    # Split by sentences or just characters if needed
                    sentences = paragraph.split(". ")
                    for sentence in sentences:
                        sentence_tokens = await anthropic_mcp.count_tokens(
                            sentence + ". ", None
                        )
                        if current_chunk_tokens + sentence_tokens &amp;amp;gt; max_tokens:
                            if current_chunk:
                                chunks.append("".join(current_chunk))
                            current_chunk = [sentence + ". "]
                            current_chunk_tokens = sentence_tokens
                        else:
                            current_chunk.append(sentence + ". ")
                            current_chunk_tokens += sentence_tokens
                else:
                    # Save the current chunk and start a new one
                    chunks.append("".join(current_chunk))
                    current_chunk = [paragraph + "\n\n"]
                    current_chunk_tokens = paragraph_tokens
            else:
                # Add paragraph to current chunk
                current_chunk.append(paragraph + "\n\n")
                current_chunk_tokens += paragraph_tokens

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append("".join(current_chunk))

        return chunks
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_diff.md"&amp;gt;# Test Code Review

## Diff

```diff
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,5 +1,8 @@
 def calculate_total(items):
+    if not items:
+        return 0
+        
     total = 0
     for item in items:
-        total += item.price
+        total += item.get('price', 0)
     return total
```&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/code_review.py"&amp;gt;import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -&amp;amp;gt; str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -&amp;amp;gt; str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
        self, output_dir: str, timestamp: str, failed_results: List
    ) -&amp;amp;gt; str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, "w", encoding="utf-8") as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -&amp;amp;gt; str:
        """Read and return content from input file."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -&amp;amp;gt; str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 🔴/🟡/🟢 | ... | ... |

Use emojis: 🔴 Critical, 🟡 Important, 🟢 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
        self, timestamp: str, from_file: str, results: LLMRunnerResults
    ) -&amp;amp;gt; Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": [],
        }

    def write_successful_results(
        self,
        results: LLMRunnerResults,
        output_dir: str,
        timestamp: str,
        summary: Dict[str, Any],
    ) -&amp;amp;gt; None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(result, response_text)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
        self, output_dir: str, timestamp: str, summary: Dict[str, Any]
    ) -&amp;amp;gt; str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
        self, from_file: str, to_file: Optional[str] = None
    ) -&amp;amp;gt; Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }

    async def review_diff_from_git(
        self, to_file: Optional[str] = None, staged_only: bool = True
    ) -&amp;amp;gt; Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ["git", "diff", "--staged"],
                    capture_output=True,
                    text=True,
                    check=True,
                )
            else:
                result = subprocess.run(
                    ["git", "diff"], capture_output=True, text=True, check=True
                )

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception("Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_code_review_live.py"&amp;gt;import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"

        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir

        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] &amp;amp;gt;= 1
        assert (
            summary["successful_reviews"] + summary["failed_reviews"]
            == summary["total_models"]
        )
        assert "timestamp" in summary

        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) &amp;amp;gt;= 1  # At least summary file should exist

        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1

        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, "r") as f:
            summary_data = json.load(f)

        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]

        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)

            # Verify file contains expected content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                assert "Code Review" in content
                assert "**Model**:" in content
                assert "**Timestamp**:" in content
                assert "**Duration**:" in content
                assert "Generated by" in content

        # Print results for visibility
        print("\n✅ Live code review test completed successfully!")
        print("📊 Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")

        if summary["successful_reviews"] &amp;amp;gt; 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")

        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(
                temp_output_dir, staged_only=False
            )

            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"

            print("\n✅ Git diff review completed!")
            print(
                f"📊 Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed"
            )

        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"

        prompt = reviewer.create_code_review_prompt(test_content)

        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "🔴" in prompt and "🟡" in prompt and "🟢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] &amp;amp;gt; 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Should contain actual review content, not just raw API response
            assert len(content) &amp;amp;gt; 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON

            print(f"✅ Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/__init__.py"&amp;gt;# Reviewer package for code review functionality
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_code_review.py"&amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import Mock, patch, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


# Module-level fixtures available to all test classes
@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def reviewer(temp_dir):
    """Create a CodeReviewer instance with temp directory."""
    return CodeReviewer(output_dir=temp_dir)


@pytest.fixture
def sample_diff_content():
    """Sample diff content for testing."""
    return """
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,6 @@
 def hello():
-    print("Hello")
+    print("Hello World")
+    return "greeting"
+
+def goodbye():
+    print("Goodbye")
"""


@pytest.fixture
def mock_model_result():
    """Create a mock successful model result."""
    return ModelResult(
        model="test-model",
        timestamp="2024-01-01T12:00:00",
        success=True,
        actual_model="test-model",
        duration_seconds=2.5,
        response={
            "choices": [
                {
                    "message": {
                        "content": "# Code Review\n\nThis is a test review response."
                    }
                }
            ]
        },
    )


@pytest.fixture
def mock_failed_result():
    """Create a mock failed model result."""
    return ModelResult(
        model="failed-model",
        timestamp="2024-01-01T12:00:00",
        success=False,
        error="API timeout error",
    )


@pytest.fixture
def mock_llm_results(mock_model_result, mock_failed_result):
    """Create mock LLMRunnerResults."""
    return LLMRunnerResults(
        successful_results=[mock_model_result],
        failed_results=[mock_failed_result],
        total_models=2,
        success_count=1,
        failure_count=1,
    )


class TestCodeReviewer:
    """Test suite for CodeReviewer class."""


class TestInitialization:
    """Test CodeReviewer initialization."""

    def test_default_initialization(self):
        """Test CodeReviewer with default parameters."""
        reviewer = CodeReviewer()
        assert reviewer.output_dir == "codereview"

    def test_custom_output_dir(self):
        """Test CodeReviewer with custom output directory."""
        custom_dir = "/tmp/custom_reviews"
        reviewer = CodeReviewer(output_dir=custom_dir)
        assert reviewer.output_dir == custom_dir


class TestExtractResponseText:
    """Test response text extraction from different model formats."""

    def test_extract_gemini_response(self, reviewer):
        """Test extracting text from Gemini response format."""
        gemini_response = {
            "candidates": [
                {"content": {"parts": [{"text": "This is a Gemini response"}]}}
            ]
        }
        result = reviewer.extract_response_text(gemini_response)
        assert result == "This is a Gemini response"

    def test_extract_openai_response(self, reviewer):
        """Test extracting text from OpenAI/XAI response format."""
        openai_response = {
            "choices": [{"message": {"content": "This is an OpenAI response"}}]
        }
        result = reviewer.extract_response_text(openai_response)
        assert result == "This is an OpenAI response"

    def test_extract_anthropic_response(self, reviewer):
        """Test extracting text from Anthropic response format."""
        anthropic_response = {"content": [{"text": "This is an Anthropic response"}]}
        result = reviewer.extract_response_text(anthropic_response)
        assert result == "This is an Anthropic response"

    def test_extract_non_dict_response(self, reviewer):
        """Test extracting text from non-dictionary response."""
        simple_response = "Simple string response"
        result = reviewer.extract_response_text(simple_response)
        assert result == "Simple string response"

    def test_extract_unknown_format(self, reviewer):
        """Test extracting text from unknown response format."""
        unknown_response = {"unknown": "format"}
        result = reviewer.extract_response_text(unknown_response)
        assert result == str(unknown_response)

    def test_extract_empty_gemini_response(self, reviewer):
        """Test extracting from empty Gemini response."""
        empty_response = {"candidates": []}
        result = reviewer.extract_response_text(empty_response)
        assert result == str(empty_response)


class TestMarkdownContent:
    """Test markdown content creation."""

    def test_create_markdown_content(self, reviewer, mock_model_result):
        """Test creating markdown content for a model result."""
        response_text = "Test review content"
        result = reviewer.create_markdown_content(mock_model_result, response_text)

        assert "# Code Review - test-model" in result
        assert "**Model**: test-model" in result
        assert "**Timestamp**: 2024-01-01T12:00:00" in result
        assert "**Duration**: 2.50 seconds" in result
        assert "Test review content" in result
        assert "*Generated by test-model via MCP Code Review Tool*" in result


class TestFileOperations:
    """Test file reading and writing operations."""

    def test_read_input_file_success(self, reviewer, temp_dir, sample_diff_content):
        """Test successfully reading an input file."""
        test_file = os.path.join(temp_dir, "test_diff.md")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        result = reviewer.read_input_file(test_file)
        assert result == sample_diff_content

    def test_read_input_file_not_found(self, reviewer):
        """Test reading a non-existent file."""
        with pytest.raises(FileNotFoundError, match="Input file .* not found"):
            reviewer.read_input_file("/nonexistent/file.md")

    def test_write_error_file(self, reviewer, temp_dir, mock_failed_result):
        """Test writing error file for failed results."""
        timestamp = "20240101_120000"
        failed_results = [mock_failed_result]

        error_filename = reviewer.write_error_file(temp_dir, timestamp, failed_results)

        assert error_filename == "errors_20240101_120000.md"
        error_path = os.path.join(temp_dir, error_filename)
        assert os.path.exists(error_path)

        with open(error_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review Errors" in content
        assert "**Failed Models**: 1" in content
        assert "### failed-model" in content
        assert "API timeout error" in content

    def test_write_summary_file(self, reviewer, temp_dir):
        """Test writing summary JSON file."""
        timestamp = "20240101_120000"
        summary = {
            "timestamp": timestamp,
            "input_file": "test.md",
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": ["model1_20240101_120000.md"],
        }

        summary_filename = reviewer.write_summary_file(temp_dir, timestamp, summary)

        assert summary_filename == "summary_20240101_120000.json"
        summary_path = os.path.join(temp_dir, summary_filename)
        assert os.path.exists(summary_path)

        with open(summary_path, "r", encoding="utf-8") as f:
            loaded_summary = json.load(f)

        assert loaded_summary == summary


class TestPromptCreation:
    """Test code review prompt creation."""

    def test_create_code_review_prompt(self, reviewer, sample_diff_content):
        """Test creating a comprehensive code review prompt."""
        prompt = reviewer.create_code_review_prompt(sample_diff_content)

        assert "comprehensive code review" in prompt
        assert sample_diff_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Suggestions" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt


class TestSummaryCreation:
    """Test summary creation and management."""

    def test_create_summary(self, reviewer, mock_llm_results):
        """Test creating a summary dictionary."""
        timestamp = "20240101_120000"
        from_file = "test.md"

        summary = reviewer.create_summary(timestamp, from_file, mock_llm_results)

        expected_summary = {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": [],
        }

        assert summary == expected_summary

    def test_write_successful_results(self, reviewer, temp_dir, mock_llm_results):
        """Test writing successful model results to files."""
        timestamp = "20240101_120000"
        summary = {"output_files": []}

        reviewer.write_successful_results(
            mock_llm_results, temp_dir, timestamp, summary
        )

        # Check that file was created
        expected_filename = "test-model_20240101_120000.md"
        expected_path = os.path.join(temp_dir, expected_filename)
        assert os.path.exists(expected_path)

        # Check that summary was updated
        assert expected_filename in summary["output_files"]

        # Check file content
        with open(expected_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review - test-model" in content
        assert "This is a test review response." in content


class TestReviewCode:
    """Test the main review_code method."""

    @pytest.mark.asyncio
    async def test_review_code_success(
        self, reviewer, temp_dir, sample_diff_content, mock_llm_results
    ):
        """Test successful code review execution."""
        # Create input file
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Mock dependencies
        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # Verify result structure
            assert result["status"] == "completed"
            assert result["output_directory"] == temp_dir
            # 1 success + 1 error + 1 summary
            assert result["files_created"] == 3

            # Verify files were created
            assert os.path.exists(
                os.path.join(temp_dir, "test-model_20240101_120000.md")
            )
            assert os.path.exists(os.path.join(temp_dir, "errors_20240101_120000.md"))
            assert os.path.exists(
                os.path.join(temp_dir, "summary_20240101_120000.json")
            )

    @pytest.mark.asyncio
    async def test_review_code_file_not_found(self, reviewer, temp_dir):
        """Test review_code with non-existent input file."""
        with pytest.raises(FileNotFoundError):
            await reviewer.review_code("/nonexistent/file.md", temp_dir)

    @pytest.mark.asyncio
    async def test_review_code_no_failures(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code with only successful results."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Create results with no failures
        success_only_results = LLMRunnerResults(
            successful_results=[
                ModelResult(
                    model="test-model",
                    timestamp="2024-01-01T12:00:00",
                    success=True,
                    actual_model="test-model",
                    duration_seconds=2.5,
                    response={"choices": [{"message": {"content": "Review content"}}]},
                )
            ],
            failed_results=[],
            total_models=1,
            success_count=1,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_only_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # 1 success + 1 summary (no error file)
            assert result["files_created"] == 2
            assert "error_file" not in result["summary"]


class TestReviewDiffFromGit:
    """Test git diff review functionality."""

    @pytest.mark.asyncio
    async def test_review_diff_from_git_staged(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing staged git diff."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            # Setup mocks
            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess call
            mock_subprocess.assert_called_once_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"
            assert result["summary"]["input_file"] == "git diff"

    @pytest.mark.asyncio
    async def test_review_diff_from_git_all_changes(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing all git changes."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=False)

            mock_subprocess.assert_called_once_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

            assert result["summary"]["source"] == "git_diff_all"

    @pytest.mark.asyncio
    async def test_review_diff_no_changes(self, reviewer, temp_dir):
        """Test git diff with no changes."""
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value.stdout = ""
            mock_subprocess.return_value.check_returncode = Mock()

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_not_found(self, reviewer, temp_dir):
        """Test git diff when git is not installed."""
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_error(self, reviewer, temp_dir):
        """Test git diff with git command error."""
        import subprocess

        with patch(
            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
        ):
            with pytest.raises(Exception, match="Git diff failed"):
                await reviewer.review_diff_from_git(temp_dir)


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_extract_response_malformed_gemini(self, reviewer):
        """Test extracting from malformed Gemini response."""
        malformed = {"candidates": [{"content": {"parts": []}}]}  # Empty parts
        result = reviewer.extract_response_text(malformed)
        assert result == str(malformed)

    def test_extract_response_malformed_openai(self, reviewer):
        """Test extracting from malformed OpenAI response."""
        malformed = {"choices": [{"message": {}}]}  # Missing content
        result = reviewer.extract_response_text(malformed)
        assert result == ""

    def test_extract_response_empty_anthropic(self, reviewer):
        """Test extracting from empty Anthropic response."""
        empty = {"content": []}
        result = reviewer.extract_response_text(empty)
        assert result == str(empty)

    @pytest.mark.asyncio
    async def test_review_code_with_default_output_dir(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code using default output directory from None."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        success_results = LLMRunnerResults(
            successful_results=[],
            failed_results=[],
            total_models=0,
            success_count=0,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
            patch("os.makedirs") as mock_makedirs,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            # Test with None to_file (should use default)
            result = await reviewer.review_code(input_file, None)

            # Should use reviewer's default output_dir
            mock_makedirs.assert_called_with(temp_dir, exist_ok=True)
            assert result["output_directory"] == temp_dir


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_code_review_integration.py"&amp;gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import patch, MagicMock, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


class TestCodeReviewIntegration:
    """Test integration between .claude/commands/model_code_review.md and code_review.py"""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_diff_content(self):
        """Sample diff content for testing"""
        return """diff --git a/src/auth.py b/src/auth.py
index 1234567..abcdefg 100644
--- a/src/auth.py
+++ b/src/auth.py
@@ -1,5 +1,10 @@
 def authenticate_user(username, password):
-    # Simple authentication
-    return username == "admin" and password == "secret"
+    # Improved authentication with validation
+    if not username or not password:
+        return False
+    
+    # TODO: Add proper password hashing
+    return username == "admin" and password == "secret123"
 
 def get_user_role(username):
     return "admin" if username == "admin" else "user"
"""

    @pytest.fixture
    def sample_diff_file(self, temp_dir, sample_diff_content):
        """Create sample diff file for testing"""
        diff_file = os.path.join(temp_dir, "test_diff.md")
        with open(diff_file, "w") as f:
            f.write(sample_diff_content)
        return diff_file

    @pytest.fixture
    def mock_llm_results(self):
        """Mock LLM runner results matching expected format"""
        successful_results = [
            ModelResult(
                model="claude-3-5-sonnet",
                success=True,
                response={
                    "content": [
                        {
                            "text": "## Code Review Analysis\n\n### Security Issues\n🔴 **Critical**: Hardcoded password in authentication logic\n\n### Recommendations\n- Implement proper password hashing\n- Add input validation"
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:00",
                duration_seconds=2.5,
                error=None,
            ),
            ModelResult(
                model="gpt-4-turbo",
                success=True,
                response={
                    "choices": [
                        {
                            "message": {
                                "content": "## Security Analysis\n\n🔴 **High Risk**: Authentication uses plaintext password comparison\n🟡 **Medium**: Missing input validation for empty credentials"
                            }
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:05",
                duration_seconds=3.1,
                error=None,
            ),
        ]

        failed_results = [
            ModelResult(
                model="gemini-pro",
                success=False,
                response=None,
                timestamp="2024-01-01T12:00:10",
                duration_seconds=0,
                error="API rate limit exceeded",
            )
        ]

        return LLMRunnerResults(
            successful_results=successful_results,
            failed_results=failed_results,
            total_models=3,
            success_count=2,
            failure_count=1,
        )

    @pytest.mark.asyncio
    async def test_code_review_from_file_integration(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test the complete file-based code review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config", "gpt": "config"}

            # Initialize reviewer with temp directory
            reviewer = CodeReviewer(output_dir=temp_dir)

            # Run review (simulates mcp__collect__run_code_review)
            result = await reviewer.review_code(sample_diff_file, temp_dir)

            # Verify return structure matches command expectations
            assert result["status"] == "completed"
            assert "summary" in result
            assert "output_directory" in result
            assert "files_created" in result

            # Verify output files were created as documented in command
            files = os.listdir(temp_dir)

            # Should have individual model reviews
            claude_files = [f for f in files if f.startswith("claude-3-5-sonnet")]
            gpt_files = [f for f in files if f.startswith("gpt-4-turbo")]
            assert len(claude_files) == 1
            assert len(gpt_files) == 1

            # Should have summary file
            summary_files = [f for f in files if f.startswith("summary_")]
            assert len(summary_files) == 1

            # Should have errors file for failed models
            error_files = [f for f in files if f.startswith("errors_")]
            assert len(error_files) == 1

            # Verify summary JSON structure
            summary_file = os.path.join(temp_dir, summary_files[0])
            with open(summary_file, "r") as f:
                summary_data = json.load(f)

            assert summary_data["total_models"] == 3
            assert summary_data["successful_reviews"] == 2
            assert summary_data["failed_reviews"] == 1
            assert len(summary_data["output_files"]) == 2

    @pytest.mark.asyncio
    async def test_git_diff_review_integration(self, temp_dir, mock_llm_results):
        """Test git diff review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
            patch("subprocess.run") as mock_subprocess,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config"}

            # Mock git diff output
            mock_subprocess.return_value = MagicMock(
                stdout="diff --git a/test.py b/test.py\n+def new_function():\n+    pass",
                returncode=0,
            )

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Test staged-only review (Option A from command)
            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess called with correct git command
            mock_subprocess.assert_called_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"

            # Test all changes review
            await reviewer.review_diff_from_git(temp_dir, staged_only=False)
            mock_subprocess.assert_called_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

    def test_claude_command_workflow_documentation(self):
        """Verify that the Claude command documentation matches code_review.py capabilities"""
        reviewer = CodeReviewer()

        # Test that CodeReviewer has methods mentioned in command
        assert hasattr(
            reviewer, "review_code"
        ), "Should support file-based review (Option B)"
        assert hasattr(
            reviewer, "review_diff_from_git"
        ), "Should support git diff review (Option A)"

        # Test that review_code signature matches command usage
        import inspect

        sig = inspect.signature(reviewer.review_code)
        assert "from_file" in sig.parameters, "Should accept from_file parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

        # Test that review_diff_from_git signature matches command usage
        sig = inspect.signature(reviewer.review_diff_from_git)
        assert "staged_only" in sig.parameters, "Should accept staged_only parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

    def test_output_file_naming_convention(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test that output files follow naming convention documented in command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {}

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Mock timestamp for predictable filenames
            with patch("reviewer.code_review.datetime") as mock_datetime:
                mock_datetime.now.return_value.strftime.return_value = "20241201_143052"

                # Run async test
                import asyncio

                asyncio.run(reviewer.review_code(sample_diff_file, temp_dir))

            files = os.listdir(temp_dir)

            # Verify naming matches documentation: {model}_YYYYMMDD_HHMMSS.md
            expected_patterns = [
                "claude-3-5-sonnet_20241201_143052.md",
                "gpt-4-turbo_20241201_143052.md",
                "summary_20241201_143052.json",
                "errors_20241201_143052.md",
            ]

            for pattern in expected_patterns:
                assert pattern in files, f"Expected file {pattern} not found in {files}"

    def test_prompt_structure_matches_command_requirements(self):
        """Test that code review prompt includes all sections mentioned in command"""
        reviewer = CodeReviewer()
        prompt = reviewer.create_code_review_prompt("sample code")

        # Verify prompt includes all required sections from command documentation
        required_sections = [
            "Overall Assessment",
            "Issues Found",
            "Security vulnerabilities",
            "Bugs and logic errors",
            "Performance issues",
            "Code quality problems",
            "Testing gaps",
            "Suggestions for Improvement",
            "Positive Aspects",
            "Risk Assessment",
            "Summary Table",
        ]

        for section in required_sections:
            assert section in prompt, f"Prompt missing required section: {section}"

        # Verify emoji risk indicators are included
        risk_emojis = ["🔴", "🟡", "🟢"]
        for emoji in risk_emojis:
            assert emoji in prompt, f"Prompt missing risk emoji: {emoji}"

    @pytest.mark.asyncio
    async def test_error_handling_matches_command_expectations(self, temp_dir):
        """Test error handling for scenarios mentioned in command troubleshooting"""
        reviewer = CodeReviewer(output_dir=temp_dir)

        # Test "No git changes found" scenario
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value = MagicMock(stdout="", returncode=0)

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

        # Test file not found scenario
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_dir)

        # Test git not available scenario
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    def test_response_text_extraction_supports_all_models(self):
        """Test that response extraction works for all model formats mentioned in command"""
        reviewer = CodeReviewer()

        # Test Anthropic Claude format
        anthropic_response = {"content": [{"text": "Claude review content"}]}
        assert (
            reviewer.extract_response_text(anthropic_response)
            == "Claude review content"
        )

        # Test OpenAI GPT format
        openai_response = {"choices": [{"message": {"content": "GPT review content"}}]}
        assert reviewer.extract_response_text(openai_response) == "GPT review content"

        # Test Google Gemini format
        gemini_response = {
            "candidates": [{"content": {"parts": [{"text": "Gemini review content"}]}}]
        }
        assert (
            reviewer.extract_response_text(gemini_response) == "Gemini review content"
        )

        # Test fallback for XAI Grok or unknown formats
        unknown_response = "Direct string response"
        assert (
            reviewer.extract_response_text(unknown_response) == "Direct string response"
        )
&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview_live/errors_20250601_085957.md"&amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_085957
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.646644

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.833688

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.262231

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.434107

            &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview_live/summary_20250601_085957.json"&amp;gt;{
  "timestamp": "20250601_085957",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_085957.md"
}&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview/errors_20250601_084959.md"&amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084959
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.675555

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.837903

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.045514

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.245033

            &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview/summary_20250601_084601.json"&amp;gt;{
  "timestamp": "20250601_084601",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084601.md"
}&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview/summary_20250601_084959.json"&amp;gt;{
  "timestamp": "20250601_084959",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084959.md"
}&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/test_codereview/errors_20250601_084601.md"&amp;gt;
            # Code Review Errors

            **Timestamp**: 20250601_084601
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:43:35.383459

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:44:24.400527

            
                ### grok-3
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:45:01.153872

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 504 Deadline Exceeded
                - **Timestamp**: 2025-06-01T08:46:01.157505

            &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/live_test_codereview/grok-3-mini-fast-latest_20250601_091219.md"&amp;gt;
            # Code Review - grok-3-mini-fast-latest

            **Model**: grok-3-mini-fast-latest
            **Timestamp**: 2025-06-01T09:11:32.476330
            **Duration**: 17.72 seconds

            ---

            Below is a comprehensive code review of the provided diff for the `calculate_total` function in `test.py`, based on the instructions. I'll address each section step by step, focusing on actionable feedback to improve code quality, security, and maintainability.

---

### 1. **Overall Assessment**

- **Brief summary of what changed and why**: The changes enhance the robustness of the `calculate_total` function. An explicit check for an empty `items` list was added to return 0, preventing potential runtime errors. Additionally, the price access was modified from direct attribute access (`item.price`) to a dictionary-based approach (`item.get('price', 0)`), likely to handle cases where `items` are dictionaries rather than custom objects. This suggests a shift in data structure (e.g., from object-oriented to dictionary-based data), possibly to accommodate more flexible input formats or to avoid `AttributeError` when `price` is missing.

- **Impact on the codebase**: The scope is narrow, affecting only this function, but the significance is moderate. It improves error handling and reduces the risk of crashes in edge cases (e.g., empty lists or missing keys), making the code more resilient. However, if this function is used across the application, the change could introduce inconsistencies if other parts of the codebase still expect `items` to be objects with attributes. Overall, it aligns with defensive programming principles.

- **Alignment with best practices**: The changes are positive, as they address common pitfalls like unhandled edge cases and attribute access errors. Using `dict.get()` with a default value follows best practices for handling optional keys, promoting safer code. However, the function could benefit from additional improvements in type safety, documentation, and testing to fully align with modern Python standards (e.g., PEP 8, type hints).

---

### 2. **Issues Found**

I analyzed the diff for potential security, bugs, performance, code quality, and testing issues. Here's a breakdown:

- **Security vulnerabilities**: 
  - No significant security issues were identified. This function performs a simple summation and doesn't involve user input, network operations, or sensitive data handling, so risks like injection or data exposure are low. However, if `items` comes from an untrusted source (e.g., user input or external API), the function could be vulnerable to malicious data (e.g., if `items` contains non-numeric values). This isn't directly addressed in the change.

- **Bugs and logic errors**:
  - **Edge case handling**: The added empty list check is good, but the function now assumes `items` elements are dictionaries (due to `item.get()`). If `items` contains non-dictionary types (e.g., custom objects or other data structures), a `AttributeError` could still occur when calling `get()`. For example, if `items` includes strings or lists, the code would fail. The original code had a similar issue with attribute access, but this change doesn't fully resolve type mismatches.
  - **Error handling**: No broader error handling is present (e.g., for non-numeric `price` values). If a dictionary has a `price` key with a non-numeric value (e.g., a string), a `TypeError` could occur during addition. The `get()` method defaults to 0 for missing keys, which is helpful, but it doesn't validate the type or value of existing keys.
  - **Logic errors**: The function works for valid inputs but could return incorrect results if `items` is not a list (e.g., a string or None). The iteration (`for item in items`) assumes `items` is iterable, which could raise a `TypeError` if it's not.

- **Performance issues**:
  - No major performance concerns. The function uses a linear O(n) loop, which is efficient for most use cases. The addition of the empty check and `get()` method adds negligible overhead. However, if `items` is very large, the function could benefit from Python's built-in `sum()` function for slight optimization.

- **Code quality problems**:
  - **Readability and maintainability**: The code is straightforward, but the shift from attribute access to dictionary access could confuse maintainers if the data structure isn't documented. There's no type hinting, docstring, or comments, making it harder to understand the function's intent or constraints. Code style is mostly fine, but the indentation in the diff looks consistent.
  - **Complexity**: Low cyclomatic complexity, which is good. However, the function could be simplified using Python's `sum()` with a generator expression, reducing code duplication.

- **Testing gaps**:
  - No tests are provided or referenced in the diff, so it's unclear if the changes are covered. Potential gaps include:
    - Tests for edge cases like empty lists, missing keys, non-dictionary items, or invalid data types.
    - Regression tests to ensure the function still works for valid inputs (e.g., lists of dictionaries or objects).
    - The added changes improve robustness, but without tests, there's no assurance that they don't introduce new bugs. Unit tests should cover various scenarios to achieve good code coverage.

---

### 3. **Suggestions for Improvement**

Here are specific, actionable recommendations to enhance the code:

- **Code structure and organization**:
  - Add type hints to clarify expected input and output types, e.g., `def calculate_total(items: list[dict]) -&amp;amp;gt; int`. This would make the function more explicit about assuming dictionary-based items and help catch type errors early with tools like mypy.
  - Consider refactoring the function to handle both dictionary and object-based inputs if needed, e.g., by checking the type of each item and using `getattr(item, 'price', 0)` for objects or `item.get('price', 0)` for dicts. This would make the function more flexible.

- **Error handling improvements**:
  - Add input validation at the start of the function, e.g., check if `items` is iterable and raise a custom error (e.g., `ValueError`) if not. Also, handle potential non-numeric `price` values by adding a type check or conversion, e.g., `total += float(item.get('price', 0))` to avoid `TypeError`.
  - For better robustness, use a try-except block around the loop to catch unexpected errors and log them or return a default value, but avoid overusing exceptions for performance reasons.

- **Performance optimizations**:
  - Replace the manual loop with a more concise and potentially faster approach using `sum()`, e.g., `return sum(item.get('price', 0) for item in items or [])`. This handles the empty case automatically and reduces code lines. The `or []` ensures that if `items` is None, it defaults to an empty list, avoiding errors.

- **Better naming and documentation**:
  - Add a docstring to the function describing its purpose, parameters, return value, and any assumptions (e.g., that items are dictionaries). Example:
    ```
    def calculate_total(items):
        """
        Calculate the total price from a list of items.

        Args:
            items (list): A list of dictionaries, each containing a 'price' key.

        Returns:
            int: The sum of all prices, or 0 if no items are provided.

        Raises:
            ValueError: If items is not iterable or contains invalid data.
        """
        # ... rest of the code
    ```
  - Improve variable naming if needed; `total` is fine, but `items` could be more descriptive (e.g., `item_list`) if the context isn't clear.

- **Refactoring opportunities**:
  - Simplify the code using `sum()` as mentioned earlier. If this function is part of a larger module, consider extracting price retrieval into a separate helper function (e.g., `get_price(item)`) to handle different data types, improving reusability and reducing duplication.

---

### 4. **Positive Aspects**

The changes demonstrate good engineering practices and improve the code's reliability:

- **Good patterns and practices**: Adding the empty list check and using `dict.get()` with a default value is a solid example of defensive programming. It prevents common errors like `AttributeError` or index errors, making the function more robust without overcomplicating the logic.
- **Clear, readable code**: The updated logic is straightforward and easy to follow. The changes maintain simplicity while addressing potential issues, which enhances maintainability.
- **Proper error handling**: The use of `get('price', 0)` elegantly handles missing keys, reducing the need for explicit try-except blocks and improving code flow.
- **Well-structured logic**: The function remains concise and focused on a single responsibility (summing prices), adhering to the single-responsibility principle.

Overall, the diff shows thoughtful improvements that make the code more resilient to real-world inputs.

---

### 5. **Risk Assessment**

- **High Risk**: None identified. The changes are localized and don't introduce breaking changes or security threats, but improper handling of data types could lead to runtime errors in certain scenarios.
- **Medium Risk**: Potential type mismatches (e.g., if `items` elements are not dictionaries) could cause errors or incorrect results. Additionally, lack of testing might lead to undetected bugs in production.
- **Low Risk**: Minor issues like missing documentation or code style inconsistencies, which don't affect functionality but could impact long-term maintainability.

---

### Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential type mismatch for `items` elements | 🟡 Important | Assumes items are dictionaries; could raise errors if elements are custom objects or other types. | Add type hints and input validation, e.g., check if item is dict or use a flexible getter function. |
| Missing error handling for non-numeric prices or non-iterable inputs | 🟡 Important | Could raise `TypeError` if `price` is not a number or if `items` is not iterable. | Add type checks or use safe conversions, e.g., `float(item.get('price', 0))`, and validate `items` at the start. |
| Lack of documentation and docstrings | 🟢 Minor | No explanation of function behavior, parameters, or assumptions, reducing readability. | Add a descriptive docstring and consider inline comments for complex logic. |
| Testing gaps for edge cases | 🟡 Important | No visible tests for empty lists, missing keys, or invalid inputs, risking undetected regressions. | Implement unit tests covering various scenarios, e.g., using pytest with cases for empty, valid, and invalid inputs. |
| Opportunity for code simplification | 🟢 Minor | Manual loop could be replaced with `sum()` for conciseness and performance. | Refactor to use `sum(item.get('price', 0) for item in items or [])` to handle edges automatically. |

This review provides a balanced, actionable critique to help refine the code. If you have additional context (e.g., the rest of the codebase or testing framework), I can refine this further!

            ---
            *Generated by grok-3-mini-fast-latest via MCP Code Review Tool*
        &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/live_test_codereview/summary_20250601_091219.json"&amp;gt;{
  "timestamp": "20250601_091219",
  "input_file": "test_diff.md",
  "total_models": 4,
  "successful_reviews": 4,
  "failed_reviews": 0,
  "output_files": [
    "gemini-2.0-flash_20250601_091219.md",
    "o3-mini-2025-01-31_20250601_091219.md",
    "grok-3-mini-fast-latest_20250601_091219.md",
    "claude-opus-4-20250514_20250601_091219.md"
  ]
}&amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/live_test_codereview/o3-mini-2025-01-31_20250601_091219.md"&amp;gt;
            # Code Review - o3-mini-2025-01-31

            **Model**: o3-mini-2025-01-31
            **Timestamp**: 2025-06-01T09:11:23.235642
            **Duration**: 9.24 seconds

            ---

            Below is a comprehensive review of the code diff:

──────────────────────────────
1. Overall Assessment
──────────────────────────────
• Summary of Changes:
 – An early exit is added to handle the case where the items list is empty (returning 0).
 – Instead of directly accessing an attribute (item.price), the code now uses the dictionary “get” method (item.get('price', 0)) to retrieve the price value.
 – This indicates a shift in expectation from an object with a price attribute to a dictionary-like object where "price" is a key.
  
• Impact on the Codebase:
 – The changes are localized to the calculate_total function in test.py.
 – The modifications improve robustness when items is empty and when an item does not contain a "price" key.
 – It may affect other parts of the system if they pass objects with a price attribute rather than dictionaries; integration testing is advised.
  
• Alignment with Best Practices:
 – Handling an empty list immediately is a good practice to avoid unnecessary computation.
 – Using item.get with a default value enhances fault tolerance for missing keys.
 – Code readability is maintained, though further documentation could be added.

──────────────────────────────
2. Issues Found
──────────────────────────────
• Security Vulnerabilities:
 – No apparent security concerns such as injection risks or data exposures. The function’s scope is narrow and does not involve external inputs.

• Bugs and Logic Errors:
 – Type Expectations: Changing from attribute access to dictionary access may break callers expecting objects with attributes. Confirm that all items are dictionaries.
 – Implicit Assumptions: The code assumes every item is a dict-like object. There’s no type-checking, so if an unexpected type is encountered, it might raise an AttributeError or TypeError.
 – Early Exit:
  • The early return for empty items is clear, but if items is None (i.e., not provided), it might lead to unexpected None errors unless the caller guarantees a list.

• Performance Issues:
 – The function’s algorithm is simple and works in O(n) time. There are no performance bottlenecks even for larger lists.
 – Memory usage is minimal.

• Code Quality Problems:
 – Readability is high; however, a brief comment explaining the change in expected item format (from object to dict) could help future maintainers.
 

            ---
            *Generated by o3-mini-2025-01-31 via MCP Code Review Tool*
        &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/live_test_codereview/claude-opus-4-20250514_20250601_091219.md"&amp;gt;
            # Code Review - claude-opus-4-20250514

            **Model**: claude-opus-4-20250514
            **Timestamp**: 2025-06-01T09:11:50.195387
            **Duration**: 29.30 seconds

            ---

            ## Code Review: calculate_total Function Enhancement

### 1. **Overall Assessment**

**Summary of Changes:**
- Added null/empty check for the `items` parameter
- Changed from attribute access (`item.price`) to dictionary access with default value (`item.get('price', 0)`)

**Impact:**
- Improves robustness by handling edge cases (empty input and missing price fields)
- Changes the expected data structure from objects with price attributes to dictionaries
- Prevents potential AttributeError and KeyError exceptions

**Best Practices Alignment:**
- ✅ Defensive programming with input validation
- ✅ Graceful handling of missing data
- ⚠️ Inconsistent data structure assumption (mixing objects and dictionaries)

### 2. **Issues Found**

**Code Quality Problems:**
1. **Inconsistent Data Model**: The change from `item.price` to `item.get('price', 0)` suggests a shift from objects to dictionaries, but this isn't documented or type-hinted
2. **Silent Failure**: Items without prices are silently treated as having price 0, which might mask data quality issues
3. **Missing Type Hints**: No type annotations to clarify expected input/output types
4. **No Documentation**: Function lacks docstring explaining purpose, parameters, and return value

**Potential Logic Issues:**
1. **Type Assumption**: Code assumes items are dictionaries but doesn't validate this
2. **Numeric Type Assumption**: No validation that 'price' values are numeric

### 3. **Suggestions for Improvement**

```python
from typing import List, Dict, Union, Optional
import logging

def calculate_total(items: Optional[List[Dict[str, Union[int, float]]]]) -&amp;amp;gt; Union[int, float]:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dictionaries with 'price' key, or None/empty list
        
    Returns:
        Total price as int or float. Returns 0 for empty/None input.
        
    Raises:
        TypeError: If price value is not numeric
        ValueError: If price value is negative
    """
    if not items:
        return 0
    
    total = 0
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            raise TypeError(f"Item at index {idx} is not a dictionary")
            
        price = item.get('price', None)
        if price is None:
            logging.warning(f"Item at index {idx} has no price field")
            continue
            
        if not isinstance(price, (int, float)):
            raise TypeError(f"Price for item at index {idx} is not numeric: {price}")
            
        if price &amp;amp;lt; 0:
            raise ValueError(f"Price for item at index {idx} is negative: {price}")
            
        total += price
        
    return total
```

**Alternative approach for mixed data types:**
```python
def calculate_total(items: Optional[List[Union[Dict, object]]]) -&amp;amp;gt; Union[int, float]:
    """Handle both dictionary and object inputs."""
    if not items:
        return 0
    
    total = 0
    for item in items:
        if isinstance(item, dict):
            price = item.get('price', 0)
        elif hasattr(item, 'price'):
            price = item.price
        else:
            price = 0
            
        total += price
        
    return total
```

### 4. **Positive Aspects**

- ✅ **Good defensive programming**: Adding the empty check prevents errors on None/empty inputs
- ✅ **Graceful degradation**: Using `.get()` with default prevents KeyError exceptions
- ✅ **Maintains backward compatibility**: Function signature remains unchanged
- ✅ **Simple and readable**: The logic remains straightforward and easy to understand

### 5. **Risk Assessment**

**Medium Risk** 🟡
- **Breaking Change**: Switching from attribute to dictionary access could break existing code that passes objects
- **Data Quality**: Silent handling of missing prices could hide data issues in production
- **Type Safety**: Lack of validation could lead to runtime errors with unexpected data types

**Mitigation Strategies:**
1. Add comprehensive unit tests covering all data type scenarios
2. Implement logging for items with missing prices
3

            ---
            *Generated by claude-opus-4-20250514 via MCP Code Review Tool*
        &amp;lt;/file&amp;gt;
  &amp;lt;file path="reviewer/live_test_codereview/gemini-2.0-flash_20250601_091219.md"&amp;gt;
            # Code Review - gemini-2.0-flash

            **Model**: gemini-2.0-flash
            **Timestamp**: 2025-06-01T09:11:16.427900
            **Duration**: 6.81 seconds

            ---

            ## Test Code Review

### 1. **Overall Assessment**

The diff introduces two key changes to the `calculate_total` function:

1.  A check for an empty `items` list, returning 0 in that case.
2.  A change in how the price is accessed: from `item.price` to `item.get('price', 0)`.

The first change handles a potential edge case, preventing errors when the input is empty. The second change makes the code more robust by handling cases where an item might not have a `price` attribute directly, but rather stores it as a dictionary key. These changes enhance the robustness and reliability of the function. The scope is relatively small, impacting only the `calculate_total` function. These changes generally align with best practices for defensive programming and error handling.

### 2. **Issues Found**

*   **Potential Type Error (🟡)**:  While `item.get('price', 0)` handles the absence of the 'price' key, it assumes the value associated with the 'price' key (if it exists) will be a number that can be added to `total`. If `item['price']` exists but is a string (e.g., "unknown"), a `TypeError` would still occur.
*   **Limited Input Validation (🟡)**: The code assumes that each `item` in `items` is a dictionary. It doesn't validate that `items` is even a list, or that each element within it is a dictionary-like object.

### 3. **Suggestions for Improvement**

*   **Type Validation and Error Handling (Important):**  Implement more robust type validation, either with `isinstance` checks or using a try-except block:

    ```python
    def calculate_total(items):
        if not items:
            return 0

        total = 0
        for item in items:
            try:
                price = item.get('price', 0)
                if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
                total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.
        return total
    ```

*   **Consider a dedicated Item class (Minor):** If the structure of `items` is fixed (i.e., always containing dictionaries with a 'price'), consider defining a dedicated `Item` class with a `price` attribute. This would improve code readability and maintainability.

*   **Add input validation (Minor):** Assert that `items` is a list and each element is either a dictionary or an object with a `get` method.

    ```python
    def calculate_total(items):
        if not isinstance(items, list):
            raise TypeError("items must be a list")

        if not items:
            return 0

        total = 0
        for item in items:
            if not hasattr(item, 'get') and not isinstance(item, dict):
                raise TypeError("Each item must be a dictionary or an object with a 'get' method")
              if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
              total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.

        return total
    ```

### 4. **Positive Aspects**

*   **Handles Empty Input (🟢):** The addition of the `if not items` check is a good practice for handling edge cases and preventing potential errors.
*   **Robust Price Access (🟢):** Using `item.get('price', 0)` is a good way to handle cases where the `price` attribute may not be directly available, providing a default value of 0 if the key is missing.

### 5. **Risk Assessment**

*   **Medium Risk**:  The lack of explicit type validation for the `price` can still lead to runtime errors. Implementing the suggested improvement involving the `try-except` block significantly mitigates this risk.

## Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential Type Error | 🟡 |  If `item

            ---
            *Generated by gemini-2.0-flash via MCP Code Review Tool*
        
&amp;lt;/file&amp;gt;
  &amp;lt;file path="migrations/20250810_02_add-github-url-to-prompt-history.sql"&amp;gt;-- Add github_url column to prompt_history table to track project association in historical records
-- depends: 20250810_01_add-projects-table

-- Add github_url column to prompt_history table
ALTER TABLE prompt_history ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Add index for efficient queries by github_url
CREATE INDEX IF NOT EXISTS idx_prompt_history_github_url ON prompt_history(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_history_github_url;
-- ALTER TABLE prompt_history DROP COLUMN github_url;&amp;lt;/file&amp;gt;
  &amp;lt;file path="migrations/20250727_01_create-prompt-tables.sql"&amp;gt;-- Create prompt tables for prompt storage, versioning, and metrics
-- depends: 

-- Current prompt table
CREATE TABLE IF NOT EXISTS prompt (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    data JSONB NOT NULL,
    version INTEGER DEFAULT 1,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Historical versions table
CREATE TABLE IF NOT EXISTS prompt_history (
    id TEXT,
    version INTEGER,
    data JSONB NOT NULL,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    change_summary TEXT,
    PRIMARY KEY (id, version)
);

-- Metrics time-series table (optimized for prompt tracking)
CREATE TABLE IF NOT EXISTS prompt_metrics (
    prompt_id TEXT,
    version INTEGER,
    metric_name TEXT,
    step INTEGER,
    value REAL,
    timestamp TIMESTAMP,
    PRIMARY KEY (prompt_id, version, metric_name, step)
);

-- Performance-critical indexes
CREATE INDEX IF NOT EXISTS idx_prompt_hash ON prompt(content_hash);
CREATE INDEX IF NOT EXISTS idx_prompt_updated ON prompt(updated_at);
CREATE INDEX IF NOT EXISTS idx_prompt_history_created ON prompt_history(created_at);
CREATE INDEX IF NOT EXISTS idx_prompt_metrics_time ON prompt_metrics(timestamp);

-- Expression indexes on JSONB fields for common queries
CREATE INDEX IF NOT EXISTS idx_prompt_status ON prompt(data -&amp;amp;gt;&amp;amp;gt; '$.status');
CREATE INDEX IF NOT EXISTS idx_prompt_type ON prompt(data -&amp;amp;gt;&amp;amp;gt; '$.type');&amp;lt;/file&amp;gt;
  &amp;lt;file path="migrations/20250810_01_add-projects-table.sql"&amp;gt;-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/test_database.py"&amp;gt;import pytest
import sqlite3
import os

from repository.database import SQLite3Database


@pytest.fixture
def test_db():
    """Create a test database instance"""
    test_db_path = "test_collect.db"
    db = SQLite3Database(db_path=test_db_path)
    yield db
    # Cleanup
    if os.path.exists(test_db_path):
        os.remove(test_db_path)


def test_database_connection_basic(test_db):
    """Test basic database connection establishment"""
    with test_db.get_connection() as conn:
        assert conn is not None
        assert isinstance(conn, sqlite3.Connection)
        # Test basic query
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_connection_read_only(test_db):
    """Test read-only database connection"""
    with test_db.get_connection(read_only=True) as conn:
        assert conn is not None
        # Should be able to read
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_row_factory(test_db):
    """Test that Row factory is properly configured"""
    with test_db.get_connection() as conn:
        cursor = conn.execute("SELECT 1 as test_col")
        row = cursor.fetchone()
        # Should be able to access by column name
        assert row["test_col"] == 1


def test_database_pragma_settings(test_db):
    """Test that PRAGMA settings are applied correctly"""
    with test_db.get_connection() as conn:
        # Check foreign keys
        cursor = conn.execute("PRAGMA foreign_keys")
        assert cursor.fetchone()[0] == 1

        # Check journal mode
        cursor = conn.execute("PRAGMA journal_mode")
        assert cursor.fetchone()[0] == "wal"

        # Check synchronous mode
        cursor = conn.execute("PRAGMA synchronous")
        assert cursor.fetchone()[0] == 1  # NORMAL = 1


def test_database_context_manager_cleanup():
    """Test that database connections are properly closed"""
    test_db_path = "test_cleanup.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with db.get_connection() as conn:
            conn.execute("SELECT 1")

        # Connection should be closed after context manager exits
        # We can't directly test if it's closed, but we can verify
        # that we can create a new connection successfully
        with db.get_connection() as conn:
            assert conn is not None

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)


def test_database_error_handling():
    """Test error handling and rollback"""
    test_db_path = "test_error.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with pytest.raises(sqlite3.Error):
            with db.get_connection() as conn:
                # This should cause an error
                conn.execute("INVALID SQL STATEMENT")

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/database.py"&amp;gt;import sqlite3

from contextlib import contextmanager
from typing import Generator


class SQLite3Database:
    def __init__(self, db_path: str = "../data/collect.db"):
        self.db_path = db_path

    # Decorator that converts this generator function into a context manager
    @contextmanager
    def get_connection(
        self, read_only: bool = False
    ) -&amp;amp;gt; Generator[sqlite3.Connection, None, None]:
        """Context manager for database connections"""
        # Setup phase: runs when entering 'with' block
        # Enable PARSE_DECLTYPES to use our custom datetime converters
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.row_factory = sqlite3.Row  # enables column access by name

        # Connection optimizations
        conn.execute("PRAGMA foreign_keys = ON")  # enables foreign key support
        if not read_only:
            # enables better concurrency
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL")  # Faster writes
        conn.execute("PRAGMA cache_size = -64000")  # 64MB cache
        # Use memory for temp tables
        conn.execute("PRAGMA temp_store = MEMORY")
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB memory-mapped I/O

        try:
            yield conn  # Pauses here, returns conn to 'with' statement
            # Code inside 'with' block runs here
            if not read_only:
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            # Cleanup phase: always runs when exiting 'with' block
            conn.close()
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/test_prompt_service.py"&amp;gt;import pytest
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import Prompt, PromptType, PromptPlanStatus, CmdCategory
from config import Config


@pytest.fixture
def prompt_service():
    """
    ## How It Works

    1. **`with db.get_connection() as conn:`**
       - Opens a database connection using a context manager
       - The `as conn` assigns the connection to the variable
       - When the `with` block exits, `conn.close()` is automatically called

    2. **`cmd_service = CmdsService(conn)`**
       - Creates the service object with the database connection
       - The service can now execute database operations

    3. **`yield cmd_service`**
       - This is pytest fixture syntax that provides the service to the test
       - `yield` pauses execution here while the test runs
       - After the test completes, execution resumes after the `yield`

    4. **Automatic cleanup**
       - When the test finishes, the `with` block exits
       - Database connection is automatically closed
       - Resources are freed

    This pattern ensures **deterministic cleanup** -
    the database connection will always be properly closed regardless of
    whether the test passes or fails.
    """
    config = Config()
    db = SQLite3Database(config.db_path)
    with db.get_connection() as conn:
        cmd_service = PromptService(conn, config)

        yield cmd_service


def test_check_dirs(prompt_service: PromptService):
    result = prompt_service.cmd_check_dirs()
    assert result is True


def test_load_cmds_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_cmds_from_disk()
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def test_load_plans_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_plans_from_disk()

    print(f"\nTotal plans loaded: {len(load_results.loaded_prompts)}")
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def create_test_prompts(prompt_service: PromptService) -&amp;amp;gt; List[Prompt]:
    prompt_content = """
    this is a test prompt for testing database persistence... blah blah
    """

    def new_cmd_prompt(prompt_content: str) -&amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return [new_cmd_prompt(prompt_content), new_plan_prompt(prompt_content)]


def test_save_prompt_in_db(prompt_service: PromptService):
    # create test cmd and plan prompt types
    pls = create_test_prompts(prompt_service)
    cmd_prompt = pls[0]
    plan_prompt = pls[1]

    try:
        # save test prompts in sqlite and verify success
        cmd_result = prompt_service.save_prompt_in_db(cmd_prompt)
        print(f"cmd_result: {cmd_result}")
        assert cmd_result.success is not False

        plan_result = prompt_service.save_prompt_in_db(plan_prompt)
        print(f"plan_result: {plan_result}")
        assert plan_result.success is not False

        # retrieve the saved test prompts from sqlite and verify they
        # match the original test cmd and plan prompts
        print(f"Retrieving cmd prompt with id: {cmd_prompt.id}")
        retrieved_cmd = prompt_service.get_prompt_by_id(cmd_prompt.id)
        print(f"Retrieved cmd: {retrieved_cmd}")
        assert retrieved_cmd is not None

        retrieved_plan = prompt_service.get_prompt_by_id(plan_prompt.id)
        assert retrieved_plan is not None

        # update prompt and increment the version
        updated_text = cmd_prompt.data.content + "UPDATED TEXT"
        cmd_prompt.data.content = updated_text

        update_result = prompt_service.update_prompt_in_db(cmd_prompt)
        assert update_result.success is True

        # retrieve the updated prompt again from the prompt table and
        # validate the changes were persisted/updated
        retrieved_prompt = prompt_service.get_prompt_by_id(cmd_prompt.id)
        assert retrieved_prompt.data.content == updated_text

        # retrieve the prompt by name
        # and validate correct prompt retrieval
        retrieved_prompt_by_name = prompt_service.get_prompt_by_name(cmd_prompt.name)
        assert retrieved_prompt_by_name is not None
        assert retrieved_prompt_by_name.id == cmd_prompt.id

    finally:
        # Clean up test data - this will ALWAYS run, even if test fails
        print("\nCleaning up test prompts...")

        cmd_cleanup = delete_prompt_completely(prompt_service, cmd_prompt.id)
        print(f"CMD cleanup result: {cmd_cleanup}")

        plan_cleanup = delete_prompt_completely(prompt_service, plan_prompt.id)
        print(f"PLAN cleanup result: {plan_cleanup}")


def delete_prompt_completely(prompt_service: PromptService, prompt_id: str):
    """
    DELETE a prompt from tables: prompt, prompt_history and prompt_metrics
    THIS IS FOR INTEGRATION TESTING ONLY - as production code should reserve
    history
    """
    cursor = prompt_service.conn.cursor()
    try:
        # start transaction
        cursor.execute("BEGIN TRANSACTION")

        # delete from prompt_history first (due to composite primary key)
        cursor.execute(
            """
                       DELETE FROM prompt_history
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_history_rows_deleted = cursor.rowcount

        # delete from prompt_metrics table if any exist
        cursor.execute(
            """
                       DELETE FROM prompt_metrics
                       WHERE prompt_id = ?
                       """,
            (prompt_id,),
        )
        prompt_metrics_rows_deleted = cursor.rowcount

        # delete from prompt table (we do this last)
        cursor.execute(
            """
                       DELETE FROM prompt
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_rows_deleted = cursor.rowcount

        prompt_service.conn.commit()
        return {
            "success": True,
            "prompt_rows": prompt_rows_deleted,
            "prompt_history_rows": prompt_history_rows_deleted,
            "prompt_metrics_rows": prompt_metrics_rows_deleted,
        }

    except Exception as e:
        prompt_service.conn.rollback()
        return {"success": False, "error": str(e)}


def test_prompt_loading(prompt_service: PromptService):
    cmds = prompt_service.load_cmds_from_disk()
    print(f"\nTotal commands loaded: {len(cmds.loaded_prompts)}")
    assert len(cmds.errors) == 0

    plans = prompt_service.load_plans_from_disk()
    print(f"\nTotal plans loaded: {len(plans.loaded_prompts)}")
    assert len(plans.errors) == 0

    prompts = cmds.loaded_prompts + plans.loaded_prompts

    results = prompt_service.bulk_save_in_db(prompts)

    bad_results = [result for result in results if not result.success]
    good_results = [result for result in results if result.success]

    print(f"\nGood Result count: {len(good_results)}")
    print(f"\nBad Result count: {len(bad_results)}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/prompt_models.py"&amp;gt;from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
from typing import Optional, List
from config import Config


class PromptPlanStatus(str, Enum):
    """Plan status types"""

    DRAFT = "draft"
    APPROVED = "approved"
    COMPLETED = "completed"


class PromptType(str, Enum):
    """Prompt types"""

    PLAN = "plan"
    CMD = "cmd"


def create_cmd_category_enum():
    """Create CmdCategory enum dynamically from config"""
    try:
        config = Config()
        subdirs = config.command_subdirs
    except Exception:
        # Fallback to default subdirs if config fails
        subdirs = ["archive", "go", "js", "mcp", "python", "tools"]

    # Build enum members dictionary
    members = {}
    for subdir in subdirs:
        members[subdir.upper()] = subdir

    # Always include UNCATEGORIZED as fallback
    members["UNCATEGORIZED"] = "uncategorized"

    # Create enum using the functional API with type=str for JSON serialization
    return Enum("CmdCategory", members, type=str)


# Create the enum instance
CmdCategory = create_cmd_category_enum()


class Project(BaseModel):
    github_url: str
    description: str
    created_at: datetime
    updated_at: datetime


class PromptData(BaseModel):
    """Structured data for prompt JSONB field"""

    type: PromptType
    status: PromptPlanStatus
    project: Optional[str]
    cmd_category: Optional[CmdCategory]
    content: str  # This is the prompt content, in markdown
    description: Optional[str] = None
    # using 'claude' or 'gemini' here to specify the dir it will write to
    # .claude/commands and .gemini/commands respectively
    tags: List[str] = Field(default_factory=list)


class Prompt(BaseModel):
    id: str
    name: str
    github_url: Optional[str]
    data: PromptData  # Structured JSONB data
    version: int
    content_hash: str
    created_at: datetime
    updated_at: datetime


class PromptCreate(BaseModel):
    id: str
    name: str
    data: PromptData
    content_hash: str
    version: Optional[int] = 1


class LoadError(BaseModel):
    filename: str
    error_message: str
    error_type: str


class PromptCreateResult(BaseModel):
    """Result of creating a new prompt"""

    success: bool
    prompt_id: str
    version: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptLoadResult(BaseModel):
    """Result of loading prompts into database"""

    loaded_prompts: List[Prompt]
    errors: Optional[List[LoadError]] = None


class PromptDeleteResult(BaseModel):
    success: bool
    prompt_id: str
    deleted: bool
    rows_affected: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptFlattenResult(BaseModel):
    """Result of flattening a prompt to disk"""

    success: bool
    prompt_id: str
    prompt_name: str
    file_path: str
    cmd_category: str
    error_message: Optional[str] = None
    error_type: Optional[str] = None
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/datetime_adapters.py"&amp;gt;"""Custom datetime adapters for SQLite3 to avoid Python 3.12 deprecation warnings.

This module provides custom adapters and converters for datetime objects when
working with SQLite databases in Python 3.12+, replacing the deprecated default
adapters.
"""

import datetime
import sqlite3


def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format.

    Args:
        val: datetime.datetime object to adapt

    Returns:
        str: ISO 8601 formatted datetime string
    """
    return val.replace(tzinfo=None).isoformat()


def adapt_date_iso(val):
    """Adapt datetime.date to ISO 8601 date format.

    Args:
        val: datetime.date object to adapt

    Returns:
        str: ISO 8601 formatted date string
    """
    return val.isoformat()


def convert_datetime_iso(val):
    """Convert ISO 8601 datetime string to datetime.datetime object.

    Args:
        val: bytes object containing ISO 8601 datetime string

    Returns:
        datetime.datetime: Parsed datetime object
    """
    return datetime.datetime.fromisoformat(val.decode())


def convert_date_iso(val):
    """Convert ISO 8601 date string to datetime.date object.

    Args:
        val: bytes object containing ISO 8601 date string

    Returns:
        datetime.date: Parsed date object
    """
    return datetime.date.fromisoformat(val.decode())


def convert_timestamp(val):
    """Convert Unix timestamp to datetime.datetime object.

    Args:
        val: bytes object containing Unix timestamp

    Returns:
        datetime.datetime: Datetime object from timestamp
    """
    return datetime.datetime.fromtimestamp(int(val))


def register_adapters():
    """Register all custom datetime adapters and converters with sqlite3.

    This function should be called once at application startup to configure
    SQLite to use our custom datetime handling instead of the deprecated
    default handlers.
    """
    # Register adapters (Python -&amp;amp;gt; SQLite)
    sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
    sqlite3.register_adapter(datetime.date, adapt_date_iso)

    # Register converters (SQLite -&amp;amp;gt; Python)
    sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
    sqlite3.register_converter("DATETIME", convert_datetime_iso)
    sqlite3.register_converter("DATE", convert_date_iso)


# Automatically register adapters when module is imported
register_adapters()
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/prompt_service.py"&amp;gt;import sqlite3
from pathlib import Path
import uuid
import json
from datetime import datetime, timezone
import hashlib
from typing import Optional, List, Tuple
from repository.prompt_models import (
    PromptLoadResult,
    LoadError,
    CmdCategory,
    PromptType,
    PromptPlanStatus,
    PromptData,
    Prompt,
    PromptCreateResult,
    PromptDeleteResult,
    PromptFlattenResult,
)
from config import Config


class PromptService:
    def __init__(self, conn: sqlite3.Connection, config: Config):
        self.conn = conn
        self.config = config
        self.plans_check_dirs()
        self.cmd_check_dirs()

    def plans_check_dirs(self) -&amp;amp;gt; bool:
        """Check if all required plan directories exist, create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        # Required directories
        required_dirs = [
            plans_dir,
            plans_dir / "drafts",
            plans_dir / "approved",
            plans_dir / "completed",
        ]

        missing_dirs = []
        created_dirs = []

        for dir_path in required_dirs:
            if not dir_path.exists():
                missing_dirs.append(dir_path)

        if missing_dirs:
            print("📁 Creating missing plan directories:")
            for missing_dir in missing_dirs:
                try:
                    missing_dir.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(missing_dir)
                    print(
                        f"   ✅ Created: {
                            missing_dir.relative_to(project_dir)}"
                    )
                except Exception as e:
                    print(
                        f"   ❌ Failed to create {
                            missing_dir.relative_to(project_dir)}: {e}"
                    )
                    return False

            if created_dirs:
                print(
                    f"📁 Successfully created {
                        len(created_dirs)} directories"
                )
        else:
            print("✅ All required plan directories exist")

        return True

    def cmd_check_dirs(self) -&amp;amp;gt; bool:
        """Check if all required command directories exist,
           create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        claude_dir = project_dir / ".claude"
        gemini_dir = project_dir / ".gemini"

        # Get subdirectories from config
        config = Config()
        subdirs = config.command_subdirs

        # Build required directories
        required_dirs = {
            "claude": [claude_dir / "commands"]
            + [claude_dir / "commands" / subdir for subdir in subdirs],
            "gemini": [gemini_dir / "commands"]
            + [gemini_dir / "commands" / subdir for subdir in subdirs],
        }

        # Check for missing directories by type
        missing_by_type = {"claude": [], "gemini": []}
        for dir_type, dirs in required_dirs.items():
            for dir_path in dirs:
                if not dir_path.exists():
                    missing_by_type[dir_type].append(dir_path)

        # Count total missing
        total_missing = sum(len(dirs) for dirs in missing_by_type.values())

        if total_missing == 0:
            print("✅ All required command directories exist")
            return True

        # Create missing directories
        print(f"📁 Creating {total_missing} missing command directories:")
        created_count = 0
        failed = False

        for dir_type, missing_dirs in missing_by_type.items():
            if missing_dirs:
                print(f"\n   {dir_type.title()} directories:")
                for missing_dir in missing_dirs:
                    try:
                        missing_dir.mkdir(parents=True, exist_ok=True)
                        created_count += 1
                        print(
                            f"   ✅ Created: {
                                missing_dir.relative_to(project_dir)}"
                        )
                    except Exception as e:
                        print(
                            f"   ❌ Failed to create {
                                missing_dir.relative_to(project_dir)}: {e}"
                        )
                        failed = True

        if created_count &amp;amp;gt; 0:
            print(f"\n📁 Successfully created {created_count} directories")

        return not failed

    def _load_cmds_from_directory(
        self, cmds_dir: Path, source: str
    ) -&amp;amp;gt; Tuple[List[Prompt], List[LoadError]]:
        """Load commands from a specific directory

        Args:
            cmds_dir: Path to the commands directory
            source: Source identifier ('claude' or 'gemini')

        Returns:
            Tuple of (prompts list, errors list)
        """
        prompts = []
        errors = []

        if not cmds_dir.exists():
            return prompts, errors

        # Loop through the files in cmds dir and load prompts first
        for file in cmds_dir.iterdir():
            if file.is_file():
                try:
                    # Check if filename adheres to naming rules
                    current_filename = file.name
                    if not self.check_filename(current_filename):
                        # Only rename files during explicit operations, not during loading
                        # Skip file renaming when just loading/reading files
                        print(
                            f"⚠️  File {
                                current_filename} doesn't follow naming convention but will not be renamed during load operation"
                        )

                    prompt_content = file.read_text()
                    prompt = self.new_prompt_model(
                        prompt_content=prompt_content,
                        name=file.name,
                        prompt_type=PromptType.CMD,
                        cmd_category=CmdCategory.UNCATEGORIZED,
                        status=PromptPlanStatus.DRAFT,
                        tags=[source],  # Add source tag
                    )
                    prompts.append(prompt)

                except Exception as e:
                    errors.append(
                        LoadError(
                            filename=str(file),
                            error_message=str(e),
                            error_type=type(e).__name__,
                        )
                    )

        # Then cycle through the subdirs, create Prompt models and append
        for sub_dir in cmds_dir.iterdir():
            if sub_dir.is_dir():
                try:
                    cmd_category = CmdCategory(sub_dir.name.lower())

                    for file in sub_dir.iterdir():
                        try:
                            if file.is_file():
                                # Check if filename adheres to naming rules
                                current_filename = file.name
                                if not self.check_filename(current_filename):
                                    # Normalize the filename
                                    fixed_filename = self.normalize_filename(
                                        current_filename
                                    )

                                    # Create new file path with normalized name
                                    new_file_path = file.parent / fixed_filename

                                    # Rename the file on disk
                                    file.rename(new_file_path)

                                    # Update file reference to the new path
                                    file = new_file_path
                                    print(
                                        f"📝 Renamed: {current_filename} → {
                                            fixed_filename}"
                                    )

                                prompt_content = file.read_text()
                                prompt = self.new_prompt_model(
                                    prompt_content=prompt_content,
                                    name=file.name,
                                    prompt_type=PromptType.CMD,
                                    cmd_category=cmd_category,
                                    status=PromptPlanStatus.DRAFT,
                                    tags=[source],  # Add source tag
                                )
                                prompts.append(prompt)

                        except Exception as e:
                            errors.append(
                                LoadError(
                                    filename=str(file),
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )
                except ValueError:
                    # Skip directories that don't match valid CmdCategory values
                    continue

        return prompts, errors

    def load_cmds_from_disk(self) -&amp;amp;gt; PromptLoadResult:
        """Load commands from both .claude and .gemini directories

        Returns:
            PromptLoadResult: Combined results from both directories
        """
        project_dir = Path(__file__).parent.parent
        claude_cmds_dir = project_dir / ".claude" / "commands"
        gemini_cmds_dir = project_dir / ".gemini" / "commands"

        all_prompts = []
        all_errors = []

        # Load from Claude directory
        claude_prompts, claude_errors = self._load_cmds_from_directory(
            claude_cmds_dir, "claude"
        )
        all_prompts.extend(claude_prompts)
        all_errors.extend(claude_errors)

        # Load from Gemini directory
        gemini_prompts, gemini_errors = self._load_cmds_from_directory(
            gemini_cmds_dir, "gemini"
        )
        all_prompts.extend(gemini_prompts)
        all_errors.extend(gemini_errors)

        return PromptLoadResult(
            loaded_prompts=all_prompts,
            errors=all_errors,
        )

    def load_plans_from_disk(self) -&amp;amp;gt; PromptLoadResult:
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        status_mapping = {
            "drafts": PromptPlanStatus.DRAFT,
            "approved": PromptPlanStatus.APPROVED,
            "completed": PromptPlanStatus.COMPLETED,
        }

        prompts = []
        errors = []

        for subdir in plans_dir.iterdir():
            if subdir.is_dir() and subdir.name in status_mapping:
                cmd_category = None
                status = status_mapping[subdir.name]
                for file in subdir.iterdir():
                    try:
                        if file.is_file():
                            # Check if filename adheres to naming rules
                            current_filename = file.name
                            if not self.check_filename(current_filename):
                                # Normalize the filename
                                fixed_filename = self.normalize_filename(
                                    current_filename
                                )

                                # Create new file path with normalized name
                                new_file_path = file.parent / fixed_filename

                                # Rename the file on disk
                                file.rename(new_file_path)

                                # Update file reference to the new path
                                file = new_file_path
                                print(
                                    f"""📝 Renamed: {current_filename} → {
                                        fixed_filename}
                                      """
                                )

                            prompts.append(
                                self.new_prompt_model(
                                    prompt_content=file.read_text(),
                                    name=file.name,
                                    prompt_type=PromptType.PLAN,
                                    github_url=self.config.github_url,
                                    cmd_category=cmd_category,
                                    status=status,
                                    project=project_dir.name,
                                )
                            )

                    except Exception as e:
                        errors.append(
                            LoadError(
                                filename=str(file),
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

        return PromptLoadResult(loaded_prompts=prompts, errors=errors)

    def normalize_filename(self, filename: str) -&amp;amp;gt; str:
        """Normalize filename to use underscores and ensure .md or .toml extension

        Args:
            filename: The original filename

        Returns:
            Normalized filename with underscores and .md or .toml extension
        """
        # Replace hyphens with underscores
        normalized = filename.replace("-", "_")

        # Check if it already has .md or .toml extension
        if normalized.endswith(".md") or normalized.endswith(".toml"):
            return normalized

        # If it has another extension, replace it with .md
        if "." in normalized:
            normalized = normalized.rsplit(".", 1)[0] + ".md"
        else:
            # No extension, add .md as default
            normalized = normalized + ".md"

        return normalized

    def check_filename(self, filename: str) -&amp;amp;gt; bool:
        """Check if filename adheres to naming rules
        (underscores and .md or .toml extension)

        Args:
            filename: The filename to check

        Returns:
            bool: True if filename follows the rules, False otherwise
        """
        # Check if filename has .md or .toml extension
        if not (filename.endswith(".md") or filename.endswith(".toml")):
            return False

        # Check if filename contains hyphens (should use underscores)
        if "-" in filename:
            return False

        return True

    def new_prompt_model(
        self,
        prompt_content: str,
        name: str,
        prompt_type: PromptType,
        github_url: Optional[str] = None,
        cmd_category: Optional[CmdCategory] = None,
        status: PromptPlanStatus = PromptPlanStatus.DRAFT,
        project: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -&amp;amp;gt; Prompt:
        if prompt_type == PromptType.CMD and not cmd_category:
            raise ValueError("CMD type prompts require a category")

        default_tags = []
        if cmd_category:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                default_tags.append(cmd_category)
            else:
                default_tags.append(cmd_category.value)
        default_tags.append(prompt_type.value)

        # Merge custom tags with default tags
        all_tags = default_tags + (tags if tags else [])

        prompt_data = PromptData(
            type=prompt_type,
            status=status,
            project=project,
            cmd_category=cmd_category,
            content=prompt_content,
            description=description,
            tags=all_tags,
        )

        content_hash = hashlib.sha256(prompt_content.encode("utf-8")).hexdigest()

        timestamp = datetime.now(timezone.utc)

        db_name = self.create_db_name(
            prompt_type=prompt_type,
            prompt_status=status,
            cmd_category=cmd_category,
            project_name=project,
            name=name,
        )

        prompt = Prompt(
            id=str(uuid.uuid4()),
            name=db_name,
            github_url=github_url,
            data=prompt_data,
            version=1,
            content_hash=content_hash,
            created_at=timestamp,
            updated_at=timestamp,
        )

        return prompt

    def create_db_name(
        self,
        prompt_type: PromptType,
        prompt_status: Optional[PromptPlanStatus],
        cmd_category: Optional[CmdCategory],
        project_name: Optional[str],
        name: str,
    ) -&amp;amp;gt; str:
        # in the directory [project]/_docs/plans:
        # there are directories: draft, approved and completed
        # we model those as PromptPlanStatus -&amp;amp;gt; see prompt_models.py
        if prompt_type == PromptType.PLAN:
            create_name = project_name + "_" + prompt_status.value + "_" + name
        if prompt_type == PromptType.CMD:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                create_name = cmd_category + "_" + name
            else:
                create_name = cmd_category.value + "_" + name

        return create_name

    def parse_db_name(self, db_name: str, prompt_type: PromptType) -&amp;amp;gt; str:
        """Extract the original filename from the database name

        Args:
            db_name: The database name
            (e.g., 'collect-approved-update_function.md')
            prompt_type: The type of prompt(PLAN or CMD)

        Returns:
            The original filename(e.g., 'update_function.md')
        """
        # split the name to a list using '_' seperator
        ls = db_name.split("_")
        # rebuild filename from the list of split words
        filename = ""
        if prompt_type == PromptType.PLAN:
            # if prompt type is PLAN: then name will include the project
            # so we need to drop the first 2 words in the db_name
            # example: collect_completed_add_claude_sdk_processing.md
            # ls = [collect, completed, add, claude, sdk, processing.md]
            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

        if prompt_type == PromptType.CMD:
            # if prompt type is CMD: then name will only include the dir/type
            # so we only need to drop the first word in ls
            # example: tools_create_database.md
            # ls = [tools, create, database.md]
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

    def check_exists(self, name: str) -&amp;amp;gt; Tuple[bool, str]:
        """Check if a prompt with the given name already exists

        Args:
            name: The prompt name to check

        Returns:
            Tuple[bool, str]: (exists, prompt_id)
            where exists is True if prompt exists,
            and prompt_id is the ID if found, empty string if not found
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT id FROM prompt WHERE name = ?", (name,))
        result = cursor.fetchone()

        if result:
            return (True, result["id"])  # Found: return True and the prompt ID
        else:
            # Not found: return False and empty string
            return (False, "")

    def save_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Initial prompt creation"
    ) -&amp;amp;gt; PromptCreateResult:
        """Create a new prompt and initialize version history
        if the prompt doesn't exist, if it already exists then
        we call `update_prompt_in_db` with the update.

        Args:
            prompt: Prompt object to create
            change_summary: Description of this change
            (default: "Initial prompt creation")

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            # Validate prompt has required fields
            if not prompt.id or not prompt.name or not prompt.data:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id if prompt.id else "",
                    version=prompt.version if prompt.version else 1,
                    error_message="Prompt missing required fields",
                    error="ValidationError",
                )

            exists, prompt_id = self.check_exists(prompt.name)
            if exists:
                # get prompt from database using prompt_id from the version
                # retrieved from the database
                prompt_from_db = self.get_prompt_by_id(prompt_id)
                # if we don't have a prompt here then we return false
                if prompt_from_db is None:
                    return PromptCreateResult(
                        success=False,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"prompt retrieval failed for {
                            prompt_id}",
                        error="ValueError",
                    )

                # otherwise we have a prompt from the database call
                # and we need to compare hashes to see if there are changes
                if prompt.content_hash == prompt_from_db.content_hash:
                    # if they are the same then the version in the db is the
                    # same as the version on disk so we return success and
                    # do nothing else.
                    return PromptCreateResult(
                        success=True,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"""
                        prompt: {prompt.name} from disk is the same as db
                        """,
                        error="",
                    )

                else:
                    # if we get here then we have changes on disk that are more
                    # current than what is in the database

                    # IMPORTANT: We override the prompt.id here because the
                    # prompt exists already and we don't have a clean way of
                    # storing the uuid with the prompt on disk.
                    # When the prompt model is created from loading from disk,
                    # we DO generate a uuid for the model at that time just in
                    # case the prompt is newly generated from the disk and is
                    # not in the db
                    prompt.id = prompt_from_db.id

                    # Important to note that we will increment the version in
                    # `self.update_prompt_in_db`, we do not increment it here
                    return self.update_prompt_in_db(prompt)

            else:  # prompt doesn't exist in the database
                # if we make it here we have a new prompt and it
                # needs to be saved to the database for the first time
                prompt_jsonb = prompt.data.model_dump_json()

                # Create new cursor for this transaction
                cursor = self.conn.cursor()

                # insert prompt into prompt table
                cursor.execute(
                    """
                    INSERT INTO prompt(
                    id,
                    name,
                    data,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?,?)
                    """,
                    (
                        prompt.id,
                        prompt.name,
                        prompt_jsonb,
                        prompt.version,
                        prompt.content_hash,
                        prompt.created_at,
                        prompt.updated_at,
                        prompt.github_url,
                    ),
                )

                # insert initial version into prompt_history table
                cursor.execute(
                    """
                    INSERT INTO prompt_history(
                    id,
                    version,
                    data,
                    content_hash,
                    created_at,
                    archived_at,
                    change_summary,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                    """,
                    (
                        prompt.id,
                        prompt.version,
                        prompt_jsonb,
                        prompt.content_hash,
                        prompt.created_at,
                        datetime.now(timezone.utc),
                        change_summary,
                        prompt.github_url,
                    ),
                )
                self.conn.commit()

                return PromptCreateResult(
                    success=True, prompt_id=prompt.id, version=prompt.version
                )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def update_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Prompt updated from disk"
    ) -&amp;amp;gt; PromptCreateResult:
        """Update an existing prompt and add to version history

        Args:
            prompt: Prompt object to update
            change_summary: Description of this change

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            cursor = self.conn.cursor()

            # first we get the existing prompt in the database
            current_prompt = self.get_prompt_by_id(prompt.id)
            if not current_prompt:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id,
                    version=prompt.version,
                    error_message=f"Prompt w id {prompt.id} not found",
                    error="NotFoundError",
                )
            # then we increment the version
            prompt.version = current_prompt.version + 1

            # we need to recalculate the hash for the udpated prompt
            # so we can properly compare for changes
            prompt.content_hash = hashlib.sha256(
                prompt.data.content.encode("utf-8")
            ).hexdigest()

            # process the PromptData model to to json
            prompt_jsonb = prompt.data.model_dump_json()

            # then we update the `updated_at` timestamp
            prompt.updated_at = datetime.now(timezone.utc)

            # Update prompt table
            # NOTE: when writing the the jsonb field `data` we use jsonb
            # when reading we use `json(data)`
            cursor.execute(
                """
                UPDATE prompt
                SET name = ?,
                    data = jsonb(?),
                    version = ?,
                    content_hash = ?,
                    updated_at = ?,
                    github_url = ?
                WHERE id = ?
                """,
                (
                    prompt.name,
                    prompt_jsonb,
                    prompt.version,
                    prompt.content_hash,
                    prompt.updated_at,
                    prompt.github_url,
                    prompt.id,
                ),
            )

            # Insert into the updated prompt into prompt_history
            cursor.execute(
                """
                INSERT INTO prompt_history(
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                """,
                (
                    prompt.id,
                    prompt.version,
                    prompt_jsonb,
                    prompt.content_hash,
                    prompt.created_at,
                    datetime.now(timezone.utc),
                    change_summary,
                    prompt.github_url,
                ),
            )

            self.conn.commit()

            return PromptCreateResult(
                success=True, prompt_id=prompt.id, version=prompt.version
            )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def get_prompt_by_id(self, prompt_id: str) -&amp;amp;gt; Optional[Prompt]:
        """Get a prompt by its ID from the database

        Args:
            prompt_id: The ID of the prompt to retrieve

        Returns:
            Optional[Prompt]: The prompt if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE id = ?
            """,
            (prompt_id,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        # Parse the JSONB data back to PromptData
        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        # Create and return the Prompt object
        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def get_prompt_by_name(self, prompt_name: str) -&amp;amp;gt; Optional[Prompt]:
        """
        Get a prompt by name from the database

        Args:
            prompt_name: The name of the prompt to retrieve.
            (should be unique)
        Returns:
            Optional[Prompt]: The prompt if found by name or None otherwise
        """

        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE name = ?
            """,
            (prompt_name,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def delete_prompt_by_id(self, prompt_id: str) -&amp;amp;gt; PromptDeleteResult:
        cursor = self.conn.cursor()
        try:
            # archive final state in prompt_history table before deletion
            # we will not be deleting the version history of the prompt
            cursor.execute(
                """
                INSERT INTO prompt_history (
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                SELECT id, version, data, content_hash, created_at, ?, ?, github_url
                FROM prompt WHERE id = ?
            """,
                (datetime.now(timezone.utc), "DELETED - Final Version", prompt_id),
            )

            # Delete only from the prompt table
            cursor.execute("DELETE FROM prompt WHERE id = ?", (prompt_id,))
            deleted_row_count = cursor.rowcount

            self.conn.commit()

            return PromptDeleteResult(
                success=True,
                prompt_id=prompt_id,
                deleted=True,
                rows_affected=deleted_row_count,
            )

        except Exception as e:
            self.conn.rollback()
            return PromptDeleteResult(
                success=False,
                prompt_id=prompt_id,
                deleted=False,
                rows_affected=0,
                error_message=str(e),
                error_type=type(e).__name__,
            )

    def bulk_save_in_db(self, prompts: List[Prompt]) -&amp;amp;gt; List[PromptCreateResult]:
        """
        Bulk load/save prompts into the database

        Args:
            plans: List of Plan objects to load into database

        Returns:
            PlanLoadResult: Summary of the loading operation
        """

        return [self.save_prompt_in_db(prompt) for prompt in prompts]

    def flatten_cmds_to_disk(self) -&amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all cmd_category prompts from database to disk directories

        Queries all CMD type prompts from database and writes them to:
        - .claude/commands/{category}/{filename}
        - .gemini/commands/{category}/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure command directories exist
            if not self.cmd_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create command directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all CMD type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;gt;&amp;amp;gt; '$.type' = 'cmd'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No CMD prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    # `**` unpacks the dictionary into key words for pydantic
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.CMD)

                    # Get category, handle None/uncategorized case
                    if prompt.data.cmd_category:
                        if isinstance(prompt.data.cmd_category, str):
                            category = prompt.data.cmd_category
                        else:
                            category = prompt.data.cmd_category.value
                    else:
                        category = "uncategorized"

                    # Determine target directory based on tags
                    target_dirs = []
                    if prompt.data.tags:
                        if "claude" in prompt.data.tags:
                            target_dirs.append("claude")
                        if "gemini" in prompt.data.tags:
                            target_dirs.append("gemini")

                    # If no source tags found, skip this prompt
                    if not target_dirs:
                        errmsg = "No source tag (claude/gemini) found in tags"
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category=category,
                                error_message=errmsg,
                                error_type="MissingSourceTag",
                            )
                        )
                        continue

                    # Write to appropriate directories based on source tags
                    for target_dir in target_dirs:
                        try:
                            target_path = (
                                project_dir
                                / f".{target_dir}"
                                / "commands"
                                / category
                                / filename
                            )

                            # Ensure parent directory exists
                            target_path.parent.mkdir(parents=True, exist_ok=True)

                            # Write content to file
                            target_path.write_text(
                                prompt.data.content, encoding="utf-8"
                            )

                            results.append(
                                PromptFlattenResult(
                                    success=True,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=str(target_path),
                                    cmd_category=category,
                                    error_message="",
                                    error_type="",
                                )
                            )

                        except Exception as e:
                            results.append(
                                PromptFlattenResult(
                                    success=False,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=(
                                        str(target_path)
                                        if "target_path" in locals()
                                        else ""
                                    ),
                                    cmd_category=category,
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results

    def flatten_plans_to_disk(self) -&amp;amp;gt; List[PromptFlattenResult]:
        """Flatten all plan prompts from database to disk directories

        Queries all PLAN type prompts from database and writes them to:
        - _docs/plans/drafts/{filename}
        - _docs/plans/approved/{filename}
        - _docs/plans/completed/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure plan directories exist
            if not self.plans_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create plan directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all PLAN type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;amp;gt;&amp;amp;gt; '$.type' = 'plan'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No PLAN prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            # Status to directory mapping
            status_dir_mapping = {
                PromptPlanStatus.DRAFT.value: "drafts",
                PromptPlanStatus.APPROVED.value: "approved",
                PromptPlanStatus.COMPLETED.value: "completed",
            }

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Validate project name for PLAN type prompts
                    if not prompt.data.project:
                        # PLAN type must have a project name
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category="",
                                error_message="PLAN type prompt missing required project name",
                                error_type="MissingProjectError",
                            )
                        )
                        continue

                    # TODO: update this to use coordinate the github_url
                    if prompt.data.project != project_dir.name:
                        # Skip this prompt - it belongs to a different project
                        continue

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.PLAN)

                    # Get status directory
                    status_value = (
                        prompt.data.status.value
                        if hasattr(prompt.data.status, "value")
                        else str(prompt.data.status)
                    )
                    status_dir = status_dir_mapping.get(status_value, "drafts")

                    # Write to appropriate status directory
                    try:
                        target_path = (
                            project_dir / "_docs" / "plans" / status_dir / filename
                        )

                        # Ensure parent directory exists
                        target_path.parent.mkdir(parents=True, exist_ok=True)

                        # Write content to file
                        target_path.write_text(prompt.data.content, encoding="utf-8")

                        results.append(
                            PromptFlattenResult(
                                success=True,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=str(target_path),
                                cmd_category=status_dir,
                                error_message="",
                                error_type="",
                            )
                        )

                    except Exception as e:
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=(
                                    str(target_path)
                                    if "target_path" in locals()
                                    else ""
                                ),
                                cmd_category=status_dir,
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results
&amp;lt;/file&amp;gt;
  &amp;lt;file path="repository/test_datetime_adapters.py"&amp;gt;"""Test the custom datetime adapters for SQLite3 compatibility."""

import pytest
import warnings
from datetime import datetime, date
from repository.database import SQLite3Database
from repository import datetime_adapters


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db_path = ":memory:"  # Use in-memory database for tests
    db = SQLite3Database(db_path)

    # Create test table
    with db.get_connection() as conn:
        conn.execute(
            """
            CREATE TABLE test_dates (
                id INTEGER PRIMARY KEY,
                created_at TIMESTAMP,
                updated_at DATETIME,
                date_only DATE,
                description TEXT
            )
        """
        )
        yield conn


def test_datetime_storage_retrieval(test_db):
    """Test that datetime objects can be stored and retrieved without warnings."""

    # Capture warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Test data
        test_datetime = datetime(2025, 1, 13, 10, 30, 45)
        test_date = date(2025, 1, 13)

        # Insert test data
        test_db.execute(
            """
            INSERT INTO test_dates (created_at, updated_at, date_only, description)
            VALUES (?, ?, ?, ?)
        """,
            (test_datetime, test_datetime, test_date, "Test record"),
        )

        test_db.commit()

        # Retrieve data
        cursor = test_db.execute(
            """
            SELECT created_at, updated_at, date_only, description 
            FROM test_dates WHERE id = 1
        """
        )
        row = cursor.fetchone()

        # Verify no deprecation warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert (
            len(deprecation_warnings) == 0
        ), f"Found deprecation warnings: {[str(dw.message) for dw in deprecation_warnings]}"

        # Verify data integrity
        assert isinstance(row["created_at"], datetime)
        assert isinstance(row["updated_at"], datetime)
        assert isinstance(row["date_only"], date)
        assert row["created_at"] == test_datetime
        assert row["updated_at"] == test_datetime
        assert row["date_only"] == test_date


def test_datetime_iso_format(test_db):
    """Test that datetimes are stored in ISO format."""

    test_datetime = datetime(2025, 1, 13, 14, 30, 45)

    # Insert using our adapter
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, description)
        VALUES (?, ?)
    """,
        (test_datetime, "ISO format test"),
    )
    test_db.commit()

    # Read raw value (bypass converter)
    cursor = test_db.execute(
        """
        SELECT CAST(created_at AS TEXT) as raw_datetime 
        FROM test_dates WHERE description = 'ISO format test'
    """
    )
    row = cursor.fetchone()

    # Verify ISO format
    expected_iso = "2025-01-13T14:30:45"
    assert row["raw_datetime"] == expected_iso


def test_adapter_functions_directly():
    """Test adapter and converter functions directly."""

    # Test datetime adapter
    test_dt = datetime(2025, 1, 13, 10, 30, 45, 123456)
    adapted = datetime_adapters.adapt_datetime_iso(test_dt)
    assert adapted == "2025-01-13T10:30:45.123456"

    # Test date adapter
    test_date = date(2025, 1, 13)
    adapted_date = datetime_adapters.adapt_date_iso(test_date)
    assert adapted_date == "2025-01-13"

    # Test datetime converter
    iso_bytes = b"2025-01-13T10:30:45.123456"
    converted = datetime_adapters.convert_datetime_iso(iso_bytes)
    assert converted == test_dt

    # Test date converter
    date_bytes = b"2025-01-13"
    converted_date = datetime_adapters.convert_date_iso(date_bytes)
    assert converted_date == test_date

    # Test timestamp converter
    timestamp_bytes = b"1736765445"  # Unix timestamp for 2025-01-13 10:30:45 UTC
    converted_ts = datetime_adapters.convert_timestamp(timestamp_bytes)
    # Note: This will be in local timezone
    assert isinstance(converted_ts, datetime)


def test_timezone_naive_handling():
    """Test that timezone info is properly stripped."""

    # Create timezone-aware datetime
    from datetime import timezone

    tz_aware = datetime(2025, 1, 13, 10, 30, 45, tzinfo=timezone.utc)

    # Adapt should strip timezone
    adapted = datetime_adapters.adapt_datetime_iso(tz_aware)
    assert adapted == "2025-01-13T10:30:45"
    assert "+00:00" not in adapted  # No timezone offset in output


def test_multiple_datetime_operations(test_db):
    """Test multiple datetime operations to ensure no warnings."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Multiple inserts
        for i in range(5):
            dt = datetime.now()
            test_db.execute(
                """
                INSERT INTO test_dates (created_at, updated_at, description)
                VALUES (?, ?, ?)
            """,
                (dt, dt, f"Record {i}"),
            )

        test_db.commit()

        # Multiple selects
        cursor = test_db.execute("SELECT * FROM test_dates")
        rows = cursor.fetchall()

        # Verify all datetimes are properly converted
        for row in rows:
            if row["created_at"]:
                assert isinstance(row["created_at"], datetime)
            if row["updated_at"]:
                assert isinstance(row["updated_at"], datetime)

        # Check for warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert len(deprecation_warnings) == 0


def test_null_datetime_handling(test_db):
    """Test that NULL datetime values are handled correctly."""

    # Insert NULL values
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, updated_at, date_only, description)
        VALUES (NULL, NULL, NULL, 'Null test')
    """
    )
    test_db.commit()

    # Retrieve NULL values
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at, date_only 
        FROM test_dates WHERE description = 'Null test'
    """
    )
    row = cursor.fetchone()

    # Verify NULLs are preserved
    assert row["created_at"] is None
    assert row["updated_at"] is None
    assert row["date_only"] is None


def test_backwards_compatibility(test_db):
    """Test that existing ISO format strings are still readable."""

    # Manually insert ISO format strings (simulating old data)
    test_db.execute(
        """
        INSERT INTO test_dates (id, created_at, updated_at, description)
        VALUES (100, '2024-12-01T10:30:45', '2024-12-01T10:30:45', 'Old format')
    """
    )
    test_db.commit()

    # Read with our converters
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at 
        FROM test_dates WHERE id = 100
    """
    )
    row = cursor.fetchone()

    # Verify conversion works
    assert isinstance(row["created_at"], datetime)
    assert row["created_at"].year == 2024
    assert row["created_at"].month == 12
    assert row["created_at"].day == 1
    assert row["created_at"].hour == 10
    assert row["created_at"].minute == 30
    assert row["created_at"].second == 45
&amp;lt;/file&amp;gt;
  &amp;lt;file path=".gemini/settings.json"&amp;gt;{
	"mcpServers": {
		"collect": {
			"command": "uv",
			"args": [
				"run",
				"python",
				"collect.py"
			],
			"workingDirectory": "/Users/benjaminmetz/python/collect",
			"enabled": true
		}
	}
}
&amp;lt;/file&amp;gt;
  &amp;lt;file path="dotfiles/.zshrc"&amp;gt;GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"

export EDITOR="nvim"
export VISUAL="nvim"
export PATH="$PATH:$HOME/.local/bin"

# path to sqlite3
export PATH="/opt/homebrew/opt/sqlite/bin:$PATH"

# path to ripgrep
export PATH="$HOME/opt/homebrew/bin/rg:$PATH"

#python uv path
. "$HOME/.local/bin/env"

# path to scripts and zig and stuff
export PATH="$HOME/bin:$PATH"

# path to npm
export PATH="$(npm config get prefix)/bin:$PATH"
# path to zig
export PATH=$PATH:~/bin/zig/
export PATH="$(brew --prefix coreutils)/libexec/gnubin:$PATH"

# dependencies needed for gemini / google token processing
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$PKG_CONFIG_PATH"
export PATH="$(brew --prefix)/bin:$PATH"
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$(brew --prefix protobuf)/lib/pkgconfig:$PKG_CONFIG_PATH"

# shortcuts to project work
alias gowork='cd $HOME/go/src/github.com/metzben &amp;amp;amp;&amp;amp;amp; ls -lhG'
alias py='cd $HOME/python &amp;amp;amp;&amp;amp;amp; ls -l --color'
alias collect='cd $HOME/python/collect &amp;amp;amp;&amp;amp;amp; source .venv/bin/activate'
alias el='cd $HOME/go/src/github.com/metzben/elephnt &amp;amp;amp;&amp;amp;amp; ls -l --color'
alias tiny='cd $HOME/go/src/github.com/metzben/tinystack &amp;amp;amp;&amp;amp;amp; ls -lhG'
alias ai='cd $HOME/python/aiwork &amp;amp;amp;&amp;amp;amp; ls -lhG'
alias mcp='cd $HOME/python/mcpwork &amp;amp;amp;&amp;amp;amp; ls -lhG'
alias base='cd $HOME/base &amp;amp;amp;&amp;amp;amp; nvim .'
alias fta='cd $HOME/python/fastta &amp;amp;amp;&amp;amp;amp; nvim .'
alias indicators='cd $HOME/python/indicators &amp;amp;amp;&amp;amp;amp; ls -l'
alias mcpstart='cd $HOME/python/startermcp &amp;amp;amp;&amp;amp;amp; ls -l'
alias tools='cd ~/bin &amp;amp;amp;&amp;amp;amp; ls -l --color'
alias plans='cd _docs/plans &amp;amp;amp;&amp;amp;amp; tree -C -L 2'

# Database function - only works in collect directory
db() {
    if [[ "$PWD" == *"/collect" ]] || [[ "$PWD" == *"/collect/"* ]]; then
        sqlite3 data/collect.db
    else
        echo "Not in collect directory. This command only works in the collect project."
    fi
}

# claude ai shortcuts
alias ask='claude -p '
alias editmcp='nvim ~/Library/Application\ Support/Claude/claude_desktop_config.json'
alias rip='claude --dangerously-skip-permissions'
alias cmds='cd "$(git rev-parse --show-toplevel)/.claude/commands" &amp;amp;amp;&amp;amp;amp; ls -l --color'
alias gms='cd "$(git rev-parse --show-toplevel)/.gemini/commands" &amp;amp;amp;&amp;amp;amp; ls -l --color'

# git shortcuts
alias gs='git status'
alias gd='git diff --staged'
alias gc='git commit -m '
alias push='git push origin main'
alias ga='git add '
alias gb='git branch'
alias gwl='git worktree list'
alias rebase='git pull --rebase origin main'
alias pull='git pull origin main'

# Worktree navigation functions
cd1() {
    local project_name=$(basename "$(pwd)")
    local wt1_path="../${project_name}-wt1"
    
    if [[ -d "$wt1_path" ]]; then
        cd "$wt1_path"
        echo "Changed to worktree 1: $(pwd)"
    else
        echo "Worktree 1 not found: $wt1_path"
        echo "Run 'trees' to create worktrees first."
    fi
}

cd2() {
    local project_name=$(basename "$(pwd)")
    local wt2_path="../${project_name}-wt2"
    
    if [[ -d "$wt2_path" ]]; then
        cd "$wt2_path"
        echo "Changed to worktree 2: $(pwd)"
    else
        echo "Worktree 2 not found: $wt2_path"
        echo "Run 'trees' to create worktrees first."
    fi
}


checkport() {
    if [ -z "$1" ]; then
        echo "Usage: checkport &amp;amp;lt;port_number&amp;amp;gt;"
        return 1
    fi
    
    if lsof -i :$1 2&amp;amp;gt;/dev/null; then
        echo "Port $1 is in use"
    else
        echo "Port $1 is available"
    fi
}

# uv shortcuts
alias env='source .venv/bin/activate'
alias da='deactivate'
alias ipy='uv run ipython'

# go shortcuts
alias run='go test -v -run'

# config shortcuts
alias src='source ~/.zshrc'
alias openz='nvim ~/.zshrc'
alias initlua='nvim $HOME/.config/nvim/init.lua'
alias ghconf='nvim $HOME/.config/ghostty/config'
alias oc='cursor .'

# misc shorty's
alias ll='ls -l --color'
alias tll='tree -C -L 2'
alias oc='cursor .'
alias onv='nvim .'
alias runz='zig run src/main.zig'
alias cperr="zig run src/main.zig 2&amp;amp;gt;&amp;amp;amp;1 | tee /dev/tty | awk '/error:/{found=1} found {print}' | pbcopy"

# ollama models
alias deep70='ollama run deepseek-r1:70b'
alias llama70='ollama run llama3.3'

# The next line updates PATH for the Google Cloud SDK.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc'; fi

# The next line enables shell command completion for gcloud.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc'; fi

alias auth='gcloud auth login'
alias auth2='gcloud auth application-default login'

export PS1='b@m %~ % '



# opencode
export PATH=/Users/benjaminmetz/.opencode/bin:$PATH
&amp;lt;/file&amp;gt;
  &amp;lt;file path="dotfiles/nvim/init.lua"&amp;gt;vim.opt.clipboard = "unnamedplus"

if vim.g.vscode then
	return
end

-- Set &amp;amp;lt;space&amp;amp;gt; as the leader key
-- See `:help mapleader`
--  NOTE: Must happen before plugins are loaded (otherwise wrong leader will be used)
vim.g.mapleader = " "
vim.g.maplocalleader = " "

-- have neovim honor the terminal opacity
vim.opt.termguicolors = true
--vim.cmd.colorscheme("catppuccin-mocha")

vim.cmd([[highlight Normal ctermbg=none guibg=none]])

-- Set to true if you have a Nerd Font installed and selected in the terminal
vim.g.have_nerd_font = true

-- [[ Setting options ]]
-- See `:help vim.opt`
-- NOTE: You can change these options as you wish!
--  For more options, you can see `:help option-list`

-- Make line numbers default
vim.opt.number = true
-- You can also add relative line numbers, to help with jumping.
--  Experiment for yourself to see if you like it!
vim.opt.relativenumber = true

-- Enable mouse mode, can be useful for resizing splits for example!
vim.opt.mouse = "a"

-- Don't show the mode, since it's already in the status line
vim.opt.showmode = true

-- Sync clipboard between OS and Neovim.
--  Schedule the setting after `UiEnter` because it can increase startup-time.
--  Remove this option if you want your OS clipboard to remain independent.
--  See `:help 'clipboard'`
vim.schedule(function()
	vim.opt.clipboard = "unnamedplus"
end)

-- Enable break indent
vim.opt.breakindent = true

-- Save undo history
vim.opt.undofile = true

-- Case-insensitive searching UNLESS \C or one or more capital letters in the search term
vim.opt.ignorecase = true
vim.opt.smartcase = true

-- Keep signcolumn on by default
vim.opt.signcolumn = "yes"

-- Decrease update time
vim.opt.updatetime = 250

-- Decrease mapped sequence wait time
-- Displays which-key popup sooner
vim.opt.timeoutlen = 300

-- Configure how new splits should be opened
vim.opt.splitright = true
vim.opt.splitbelow = true

-- Sets how neovim will display certain whitespace characters in the editor.
--  See `:help 'list'`
--  and `:help 'listchars'`
vim.opt.list = true
vim.opt.listchars = { tab = "» ", trail = "·", nbsp = "␣" }

-- Preview substitutions live, as you type!
vim.opt.inccommand = "split"

-- Show which line your cursor is on
vim.opt.cursorline = true

-- Minimal number of screen lines to keep above and below the cursor.
vim.opt.scrolloff = 15

vim.o.foldmethod = "expr"
vim.o.foldexpr = "nvim_treesitter#foldexpr()"
vim.o.foldenable = true
vim.o.foldlevel = 99 -- Keep folds open by default

--vim.cmd([[packadd packer.nvim]])

-- [[ Basic Keymaps ]]
--  See `:help vim.keymap.set()`

-- Clear highlights on search when pressing &amp;amp;lt;Esc&amp;amp;gt; in normal mode
--  See `:help hlsearch`
vim.keymap.set("n", "&amp;amp;lt;Esc&amp;amp;gt;", "&amp;amp;lt;cmd&amp;amp;gt;nohlsearch&amp;amp;lt;CR&amp;amp;gt;")
-- when in insert mode pressing j and j again will &amp;amp;lt;Esc&amp;amp;gt;'
vim.keymap.set("i", "&amp;amp;lt;D-j&amp;amp;gt;", "&amp;amp;lt;Esc&amp;amp;gt;", { noremap = true, silent = true })
vim.keymap.set("n", "a", "A", { noremap = true, silent = true })
vim.keymap.set("n", "4", "$", { noremap = true, silent = true })

-- Diagnostic keymaps
vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;q", vim.diagnostic.setloclist, { desc = "Open diagnostic [Q]uickfix list" })
vim.keymap.set("n", "[d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })
vim.keymap.set("n", "]d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })

-- for people to discover. Otherwise, you normally need to press &amp;amp;lt;C-\&amp;amp;gt;&amp;amp;lt;C-n&amp;amp;gt;, which
-- is not what someone will guess without a bit more experience.
--
-- NOTE: This won't work in all terminal emulators/tmux/etc. Try your own mapping
-- or just use &amp;amp;lt;C-\&amp;amp;gt;&amp;amp;lt;C-n&amp;amp;gt; to exit terminal mode
vim.keymap.set("t", "&amp;amp;lt;Esc&amp;amp;gt;&amp;amp;lt;Esc&amp;amp;gt;", "&amp;amp;lt;C-\\&amp;amp;gt;&amp;amp;lt;C-n&amp;amp;gt;", { desc = "Exit terminal mode" })

-- Keybinds to make split navigation easier.
-- Use CTRL+&amp;amp;lt;hjkl&amp;amp;gt; to switch between windows
--
--  See `:help wincmd` for a list of all window commands
vim.keymap.set("n", "&amp;amp;lt;C-h&amp;amp;gt;", "&amp;amp;lt;C-w&amp;amp;gt;&amp;amp;lt;C-h&amp;amp;gt;", { desc = "Move focus to the left window" })
vim.keymap.set("n", "&amp;amp;lt;C-l&amp;amp;gt;", "&amp;amp;lt;C-w&amp;amp;gt;&amp;amp;lt;C-l&amp;amp;gt;", { desc = "Move focus to the right window" })
vim.keymap.set("n", "&amp;amp;lt;C-j&amp;amp;gt;", "&amp;amp;lt;C-w&amp;amp;gt;&amp;amp;lt;C-j&amp;amp;gt;", { desc = "Move focus to the lower window" })
vim.keymap.set("n", "&amp;amp;lt;C-k&amp;amp;gt;", "&amp;amp;lt;C-w&amp;amp;gt;&amp;amp;lt;C-k&amp;amp;gt;", { desc = "Move focus to the upper window" })

vim.api.nvim_create_autocmd("BufEnter", {
	pattern = "$HOME/go/src/github.com/metzben/*",
	callback = function()
		vim.cmd("colorscheme onedark")
	end,
})

-- Ensure Packer is loaded
vim.cmd([[packadd packer.nvim]])

-- [[ Basic Autocommands ]]
--  See `:help lua-guide-autocommands`

-- Highlight when yanking (copying) text
--  Try it with `yap` in normal mode
--  See `:help vim.highlight.on_yank()`
vim.api.nvim_create_autocmd("TextYankPost", {
	desc = "Highlight when yanking (copying) text",
	group = vim.api.nvim_create_augroup("kickstart-highlight-yank", { clear = true }),
	callback = function()
		vim.highlight.on_yank()
	end,
})

-- [[ Install `lazy.nvim` plugin manager ]]
--    See `:help lazy.nvim.txt` or https://github.com/folke/lazy.nvim for more info
local lazypath = vim.fn.stdpath("data") .. "/lazy/lazy.nvim"
if not (vim.uv or vim.loop).fs_stat(lazypath) then
	local lazyrepo = "https://github.com/folke/lazy.nvim.git"
	local out = vim.fn.system({ "git", "clone", "--filter=blob:none", "--branch=stable", lazyrepo, lazypath })
	if vim.v.shell_error ~= 0 then
		error("Error cloning lazy.nvim:\n" .. out)
	end
end ---@diagnostic disable-next-line: undefined-field
vim.opt.rtp:prepend(lazypath)

-- [[ Configure and install plugins ]]
--
--  To check the current status of your plugins, run
--    :Lazy
--
--  You can press `?` in this menu for help. Use `:q` to close the window
--
--  To update plugins you can run
--    :Lazy update
--
-- NOTE: Here is where you install your plugins.
require("lazy").setup({
	-- NOTE: Plugins can be added with a link (or for a github repo: 'owner/repo' link).
	"tpope/vim-sleuth", -- Detect tabstop and shiftwidth automatically

	-- NOTE: Plugins can also be added by using a table,
	-- with the first argument being the link and the following
	-- keys can be used to configure plugin behavior/loading/etc.
	--
	-- Use `opts = {}` to force a plugin to be loaded.
	--

	-- Here is a more advanced example where we pass configuration
	-- options to `gitsigns.nvim`. This is equivalent to the following Lua:
	--    require('gitsigns').setup({ ... })
	--
	-- See `:help gitsigns` to understand what the configuration keys do
	{ -- Adds git related signs to the gutter, as well as utilities for managing changes
		"lewis6991/gitsigns.nvim",
		opts = {
			signs = {
				add = { text = "+" },
				change = { text = "~" },
				delete = { text = "_" },
				topdelete = { text = "‾" },
				changedelete = { text = "~" },
			},
		},
	},

	-- NOTE: Plugins can also be configured to run Lua code when they are loaded.
	--
	-- This is often very useful to both group configuration, as well as handle
	-- lazy loading plugins that don't need to be loaded immediately at startup.
	--
	-- For example, in the following configuration, we use:
	--  event = 'VimEnter'
	--
	-- which loads which-key before all the UI elements are loaded. Events can be
	-- normal autocommands events (`:help autocmd-events`).
	--
	-- Then, because we use the `opts` key (recommended), the configuration runs
	-- after the plugin has been loaded as `require(MODULE).setup(opts)`.

	{ -- Useful plugin to show you pending keybinds.
		"folke/which-key.nvim",
		event = "VimEnter", -- Sets the loading event to 'VimEnter'
		opts = {
			icons = {
				-- set icon mappings to true if you have a Nerd Font
				mappings = vim.g.have_nerd_font,
				-- If you are using a Nerd Font: set icons.keys to an empty table which will use the
				-- default which-key.nvim defined Nerd Font icons, otherwise define a string table
				keys = vim.g.have_nerd_font and {} or {
					Up = "&amp;amp;lt;Up&amp;amp;gt; ",
					Down = "&amp;amp;lt;Down&amp;amp;gt; ",
					Left = "&amp;amp;lt;Left&amp;amp;gt; ",
					Right = "&amp;amp;lt;Right&amp;amp;gt; ",
					C = "&amp;amp;lt;C-…&amp;amp;gt; ",
					M = "&amp;amp;lt;M-…&amp;amp;gt; ",
					D = "&amp;amp;lt;D-…&amp;amp;gt; ",
					S = "&amp;amp;lt;S-…&amp;amp;gt; ",
					CR = "&amp;amp;lt;CR&amp;amp;gt; ",
					Esc = "&amp;amp;lt;Esc&amp;amp;gt; ",
					ScrollWheelDown = "&amp;amp;lt;ScrollWheelDown&amp;amp;gt; ",
					ScrollWheelUp = "&amp;amp;lt;ScrollWheelUp&amp;amp;gt; ",
					NL = "&amp;amp;lt;NL&amp;amp;gt; ",
					BS = "&amp;amp;lt;BS&amp;amp;gt; ",
					Space = "&amp;amp;lt;Space&amp;amp;gt; ",
					Tab = "&amp;amp;lt;Tab&amp;amp;gt; ",
					F1 = "&amp;amp;lt;F1&amp;amp;gt;",
					F2 = "&amp;amp;lt;F2&amp;amp;gt;",
					F3 = "&amp;amp;lt;F3&amp;amp;gt;",
					F4 = "&amp;amp;lt;F4&amp;amp;gt;",
					F5 = "&amp;amp;lt;F5&amp;amp;gt;",
					F6 = "&amp;amp;lt;F6&amp;amp;gt;",
					F7 = "&amp;amp;lt;F7&amp;amp;gt;",
					F8 = "&amp;amp;lt;F8&amp;amp;gt;",
					F9 = "&amp;amp;lt;F9&amp;amp;gt;",
					F10 = "&amp;amp;lt;F10&amp;amp;gt;",
					F11 = "&amp;amp;lt;F11&amp;amp;gt;",
					F12 = "&amp;amp;lt;F12&amp;amp;gt;",
				},
			},

			-- Document existing key chains
			spec = {
				{ "&amp;amp;lt;leader&amp;amp;gt;c", group = "[C]ode", mode = { "n", "x" } },
				{ "&amp;amp;lt;leader&amp;amp;gt;d", group = "[D]ocument" },
				{ "&amp;amp;lt;leader&amp;amp;gt;r", group = "[R]ename" },
				{ "&amp;amp;lt;leader&amp;amp;gt;s", group = "[S]earch" },
				{ "&amp;amp;lt;leader&amp;amp;gt;w", group = "[W]orkspace" },
				{ "&amp;amp;lt;leader&amp;amp;gt;t", group = "[T]oggle" },
				{ "&amp;amp;lt;leader&amp;amp;gt;h", group = "Git [H]unk", mode = { "n", "v" } },
			},
		},
	},

	-- NOTE: Plugins can specify dependencies.
	--
	-- The dependencies are proper plugin specifications as well - anything
	-- you do for a plugin at the top level, you can do for a dependency.
	--
	-- Use the `dependencies` key to specify the dependencies of a particular plugin

	{ -- Fuzzy Finder (files, lsp, etc)
		"nvim-telescope/telescope.nvim",
		event = "VimEnter",
		branch = "0.1.x",
		dependencies = {
			"nvim-lua/plenary.nvim",
			{ -- If encountering errors, see telescope-fzf-native README for installation instructions
				"nvim-telescope/telescope-fzf-native.nvim",

				-- `build` is used to run some command when the plugin is installed/updated.
				-- This is only run then, not every time Neovim starts up.
				build = "make",

				-- `cond` is a condition used to determine whether this plugin should be
				-- installed and loaded.
				cond = function()
					return vim.fn.executable("make") == 1
				end,
			},
			{ "nvim-telescope/telescope-ui-select.nvim" },

			-- Useful for getting pretty icons, but requires a Nerd Font.
			{ "nvim-tree/nvim-web-devicons", enabled = vim.g.have_nerd_font },
		},
		config = function()
			-- Telescope is a fuzzy finder that comes with a lot of different things that
			-- it can fuzzy find! It's more than just a "file finder", it can search
			-- many different aspects of Neovim, your workspace, LSP, and more!
			--
			-- The easiest way to use Telescope, is to start by doing something like:
			--  :Telescope help_tags
			--
			-- After running this command, a window will open up and you're able to
			-- type in the prompt window. You'll see a list of `help_tags` options and
			-- a corresponding preview of the help.
			--
			-- Two important keymaps to use while in Telescope are:
			--  - Insert mode: &amp;amp;lt;c-/&amp;amp;gt;
			--  - Normal mode: ?
			--
			-- This opens a window that shows you all of the keymaps for the current
			-- Telescope picker. This is really useful to discover what Telescope can
			-- do as well as how to actually do it!

			-- [[ Configure Telescope ]]
			-- See `:help telescope` and `:help telescope.setup()`
			require("telescope").setup({
				-- You can put your default mappings / updates / etc. in here
				--  All the info you're looking for is in `:help telescope.setup()`
				--
				-- defaults = {
				--   mappings = {
				--     i = { ['&amp;amp;lt;c-enter&amp;amp;gt;'] = 'to_fuzzy_refine' },
				--   },
				-- },
				-- pickers = {}
				extensions = {
					["ui-select"] = {
						require("telescope.themes").get_dropdown(),
					},
				},
			})

			-- Enable Telescope extensions if they are installed
			pcall(require("telescope").load_extension, "fzf")
			pcall(require("telescope").load_extension, "ui-select")

			-- See `:help telescope.builtin`
			local builtin = require("telescope.builtin")
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sh", builtin.help_tags, { desc = "[S]earch [H]elp" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sk", builtin.keymaps, { desc = "[S]earch [K]eymaps" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sf", function()
				builtin.find_files({ hidden = true, no_ignore = true })
			end, { desc = "[S]earch [F]iles" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;ss", builtin.builtin, { desc = "[S]earch [S]elect Telescope" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sw", builtin.grep_string, { desc = "[S]earch current [W]ord" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sg", builtin.live_grep, { desc = "[S]earch by [G]rep" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sd", builtin.diagnostics, { desc = "[S]earch [D]iagnostics" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sr", builtin.resume, { desc = "[S]earch [R]esume" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;s.", builtin.oldfiles, { desc = '[S]earch Recent Files ("." for repeat)' })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;&amp;amp;lt;leader&amp;amp;gt;", builtin.buffers, { desc = "[ ] Find existing buffers" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;af", builtin.current_buffer_fuzzy_find, { desc = "[S]earch in existing file" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;nf", "]m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;pf", "[m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;r", vim.lsp.buf.rename, { desc = "LSP Rename" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;d", "&amp;amp;lt;cmd&amp;amp;gt;Telescope diagnostics&amp;amp;lt;CR&amp;amp;gt;")
			vim.api.nvim_set_keymap("n", "&amp;amp;lt;leader&amp;amp;gt;c", "~", { noremap = true, silent = true })
			vim.keymap.set(
				"n",
				"&amp;amp;lt;leader&amp;amp;gt;O",
				":put! _&amp;amp;lt;CR&amp;amp;gt;",
				{ desc = "Add blank line above without entering edit mode" }
			)

			vim.keymap.set("v", "&amp;amp;lt;leader&amp;amp;gt;/", ":norm I//&amp;amp;lt;CR&amp;amp;gt;", { desc = "Comment selected block" })
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;/", "gcc", { desc = "Toggle comment on current line" })
			-- Slightly advanced example of overriding default behavior and theme
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;[", function()
				-- You can pass additional configuration to Telescope to change the theme, layout, etc.
				builtin.current_buffer_fuzzy_find(require("telescope.themes").get_dropdown({
					winblend = 10,
					previewer = false,
				}))
			end, { desc = "[/] Fuzzily search in current buffer" })

			-- It's also possible to pass additional configuration options.
			--  See `:help telescope.builtin.live_grep()` for information about particular keys
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;s/", function()
				builtin.live_grep({
					grep_open_files = true,
					prompt_title = "Live Grep in Open Files",
				})
			end, { desc = "[S]earch [/] in Open Files" })

			-- Shortcut for searching your Neovim configuration files
			vim.keymap.set("n", "&amp;amp;lt;leader&amp;amp;gt;sn", function()
				builtin.find_files({ cwd = vim.fn.stdpath("config") })
			end, { desc = "[S]earch [N]eovim files" })
		end,
	},

	-- LSP Plugins
	{
		-- `lazydev` configures Lua LSP for your Neovim config, runtime and plugins
		-- used for completion, annotations and signatures of Neovim apis
		"folke/lazydev.nvim",
		ft = "lua",
		opts = {
			library = {
				-- Load luvit types when the `vim.uv` word is found
				{ path = "luvit-meta/library", words = { "vim%.uv" } },
			},
		},
	},
	{ "Bilal2453/luvit-meta", lazy = true },
	{
		-- Main LSP Configuration
		"neovim/nvim-lspconfig",
		dependencies = {
			-- Automatically install LSPs and related tools to stdpath for Neovim
			{ "williamboman/mason.nvim", config = true }, -- NOTE: Must be loaded before dependants
			"williamboman/mason-lspconfig.nvim",
			"WhoIsSethDaniel/mason-tool-installer.nvim",

			-- Useful status updates for LSP.
			-- NOTE: `opts = {}` is the same as calling `require('fidget').setup({})`
			{ "j-hui/fidget.nvim", opts = {} },

			-- Allows extra capabilities provided by nvim-cmp
			"hrsh7th/cmp-nvim-lsp",
		},
		config = function()
			-- Brief aside: **What is LSP?**
			--
			-- LSP is an initialism you've probably heard, but might not understand what it is.
			--
			-- LSP stands for Language Server Protocol. It's a protocol that helps editors
			-- and language tooling communicate in a standardized fashion.
			--
			-- In general, you have a "server" which is some tool built to understand a particular
			-- language (such as `gopls`, `lua_ls`, `rust_analyzer`, etc.). These Language Servers
			-- (sometimes called LSP servers, but that's kind of like ATM Machine) are standalone
			-- processes that communicate with some "client" - in this case, Neovim!
			--
			-- LSP provides Neovim with features like:
			--  - Go to definition
			--  - Find references
			--  - Autocompletion
			--  - Symbol Search
			--  - and more!
			--
			-- Thus, Language Servers are external tools that must be installed separately from
			-- Neovim. This is where `mason` and related plugins come into play.
			--
			-- If you're wondering about lsp vs treesitter, you can check out the wonderfully
			-- and elegantly composed help section, `:help lsp-vs-treesitter`

			--  This function gets run when an LSP attaches to a particular buffer.
			--    That is to say, every time a new file is opened that is associated with
			--    an lsp (for example, opening `main.rs` is associated with `rust_analyzer`) this
			--    function will be executed to configure the current buffer
			vim.api.nvim_create_autocmd("LspAttach", {
				group = vim.api.nvim_create_augroup("kickstart-lsp-attach", { clear = true }),
				callback = function(event)
					-- NOTE: Remember that Lua is a real programming language, and as such it is possible
					-- to define small helper and utility functions so you don't have to repeat yourself.
					--
					-- In this case, we create a function that lets us more easily define mappings specific
					-- for LSP related items. It sets the mode, buffer and description for us each time.
					local map = function(keys, func, desc, mode)
						mode = mode or "n"
						vim.keymap.set(mode, keys, func, { buffer = event.buf, desc = "LSP: " .. desc })
					end

					-- Jump to the definition of the word under your cursor.
					--  This is where a variable was first declared, or where a function is defined, etc.
					--  To jump back, press &amp;amp;lt;C-t&amp;amp;gt;.
					map("gd", require("telescope.builtin").lsp_definitions, "[G]oto [D]efinition")

					-- Find references for the word under your cursor.
					map("gr", require("telescope.builtin").lsp_references, "[G]oto [R]eferences")

					-- Jump to the implementation of the word under your cursor.
					--  Useful when your language has ways of declaring types without an actual implementation.
					map("gI", require("telescope.builtin").lsp_implementations, "[G]oto [I]mplementation")

					-- Jump to the type of the word under your cursor.
					--  Useful when you're not sure what type a variable is and you want to see
					--  the definition of its *type*, not where it was *defined*.
					map("&amp;amp;lt;leader&amp;amp;gt;D", require("telescope.builtin").lsp_type_definitions, "Type [D]efinition")

					-- Fuzzy find all the symbols in your current document.
					--  Symbols are things like variables, functions, types, etc.
					map("&amp;amp;lt;leader&amp;amp;gt;ds", require("telescope.builtin").lsp_document_symbols, "[D]ocument [S]ymbols")

					-- Fuzzy find all the symbols in your current workspace.
					--  Similar to document symbols, except searches over your entire project.
					map(
						"&amp;amp;lt;leader&amp;amp;gt;ws",
						require("telescope.builtin").lsp_dynamic_workspace_symbols,
						"[W]orkspace [S]ymbols"
					)

					-- Rename the variable under your cursor.
					--  Most Language Servers support renaming across files, etc.
					map("&amp;amp;lt;leader&amp;amp;gt;rn", vim.lsp.buf.rename, "[R]e[n]ame")

					-- Execute a code action, usually your cursor needs to be on top of an error
					-- or a suggestion from your LSP for this to activate.
					map("&amp;amp;lt;leader&amp;amp;gt;ca", vim.lsp.buf.code_action, "[C]ode [A]ction", { "n", "x" })

					-- WARN: This is not Goto Definition, this is Goto Declaration.
					--  For example, in C this would take you to the header.
					map("gD", vim.lsp.buf.declaration, "[G]oto [D]eclaration")

					-- The following two autocommands are used to highlight references of the
					-- word under your cursor when your cursor rests there for a little while.
					--    See `:help CursorHold` for information about when this is executed
					--
					-- When you move your cursor, the highlights will be cleared (the second autocommand).
					local client = vim.lsp.get_client_by_id(event.data.client_id)
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_documentHighlight) then
						local highlight_augroup =
							vim.api.nvim_create_augroup("kickstart-lsp-highlight", { clear = false })
						vim.api.nvim_create_autocmd({ "CursorHold", "CursorHoldI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.document_highlight,
						})

						vim.api.nvim_create_autocmd({ "CursorMoved", "CursorMovedI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.clear_references,
						})

						vim.api.nvim_create_autocmd("LspDetach", {
							group = vim.api.nvim_create_augroup("kickstart-lsp-detach", { clear = true }),
							callback = function(event2)
								vim.lsp.buf.clear_references()
								vim.api.nvim_clear_autocmds({ group = "kickstart-lsp-highlight", buffer = event2.buf })
							end,
						})
					end

					-- The following code creates a keymap to toggle inlay hints in your
					-- code, if the language server you are using supports them
					--
					-- This may be unwanted, since they displace some of your code
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_inlayHint) then
						map("&amp;amp;lt;leader&amp;amp;gt;th", function()
							vim.lsp.inlay_hint.enable(not vim.lsp.inlay_hint.is_enabled({ bufnr = event.buf }))
						end, "[T]oggle Inlay [H]ints")
					end
				end,
			})

			-- Change diagnostic symbols in the sign column (gutter)
			-- if vim.g.have_nerd_font then
			--   local signs = { ERROR = '', WARN = '', INFO = '', HINT = '' }
			--   local diagnostic_signs = {}
			--   for type, icon in pairs(signs) do
			--     diagnostic_signs[vim.diagnostic.severity[type]] = icon
			--   end
			--   vim.diagnostic.config { signs = { text = diagnostic_signs } }
			-- end

			-- LSP servers and clients are able to communicate to each other what features they support.
			--  By default, Neovim doesn't support everything that is in the LSP specification.
			--  When you add nvim-cmp, luasnip, etc. Neovim now has *more* capabilities.
			--  So, we create new capabilities with nvim cmp, and then broadcast that to the servers.
			local capabilities = vim.lsp.protocol.make_client_capabilities()
			capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

			-- Enable the following language servers
			--  Feel free to add/remove any LSPs that you want here. They will automatically be installed.
			--
			--  Add any additional override configuration in the following tables. Available keys are:
			--  - cmd (table): Override the default command used to start the server
			--  - filetypes (table): Override the default list of associated filetypes for the server
			--  - capabilities (table): Override fields in capabilities. Can be used to disable certain LSP features.
			--  - settings (table): Override the default settings passed when initializing the server.
			--        For example, to see the options for `lua_ls`, you could go to: https://luals.github.io/wiki/settings/
			local servers = {
				-- clangd = {},
				-- gopls = {},
				-- pyright = {},
				-- rust_analyzer = {},
				-- ... etc. See `:help lspconfig-all` for a list of all the pre-configured LSPs
				--
				-- Some languages (like typescript) have entire language plugins that can be useful:
				--    https://github.com/pmizio/typescript-tools.nvim
				--
				-- But for many setups, the LSP (`ts_ls`) will work just fine
				-- ts_ls = {},
				--

				lua_ls = {
					-- cmd = { ... },
					-- filetypes = { ... },
					-- capabilities = {},
					settings = {
						Lua = {
							completion = {
								callSnippet = "Replace",
							},
							-- You can toggle below to ignore Lua_LS's noisy `missing-fields` warnings
							-- diagnostics = { disable = { 'missing-fields' } },
						},
					},
				},
			}

			-- Ensure the servers and tools above are installed
			--  To check the current status of installed tools and/or manually install
			--  other tools, you can run
			--    :Mason
			--
			--  You can press `g?` for help in this menu.
			require("mason").setup()

			-- You can add other tools here that you want Mason to install
			-- for you, so that they are available from within Neovim.
			local ensure_installed = vim.tbl_keys(servers or {})
			vim.list_extend(ensure_installed, {
				"stylua", -- Used to format Lua code
			})
			require("mason-tool-installer").setup({ ensure_installed = ensure_installed })

			require("mason-lspconfig").setup({
				handlers = {
					function(server_name)
						local server = servers[server_name] or {}
						-- This handles overriding only values explicitly passed
						-- by the server configuration above. Useful when disabling
						-- certain features of an LSP (for example, turning off formatting for ts_ls)
						server.capabilities = vim.tbl_deep_extend("force", {}, capabilities, server.capabilities or {})
						require("lspconfig")[server_name].setup(server)
					end,
				},
			})
		end,
	},

	{ -- Autoformat
		"stevearc/conform.nvim",
		event = { "BufWritePre" },
		cmd = { "ConformInfo" },
		keys = {
			{
				"&amp;amp;lt;leader&amp;amp;gt;f",
				function()
					require("conform").format({ async = true, lsp_format = "fallback" })
				end,
				mode = "",
				desc = "[F]ormat buffer",
			},
		},
		opts = {
			notify_on_error = false,
			format_on_save = function(bufnr)
				-- Disable "format_on_save lsp_fallback" for languages that don't
				-- have a well standardized coding style. You can add additional
				-- languages here or re-enable it for the disabled ones.
				local disable_filetypes = { c = true, cpp = true }
				local lsp_format_opt
				if disable_filetypes[vim.bo[bufnr].filetype] then
					lsp_format_opt = "never"
				else
					lsp_format_opt = "fallback"
				end
				return {
					timeout_ms = 500,
					lsp_format = lsp_format_opt,
				}
			end,
			formatters_by_ft = {
				lua = { "stylua" },
				-- Conform can also run multiple formatters sequentially
				-- python = { "isort", "black" },
				--
				-- You can use 'stop_after_first' to run the first available formatter from the list
				-- javascript = { "prettierd", "prettier", stop_after_first = true },
			},
		},
	},

	{ -- Autocompletion
		"hrsh7th/nvim-cmp",
		event = "InsertEnter",
		dependencies = {
			-- Snippet Engine &amp;amp;amp; its associated nvim-cmp source
			{
				"L3MON4D3/LuaSnip",
				build = (function()
					-- Build Step is needed for regex support in snippets.
					-- This step is not supported in many windows environments.
					-- Remove the below condition to re-enable on windows.
					if vim.fn.has("win32") == 1 or vim.fn.executable("make") == 0 then
						return
					end
					return "make install_jsregexp"
				end)(),
				dependencies = {
					-- `friendly-snippets` contains a variety of premade snippets.
					--    See the README about individual language/framework/plugin snippets:
					--    https://github.com/rafamadriz/friendly-snippets
					-- {
					--   'rafamadriz/friendly-snippets',
					--   config = function()
					--     require('luasnip.loaders.from_vscode').lazy_load()
					--   end,
					-- },
				},
			},
			"saadparwaiz1/cmp_luasnip",

			-- Adds other completion capabilities.
			--  nvim-cmp does not ship with all sources by default. They are split
			--  into multiple repos for maintenance purposes.
			"hrsh7th/cmp-nvim-lsp",
			"hrsh7th/cmp-path",
		},
		config = function()
			-- See `:help cmp`
			local cmp = require("cmp")
			local luasnip = require("luasnip")
			luasnip.config.setup({})

			cmp.setup({
				snippet = {
					expand = function(args)
						luasnip.lsp_expand(args.body)
					end,
				},
				completion = { completeopt = "menu,menuone,noinsert" },

				-- For an understanding of why these mappings were
				-- chosen, you will need to read `:help ins-completion`
				--
				-- No, but seriously. Please read `:help ins-completion`, it is really good!
				mapping = cmp.mapping.preset.insert({
					-- Select the [n]ext item
					["&amp;amp;lt;C-n&amp;amp;gt;"] = cmp.mapping.select_next_item(),
					-- Select the [p]revious item
					["&amp;amp;lt;C-p&amp;amp;gt;"] = cmp.mapping.select_prev_item(),

					-- Scroll the documentation window [b]ack / [f]orward
					["&amp;amp;lt;C-b&amp;amp;gt;"] = cmp.mapping.scroll_docs(-4),
					["&amp;amp;lt;C-f&amp;amp;gt;"] = cmp.mapping.scroll_docs(4),

					-- Accept ([y]es) the completion.
					--  This will auto-import if your LSP supports it.
					--  This will expand snippets if the LSP sent a snippet.
					["&amp;amp;lt;C-y&amp;amp;gt;"] = cmp.mapping.confirm({ select = true }),

					-- If you prefer more traditional completion keymaps,
					-- you can uncomment the following lines
					--['&amp;amp;lt;CR&amp;amp;gt;'] = cmp.mapping.confirm { select = true },
					--['&amp;amp;lt;Tab&amp;amp;gt;'] = cmp.mapping.select_next_item(),
					--['&amp;amp;lt;S-Tab&amp;amp;gt;'] = cmp.mapping.select_prev_item(),

					-- Manually trigger a completion from nvim-cmp.
					--  Generally you don't need this, because nvim-cmp will display
					--  completions whenever it has completion options available.
					["&amp;amp;lt;C-Space&amp;amp;gt;"] = cmp.mapping.complete({}),

					-- Think of &amp;amp;lt;c-l&amp;amp;gt; as moving to the right of your snippet expansion.
					--  So if you have a snippet that's like:
					--  function $name($args)
					--    $body
					--  end
					--
					-- &amp;amp;lt;c-l&amp;amp;gt; will move you to the right of each of the expansion locations.
					-- &amp;amp;lt;c-h&amp;amp;gt; is similar, except moving you backwards.
					["&amp;amp;lt;C-l&amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.expand_or_locally_jumpable() then
							luasnip.expand_or_jump()
						end
					end, { "i", "s" }),
					["&amp;amp;lt;C-h&amp;amp;gt;"] = cmp.mapping(function()
						if luasnip.locally_jumpable(-1) then
							luasnip.jump(-1)
						end
					end, { "i", "s" }),

					-- For more advanced Luasnip keymaps (e.g. selecting choice nodes, expansion) see:
					--    https://github.com/L3MON4D3/LuaSnip?tab=readme-ov-file#keymaps
				}),
				sources = {
					{
						name = "lazydev",
						-- set group index to 0 to skip loading LuaLS completions as lazydev recommends it
						group_index = 0,
					},
					{ name = "nvim_lsp" },
					{ name = "luasnip" },
					{ name = "path" },
				},
			})
		end,
	},

	{ -- You can easily change to a different colorscheme.
		-- Change the name of the colorscheme plugin below, and then
		-- change the command in the config to whatever the name of that colorscheme is.
		--
		-- If you want to see what colorschemes are already installed, you can use `:Telescope colorscheme`.
		"folke/tokyonight.nvim",
		priority = 1000, -- Make sure to load this before all the other start plugins.
		init = function()
			-- Load the colorscheme here.
			-- Like many other themes, this one has different styles, and you could load
			-- any other, such as 'tokyonight-storm', 'tokyonight-moon', or 'tokyonight-day'.
			vim.cmd.colorscheme("tokyonight-night")

			-- You can configure highlights by doing something like:
			vim.cmd.hi("Comment gui=none")
		end,
	},
	{ -- Colorscheme
		"catppuccin/nvim",
		name = "catppuccin",
		priority = 1000,
		config = function()
			require("catppuccin").setup({
				flavour = "mocha",
				transparent_background = true,
				term_colors = true,
				integrations = {
					telescope = true,
					mason = true,
					which_key = true,
				},
			})
			-- Force loading the colorscheme
			vim.cmd.colorscheme("catppuccin-mocha")
		end,
	},
	--calming Japanese inspired theme with muted tones strings are configurable
	{
		"rebelot/kanagawa.nvim",
		priority = 1000,
		config = function()
			require("kanagawa").setup({
				overrides = function(colors)
					return {
						String = { fg = colors.crystalBlue }, -- Customize string color
					}
				end,
			})
			vim.cmd("colorscheme kanagawa")
		end,
	},

	-- based on one dark pro from vs code
	{
		"olimorris/onedarkpro.nvim",
		priority = 1000,
		config = function()
			require("onedarkpro").setup({
				theme = "onedark", -- Choose "onedark" or "onelight"
			})
			vim.cmd("colorscheme onedark")
		end,
	},

	-- Highlight todo, notes, etc in comments
	{
		"folke/todo-comments.nvim",
		event = "VimEnter",
		dependencies = { "nvim-lua/plenary.nvim" },
		opts = { signs = false },
	},

	{ -- Collection of various small independent plugins/modules
		"echasnovski/mini.nvim",
		config = function()
			-- Better Around/Inside textobjects
			--
			-- Examples:
			--  - va)  - [V]isually select [A]round [)]paren
			--  - yinq - [Y]ank [I]nside [N]ext [Q]uote
			--  - ci'  - [C]hange [I]nside [']quote
			require("mini.ai").setup({ n_lines = 500 })

			-- Add/delete/replace surroundings (brackets, quotes, etc.)
			--
			-- - saiw) - [S]urround [A]dd [I]nner [W]ord [)]Paren
			-- - sd'   - [S]urround [D]elete [']quotes
			-- - sr)'  - [S]urround [R]eplace [)] [']
			require("mini.surround").setup()

			-- Simple and easy statusline.
			--  You could remove this setup call if you don't like it,
			--  and try some other statusline plugin
			local statusline = require("mini.statusline")
			-- set use_icons to true if you have a Nerd Font
			statusline.setup({ use_icons = vim.g.have_nerd_font })

			-- You can configure sections in the statusline by overriding their
			-- default behavior. For example, here we set the section for
			-- cursor location to LINE:COLUMN
			---@diagnostic disable-next-line: duplicate-set-field
			statusline.section_location = function()
				return "%2l:%-2v"
			end

			-- ... and there is more!
			--  Check out: https://github.com/echasnovski/mini.nvim
		end,
	},
	{ -- Highlight, edit, and navigate code
		"nvim-treesitter/nvim-treesitter",
		build = ":TSUpdate",
		main = "nvim-treesitter.configs", -- Sets main module to use for opts
		-- [[ Configure Treesitter ]] See `:help nvim-treesitter`
		opts = {
			ensure_installed = {
				"bash",
				"c",
				"diff",
				"html",
				"lua",
				"luadoc",
				"markdown",
				"markdown_inline",
				"query",
				"vim",
				"vimdoc",
			},
			-- Autoinstall languages that are not installed
			auto_install = true,
			highlight = {
				enable = true,
				-- Some languages depend on vim's regex highlighting system (such as Ruby) for indent rules.
				--  If you are experiencing weird indenting issues, add the language to
				--  the list of additional_vim_regex_highlighting and disabled languages for indent.
				additional_vim_regex_highlighting = { "ruby" },
			},
			indent = { enable = true, disable = { "ruby" } },
		},
		-- There are additional nvim-treesitter modules that you can use to interact
		-- with nvim-treesitter. You should go explore a few and see what interests you:
		--
		--    - Incremental selection: Included, see `:help nvim-treesitter-incremental-selection-mod`
		--    - Show your current context: https://github.com/nvim-treesitter/nvim-treesitter-context
		--    - Treesitter + textobjects: https://github.com/nvim-treesitter/nvim-treesitter-textobjects
	},

	-- The following comments only work if you have downloaded the kickstart repo, not just copy pasted the
	-- init.lua. If you want these files, they are in the repository, so you can just download them and
	-- place them in the correct locations.

	-- NOTE: Next step on your Neovim journey: Add/Configure additional plugins for Kickstart
	--
	--  Here are some example plugins that I've included in the Kickstart repository.
	--  Uncomment any of the lines below to enable them (you will need to restart nvim).
	--
	-- require 'kickstart.plugins.debug',
	-- require 'kickstart.plugins.indent_line',
	-- require 'kickstart.plugins.lint',
	-- require 'kickstart.plugins.autopairs',
	-- require 'kickstart.plugins.neo-tree',
	-- require 'kickstart.plugins.gitsigns', -- adds gitsigns recommend keymaps

	-- NOTE: The import below can automatically add your own plugins, configuration, etc from `lua/custom/plugins/*.lua`
	--    This is the easiest way to modularize your config.
	--
	--  Uncomment the following line and add your plugins to `lua/custom/plugins/*.lua` to get going.
	-- { import = 'custom.plugins' },
	--
	-- For additional information with loading, sourcing and examples see `:help lazy.nvim-🔌-plugin-spec`
	-- Or use telescope!
	-- In normal mode type `&amp;amp;lt;space&amp;amp;gt;sh` then write `lazy.nvim-plugin`
	-- you can continue same window with `&amp;amp;lt;space&amp;amp;gt;sr` which resumes last telescope search
}, {
	ui = {
		-- If you are using a Nerd Font: set icons to an empty table which will use the
		-- default lazy.nvim defined Nerd Font icons, otherwise define a unicode icons table
		icons = vim.g.have_nerd_font and {} or {
			cmd = "⌘",
			config = "🛠",
			event = "📅",
			ft = "📂",
			init = "⚙",
			keys = "🗝",
			plugin = "🔌",
			runtime = "💻",
			require = "🌙",
			source = "📄",
			start = "🚀",
			task = "📌",
			lazy = "💤 ",
		},
	},
})

-- At the very end of your init.lua
vim.api.nvim_create_autocmd("UIEnter", {
	callback = function()
		if vim.g.colors_name ~= "catppuccin-mocha" then
			vim.cmd.colorscheme("catppuccin-mocha")
		end
	end,
	group = vim.api.nvim_create_augroup("EnforceCatppuccin", { clear = true }),
})

-- The line beneath this is called `modeline`. See `:help modeline`
-- vim: ts=2 sts=2 sw=2 et
&amp;lt;/file&amp;gt;
  &amp;lt;file path="dotfiles/ghostty/config"&amp;gt;background-opacity = 0.82

theme = catppuccin-mocha
keybind = shift+enter=text:\n
macos-titlebar-style = hidden

keybind = ctrl+3=reload_config
keybind = global:cmd+shift+space=toggle_quick_terminal
&amp;lt;/file&amp;gt;
  &amp;lt;file path=".ruff_cache/.gitignore"&amp;gt;# Automatically created by ruff.
*
&amp;lt;/file&amp;gt;
  &amp;lt;file path="resources/sqlite3-commands.md"&amp;gt;# SQLite3 Common Commands Reference

## Creating a Database

To create a SQLite3 database, simply run:

```bash
sqlite3 database_name.db
```

This will:
- Create a new database file if it doesn't exist
- Open the database if it already exists
- Start the sqlite3 interactive shell

Example:
```bash
sqlite3 myapp.db
```

You can also create a database and run a command:
```bash
sqlite3 myapp.db "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT);"
```

The database file is created when you:
- Create the first table
- Insert the first data
- Or explicitly save with `.save database_name.db`

## Meta Commands (dot commands)

- `.tables` - List all tables
- `.schema [table]` - Show CREATE statements
- `.quit` or `.exit` - Exit sqlite3
- `.help` - Show all commands
- `.databases` - List attached databases
- `.headers on/off` - Show/hide column headers
- `.mode column` - Pretty-print output
- `.width` - Set column widths
- `.import FILE TABLE` - Import CSV data
- `.output FILE` - Redirect output to file
- `.dump` - Export database as SQL

## SQL Commands

- `SELECT * FROM table;` - Query data
- `INSERT INTO table VALUES (...);` - Insert data
- `UPDATE table SET col=val WHERE ...;` - Update data
- `DELETE FROM table WHERE ...;` - Delete data
- `CREATE TABLE ...` - Create table
- `DROP TABLE table;` - Delete table
- `ALTER TABLE ...` - Modify table
- `CREATE INDEX ...` - Create index
- `PRAGMA table_info(table);` - Show table structure
- `VACUUM;` - Optimize database&amp;lt;/file&amp;gt;
  &amp;lt;file path="resources/git-worktrees.md"&amp;gt;# Git Worktrees Guide

Git worktrees allow you to have multiple branches checked out simultaneously in different directories. This is incredibly useful when you need to work on multiple features, review PRs, or quickly switch contexts without stashing changes.

## What are Git Worktrees?

A git worktree is a linked working tree that shares the same repository but allows you to have different branches checked out in different directories. All worktrees share:
- The same `.git` directory (repository data)
- The same remote configurations
- The same stash entries
- The same commit history

## Creating a Worktree

### Basic Syntax
```bash
git worktree add &amp;amp;lt;path&amp;amp;gt; &amp;amp;lt;branch&amp;amp;gt;
```

### Examples

#### Create worktree from existing branch
```bash
# Create a worktree for the 'feature/auth' branch in a new directory
git worktree add ../myproject-auth feature/auth

# Create worktree in a specific location
git worktree add /tmp/hotfix hotfix/urgent-bug
```

#### Create worktree with a new branch
```bash
# Create a new branch and worktree simultaneously
git worktree add -b feature/new-ui ../myproject-new-ui

# Create from a specific commit or tag
git worktree add -b release/v2.0 ../myproject-v2 v2.0-tag
```

#### Practical Example
```bash
# You're working on main in /Users/you/myproject
cd /Users/you/myproject

# Create a worktree for a new feature
git worktree add -b feature/payment-integration ../myproject-payments

# Now you have:
# /Users/you/myproject (main branch)
# /Users/you/myproject-payments (feature/payment-integration branch)

# Navigate to the new worktree
cd ../myproject-payments

# Work on your feature
echo "Payment module" &amp;amp;gt; payment.py
git add payment.py
git commit -m "Add payment module"
```

## Working with Worktrees

### List all worktrees
```bash
git worktree list
# Output:
# /Users/you/myproject         abc1234 [main]
# /Users/you/myproject-payments def5678 [feature/payment-integration]
```

### Switch between worktrees
Simply use `cd` to navigate between directories:
```bash
cd /Users/you/myproject          # main branch
cd /Users/you/myproject-payments  # feature branch
```

## Merging Worktree Changes Back to Main

Since worktrees share the same repository, merging is straightforward:

### Step 1: Commit changes in your worktree
```bash
cd /Users/you/myproject-payments
git add .
git commit -m "Complete payment integration"
git push -u origin feature/payment-integration
```

### Step 2: Switch to main (in any worktree or main directory)
```bash
cd /Users/you/myproject  # Or stay in any worktree
git checkout main
git pull origin main     # Ensure main is up to date
```

### Step 3: Merge the feature branch
```bash
# Simple merge
git merge feature/payment-integration

# Or merge with a merge commit (recommended for features)
git merge --no-ff feature/payment-integration

# Or rebase if you prefer linear history
git rebase main feature/payment-integration
```

### Step 4: Push to remote
```bash
git push origin main
```

### Step 5: Clean up (optional)
```bash
# Delete the local branch
git branch -d feature/payment-integration

# Delete the remote branch
git push origin --delete feature/payment-integration

# Remove the worktree
git worktree remove /Users/you/myproject-payments
```

## Alternative: Using Pull Requests

For team workflows, you might prefer Pull Requests:

```bash
# 1. In your worktree, push the branch
cd /Users/you/myproject-payments
git push -u origin feature/payment-integration

# 2. Create PR via GitHub/GitLab/Bitbucket web interface

# 3. After PR is merged, clean up locally
git worktree remove /Users/you/myproject-payments
git branch -d feature/payment-integration
```

## Worktree Management

### Remove a worktree
```bash
# Remove worktree (must be clean with no uncommitted changes)
git worktree remove /path/to/worktree

# Force removal (discards local changes)
git worktree remove --force /path/to/worktree
```

### Prune stale worktrees
```bash
# Remove worktree references if directory was deleted manually
git worktree prune
```

### Lock/unlock a worktree
```bash
# Prevent a worktree from being pruned
git worktree lock /path/to/worktree

# Unlock it later
git worktree unlock /path/to/worktree
```

## Best Practices

1. **Use descriptive paths**: Name worktree directories after their purpose
   ```bash
   git worktree add ../project-bugfix-auth bugfix/auth-issue
   git worktree add ../project-feature-api feature/new-api
   ```

2. **Keep worktrees organized**: Use a consistent structure
   ```bash
   ~/work/
     myproject/          # main
     myproject-feature1/ # feature branch
     myproject-hotfix/   # hotfix branch
   ```

3. **Clean up regularly**: Remove worktrees when done
   ```bash
   git worktree list
   git worktree remove &amp;amp;lt;path&amp;amp;gt;
   ```

4. **Don't share worktree directories**: Each developer should create their own

5. **Commit before switching**: Although you can leave changes uncommitted, it's cleaner to commit or stash first

## Common Issues and Solutions

### "fatal: '&amp;amp;lt;branch&amp;amp;gt;' is already checked out at '&amp;amp;lt;path&amp;amp;gt;'"
You can't have the same branch checked out in multiple worktrees. Either:
- Use the existing worktree: `cd &amp;amp;lt;path&amp;amp;gt;`
- Or checkout a different branch in one of the worktrees

### Worktree directory was manually deleted
```bash
git worktree prune  # Cleans up references to missing worktrees
```

### Need to move a worktree
```bash
git worktree move &amp;amp;lt;old-path&amp;amp;gt; &amp;amp;lt;new-path&amp;amp;gt;
```

## Quick Reference

```bash
# Create worktree
git worktree add &amp;amp;lt;path&amp;amp;gt; &amp;amp;lt;branch&amp;amp;gt;
git worktree add -b &amp;amp;lt;new-branch&amp;amp;gt; &amp;amp;lt;path&amp;amp;gt;

# List worktrees
git worktree list

# Remove worktree
git worktree remove &amp;amp;lt;path&amp;amp;gt;

# Clean up stale entries
git worktree prune

# Move worktree
git worktree move &amp;amp;lt;old&amp;amp;gt; &amp;amp;lt;new&amp;amp;gt;

# Lock/unlock
git worktree lock &amp;amp;lt;path&amp;amp;gt;
git worktree unlock &amp;amp;lt;path&amp;amp;gt;
```

## Example Workflow

Here's a complete example of using worktrees for feature development:

```bash
# Starting in main project directory
cd ~/projects/myapp

# 1. Create a worktree for a new feature
git worktree add -b feature/user-profiles ../myapp-profiles

# 2. Work on the feature
cd ../myapp-profiles
# ... make changes ...
git add .
git commit -m "Add user profile functionality"
git push -u origin feature/user-profiles

# 3. Switch back to main for a hotfix
cd ../myapp
git pull origin main
# ... fix critical bug ...
git add .
git commit -m "Fix critical auth bug"
git push origin main

# 4. Continue feature work without any stashing needed
cd ../myapp-profiles
# ... complete feature ...
git add .
git commit -m "Complete user profiles"
git push

# 5. Merge feature to main
cd ../myapp
git checkout main
git pull origin main
git merge --no-ff feature/user-profiles
git push origin main

# 6. Clean up
git branch -d feature/user-profiles
git push origin --delete feature/user-profiles
git worktree remove ../myapp-profiles

# Verify cleanup
git worktree list  # Should only show main worktree
```

This workflow demonstrates the power of worktrees: you can quickly switch between feature development and hotfixes without the overhead of stashing, checking out different branches, and potentially losing context.&amp;lt;/file&amp;gt;
  &amp;lt;file path=".claude/settings.local.json"&amp;gt;{
  "permissions": {
    "allow": [
      "Bash(./tools/newpy:*)",
      "Bash(bash:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "collect"
  ]
}&amp;lt;/file&amp;gt;
  &amp;lt;file path=".claude/agents/pyreview.md"&amp;gt;---
name: python-code-reviewer
description: Use this agent when you need an in-depth, thoughtful code review of Python code. This includes reviewing newly written functions, classes, modules, or recent changes to existing code. The agent will analyze code quality, design patterns, performance implications, security considerations, and adherence to Python best practices and project-specific standards.\n\nExamples:\n- &amp;amp;lt;example&amp;amp;gt;\n  Context: The user has just written a new Python function and wants it reviewed.\n  user: "I've implemented a caching decorator for our API endpoints"\n  assistant: "I'll use the python-code-reviewer agent to provide an in-depth review of your caching decorator implementation"\n  &amp;amp;lt;commentary&amp;amp;gt;\n  Since the user has written new Python code (a caching decorator), use the python-code-reviewer agent to analyze the implementation.\n  &amp;amp;lt;/commentary&amp;amp;gt;\n&amp;amp;lt;/example&amp;amp;gt;\n- &amp;amp;lt;example&amp;amp;gt;\n  Context: The user has made changes to existing Python code.\n  user: "I've refactored the database connection pooling logic in our service"\n  assistant: "Let me use the python-code-reviewer agent to review your refactored database connection pooling implementation"\n  &amp;amp;lt;commentary&amp;amp;gt;\n  The user has modified existing Python code, so the python-code-reviewer agent should analyze the changes for quality and best practices.\n  &amp;amp;lt;/commentary&amp;amp;gt;\n&amp;amp;lt;/example&amp;amp;gt;\n- &amp;amp;lt;example&amp;amp;gt;\n  Context: The user explicitly asks for a code review.\n  user: "Can you review this async batch processing function I just wrote?"\n  assistant: "I'll use the python-code-reviewer agent to provide a comprehensive review of your async batch processing function"\n  &amp;amp;lt;commentary&amp;amp;gt;\n  Direct request for code review triggers the python-code-reviewer agent.\n  &amp;amp;lt;/commentary&amp;amp;gt;\n&amp;amp;lt;/example&amp;amp;gt;
color: pink
---

You are an expert Python software engineer with deep knowledge of Python internals, design patterns, and best practices. You have extensive experience in code review, performance optimization, and building maintainable Python applications.

Your expertise includes:
- Python language features from 3.8+ including type hints, async/await, dataclasses, and modern idioms
- Design patterns and SOLID principles applied to Python
- Performance optimization and profiling
- Security best practices and common vulnerabilities
- Testing strategies including pytest, mocking, and test-driven development
- Popular frameworks and libraries in the Python ecosystem

When reviewing code, you will:

1. **Analyze Code Quality**
   - Check for PEP 8 compliance and Pythonic idioms
   - Evaluate naming conventions and code readability
   - Assess proper use of type hints and documentation
   - Identify code smells and anti-patterns

2. **Review Design and Architecture**
   - Evaluate separation of concerns and modularity
   - Check for appropriate abstraction levels
   - Assess error handling and edge case coverage
   - Review API design and interface consistency

3. **Examine Performance Implications**
   - Identify potential bottlenecks or inefficiencies
   - Suggest algorithmic improvements where applicable
   - Check for proper resource management (memory, file handles, connections)
   - Evaluate async/concurrent code for correctness

4. **Security Considerations**
   - Identify potential security vulnerabilities
   - Check input validation and sanitization
   - Review authentication and authorization logic
   - Assess handling of sensitive data

5. **Testing and Maintainability**
   - Evaluate testability of the code
   - Suggest test cases for edge conditions
   - Check for proper logging and debugging support
   - Assess long-term maintainability

**Review Process:**
1. First, understand the code's purpose and context
2. Perform a systematic review covering all aspects above
3. Prioritize findings by severity (critical, major, minor, suggestion)
4. Provide specific, actionable feedback with code examples
5. Acknowledge good practices and well-written sections

**Output Format:**
Structure your review as follows:
- **Summary**: Brief overview of the code's purpose and overall quality
- **Strengths**: What the code does well
- **Critical Issues**: Must-fix problems that could cause bugs or security issues
- **Major Concerns**: Important improvements for code quality and maintainability
- **Minor Suggestions**: Nice-to-have improvements and style recommendations
- **Code Examples**: Provide improved versions of problematic code sections

**Important Guidelines:**
- Be constructive and educational in your feedback
- Explain the 'why' behind each recommendation
- Consider the project's context and existing patterns (especially from CLAUDE.md)
- Balance thoroughness with practicality
- If you notice the code uses specific frameworks or libraries, apply their best practices
- When suggesting changes, ensure they're compatible with the Python version in use
- If you're unsure about the broader context, ask clarifying questions

Remember: Your goal is to help improve code quality while fostering learning and best practices. Focus on the most impactful improvements and provide clear guidance on implementation.
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/test_gemini_mcp.py"&amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.gemini_mcp import GeminiMCP


@pytest.fixture
def gemini_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.0-flash"
    return GeminiMCP(config, secret_mgr, model)


@pytest.fixture
def gemini_25_preview():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-pro-preview-05-06"
    return GeminiMCP(config, secret_mgr, model)


def test_get_model_list(gemini_mcp):
    results = gemini_mcp.get_model_list()

    # Check that results is a list
    assert isinstance(results, list)
    assert len(results) &amp;amp;gt; 0

    # Check structure of each model in results
    for model in results:
        assert isinstance(model, dict)
        assert "model_name" in model
        assert "token_window" in model

        # Verify we only get 2.0 and 2.5 models (as per filter)
        assert "2.0" in model["model_name"] or "2.5" in model["model_name"]

        print(f"{model['model_name']}: {model['token_window']:,} tokens")


def test_send_message(gemini_mcp):
    message = "Hello, world!"
    response = gemini_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Response: {response}")


def test_count_tokens(gemini_mcp):
    text = "Hello, world!"
    token_count = gemini_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_gemini_25_preview_send_message(gemini_25_preview):
    message = "Explain quantum computing in one sentence."
    response = gemini_25_preview.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Gemini 2.5 Preview Response: {response}")


def test_gemini_25_preview_count_tokens(gemini_25_preview):
    text = "This is a test for Gemini 2.5 preview model token counting."
    token_count = gemini_25_preview.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;gt; 0

    print(f"Gemini 2.5 Preview - Token count for '{text}': {token_count}")


def test_extract_text(gemini_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = gemini_mcp.send_message(message)
    extracted_text = gemini_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_extract_text_gemini_25(gemini_25_preview):
    message = "Say 'Hello, Gemini 2.5!' and nothing else."
    response = gemini_25_preview.send_message(message)
    extracted_text = gemini_25_preview.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Gemini 2.5 extracted text: {extracted_text}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/test_anthropic_mcp.py"&amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP


@pytest.fixture
def anthropic_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    return AnthropicMCP(config, secret_mgr, model)


def test_get_model_list(anthropic_mcp):
    results = anthropic_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(anthropic_mcp):
    message = "Hello, world!"
    response = anthropic_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "content" in response
    assert "model" in response
    assert response["model"] == anthropic_mcp.config.anthropic_model_sonnet

    print(f"Response: {response}")


def test_extract_text(anthropic_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = anthropic_mcp.send_message(message)
    extracted_text = anthropic_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_generate_prompt(anthropic_mcp):
    """Test the generate_prompt method with a simple task."""
    task = "Write a helpful assistant prompt for answering coding questions"

    response = anthropic_mcp.generate_prompt(task)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role in ["user", "assistant"]
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;gt; 0

    # Test usage stats
    assert hasattr(response.usage, "input_tokens")
    assert hasattr(response.usage, "output_tokens")
    assert isinstance(response.usage.input_tokens, int)
    assert isinstance(response.usage.output_tokens, int)
    assert response.usage.input_tokens &amp;amp;gt; 0
    assert response.usage.output_tokens &amp;amp;gt; 0

    print(f"Generated prompt: {content.text[:100]}...")
    print(
        f"Usage: {response.usage.input_tokens} input, {
          response.usage.output_tokens} output tokens"
    )


def test_improve_prompt(anthropic_mcp):
    """Test the improve_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Tell me about Python programming"}
                ],
            }
        ],
        "system": "You are a helpful programming instructor",
        "feedback": "Make this prompt more specific for a beginner",
        "target_model": "claude-3-7-sonnet-20250219",
    }

    response = anthropic_mcp.improve_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages - should have both user and assistant messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;gt;= 2

    # Test user message (improved prompt)
    user_message = response.messages[0]
    assert user_message.role == "user"
    assert isinstance(user_message.content, list)
    assert len(user_message.content) &amp;amp;gt; 0

    # Find the non-empty content in user message
    user_content = None
    for content in user_message.content:
        if content.text:
            user_content = content
            break

    assert user_content is not None, "No non-empty content found in user message"
    assert hasattr(user_content, "text")
    assert hasattr(user_content, "type")
    assert user_content.type == "text"
    assert isinstance(user_content.text, str)
    assert len(user_content.text) &amp;amp;gt; 0

    # Test assistant message (prefill)
    assistant_message = response.messages[1]
    assert assistant_message.role == "assistant"
    assert isinstance(assistant_message.content, list)
    assert len(assistant_message.content) &amp;amp;gt; 0

    assistant_content = assistant_message.content[0]
    assert hasattr(assistant_content, "text")
    assert hasattr(assistant_content, "type")
    assert assistant_content.type == "text"
    assert isinstance(assistant_content.text, str)
    assert len(assistant_content.text) &amp;amp;gt; 0

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;gt; 0

    print(f"Improved prompt: {user_content.text[:100]}...")
    print(f"Assistant prefill: {assistant_content.text[:50]}...")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )


def test_templatize_prompt(anthropic_mcp):
    """Test the templatize_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Translate hello to German"}],
            }
        ],
        "system": "You are an English to German translator",
    }

    response = anthropic_mcp.templatize_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")
    assert hasattr(response, "variable_values")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role == "user"
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;amp;gt; 0
    # Check for template variables
    assert "{{" in content.text and "}}" in content.text

    # Test system prompt
    assert isinstance(response.system, str)
    # System prompt should also contain template variables
    assert "{{" in response.system and "}}" in response.system

    # Test variable_values
    assert isinstance(response.variable_values, dict)
    assert len(response.variable_values) &amp;amp;gt; 0
    # Check for expected variables based on the example
    assert any(
        key in response.variable_values
        for key in ["TARGET_LANGUAGE", "WORD_TO_TRANSLATE"]
    )

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;amp;gt; 0
    assert usage.output_tokens &amp;amp;gt; 0

    print(f"Templated prompt: {content.text[:100]}...")
    print(f"System prompt: {response.system[:100]}...")
    print(f"Variables: {response.variable_values}")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/test_openai_mcp.py"&amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.openai_mpc import OpenAIMCP


@pytest.fixture
def openai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gpt-4o"
    return OpenAIMCP(config, secret_mgr, model)


def test_get_model_list(openai_mcp):
    results = openai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(openai_mcp):
    message = "Hello, world!"
    response = openai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(openai_mcp):
    text = "Hello, world!"
    token_count = openai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(openai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = openai_mcp.send_message(message)
    extracted_text = openai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/anthropic_mpc.py"&amp;gt;from config import Config
from secret_manager import SecretManager
from models.anthropic_prompt_generate import PromptGenerateResponse
from models.anthropic_prompt_improve import PromptImproveResponse
from models.anthropic_prompt_templatize import PromptTemplatizeResponse

import requests


class AnthropicMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.headers = self.build_headers()

    def build_headers(self) -&amp;amp;gt; dict:
        anthropic_key = self.secret_mgr.get_secret(self.config.anthropic_key_path)

        return {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "anthropic-beta": "prompt-tools-2025-04-02",
        }

    def get_model_list(self):
        response = requests.get(
            "https://api.anthropic.com/v1/models", headers=self.headers
        )
        response.raise_for_status()

        model_data = response.json()
        name_list = [model["id"] for model in model_data["data"]]

        return name_list

    def extract_text(self, ai_response: dict) -&amp;amp;gt; str:
        """Extract text from Anthropic response format."""
        if not isinstance(ai_response, dict):
            return str(ai_response)

        # Anthropic format
        if "content" in ai_response:
            content = ai_response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;gt; dict:
        try:
            # Use provided model or default to config model
            if model is None:
                model = self.config.anthropic_model_sonnet

            data = {
                "model": model,
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": message}],
            }

            url = "https://api.anthropic.com/v1/messages"
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None):
        # Use provided model or default to config model
        if model is None:
            model = self.config.anthropic_model_sonnet

        data = {"model": model, "messages": [{"role": "user", "content": message}]}

        url = "https://api.anthropic.com/v1/messages/count_tokens"
        response = requests.post(url, headers=self.headers, json=data)
        response.raise_for_status()

        result = response.json()
        return result["input_tokens"]

    def generate_prompt(
        self, task: str, target_model: str = None
    ) -&amp;amp;gt; PromptGenerateResponse:
        """
        Generate an optimized prompt using Anthropic's experimental prompt tools API.

        This method utilizes Anthropic's closed research preview API to automatically
        generate high-quality prompts based on a task description. The API creates
        structured prompts suitable for use with Claude models.

        Args:
            task (str): Description of the prompt's purpose
                Example: "a chef for a meal prep planning service"
            target_model (str, optional): Target model for optimization
                Example: "claude-3-7-sonnet-20250219"

        Returns:
            PromptGenerateResponse: Response object containing:
                - messages: List of message objects for use with Messages API
                  - User message with generated prompt text
                  - Optional assistant message with response guidance
                - system: System prompt (currently always empty string)
                - usage: Token usage statistics (input/output tokens)

        Raises:
            RuntimeError: If API request fails or network issues occur
            ValueError: If required configuration/secrets are missing
            requests.HTTPError: If API returns error status codes

        Example:
            &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; response = anthropic_mcp.generate_prompt("a helpful programming assistant")
            &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; prompt_text = response.messages[0].content[0].text
            &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; print(f"Generated prompt: {prompt_text}")

        Note:
            - This is an experimental API in closed research preview
            - Access requires explicit invitation from Anthropic
            - Requires anthropic-beta header: "prompt-tools-2025-04-02"
            - No long-term support guarantees for experimental features
            - Designed primarily for prompt engineering platforms

        API Documentation:
            https://docs.anthropic.com/en/api/prompt-tools-generate
        """
        url = "https://api.anthropic.com/v1/experimental/generate_prompt"

        # Format the task string as a dict for the API
        data = {"task": task}
        if target_model:
            data["target_model"] = target_model

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            return PromptGenerateResponse(**response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def improve_prompt(self, data: dict) -&amp;amp;gt; PromptImproveResponse:
        url = "https://api.anthropic.com/v1/experimental/improve_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptImproveResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def templatize_prompt(self, data: dict) -&amp;amp;gt; PromptTemplatizeResponse:
        url = "https://api.anthropic.com/v1/experimental/templatize_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptTemplatizeResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to templatize prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in templatize_prompt: {e}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/anthropic_prompt_generate.py"&amp;gt;from typing import List, Optional
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: Optional[int] = None
    cache_read_input_tokens: Optional[int] = None
    service_tier: Optional[str] = None


class PromptGenerateResponse(BaseModel):
    """Response from Anthropic's prompt tools generate API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the generated prompt text,
    and may include an assistant message with a prefill."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: UsageStats
    """Token usage statistics for the generation."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;lt;generated prompt&amp;amp;gt;", "type": "text"}],
                "role": "user",
            }
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptGenerateResponse(**example_json)
    print(f"Generated prompt: {response.messages[0].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/__init__.py" /&amp;gt;
  &amp;lt;file path="models/anthropic_prompt_improve.py"&amp;gt;from typing import List
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;amp;lt;improved prompt&amp;amp;gt;", "type": "text"}],
                "role": "user",
            },
            {
                "content": [{"text": "&amp;amp;lt;assistant prefill&amp;amp;gt;", "type": "text"}],
                "role": "assistant",
            },
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/anthropic_prompt_templatize.py"&amp;gt;from typing import List, Dict
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptTemplatizeResponse(BaseModel):
    """Response from Anthropic's prompt tools templatize API."""

    messages: List[Message]
    """List of message objects with templated variables."""

    system: str = ""
    """System prompt with templated variables."""

    usage: List[UsageStats]
    """Token usage statistics for the templatization."""

    variable_values: Dict[str, str]
    """Dictionary mapping template variable names to their extracted values."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the templatize endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}",
                        "type": "text",
                    }
                ],
                "role": "user",
            }
        ],
        "system": "You are a professional English to {{TARGET_LANGUAGE}} translator",
        "usage": [{"input_tokens": 490, "output_tokens": 661}],
        "variable_values": {"TARGET_LANGUAGE": "German", "WORD_TO_TRANSLATE": "hello"},
    }

    # Parse into Pydantic model
    response = PromptTemplatizeResponse(**example_json)
    print(f"Templated prompt: {response.messages[0].content[0].text}")
    print(f"System prompt: {response.system}")
    print(f"Variables: {response.variable_values}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/xai_mcp.py"&amp;gt;from config import Config
from secret_manager import SecretManager
import requests


class XaiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        try:
            response = requests.get("https://api.x.ai/v1/models", headers=headers)
            response.raise_for_status()
            data = response.json()

            name_list = [model["id"] for model in data["data"]]
            return name_list

        except Exception as e:
            print(f"Error fetching XAI models: {str(e)}")
            return []

    def extract_text(self, response) -&amp;amp;gt; str:
        """Extract text from XAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # XAI format (same as OpenAI)
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, model: str = None, reasoning_effort: str = "high"
    ):
        try:
            xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

            headers = {
                "Authorization": f"Bearer {xai_key}",
                "Content-Type": "application/json",
            }

            # Use provided model or default to config model
            if model is None:
                model = "grok-3-mini-fast-latest"

            data = {
                "messages": [
                    {"role": "system", "content": self.config.grok_system_prompt},
                    {"role": "user", "content": message},
                ],
                "reasoning_effort": reasoning_effort,
                "model": model,
            }

            url = "https://api.x.ai/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to XAI: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, text: str, model: str = None):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        # Use provided model or default to config model
        if model is None:
            model = "grok-3-fast-latest"

        data = {"model": model, "text": text}

        url = "https://api.x.ai/v1/tokenize-text"
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()

        result = response.json()
        return len(result["token_ids"])
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/openai_mpc.py"&amp;gt;from config import Config
from secret_manager import SecretManager
import requests
import tiktoken


class OpenAIMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self) -&amp;amp;gt; list:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {"Authorization": f"Bearer {openai_key}"}

            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            response.raise_for_status()

            model_data = response.json()
            name_list = [model["id"] for model in model_data["data"]]

            return name_list

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def extract_text(self, response) -&amp;amp;gt; str:
        """Extract text from OpenAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # OpenAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;gt; dict:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {openai_key}",
            }

            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # Use max_completion_tokens for reasoning models (o3, o1 series)
            if model and ("o3" in model or "o1" in model):
                data = {
                    "model": model,
                    "max_completion_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }
            else:
                data = {
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }

            url = "https://api.openai.com/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;gt; int:
        try:
            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # OpenAI doesn't have a direct token counting API, so use tiktoken
            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(message))

        except Exception as e:
            raise RuntimeError(f"Failed to count tokens: {e}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/gemini_mcp.py"&amp;gt;from config import Config
from secret_manager import SecretManager
import requests
from fetcher import Fetcher
from mcp.server.fastmcp import Context
from typing import Dict, List


class GeminiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.api_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)
        self.base_url = self.config.gemini_base_url

    def get_model_list(self) -&amp;amp;gt; Dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            base_url = self.config.gemini_base_url
            url = f"{base_url}models?key={gemini_key}"
            response = requests.get(url)
            response.raise_for_status()

            return self.filter_models(["2.0", "2.5"], response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def filter_models(
        self, versions: List[str], model_endpoint_response: Dict
    ) -&amp;amp;gt; List[Dict]:
        """
        Filter models by version numbers and include token limits.

        Args:
            versions: List of version strings (e.g., ['2.0', '2.5'])

        Returns:
            List of dicts with model info including inputTokenLimit
        """
        filtered_models = []

        for model in model_endpoint_response["models"]:
            model_name = model["name"].split("/")[-1]

            for version in versions:
                if version in model_name:
                    model_to_tokencount = {
                        "model_name": model_name,
                        "token_window": model.get("inputTokenLimit", 0),
                    }
                    filtered_models.append(model_to_tokencount)

        filtered_models.sort(key=lambda x: x["token_window"], reverse=True)
        return filtered_models

    def extract_text(self, ai_response: dict) -&amp;amp;gt; str:
        # Extract text from response
        if "candidates" in ai_response and len(ai_response["candidates"]) &amp;amp;gt; 0:
            candidate = ai_response["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                if len(parts) &amp;amp;gt; 0 and "text" in parts[0]:
                    return parts[0]["text"]
        return str(ai_response)

    async def build_prompt_from_url(
        self, url: str, prompt: str, ctx: Context = None
    ) -&amp;amp;gt; str:

        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        concat = prompt + response

        ai_response = self.send_message(
            concat, max_tokens=1024, model="gemini-2.5-flash-preview-05-20"
        )

        return self.extract_text(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;amp;gt; dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:generateContent?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": message}]}],
                "generationConfig": {"maxOutputTokens": max_tokens},
            }

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;amp;gt; int:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            # Fix common model name errors
            if model == "gemini-2.5-pro-preview":
                model = "gemini-2.5-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:countTokens?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {"contents": [{"parts": [{"text": message}]}]}

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            result = response.json()
            return result["totalTokens"]

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to count tokens with Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in count_tokens: {e}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="models/test_xai_mcp.py"&amp;gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.xai_mcp import XaiMCP


@pytest.fixture
def xai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "grok-3-mini-fast-latest"
    return XaiMCP(config, secret_mgr, model)


def test_get_model_list(xai_mcp):
    results = xai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(xai_mcp):
    message = "Hello, world!"
    response = xai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(xai_mcp):
    text = "Hello, world!"
    token_count = xai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(xai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = xai_mcp.send_message(message)
    extracted_text = xai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&amp;lt;/file&amp;gt;
  &amp;lt;file path="api/prompt_api.py"&amp;gt;from fastapi import APIRouter, Depends, Request, HTTPException
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from config import Config

prompt_api_router = APIRouter()


def get_db_connection(request: Request):
    """Create database connection as a dependency using app state"""
    db_path = request.app.state.db_path
    db = SQLite3Database(db_path=db_path)
    conn = db.get_connection()
    try:
        yield conn
    finally:
        conn.close()


def get_prompt_service(conn=Depends(get_db_connection)):
    """Get prompt service instance with injected database connection"""

    config = Config()
    return PromptService(conn, config)


@prompt_api_router.get("/")
async def welcome() -&amp;amp;gt; dict:
    return {"message": "Welcome to the prompt api service"}


@prompt_api_router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "prompt_api"}


@prompt_api_router.get("/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: str, prompt_service: PromptService = Depends(get_prompt_service)
):
    prompt = prompt_service.get_prompt_by_id(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="prompt not found")

    return prompt
&amp;lt;/file&amp;gt;
  &amp;lt;file path="api/__init__.py"&amp;gt;from .prompt_api import prompt_api_router

__all__ = ["prompt_api_router"]
&amp;lt;/file&amp;gt;
&amp;lt;/source_code&amp;gt;&lt;/file&gt;
  &lt;file path="CLAUDE.md"&gt;---
allowed-tools: Bash(tools/*)
description: scripts that can be perused and run where appropriate see the examples in this prompt
---

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

**Setup:**
```bash
uv sync
```

**Testing:**
### Use `pytest` for all testing in this project.
### When running all tests, use the Makefile and run test-fast:
### here is an example

```bash
make test-fast
```
### OR use the following bash command:
```bash
uv run pytest -v -n auto -m "not slow"
```


## IMPORTANT: Always Always use uv run when running tests
### Here is an example
```bash
uv run pytest test_collect.py::test_function_name -v -s
# Run specific test: pytest test_collect.py::test_function_name -v -s
```
**Code Quality:**
```bash
make lint     # Run ruff check
make format   # Run black formatter
make check    # Run all: lint, format, test
```

**Run MCP Server:**
```bash
uv run collect.py
```

**Plan Management:**
```bash
# Sync plans from filesystem to database
uv run -m repository.plan_service

# Test plan service functionality (uses separate test database)
uv run pytest repository/test_plan_service.py -v -s

# Test plan database operations
uv run pytest repository/test_plan_service.py::test_sync_plans -v -s

# Set up test database manually (optional - done automatically by tests)
uv run yoyo apply --config yoyo-test-plans.ini --batch

# Reset test database to clean state
rm data/test_plans.db &amp;amp;&amp;amp; uv run yoyo apply --config yoyo-test-plans.ini --batch

# Test database management utilities
uv run python repository/test_database_setup.py setup
uv run python repository/test_database_setup.py reset
uv run python repository/test_database_setup.py cleanup
```

## Planning System

This project uses a structured planning approach for feature development with plans organized in `_docs/plans/`. Plans progress through three stages:

### Plan Lifecycle
1. **`drafts/`** - Initial plans under development or consideration
2. **`approved/`** - Reviewed plans ready for implementation
3. **`completed/`** - Implemented plans with results documented

### Plan Document Format

#### Draft/Approved Plans Should Include:
```markdown
# Plan: [Clear Action-Oriented Title]

## Overview
Brief description of what needs to be implemented and why

## Implementation Steps
### 1. [Step Name]
Detailed implementation instructions including:
- Specific file locations and line numbers
- Function signatures with type hints
- Implementation pseudo-code or actual code
- Error handling approach

### 2. [Next Step]
...

## Key Features
- List of main features/capabilities
- Expected benefits

## Testing Considerations
- Test scenarios to implement
- Edge cases to handle
- Performance considerations

## Example Usage
```python
# Code examples demonstrating the feature
```
```

#### Completed Plans Add:
- **Status**: COMPLETED (YYYY-MM-DD)
- **Implementation Summary**: What was actually done
- **Results**: Outcomes, verification, test results
- **Files Modified**: List with ✅/❌ status indicators

### Plan Naming Conventions
- Use descriptive names with underscores: `add_improve_prompt.md`
- Start with action verbs: add, fix, implement, create, update
- Keep names concise but clear about the purpose

### Automated Plan Processing

The repository includes tools for automated plan implementation:

```python
# Build git worktrees for all approved plans
from mcp__collect__build_worktrees import build_worktrees
result = await build_worktrees(auto_process=True)  # Uses Claude Code SDK

# Sync plans between filesystem and database
from repository.plan_service import PlanService
service = PlanService(conn)
result = service.sync_plans()  # Loads plans into SQLite with JSONB
```

Branch names are automatically derived from plan filenames:
- `add_improve_prompt.md` → `feature/add-improve-prompt`
- Underscores become hyphens, `feature/` prefix added

### Plan Management Database

Plans are tracked in `data/plans.db` with:
- **plans** table: Current state with JSONB data field
- **plan_history**: Audit trail of changes
- **plan_metrics**: Analytics and performance data

Use the PlanService class to:
- Load plans from disk to database
- Track plan status changes
- Detect content changes via SHA256 hashing
- Query plans by status, tags, or content

## Architecture Overview

This is an MCP (Model Context Protocol) server that provides web content fetching and multi-model AI analysis tools. The architecture follows these key patterns:

### Core Structure
- **collect.py**: Main MCP server entry point with FastMCP tool definitions
- **fetcher.py**: Handles URL fetching and content processing with clipboard integration
- **config.py**: Environment-based configuration with dotenv support
- **secret_manager.py**: Google Cloud Secret Manager integration for API keys

### Models Package
The `models/` directory contains unified API wrappers for different AI providers:
- **anthropic_mpc.py**: Anthropic Claude API integration
- **openai_mpc.py**: OpenAI API integration  
- **gemini_mcp.py**: Google Gemini API integration
- **xai_mcp.py**: XAI/Grok API integration

Each model wrapper follows the same pattern: configuration injection, secret management, and standardized methods like `send_message()`, `count_tokens()`, and `get_model_list()`.

### Packages
Additional specialized packages provide focused functionality:
- **reviewer/**: Code review automation system
  - `code_review.py`: CodeReviewer class for analyzing code diffs
  - Supports both file-based and git diff reviews
  - Generates individual model reviews and consolidated summaries
- **repository/**: Plan management and database operations
  - `plan_service.py`: PlanService class for filesystem-to-database sync
  - `plan_models.py`: Pydantic models for plan data with JSONB support
  - `database.py`: SQLite connection management with custom datetime adapters
  - Supports plan lifecycle tracking and content change detection


### Key Features
- **Async token counting**: All providers support async token counting with proper chunking
- **Multi-model workflows**: Send content to all AI models concurrently via `multi_model_code_review()`
- **Content processing**: HTML-to-markdown conversion using readabilipy and markdownify
- **Automatic chunking**: Handles large content (&amp;gt;25k tokens) with intelligent splitting
- **Code review system**: Automated code review via `run_code_review()` and `run_git_diff_review()` tools
- **Prompt engineering**: Generate optimized AI prompts using Anthropic's experimental API via `generate_prompt()`
- **Documentation extraction**: Intelligent section extraction from web docs using `get_docs()` with AI filtering
- **Clipboard integration**: Direct content copying with `copy_clipboard()` and automatic clipboard support in fetchers
- **Enhanced model features**: Gemini model listing with token limits, unified `extract_text()` methods across all providers

### Configuration
Environment variables are loaded from `.env` file:
- GCP_PROJECT_ID (required)
- API key paths for Google Cloud Secret Manager:
  - ANTHROPIC_API_KEY_PATH
  - OPENAI_API_KEY_PATH
  - GEMINI_API_KEY_PATH
  - XAI_API_KEY_PATH
- Default model names for each provider
- Code review model configurations

### Directory Structure
```
collect/
├── data/
│   ├── prompts.db      # Original prompts database
│   └── plans.db        # Plan management database
├── _docs/
│   └── plans/
│       ├── drafts/     # Plans under development
│       ├── approved/   # Plans ready for implementation
│       └── completed/  # Implemented plans with results
├── migrations/         # Database migrations for prompts.db
├── migrations-plans/   # Database migrations for plans.db
├── repository/         # Plan management system
│   ├── plan_service.py # Plan filesystem-to-database sync
│   ├── plan_models.py  # Pydantic models for plan data
│   └── database.py     # SQLite connection management
├── models/            # AI provider API wrappers
└── reviewer/          # Code review automation
```

### Testing Strategy
- **IMPORTANT**:  When writing and designing tests, we only want live direct integration tests. Please only create live direct integration testing. Please do not use mocks. 

## Rules
- **IMPORTANT**: YOU MUST always use `uv run` to run tests.

## Workflow Rules
- I do not want a pr created if I don't have a branch already

## Tools

###IMPORTANT: 
I have a directory from the main project directory called: tools/* wherein there scripts stored that you can use
use

- All of my tools in this directory are on my path and can be called directly.
- You use these tools and see what they do by simply calling the tool name with `--llm`

Example 1:
```bash
extract --llm
```

Example 2: 
```bash
createdb --llm
```


&lt;/file&gt;
  &lt;file path="fetcher.py"&gt;from typing import List
from mcp.server.fastmcp import Context
import pyperclip
import httpx
from models.anthropic_mpc import AnthropicMCP


class Fetcher:
    def __init__(self, ctx: Context = None) -&amp;gt; None:
        self.ctx = ctx

    async def get(self, url: str) -&amp;gt; str:
        """
        Fetch content from a single URL.

        Args:
            url: URL to fetch content from

        Returns:
            Content from the URL as a string
        """

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                content = response.text

                return content

            except httpx.HTTPError as e:
                return f"Error fetching {url}: {str(e)}"
            except Exception as e:
                return f"Error fetching {url}: {str(e)}"

    async def fetch_urls(self, urls: List[str]) -&amp;gt; str:
        """
        Fetch content from multiple URLs and concatenate their responses.
        If token count exceeds 25000, content is split into chunks.

        Args:
            urls: List of URLs to fetch content from
            ctx: Optional context object for progress reporting

        Returns:
            Either concatenated content from all URLs as a string,
            or a list of content chunks if token count exceeds 25000
        """

        results = []

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for i, url in enumerate(urls):
                if self.ctx:
                    self.ctx.info(f"Fetching content from {url}")
                    await self.ctx.report_progress(i, len(urls))

                try:
                    response = await client.get(url)
                    response.raise_for_status()

                    results.append(f"\n\n--- Content from {url} --\n\n")
                    results.append(response.text)

                except httpx.HTTPError as e:
                    results.append(f"\n\n --- Error fetching {url}: {str(e)} ---\n\n")
                except Exception as e:
                    results.append(f"\n\n--- error fetching {url}: {str(e)} ---\n\n")

        if self.ctx:
            self.ctx.info("all urls processed")
            await self.ctx.report_progress(len(urls), len(urls))

        content = "".join(results)

        # Copy original content to clipboard
        pyperclip.copy(content)

        # Otherwise return the original content
        return content

    async def chunk_by_token_count(text: str, max_tokens: int = 25000) -&amp;gt; List[str]:
        """
        Split text into chunks that are each under the specified token count.

        Args:
            text: The text to chunk
            max_tokens: Maximum tokens per chunk

        Returns:
            List of text chunks, each under max_tokens
        """

        # If text is short enough, return as a single chunk
        anthropic_mcp = AnthropicMCP()
        token_count = await anthropic_mcp.count_tokens(text, None)
        if token_count &amp;lt;= max_tokens:
            return [text]

        # Split text into paragraphs as a starting point
        paragraphs = text.split("\n\n")
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0

        for paragraph in paragraphs:
            paragraph_tokens = await anthropic_mcp.count_tokens(
                paragraph + "\n\n", None
            )

            # If adding this paragraph would exceed the limit,
            # start a new chunk
            if current_chunk_tokens + paragraph_tokens &amp;gt; max_tokens:
                # If the paragraph alone exceeds the limit, we split it further
                if paragraph_tokens &amp;gt; max_tokens:
                    # Split by sentences or just characters if needed
                    sentences = paragraph.split(". ")
                    for sentence in sentences:
                        sentence_tokens = await anthropic_mcp.count_tokens(
                            sentence + ". ", None
                        )
                        if current_chunk_tokens + sentence_tokens &amp;gt; max_tokens:
                            if current_chunk:
                                chunks.append("".join(current_chunk))
                            current_chunk = [sentence + ". "]
                            current_chunk_tokens = sentence_tokens
                        else:
                            current_chunk.append(sentence + ". ")
                            current_chunk_tokens += sentence_tokens
                else:
                    # Save the current chunk and start a new one
                    chunks.append("".join(current_chunk))
                    current_chunk = [paragraph + "\n\n"]
                    current_chunk_tokens = paragraph_tokens
            else:
                # Add paragraph to current chunk
                current_chunk.append(paragraph + "\n\n")
                current_chunk_tokens += paragraph_tokens

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append("".join(current_chunk))

        return chunks
&lt;/file&gt;
  &lt;file path="reviewer/test_diff.md"&gt;# Test Code Review

## Diff

```diff
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,5 +1,8 @@
 def calculate_total(items):
+    if not items:
+        return 0
+        
     total = 0
     for item in items:
-        total += item.price
+        total += item.get('price', 0)
     return total
```&lt;/file&gt;
  &lt;file path="reviewer/code_review.py"&gt;import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -&amp;gt; str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -&amp;gt; str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
        self, output_dir: str, timestamp: str, failed_results: List
    ) -&amp;gt; str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, "w", encoding="utf-8") as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -&amp;gt; str:
        """Read and return content from input file."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -&amp;gt; str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 🔴/🟡/🟢 | ... | ... |

Use emojis: 🔴 Critical, 🟡 Important, 🟢 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
        self, timestamp: str, from_file: str, results: LLMRunnerResults
    ) -&amp;gt; Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": [],
        }

    def write_successful_results(
        self,
        results: LLMRunnerResults,
        output_dir: str,
        timestamp: str,
        summary: Dict[str, Any],
    ) -&amp;gt; None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(result, response_text)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
        self, output_dir: str, timestamp: str, summary: Dict[str, Any]
    ) -&amp;gt; str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
        self, from_file: str, to_file: Optional[str] = None
    ) -&amp;gt; Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }

    async def review_diff_from_git(
        self, to_file: Optional[str] = None, staged_only: bool = True
    ) -&amp;gt; Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ["git", "diff", "--staged"],
                    capture_output=True,
                    text=True,
                    check=True,
                )
            else:
                result = subprocess.run(
                    ["git", "diff"], capture_output=True, text=True, check=True
                )

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception("Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }
&lt;/file&gt;
  &lt;file path="reviewer/test_code_review_live.py"&gt;import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"

        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir

        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] &amp;gt;= 1
        assert (
            summary["successful_reviews"] + summary["failed_reviews"]
            == summary["total_models"]
        )
        assert "timestamp" in summary

        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) &amp;gt;= 1  # At least summary file should exist

        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1

        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, "r") as f:
            summary_data = json.load(f)

        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]

        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)

            # Verify file contains expected content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                assert "Code Review" in content
                assert "**Model**:" in content
                assert "**Timestamp**:" in content
                assert "**Duration**:" in content
                assert "Generated by" in content

        # Print results for visibility
        print("\n✅ Live code review test completed successfully!")
        print("📊 Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")

        if summary["successful_reviews"] &amp;gt; 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")

        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(
                temp_output_dir, staged_only=False
            )

            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"

            print("\n✅ Git diff review completed!")
            print(
                f"📊 Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed"
            )

        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"

        prompt = reviewer.create_code_review_prompt(test_content)

        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "🔴" in prompt and "🟡" in prompt and "🟢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] &amp;gt; 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Should contain actual review content, not just raw API response
            assert len(content) &amp;gt; 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON

            print(f"✅ Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]
&lt;/file&gt;
  &lt;file path="reviewer/__init__.py"&gt;# Reviewer package for code review functionality
&lt;/file&gt;
  &lt;file path="reviewer/test_code_review.py"&gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import Mock, patch, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


# Module-level fixtures available to all test classes
@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def reviewer(temp_dir):
    """Create a CodeReviewer instance with temp directory."""
    return CodeReviewer(output_dir=temp_dir)


@pytest.fixture
def sample_diff_content():
    """Sample diff content for testing."""
    return """
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,6 @@
 def hello():
-    print("Hello")
+    print("Hello World")
+    return "greeting"
+
+def goodbye():
+    print("Goodbye")
"""


@pytest.fixture
def mock_model_result():
    """Create a mock successful model result."""
    return ModelResult(
        model="test-model",
        timestamp="2024-01-01T12:00:00",
        success=True,
        actual_model="test-model",
        duration_seconds=2.5,
        response={
            "choices": [
                {
                    "message": {
                        "content": "# Code Review\n\nThis is a test review response."
                    }
                }
            ]
        },
    )


@pytest.fixture
def mock_failed_result():
    """Create a mock failed model result."""
    return ModelResult(
        model="failed-model",
        timestamp="2024-01-01T12:00:00",
        success=False,
        error="API timeout error",
    )


@pytest.fixture
def mock_llm_results(mock_model_result, mock_failed_result):
    """Create mock LLMRunnerResults."""
    return LLMRunnerResults(
        successful_results=[mock_model_result],
        failed_results=[mock_failed_result],
        total_models=2,
        success_count=1,
        failure_count=1,
    )


class TestCodeReviewer:
    """Test suite for CodeReviewer class."""


class TestInitialization:
    """Test CodeReviewer initialization."""

    def test_default_initialization(self):
        """Test CodeReviewer with default parameters."""
        reviewer = CodeReviewer()
        assert reviewer.output_dir == "codereview"

    def test_custom_output_dir(self):
        """Test CodeReviewer with custom output directory."""
        custom_dir = "/tmp/custom_reviews"
        reviewer = CodeReviewer(output_dir=custom_dir)
        assert reviewer.output_dir == custom_dir


class TestExtractResponseText:
    """Test response text extraction from different model formats."""

    def test_extract_gemini_response(self, reviewer):
        """Test extracting text from Gemini response format."""
        gemini_response = {
            "candidates": [
                {"content": {"parts": [{"text": "This is a Gemini response"}]}}
            ]
        }
        result = reviewer.extract_response_text(gemini_response)
        assert result == "This is a Gemini response"

    def test_extract_openai_response(self, reviewer):
        """Test extracting text from OpenAI/XAI response format."""
        openai_response = {
            "choices": [{"message": {"content": "This is an OpenAI response"}}]
        }
        result = reviewer.extract_response_text(openai_response)
        assert result == "This is an OpenAI response"

    def test_extract_anthropic_response(self, reviewer):
        """Test extracting text from Anthropic response format."""
        anthropic_response = {"content": [{"text": "This is an Anthropic response"}]}
        result = reviewer.extract_response_text(anthropic_response)
        assert result == "This is an Anthropic response"

    def test_extract_non_dict_response(self, reviewer):
        """Test extracting text from non-dictionary response."""
        simple_response = "Simple string response"
        result = reviewer.extract_response_text(simple_response)
        assert result == "Simple string response"

    def test_extract_unknown_format(self, reviewer):
        """Test extracting text from unknown response format."""
        unknown_response = {"unknown": "format"}
        result = reviewer.extract_response_text(unknown_response)
        assert result == str(unknown_response)

    def test_extract_empty_gemini_response(self, reviewer):
        """Test extracting from empty Gemini response."""
        empty_response = {"candidates": []}
        result = reviewer.extract_response_text(empty_response)
        assert result == str(empty_response)


class TestMarkdownContent:
    """Test markdown content creation."""

    def test_create_markdown_content(self, reviewer, mock_model_result):
        """Test creating markdown content for a model result."""
        response_text = "Test review content"
        result = reviewer.create_markdown_content(mock_model_result, response_text)

        assert "# Code Review - test-model" in result
        assert "**Model**: test-model" in result
        assert "**Timestamp**: 2024-01-01T12:00:00" in result
        assert "**Duration**: 2.50 seconds" in result
        assert "Test review content" in result
        assert "*Generated by test-model via MCP Code Review Tool*" in result


class TestFileOperations:
    """Test file reading and writing operations."""

    def test_read_input_file_success(self, reviewer, temp_dir, sample_diff_content):
        """Test successfully reading an input file."""
        test_file = os.path.join(temp_dir, "test_diff.md")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        result = reviewer.read_input_file(test_file)
        assert result == sample_diff_content

    def test_read_input_file_not_found(self, reviewer):
        """Test reading a non-existent file."""
        with pytest.raises(FileNotFoundError, match="Input file .* not found"):
            reviewer.read_input_file("/nonexistent/file.md")

    def test_write_error_file(self, reviewer, temp_dir, mock_failed_result):
        """Test writing error file for failed results."""
        timestamp = "20240101_120000"
        failed_results = [mock_failed_result]

        error_filename = reviewer.write_error_file(temp_dir, timestamp, failed_results)

        assert error_filename == "errors_20240101_120000.md"
        error_path = os.path.join(temp_dir, error_filename)
        assert os.path.exists(error_path)

        with open(error_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review Errors" in content
        assert "**Failed Models**: 1" in content
        assert "### failed-model" in content
        assert "API timeout error" in content

    def test_write_summary_file(self, reviewer, temp_dir):
        """Test writing summary JSON file."""
        timestamp = "20240101_120000"
        summary = {
            "timestamp": timestamp,
            "input_file": "test.md",
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": ["model1_20240101_120000.md"],
        }

        summary_filename = reviewer.write_summary_file(temp_dir, timestamp, summary)

        assert summary_filename == "summary_20240101_120000.json"
        summary_path = os.path.join(temp_dir, summary_filename)
        assert os.path.exists(summary_path)

        with open(summary_path, "r", encoding="utf-8") as f:
            loaded_summary = json.load(f)

        assert loaded_summary == summary


class TestPromptCreation:
    """Test code review prompt creation."""

    def test_create_code_review_prompt(self, reviewer, sample_diff_content):
        """Test creating a comprehensive code review prompt."""
        prompt = reviewer.create_code_review_prompt(sample_diff_content)

        assert "comprehensive code review" in prompt
        assert sample_diff_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Suggestions" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt


class TestSummaryCreation:
    """Test summary creation and management."""

    def test_create_summary(self, reviewer, mock_llm_results):
        """Test creating a summary dictionary."""
        timestamp = "20240101_120000"
        from_file = "test.md"

        summary = reviewer.create_summary(timestamp, from_file, mock_llm_results)

        expected_summary = {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": [],
        }

        assert summary == expected_summary

    def test_write_successful_results(self, reviewer, temp_dir, mock_llm_results):
        """Test writing successful model results to files."""
        timestamp = "20240101_120000"
        summary = {"output_files": []}

        reviewer.write_successful_results(
            mock_llm_results, temp_dir, timestamp, summary
        )

        # Check that file was created
        expected_filename = "test-model_20240101_120000.md"
        expected_path = os.path.join(temp_dir, expected_filename)
        assert os.path.exists(expected_path)

        # Check that summary was updated
        assert expected_filename in summary["output_files"]

        # Check file content
        with open(expected_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review - test-model" in content
        assert "This is a test review response." in content


class TestReviewCode:
    """Test the main review_code method."""

    @pytest.mark.asyncio
    async def test_review_code_success(
        self, reviewer, temp_dir, sample_diff_content, mock_llm_results
    ):
        """Test successful code review execution."""
        # Create input file
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Mock dependencies
        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # Verify result structure
            assert result["status"] == "completed"
            assert result["output_directory"] == temp_dir
            # 1 success + 1 error + 1 summary
            assert result["files_created"] == 3

            # Verify files were created
            assert os.path.exists(
                os.path.join(temp_dir, "test-model_20240101_120000.md")
            )
            assert os.path.exists(os.path.join(temp_dir, "errors_20240101_120000.md"))
            assert os.path.exists(
                os.path.join(temp_dir, "summary_20240101_120000.json")
            )

    @pytest.mark.asyncio
    async def test_review_code_file_not_found(self, reviewer, temp_dir):
        """Test review_code with non-existent input file."""
        with pytest.raises(FileNotFoundError):
            await reviewer.review_code("/nonexistent/file.md", temp_dir)

    @pytest.mark.asyncio
    async def test_review_code_no_failures(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code with only successful results."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Create results with no failures
        success_only_results = LLMRunnerResults(
            successful_results=[
                ModelResult(
                    model="test-model",
                    timestamp="2024-01-01T12:00:00",
                    success=True,
                    actual_model="test-model",
                    duration_seconds=2.5,
                    response={"choices": [{"message": {"content": "Review content"}}]},
                )
            ],
            failed_results=[],
            total_models=1,
            success_count=1,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_only_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # 1 success + 1 summary (no error file)
            assert result["files_created"] == 2
            assert "error_file" not in result["summary"]


class TestReviewDiffFromGit:
    """Test git diff review functionality."""

    @pytest.mark.asyncio
    async def test_review_diff_from_git_staged(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing staged git diff."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            # Setup mocks
            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess call
            mock_subprocess.assert_called_once_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"
            assert result["summary"]["input_file"] == "git diff"

    @pytest.mark.asyncio
    async def test_review_diff_from_git_all_changes(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing all git changes."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=False)

            mock_subprocess.assert_called_once_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

            assert result["summary"]["source"] == "git_diff_all"

    @pytest.mark.asyncio
    async def test_review_diff_no_changes(self, reviewer, temp_dir):
        """Test git diff with no changes."""
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value.stdout = ""
            mock_subprocess.return_value.check_returncode = Mock()

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_not_found(self, reviewer, temp_dir):
        """Test git diff when git is not installed."""
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_error(self, reviewer, temp_dir):
        """Test git diff with git command error."""
        import subprocess

        with patch(
            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
        ):
            with pytest.raises(Exception, match="Git diff failed"):
                await reviewer.review_diff_from_git(temp_dir)


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_extract_response_malformed_gemini(self, reviewer):
        """Test extracting from malformed Gemini response."""
        malformed = {"candidates": [{"content": {"parts": []}}]}  # Empty parts
        result = reviewer.extract_response_text(malformed)
        assert result == str(malformed)

    def test_extract_response_malformed_openai(self, reviewer):
        """Test extracting from malformed OpenAI response."""
        malformed = {"choices": [{"message": {}}]}  # Missing content
        result = reviewer.extract_response_text(malformed)
        assert result == ""

    def test_extract_response_empty_anthropic(self, reviewer):
        """Test extracting from empty Anthropic response."""
        empty = {"content": []}
        result = reviewer.extract_response_text(empty)
        assert result == str(empty)

    @pytest.mark.asyncio
    async def test_review_code_with_default_output_dir(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code using default output directory from None."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        success_results = LLMRunnerResults(
            successful_results=[],
            failed_results=[],
            total_models=0,
            success_count=0,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
            patch("os.makedirs") as mock_makedirs,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            # Test with None to_file (should use default)
            result = await reviewer.review_code(input_file, None)

            # Should use reviewer's default output_dir
            mock_makedirs.assert_called_with(temp_dir, exist_ok=True)
            assert result["output_directory"] == temp_dir


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
&lt;/file&gt;
  &lt;file path="reviewer/test_code_review_integration.py"&gt;import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import patch, MagicMock, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


class TestCodeReviewIntegration:
    """Test integration between .claude/commands/model_code_review.md and code_review.py"""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_diff_content(self):
        """Sample diff content for testing"""
        return """diff --git a/src/auth.py b/src/auth.py
index 1234567..abcdefg 100644
--- a/src/auth.py
+++ b/src/auth.py
@@ -1,5 +1,10 @@
 def authenticate_user(username, password):
-    # Simple authentication
-    return username == "admin" and password == "secret"
+    # Improved authentication with validation
+    if not username or not password:
+        return False
+    
+    # TODO: Add proper password hashing
+    return username == "admin" and password == "secret123"
 
 def get_user_role(username):
     return "admin" if username == "admin" else "user"
"""

    @pytest.fixture
    def sample_diff_file(self, temp_dir, sample_diff_content):
        """Create sample diff file for testing"""
        diff_file = os.path.join(temp_dir, "test_diff.md")
        with open(diff_file, "w") as f:
            f.write(sample_diff_content)
        return diff_file

    @pytest.fixture
    def mock_llm_results(self):
        """Mock LLM runner results matching expected format"""
        successful_results = [
            ModelResult(
                model="claude-3-5-sonnet",
                success=True,
                response={
                    "content": [
                        {
                            "text": "## Code Review Analysis\n\n### Security Issues\n🔴 **Critical**: Hardcoded password in authentication logic\n\n### Recommendations\n- Implement proper password hashing\n- Add input validation"
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:00",
                duration_seconds=2.5,
                error=None,
            ),
            ModelResult(
                model="gpt-4-turbo",
                success=True,
                response={
                    "choices": [
                        {
                            "message": {
                                "content": "## Security Analysis\n\n🔴 **High Risk**: Authentication uses plaintext password comparison\n🟡 **Medium**: Missing input validation for empty credentials"
                            }
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:05",
                duration_seconds=3.1,
                error=None,
            ),
        ]

        failed_results = [
            ModelResult(
                model="gemini-pro",
                success=False,
                response=None,
                timestamp="2024-01-01T12:00:10",
                duration_seconds=0,
                error="API rate limit exceeded",
            )
        ]

        return LLMRunnerResults(
            successful_results=successful_results,
            failed_results=failed_results,
            total_models=3,
            success_count=2,
            failure_count=1,
        )

    @pytest.mark.asyncio
    async def test_code_review_from_file_integration(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test the complete file-based code review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config", "gpt": "config"}

            # Initialize reviewer with temp directory
            reviewer = CodeReviewer(output_dir=temp_dir)

            # Run review (simulates mcp__collect__run_code_review)
            result = await reviewer.review_code(sample_diff_file, temp_dir)

            # Verify return structure matches command expectations
            assert result["status"] == "completed"
            assert "summary" in result
            assert "output_directory" in result
            assert "files_created" in result

            # Verify output files were created as documented in command
            files = os.listdir(temp_dir)

            # Should have individual model reviews
            claude_files = [f for f in files if f.startswith("claude-3-5-sonnet")]
            gpt_files = [f for f in files if f.startswith("gpt-4-turbo")]
            assert len(claude_files) == 1
            assert len(gpt_files) == 1

            # Should have summary file
            summary_files = [f for f in files if f.startswith("summary_")]
            assert len(summary_files) == 1

            # Should have errors file for failed models
            error_files = [f for f in files if f.startswith("errors_")]
            assert len(error_files) == 1

            # Verify summary JSON structure
            summary_file = os.path.join(temp_dir, summary_files[0])
            with open(summary_file, "r") as f:
                summary_data = json.load(f)

            assert summary_data["total_models"] == 3
            assert summary_data["successful_reviews"] == 2
            assert summary_data["failed_reviews"] == 1
            assert len(summary_data["output_files"]) == 2

    @pytest.mark.asyncio
    async def test_git_diff_review_integration(self, temp_dir, mock_llm_results):
        """Test git diff review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
            patch("subprocess.run") as mock_subprocess,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config"}

            # Mock git diff output
            mock_subprocess.return_value = MagicMock(
                stdout="diff --git a/test.py b/test.py\n+def new_function():\n+    pass",
                returncode=0,
            )

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Test staged-only review (Option A from command)
            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess called with correct git command
            mock_subprocess.assert_called_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"

            # Test all changes review
            await reviewer.review_diff_from_git(temp_dir, staged_only=False)
            mock_subprocess.assert_called_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

    def test_claude_command_workflow_documentation(self):
        """Verify that the Claude command documentation matches code_review.py capabilities"""
        reviewer = CodeReviewer()

        # Test that CodeReviewer has methods mentioned in command
        assert hasattr(
            reviewer, "review_code"
        ), "Should support file-based review (Option B)"
        assert hasattr(
            reviewer, "review_diff_from_git"
        ), "Should support git diff review (Option A)"

        # Test that review_code signature matches command usage
        import inspect

        sig = inspect.signature(reviewer.review_code)
        assert "from_file" in sig.parameters, "Should accept from_file parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

        # Test that review_diff_from_git signature matches command usage
        sig = inspect.signature(reviewer.review_diff_from_git)
        assert "staged_only" in sig.parameters, "Should accept staged_only parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

    def test_output_file_naming_convention(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test that output files follow naming convention documented in command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {}

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Mock timestamp for predictable filenames
            with patch("reviewer.code_review.datetime") as mock_datetime:
                mock_datetime.now.return_value.strftime.return_value = "20241201_143052"

                # Run async test
                import asyncio

                asyncio.run(reviewer.review_code(sample_diff_file, temp_dir))

            files = os.listdir(temp_dir)

            # Verify naming matches documentation: {model}_YYYYMMDD_HHMMSS.md
            expected_patterns = [
                "claude-3-5-sonnet_20241201_143052.md",
                "gpt-4-turbo_20241201_143052.md",
                "summary_20241201_143052.json",
                "errors_20241201_143052.md",
            ]

            for pattern in expected_patterns:
                assert pattern in files, f"Expected file {pattern} not found in {files}"

    def test_prompt_structure_matches_command_requirements(self):
        """Test that code review prompt includes all sections mentioned in command"""
        reviewer = CodeReviewer()
        prompt = reviewer.create_code_review_prompt("sample code")

        # Verify prompt includes all required sections from command documentation
        required_sections = [
            "Overall Assessment",
            "Issues Found",
            "Security vulnerabilities",
            "Bugs and logic errors",
            "Performance issues",
            "Code quality problems",
            "Testing gaps",
            "Suggestions for Improvement",
            "Positive Aspects",
            "Risk Assessment",
            "Summary Table",
        ]

        for section in required_sections:
            assert section in prompt, f"Prompt missing required section: {section}"

        # Verify emoji risk indicators are included
        risk_emojis = ["🔴", "🟡", "🟢"]
        for emoji in risk_emojis:
            assert emoji in prompt, f"Prompt missing risk emoji: {emoji}"

    @pytest.mark.asyncio
    async def test_error_handling_matches_command_expectations(self, temp_dir):
        """Test error handling for scenarios mentioned in command troubleshooting"""
        reviewer = CodeReviewer(output_dir=temp_dir)

        # Test "No git changes found" scenario
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value = MagicMock(stdout="", returncode=0)

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

        # Test file not found scenario
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_dir)

        # Test git not available scenario
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    def test_response_text_extraction_supports_all_models(self):
        """Test that response extraction works for all model formats mentioned in command"""
        reviewer = CodeReviewer()

        # Test Anthropic Claude format
        anthropic_response = {"content": [{"text": "Claude review content"}]}
        assert (
            reviewer.extract_response_text(anthropic_response)
            == "Claude review content"
        )

        # Test OpenAI GPT format
        openai_response = {"choices": [{"message": {"content": "GPT review content"}}]}
        assert reviewer.extract_response_text(openai_response) == "GPT review content"

        # Test Google Gemini format
        gemini_response = {
            "candidates": [{"content": {"parts": [{"text": "Gemini review content"}]}}]
        }
        assert (
            reviewer.extract_response_text(gemini_response) == "Gemini review content"
        )

        # Test fallback for XAI Grok or unknown formats
        unknown_response = "Direct string response"
        assert (
            reviewer.extract_response_text(unknown_response) == "Direct string response"
        )
&lt;/file&gt;
  &lt;file path="reviewer/test_codereview_live/errors_20250601_085957.md"&gt;
            # Code Review Errors

            **Timestamp**: 20250601_085957
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.646644

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.833688

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.262231

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.434107

            &lt;/file&gt;
  &lt;file path="reviewer/test_codereview_live/summary_20250601_085957.json"&gt;{
  "timestamp": "20250601_085957",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_085957.md"
}&lt;/file&gt;
  &lt;file path="reviewer/test_codereview/errors_20250601_084959.md"&gt;
            # Code Review Errors

            **Timestamp**: 20250601_084959
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.675555

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.837903

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.045514

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.245033

            &lt;/file&gt;
  &lt;file path="reviewer/test_codereview/summary_20250601_084601.json"&gt;{
  "timestamp": "20250601_084601",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084601.md"
}&lt;/file&gt;
  &lt;file path="reviewer/test_codereview/summary_20250601_084959.json"&gt;{
  "timestamp": "20250601_084959",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084959.md"
}&lt;/file&gt;
  &lt;file path="reviewer/test_codereview/errors_20250601_084601.md"&gt;
            # Code Review Errors

            **Timestamp**: 20250601_084601
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:43:35.383459

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:44:24.400527

            
                ### grok-3
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:45:01.153872

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 504 Deadline Exceeded
                - **Timestamp**: 2025-06-01T08:46:01.157505

            &lt;/file&gt;
  &lt;file path="reviewer/live_test_codereview/grok-3-mini-fast-latest_20250601_091219.md"&gt;
            # Code Review - grok-3-mini-fast-latest

            **Model**: grok-3-mini-fast-latest
            **Timestamp**: 2025-06-01T09:11:32.476330
            **Duration**: 17.72 seconds

            ---

            Below is a comprehensive code review of the provided diff for the `calculate_total` function in `test.py`, based on the instructions. I'll address each section step by step, focusing on actionable feedback to improve code quality, security, and maintainability.

---

### 1. **Overall Assessment**

- **Brief summary of what changed and why**: The changes enhance the robustness of the `calculate_total` function. An explicit check for an empty `items` list was added to return 0, preventing potential runtime errors. Additionally, the price access was modified from direct attribute access (`item.price`) to a dictionary-based approach (`item.get('price', 0)`), likely to handle cases where `items` are dictionaries rather than custom objects. This suggests a shift in data structure (e.g., from object-oriented to dictionary-based data), possibly to accommodate more flexible input formats or to avoid `AttributeError` when `price` is missing.

- **Impact on the codebase**: The scope is narrow, affecting only this function, but the significance is moderate. It improves error handling and reduces the risk of crashes in edge cases (e.g., empty lists or missing keys), making the code more resilient. However, if this function is used across the application, the change could introduce inconsistencies if other parts of the codebase still expect `items` to be objects with attributes. Overall, it aligns with defensive programming principles.

- **Alignment with best practices**: The changes are positive, as they address common pitfalls like unhandled edge cases and attribute access errors. Using `dict.get()` with a default value follows best practices for handling optional keys, promoting safer code. However, the function could benefit from additional improvements in type safety, documentation, and testing to fully align with modern Python standards (e.g., PEP 8, type hints).

---

### 2. **Issues Found**

I analyzed the diff for potential security, bugs, performance, code quality, and testing issues. Here's a breakdown:

- **Security vulnerabilities**: 
  - No significant security issues were identified. This function performs a simple summation and doesn't involve user input, network operations, or sensitive data handling, so risks like injection or data exposure are low. However, if `items` comes from an untrusted source (e.g., user input or external API), the function could be vulnerable to malicious data (e.g., if `items` contains non-numeric values). This isn't directly addressed in the change.

- **Bugs and logic errors**:
  - **Edge case handling**: The added empty list check is good, but the function now assumes `items` elements are dictionaries (due to `item.get()`). If `items` contains non-dictionary types (e.g., custom objects or other data structures), a `AttributeError` could still occur when calling `get()`. For example, if `items` includes strings or lists, the code would fail. The original code had a similar issue with attribute access, but this change doesn't fully resolve type mismatches.
  - **Error handling**: No broader error handling is present (e.g., for non-numeric `price` values). If a dictionary has a `price` key with a non-numeric value (e.g., a string), a `TypeError` could occur during addition. The `get()` method defaults to 0 for missing keys, which is helpful, but it doesn't validate the type or value of existing keys.
  - **Logic errors**: The function works for valid inputs but could return incorrect results if `items` is not a list (e.g., a string or None). The iteration (`for item in items`) assumes `items` is iterable, which could raise a `TypeError` if it's not.

- **Performance issues**:
  - No major performance concerns. The function uses a linear O(n) loop, which is efficient for most use cases. The addition of the empty check and `get()` method adds negligible overhead. However, if `items` is very large, the function could benefit from Python's built-in `sum()` function for slight optimization.

- **Code quality problems**:
  - **Readability and maintainability**: The code is straightforward, but the shift from attribute access to dictionary access could confuse maintainers if the data structure isn't documented. There's no type hinting, docstring, or comments, making it harder to understand the function's intent or constraints. Code style is mostly fine, but the indentation in the diff looks consistent.
  - **Complexity**: Low cyclomatic complexity, which is good. However, the function could be simplified using Python's `sum()` with a generator expression, reducing code duplication.

- **Testing gaps**:
  - No tests are provided or referenced in the diff, so it's unclear if the changes are covered. Potential gaps include:
    - Tests for edge cases like empty lists, missing keys, non-dictionary items, or invalid data types.
    - Regression tests to ensure the function still works for valid inputs (e.g., lists of dictionaries or objects).
    - The added changes improve robustness, but without tests, there's no assurance that they don't introduce new bugs. Unit tests should cover various scenarios to achieve good code coverage.

---

### 3. **Suggestions for Improvement**

Here are specific, actionable recommendations to enhance the code:

- **Code structure and organization**:
  - Add type hints to clarify expected input and output types, e.g., `def calculate_total(items: list[dict]) -&amp;gt; int`. This would make the function more explicit about assuming dictionary-based items and help catch type errors early with tools like mypy.
  - Consider refactoring the function to handle both dictionary and object-based inputs if needed, e.g., by checking the type of each item and using `getattr(item, 'price', 0)` for objects or `item.get('price', 0)` for dicts. This would make the function more flexible.

- **Error handling improvements**:
  - Add input validation at the start of the function, e.g., check if `items` is iterable and raise a custom error (e.g., `ValueError`) if not. Also, handle potential non-numeric `price` values by adding a type check or conversion, e.g., `total += float(item.get('price', 0))` to avoid `TypeError`.
  - For better robustness, use a try-except block around the loop to catch unexpected errors and log them or return a default value, but avoid overusing exceptions for performance reasons.

- **Performance optimizations**:
  - Replace the manual loop with a more concise and potentially faster approach using `sum()`, e.g., `return sum(item.get('price', 0) for item in items or [])`. This handles the empty case automatically and reduces code lines. The `or []` ensures that if `items` is None, it defaults to an empty list, avoiding errors.

- **Better naming and documentation**:
  - Add a docstring to the function describing its purpose, parameters, return value, and any assumptions (e.g., that items are dictionaries). Example:
    ```
    def calculate_total(items):
        """
        Calculate the total price from a list of items.

        Args:
            items (list): A list of dictionaries, each containing a 'price' key.

        Returns:
            int: The sum of all prices, or 0 if no items are provided.

        Raises:
            ValueError: If items is not iterable or contains invalid data.
        """
        # ... rest of the code
    ```
  - Improve variable naming if needed; `total` is fine, but `items` could be more descriptive (e.g., `item_list`) if the context isn't clear.

- **Refactoring opportunities**:
  - Simplify the code using `sum()` as mentioned earlier. If this function is part of a larger module, consider extracting price retrieval into a separate helper function (e.g., `get_price(item)`) to handle different data types, improving reusability and reducing duplication.

---

### 4. **Positive Aspects**

The changes demonstrate good engineering practices and improve the code's reliability:

- **Good patterns and practices**: Adding the empty list check and using `dict.get()` with a default value is a solid example of defensive programming. It prevents common errors like `AttributeError` or index errors, making the function more robust without overcomplicating the logic.
- **Clear, readable code**: The updated logic is straightforward and easy to follow. The changes maintain simplicity while addressing potential issues, which enhances maintainability.
- **Proper error handling**: The use of `get('price', 0)` elegantly handles missing keys, reducing the need for explicit try-except blocks and improving code flow.
- **Well-structured logic**: The function remains concise and focused on a single responsibility (summing prices), adhering to the single-responsibility principle.

Overall, the diff shows thoughtful improvements that make the code more resilient to real-world inputs.

---

### 5. **Risk Assessment**

- **High Risk**: None identified. The changes are localized and don't introduce breaking changes or security threats, but improper handling of data types could lead to runtime errors in certain scenarios.
- **Medium Risk**: Potential type mismatches (e.g., if `items` elements are not dictionaries) could cause errors or incorrect results. Additionally, lack of testing might lead to undetected bugs in production.
- **Low Risk**: Minor issues like missing documentation or code style inconsistencies, which don't affect functionality but could impact long-term maintainability.

---

### Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential type mismatch for `items` elements | 🟡 Important | Assumes items are dictionaries; could raise errors if elements are custom objects or other types. | Add type hints and input validation, e.g., check if item is dict or use a flexible getter function. |
| Missing error handling for non-numeric prices or non-iterable inputs | 🟡 Important | Could raise `TypeError` if `price` is not a number or if `items` is not iterable. | Add type checks or use safe conversions, e.g., `float(item.get('price', 0))`, and validate `items` at the start. |
| Lack of documentation and docstrings | 🟢 Minor | No explanation of function behavior, parameters, or assumptions, reducing readability. | Add a descriptive docstring and consider inline comments for complex logic. |
| Testing gaps for edge cases | 🟡 Important | No visible tests for empty lists, missing keys, or invalid inputs, risking undetected regressions. | Implement unit tests covering various scenarios, e.g., using pytest with cases for empty, valid, and invalid inputs. |
| Opportunity for code simplification | 🟢 Minor | Manual loop could be replaced with `sum()` for conciseness and performance. | Refactor to use `sum(item.get('price', 0) for item in items or [])` to handle edges automatically. |

This review provides a balanced, actionable critique to help refine the code. If you have additional context (e.g., the rest of the codebase or testing framework), I can refine this further!

            ---
            *Generated by grok-3-mini-fast-latest via MCP Code Review Tool*
        &lt;/file&gt;
  &lt;file path="reviewer/live_test_codereview/summary_20250601_091219.json"&gt;{
  "timestamp": "20250601_091219",
  "input_file": "test_diff.md",
  "total_models": 4,
  "successful_reviews": 4,
  "failed_reviews": 0,
  "output_files": [
    "gemini-2.0-flash_20250601_091219.md",
    "o3-mini-2025-01-31_20250601_091219.md",
    "grok-3-mini-fast-latest_20250601_091219.md",
    "claude-opus-4-20250514_20250601_091219.md"
  ]
}&lt;/file&gt;
  &lt;file path="reviewer/live_test_codereview/o3-mini-2025-01-31_20250601_091219.md"&gt;
            # Code Review - o3-mini-2025-01-31

            **Model**: o3-mini-2025-01-31
            **Timestamp**: 2025-06-01T09:11:23.235642
            **Duration**: 9.24 seconds

            ---

            Below is a comprehensive review of the code diff:

──────────────────────────────
1. Overall Assessment
──────────────────────────────
• Summary of Changes:
 – An early exit is added to handle the case where the items list is empty (returning 0).
 – Instead of directly accessing an attribute (item.price), the code now uses the dictionary “get” method (item.get('price', 0)) to retrieve the price value.
 – This indicates a shift in expectation from an object with a price attribute to a dictionary-like object where "price" is a key.
  
• Impact on the Codebase:
 – The changes are localized to the calculate_total function in test.py.
 – The modifications improve robustness when items is empty and when an item does not contain a "price" key.
 – It may affect other parts of the system if they pass objects with a price attribute rather than dictionaries; integration testing is advised.
  
• Alignment with Best Practices:
 – Handling an empty list immediately is a good practice to avoid unnecessary computation.
 – Using item.get with a default value enhances fault tolerance for missing keys.
 – Code readability is maintained, though further documentation could be added.

──────────────────────────────
2. Issues Found
──────────────────────────────
• Security Vulnerabilities:
 – No apparent security concerns such as injection risks or data exposures. The function’s scope is narrow and does not involve external inputs.

• Bugs and Logic Errors:
 – Type Expectations: Changing from attribute access to dictionary access may break callers expecting objects with attributes. Confirm that all items are dictionaries.
 – Implicit Assumptions: The code assumes every item is a dict-like object. There’s no type-checking, so if an unexpected type is encountered, it might raise an AttributeError or TypeError.
 – Early Exit:
  • The early return for empty items is clear, but if items is None (i.e., not provided), it might lead to unexpected None errors unless the caller guarantees a list.

• Performance Issues:
 – The function’s algorithm is simple and works in O(n) time. There are no performance bottlenecks even for larger lists.
 – Memory usage is minimal.

• Code Quality Problems:
 – Readability is high; however, a brief comment explaining the change in expected item format (from object to dict) could help future maintainers.
 

            ---
            *Generated by o3-mini-2025-01-31 via MCP Code Review Tool*
        &lt;/file&gt;
  &lt;file path="reviewer/live_test_codereview/claude-opus-4-20250514_20250601_091219.md"&gt;
            # Code Review - claude-opus-4-20250514

            **Model**: claude-opus-4-20250514
            **Timestamp**: 2025-06-01T09:11:50.195387
            **Duration**: 29.30 seconds

            ---

            ## Code Review: calculate_total Function Enhancement

### 1. **Overall Assessment**

**Summary of Changes:**
- Added null/empty check for the `items` parameter
- Changed from attribute access (`item.price`) to dictionary access with default value (`item.get('price', 0)`)

**Impact:**
- Improves robustness by handling edge cases (empty input and missing price fields)
- Changes the expected data structure from objects with price attributes to dictionaries
- Prevents potential AttributeError and KeyError exceptions

**Best Practices Alignment:**
- ✅ Defensive programming with input validation
- ✅ Graceful handling of missing data
- ⚠️ Inconsistent data structure assumption (mixing objects and dictionaries)

### 2. **Issues Found**

**Code Quality Problems:**
1. **Inconsistent Data Model**: The change from `item.price` to `item.get('price', 0)` suggests a shift from objects to dictionaries, but this isn't documented or type-hinted
2. **Silent Failure**: Items without prices are silently treated as having price 0, which might mask data quality issues
3. **Missing Type Hints**: No type annotations to clarify expected input/output types
4. **No Documentation**: Function lacks docstring explaining purpose, parameters, and return value

**Potential Logic Issues:**
1. **Type Assumption**: Code assumes items are dictionaries but doesn't validate this
2. **Numeric Type Assumption**: No validation that 'price' values are numeric

### 3. **Suggestions for Improvement**

```python
from typing import List, Dict, Union, Optional
import logging

def calculate_total(items: Optional[List[Dict[str, Union[int, float]]]]) -&amp;gt; Union[int, float]:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dictionaries with 'price' key, or None/empty list
        
    Returns:
        Total price as int or float. Returns 0 for empty/None input.
        
    Raises:
        TypeError: If price value is not numeric
        ValueError: If price value is negative
    """
    if not items:
        return 0
    
    total = 0
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            raise TypeError(f"Item at index {idx} is not a dictionary")
            
        price = item.get('price', None)
        if price is None:
            logging.warning(f"Item at index {idx} has no price field")
            continue
            
        if not isinstance(price, (int, float)):
            raise TypeError(f"Price for item at index {idx} is not numeric: {price}")
            
        if price &amp;lt; 0:
            raise ValueError(f"Price for item at index {idx} is negative: {price}")
            
        total += price
        
    return total
```

**Alternative approach for mixed data types:**
```python
def calculate_total(items: Optional[List[Union[Dict, object]]]) -&amp;gt; Union[int, float]:
    """Handle both dictionary and object inputs."""
    if not items:
        return 0
    
    total = 0
    for item in items:
        if isinstance(item, dict):
            price = item.get('price', 0)
        elif hasattr(item, 'price'):
            price = item.price
        else:
            price = 0
            
        total += price
        
    return total
```

### 4. **Positive Aspects**

- ✅ **Good defensive programming**: Adding the empty check prevents errors on None/empty inputs
- ✅ **Graceful degradation**: Using `.get()` with default prevents KeyError exceptions
- ✅ **Maintains backward compatibility**: Function signature remains unchanged
- ✅ **Simple and readable**: The logic remains straightforward and easy to understand

### 5. **Risk Assessment**

**Medium Risk** 🟡
- **Breaking Change**: Switching from attribute to dictionary access could break existing code that passes objects
- **Data Quality**: Silent handling of missing prices could hide data issues in production
- **Type Safety**: Lack of validation could lead to runtime errors with unexpected data types

**Mitigation Strategies:**
1. Add comprehensive unit tests covering all data type scenarios
2. Implement logging for items with missing prices
3

            ---
            *Generated by claude-opus-4-20250514 via MCP Code Review Tool*
        &lt;/file&gt;
  &lt;file path="reviewer/live_test_codereview/gemini-2.0-flash_20250601_091219.md"&gt;
            # Code Review - gemini-2.0-flash

            **Model**: gemini-2.0-flash
            **Timestamp**: 2025-06-01T09:11:16.427900
            **Duration**: 6.81 seconds

            ---

            ## Test Code Review

### 1. **Overall Assessment**

The diff introduces two key changes to the `calculate_total` function:

1.  A check for an empty `items` list, returning 0 in that case.
2.  A change in how the price is accessed: from `item.price` to `item.get('price', 0)`.

The first change handles a potential edge case, preventing errors when the input is empty. The second change makes the code more robust by handling cases where an item might not have a `price` attribute directly, but rather stores it as a dictionary key. These changes enhance the robustness and reliability of the function. The scope is relatively small, impacting only the `calculate_total` function. These changes generally align with best practices for defensive programming and error handling.

### 2. **Issues Found**

*   **Potential Type Error (🟡)**:  While `item.get('price', 0)` handles the absence of the 'price' key, it assumes the value associated with the 'price' key (if it exists) will be a number that can be added to `total`. If `item['price']` exists but is a string (e.g., "unknown"), a `TypeError` would still occur.
*   **Limited Input Validation (🟡)**: The code assumes that each `item` in `items` is a dictionary. It doesn't validate that `items` is even a list, or that each element within it is a dictionary-like object.

### 3. **Suggestions for Improvement**

*   **Type Validation and Error Handling (Important):**  Implement more robust type validation, either with `isinstance` checks or using a try-except block:

    ```python
    def calculate_total(items):
        if not items:
            return 0

        total = 0
        for item in items:
            try:
                price = item.get('price', 0)
                if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
                total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.
        return total
    ```

*   **Consider a dedicated Item class (Minor):** If the structure of `items` is fixed (i.e., always containing dictionaries with a 'price'), consider defining a dedicated `Item` class with a `price` attribute. This would improve code readability and maintainability.

*   **Add input validation (Minor):** Assert that `items` is a list and each element is either a dictionary or an object with a `get` method.

    ```python
    def calculate_total(items):
        if not isinstance(items, list):
            raise TypeError("items must be a list")

        if not items:
            return 0

        total = 0
        for item in items:
            if not hasattr(item, 'get') and not isinstance(item, dict):
                raise TypeError("Each item must be a dictionary or an object with a 'get' method")
              if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
              total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.

        return total
    ```

### 4. **Positive Aspects**

*   **Handles Empty Input (🟢):** The addition of the `if not items` check is a good practice for handling edge cases and preventing potential errors.
*   **Robust Price Access (🟢):** Using `item.get('price', 0)` is a good way to handle cases where the `price` attribute may not be directly available, providing a default value of 0 if the key is missing.

### 5. **Risk Assessment**

*   **Medium Risk**:  The lack of explicit type validation for the `price` can still lead to runtime errors. Implementing the suggested improvement involving the `try-except` block significantly mitigates this risk.

## Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential Type Error | 🟡 |  If `item

            ---
            *Generated by gemini-2.0-flash via MCP Code Review Tool*
        
&lt;/file&gt;
  &lt;file path="migrations/20250810_02_add-github-url-to-prompt-history.sql"&gt;-- Add github_url column to prompt_history table to track project association in historical records
-- depends: 20250810_01_add-projects-table

-- Add github_url column to prompt_history table
ALTER TABLE prompt_history ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Add index for efficient queries by github_url
CREATE INDEX IF NOT EXISTS idx_prompt_history_github_url ON prompt_history(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_history_github_url;
-- ALTER TABLE prompt_history DROP COLUMN github_url;&lt;/file&gt;
  &lt;file path="migrations/20250727_01_create-prompt-tables.sql"&gt;-- Create prompt tables for prompt storage, versioning, and metrics
-- depends: 

-- Current prompt table
CREATE TABLE IF NOT EXISTS prompt (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    data JSONB NOT NULL,
    version INTEGER DEFAULT 1,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Historical versions table
CREATE TABLE IF NOT EXISTS prompt_history (
    id TEXT,
    version INTEGER,
    data JSONB NOT NULL,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    change_summary TEXT,
    PRIMARY KEY (id, version)
);

-- Metrics time-series table (optimized for prompt tracking)
CREATE TABLE IF NOT EXISTS prompt_metrics (
    prompt_id TEXT,
    version INTEGER,
    metric_name TEXT,
    step INTEGER,
    value REAL,
    timestamp TIMESTAMP,
    PRIMARY KEY (prompt_id, version, metric_name, step)
);

-- Performance-critical indexes
CREATE INDEX IF NOT EXISTS idx_prompt_hash ON prompt(content_hash);
CREATE INDEX IF NOT EXISTS idx_prompt_updated ON prompt(updated_at);
CREATE INDEX IF NOT EXISTS idx_prompt_history_created ON prompt_history(created_at);
CREATE INDEX IF NOT EXISTS idx_prompt_metrics_time ON prompt_metrics(timestamp);

-- Expression indexes on JSONB fields for common queries
CREATE INDEX IF NOT EXISTS idx_prompt_status ON prompt(data -&amp;gt;&amp;gt; '$.status');
CREATE INDEX IF NOT EXISTS idx_prompt_type ON prompt(data -&amp;gt;&amp;gt; '$.type');&lt;/file&gt;
  &lt;file path="migrations/20250810_01_add-projects-table.sql"&gt;-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;&lt;/file&gt;
  &lt;file path="repository/test_database.py"&gt;import pytest
import sqlite3
import os

from repository.database import SQLite3Database


@pytest.fixture
def test_db():
    """Create a test database instance"""
    test_db_path = "test_collect.db"
    db = SQLite3Database(db_path=test_db_path)
    yield db
    # Cleanup
    if os.path.exists(test_db_path):
        os.remove(test_db_path)


def test_database_connection_basic(test_db):
    """Test basic database connection establishment"""
    with test_db.get_connection() as conn:
        assert conn is not None
        assert isinstance(conn, sqlite3.Connection)
        # Test basic query
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_connection_read_only(test_db):
    """Test read-only database connection"""
    with test_db.get_connection(read_only=True) as conn:
        assert conn is not None
        # Should be able to read
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_row_factory(test_db):
    """Test that Row factory is properly configured"""
    with test_db.get_connection() as conn:
        cursor = conn.execute("SELECT 1 as test_col")
        row = cursor.fetchone()
        # Should be able to access by column name
        assert row["test_col"] == 1


def test_database_pragma_settings(test_db):
    """Test that PRAGMA settings are applied correctly"""
    with test_db.get_connection() as conn:
        # Check foreign keys
        cursor = conn.execute("PRAGMA foreign_keys")
        assert cursor.fetchone()[0] == 1

        # Check journal mode
        cursor = conn.execute("PRAGMA journal_mode")
        assert cursor.fetchone()[0] == "wal"

        # Check synchronous mode
        cursor = conn.execute("PRAGMA synchronous")
        assert cursor.fetchone()[0] == 1  # NORMAL = 1


def test_database_context_manager_cleanup():
    """Test that database connections are properly closed"""
    test_db_path = "test_cleanup.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with db.get_connection() as conn:
            conn.execute("SELECT 1")

        # Connection should be closed after context manager exits
        # We can't directly test if it's closed, but we can verify
        # that we can create a new connection successfully
        with db.get_connection() as conn:
            assert conn is not None

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)


def test_database_error_handling():
    """Test error handling and rollback"""
    test_db_path = "test_error.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with pytest.raises(sqlite3.Error):
            with db.get_connection() as conn:
                # This should cause an error
                conn.execute("INVALID SQL STATEMENT")

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)
&lt;/file&gt;
  &lt;file path="repository/database.py"&gt;import sqlite3

from contextlib import contextmanager
from typing import Generator


class SQLite3Database:
    def __init__(self, db_path: str = "../data/collect.db"):
        self.db_path = db_path

    # Decorator that converts this generator function into a context manager
    @contextmanager
    def get_connection(
        self, read_only: bool = False
    ) -&amp;gt; Generator[sqlite3.Connection, None, None]:
        """Context manager for database connections"""
        # Setup phase: runs when entering 'with' block
        # Enable PARSE_DECLTYPES to use our custom datetime converters
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.row_factory = sqlite3.Row  # enables column access by name

        # Connection optimizations
        conn.execute("PRAGMA foreign_keys = ON")  # enables foreign key support
        if not read_only:
            # enables better concurrency
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL")  # Faster writes
        conn.execute("PRAGMA cache_size = -64000")  # 64MB cache
        # Use memory for temp tables
        conn.execute("PRAGMA temp_store = MEMORY")
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB memory-mapped I/O

        try:
            yield conn  # Pauses here, returns conn to 'with' statement
            # Code inside 'with' block runs here
            if not read_only:
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            # Cleanup phase: always runs when exiting 'with' block
            conn.close()
&lt;/file&gt;
  &lt;file path="repository/test_prompt_service.py"&gt;import pytest
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import Prompt, PromptType, PromptPlanStatus, CmdCategory
from config import Config


@pytest.fixture
def prompt_service():
    """
    ## How It Works

    1. **`with db.get_connection() as conn:`**
       - Opens a database connection using a context manager
       - The `as conn` assigns the connection to the variable
       - When the `with` block exits, `conn.close()` is automatically called

    2. **`cmd_service = CmdsService(conn)`**
       - Creates the service object with the database connection
       - The service can now execute database operations

    3. **`yield cmd_service`**
       - This is pytest fixture syntax that provides the service to the test
       - `yield` pauses execution here while the test runs
       - After the test completes, execution resumes after the `yield`

    4. **Automatic cleanup**
       - When the test finishes, the `with` block exits
       - Database connection is automatically closed
       - Resources are freed

    This pattern ensures **deterministic cleanup** -
    the database connection will always be properly closed regardless of
    whether the test passes or fails.
    """
    config = Config()
    db = SQLite3Database(config.db_path)
    with db.get_connection() as conn:
        cmd_service = PromptService(conn, config)

        yield cmd_service


def test_check_dirs(prompt_service: PromptService):
    result = prompt_service.cmd_check_dirs()
    assert result is True


def test_load_cmds_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_cmds_from_disk()
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def test_load_plans_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_plans_from_disk()

    print(f"\nTotal plans loaded: {len(load_results.loaded_prompts)}")
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def create_test_prompts(prompt_service: PromptService) -&amp;gt; List[Prompt]:
    prompt_content = """
    this is a test prompt for testing database persistence... blah blah
    """

    def new_cmd_prompt(prompt_content: str) -&amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&amp;gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return [new_cmd_prompt(prompt_content), new_plan_prompt(prompt_content)]


def test_save_prompt_in_db(prompt_service: PromptService):
    # create test cmd and plan prompt types
    pls = create_test_prompts(prompt_service)
    cmd_prompt = pls[0]
    plan_prompt = pls[1]

    try:
        # save test prompts in sqlite and verify success
        cmd_result = prompt_service.save_prompt_in_db(cmd_prompt)
        print(f"cmd_result: {cmd_result}")
        assert cmd_result.success is not False

        plan_result = prompt_service.save_prompt_in_db(plan_prompt)
        print(f"plan_result: {plan_result}")
        assert plan_result.success is not False

        # retrieve the saved test prompts from sqlite and verify they
        # match the original test cmd and plan prompts
        print(f"Retrieving cmd prompt with id: {cmd_prompt.id}")
        retrieved_cmd = prompt_service.get_prompt_by_id(cmd_prompt.id)
        print(f"Retrieved cmd: {retrieved_cmd}")
        assert retrieved_cmd is not None

        retrieved_plan = prompt_service.get_prompt_by_id(plan_prompt.id)
        assert retrieved_plan is not None

        # update prompt and increment the version
        updated_text = cmd_prompt.data.content + "UPDATED TEXT"
        cmd_prompt.data.content = updated_text

        update_result = prompt_service.update_prompt_in_db(cmd_prompt)
        assert update_result.success is True

        # retrieve the updated prompt again from the prompt table and
        # validate the changes were persisted/updated
        retrieved_prompt = prompt_service.get_prompt_by_id(cmd_prompt.id)
        assert retrieved_prompt.data.content == updated_text

        # retrieve the prompt by name
        # and validate correct prompt retrieval
        retrieved_prompt_by_name = prompt_service.get_prompt_by_name(cmd_prompt.name)
        assert retrieved_prompt_by_name is not None
        assert retrieved_prompt_by_name.id == cmd_prompt.id

    finally:
        # Clean up test data - this will ALWAYS run, even if test fails
        print("\nCleaning up test prompts...")

        cmd_cleanup = delete_prompt_completely(prompt_service, cmd_prompt.id)
        print(f"CMD cleanup result: {cmd_cleanup}")

        plan_cleanup = delete_prompt_completely(prompt_service, plan_prompt.id)
        print(f"PLAN cleanup result: {plan_cleanup}")


def delete_prompt_completely(prompt_service: PromptService, prompt_id: str):
    """
    DELETE a prompt from tables: prompt, prompt_history and prompt_metrics
    THIS IS FOR INTEGRATION TESTING ONLY - as production code should reserve
    history
    """
    cursor = prompt_service.conn.cursor()
    try:
        # start transaction
        cursor.execute("BEGIN TRANSACTION")

        # delete from prompt_history first (due to composite primary key)
        cursor.execute(
            """
                       DELETE FROM prompt_history
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_history_rows_deleted = cursor.rowcount

        # delete from prompt_metrics table if any exist
        cursor.execute(
            """
                       DELETE FROM prompt_metrics
                       WHERE prompt_id = ?
                       """,
            (prompt_id,),
        )
        prompt_metrics_rows_deleted = cursor.rowcount

        # delete from prompt table (we do this last)
        cursor.execute(
            """
                       DELETE FROM prompt
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_rows_deleted = cursor.rowcount

        prompt_service.conn.commit()
        return {
            "success": True,
            "prompt_rows": prompt_rows_deleted,
            "prompt_history_rows": prompt_history_rows_deleted,
            "prompt_metrics_rows": prompt_metrics_rows_deleted,
        }

    except Exception as e:
        prompt_service.conn.rollback()
        return {"success": False, "error": str(e)}


def test_prompt_loading(prompt_service: PromptService):
    cmds = prompt_service.load_cmds_from_disk()
    print(f"\nTotal commands loaded: {len(cmds.loaded_prompts)}")
    assert len(cmds.errors) == 0

    plans = prompt_service.load_plans_from_disk()
    print(f"\nTotal plans loaded: {len(plans.loaded_prompts)}")
    assert len(plans.errors) == 0

    prompts = cmds.loaded_prompts + plans.loaded_prompts

    results = prompt_service.bulk_save_in_db(prompts)

    bad_results = [result for result in results if not result.success]
    good_results = [result for result in results if result.success]

    print(f"\nGood Result count: {len(good_results)}")
    print(f"\nBad Result count: {len(bad_results)}")
&lt;/file&gt;
  &lt;file path="repository/prompt_models.py"&gt;from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
from typing import Optional, List
from config import Config


class PromptPlanStatus(str, Enum):
    """Plan status types"""

    DRAFT = "draft"
    APPROVED = "approved"
    COMPLETED = "completed"


class PromptType(str, Enum):
    """Prompt types"""

    PLAN = "plan"
    CMD = "cmd"


def create_cmd_category_enum():
    """Create CmdCategory enum dynamically from config"""
    try:
        config = Config()
        subdirs = config.command_subdirs
    except Exception:
        # Fallback to default subdirs if config fails
        subdirs = ["archive", "go", "js", "mcp", "python", "tools"]

    # Build enum members dictionary
    members = {}
    for subdir in subdirs:
        members[subdir.upper()] = subdir

    # Always include UNCATEGORIZED as fallback
    members["UNCATEGORIZED"] = "uncategorized"

    # Create enum using the functional API with type=str for JSON serialization
    return Enum("CmdCategory", members, type=str)


# Create the enum instance
CmdCategory = create_cmd_category_enum()


class Project(BaseModel):
    github_url: str
    description: str
    created_at: datetime
    updated_at: datetime


class PromptData(BaseModel):
    """Structured data for prompt JSONB field"""

    type: PromptType
    status: PromptPlanStatus
    project: Optional[str]
    cmd_category: Optional[CmdCategory]
    content: str  # This is the prompt content, in markdown
    description: Optional[str] = None
    # using 'claude' or 'gemini' here to specify the dir it will write to
    # .claude/commands and .gemini/commands respectively
    tags: List[str] = Field(default_factory=list)


class Prompt(BaseModel):
    id: str
    name: str
    github_url: Optional[str]
    data: PromptData  # Structured JSONB data
    version: int
    content_hash: str
    created_at: datetime
    updated_at: datetime


class PromptCreate(BaseModel):
    id: str
    name: str
    data: PromptData
    content_hash: str
    version: Optional[int] = 1


class LoadError(BaseModel):
    filename: str
    error_message: str
    error_type: str


class PromptCreateResult(BaseModel):
    """Result of creating a new prompt"""

    success: bool
    prompt_id: str
    version: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptLoadResult(BaseModel):
    """Result of loading prompts into database"""

    loaded_prompts: List[Prompt]
    errors: Optional[List[LoadError]] = None


class PromptDeleteResult(BaseModel):
    success: bool
    prompt_id: str
    deleted: bool
    rows_affected: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptFlattenResult(BaseModel):
    """Result of flattening a prompt to disk"""

    success: bool
    prompt_id: str
    prompt_name: str
    file_path: str
    cmd_category: str
    error_message: Optional[str] = None
    error_type: Optional[str] = None
&lt;/file&gt;
  &lt;file path="repository/datetime_adapters.py"&gt;"""Custom datetime adapters for SQLite3 to avoid Python 3.12 deprecation warnings.

This module provides custom adapters and converters for datetime objects when
working with SQLite databases in Python 3.12+, replacing the deprecated default
adapters.
"""

import datetime
import sqlite3


def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format.

    Args:
        val: datetime.datetime object to adapt

    Returns:
        str: ISO 8601 formatted datetime string
    """
    return val.replace(tzinfo=None).isoformat()


def adapt_date_iso(val):
    """Adapt datetime.date to ISO 8601 date format.

    Args:
        val: datetime.date object to adapt

    Returns:
        str: ISO 8601 formatted date string
    """
    return val.isoformat()


def convert_datetime_iso(val):
    """Convert ISO 8601 datetime string to datetime.datetime object.

    Args:
        val: bytes object containing ISO 8601 datetime string

    Returns:
        datetime.datetime: Parsed datetime object
    """
    return datetime.datetime.fromisoformat(val.decode())


def convert_date_iso(val):
    """Convert ISO 8601 date string to datetime.date object.

    Args:
        val: bytes object containing ISO 8601 date string

    Returns:
        datetime.date: Parsed date object
    """
    return datetime.date.fromisoformat(val.decode())


def convert_timestamp(val):
    """Convert Unix timestamp to datetime.datetime object.

    Args:
        val: bytes object containing Unix timestamp

    Returns:
        datetime.datetime: Datetime object from timestamp
    """
    return datetime.datetime.fromtimestamp(int(val))


def register_adapters():
    """Register all custom datetime adapters and converters with sqlite3.

    This function should be called once at application startup to configure
    SQLite to use our custom datetime handling instead of the deprecated
    default handlers.
    """
    # Register adapters (Python -&amp;gt; SQLite)
    sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
    sqlite3.register_adapter(datetime.date, adapt_date_iso)

    # Register converters (SQLite -&amp;gt; Python)
    sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
    sqlite3.register_converter("DATETIME", convert_datetime_iso)
    sqlite3.register_converter("DATE", convert_date_iso)


# Automatically register adapters when module is imported
register_adapters()
&lt;/file&gt;
  &lt;file path="repository/prompt_service.py"&gt;import sqlite3
from pathlib import Path
import uuid
import json
from datetime import datetime, timezone
import hashlib
from typing import Optional, List, Tuple
from repository.prompt_models import (
    PromptLoadResult,
    LoadError,
    CmdCategory,
    PromptType,
    PromptPlanStatus,
    PromptData,
    Prompt,
    PromptCreateResult,
    PromptDeleteResult,
    PromptFlattenResult,
)
from config import Config


class PromptService:
    def __init__(self, conn: sqlite3.Connection, config: Config):
        self.conn = conn
        self.config = config
        self.plans_check_dirs()
        self.cmd_check_dirs()

    def plans_check_dirs(self) -&amp;gt; bool:
        """Check if all required plan directories exist, create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        # Required directories
        required_dirs = [
            plans_dir,
            plans_dir / "drafts",
            plans_dir / "approved",
            plans_dir / "completed",
        ]

        missing_dirs = []
        created_dirs = []

        for dir_path in required_dirs:
            if not dir_path.exists():
                missing_dirs.append(dir_path)

        if missing_dirs:
            print("📁 Creating missing plan directories:")
            for missing_dir in missing_dirs:
                try:
                    missing_dir.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(missing_dir)
                    print(
                        f"   ✅ Created: {
                            missing_dir.relative_to(project_dir)}"
                    )
                except Exception as e:
                    print(
                        f"   ❌ Failed to create {
                            missing_dir.relative_to(project_dir)}: {e}"
                    )
                    return False

            if created_dirs:
                print(
                    f"📁 Successfully created {
                        len(created_dirs)} directories"
                )
        else:
            print("✅ All required plan directories exist")

        return True

    def cmd_check_dirs(self) -&amp;gt; bool:
        """Check if all required command directories exist,
           create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        claude_dir = project_dir / ".claude"
        gemini_dir = project_dir / ".gemini"

        # Get subdirectories from config
        config = Config()
        subdirs = config.command_subdirs

        # Build required directories
        required_dirs = {
            "claude": [claude_dir / "commands"]
            + [claude_dir / "commands" / subdir for subdir in subdirs],
            "gemini": [gemini_dir / "commands"]
            + [gemini_dir / "commands" / subdir for subdir in subdirs],
        }

        # Check for missing directories by type
        missing_by_type = {"claude": [], "gemini": []}
        for dir_type, dirs in required_dirs.items():
            for dir_path in dirs:
                if not dir_path.exists():
                    missing_by_type[dir_type].append(dir_path)

        # Count total missing
        total_missing = sum(len(dirs) for dirs in missing_by_type.values())

        if total_missing == 0:
            print("✅ All required command directories exist")
            return True

        # Create missing directories
        print(f"📁 Creating {total_missing} missing command directories:")
        created_count = 0
        failed = False

        for dir_type, missing_dirs in missing_by_type.items():
            if missing_dirs:
                print(f"\n   {dir_type.title()} directories:")
                for missing_dir in missing_dirs:
                    try:
                        missing_dir.mkdir(parents=True, exist_ok=True)
                        created_count += 1
                        print(
                            f"   ✅ Created: {
                                missing_dir.relative_to(project_dir)}"
                        )
                    except Exception as e:
                        print(
                            f"   ❌ Failed to create {
                                missing_dir.relative_to(project_dir)}: {e}"
                        )
                        failed = True

        if created_count &amp;gt; 0:
            print(f"\n📁 Successfully created {created_count} directories")

        return not failed

    def _load_cmds_from_directory(
        self, cmds_dir: Path, source: str
    ) -&amp;gt; Tuple[List[Prompt], List[LoadError]]:
        """Load commands from a specific directory

        Args:
            cmds_dir: Path to the commands directory
            source: Source identifier ('claude' or 'gemini')

        Returns:
            Tuple of (prompts list, errors list)
        """
        prompts = []
        errors = []

        if not cmds_dir.exists():
            return prompts, errors

        # Loop through the files in cmds dir and load prompts first
        for file in cmds_dir.iterdir():
            if file.is_file():
                try:
                    # Check if filename adheres to naming rules
                    current_filename = file.name
                    if not self.check_filename(current_filename):
                        # Only rename files during explicit operations, not during loading
                        # Skip file renaming when just loading/reading files
                        print(
                            f"⚠️  File {
                                current_filename} doesn't follow naming convention but will not be renamed during load operation"
                        )

                    prompt_content = file.read_text()
                    prompt = self.new_prompt_model(
                        prompt_content=prompt_content,
                        name=file.name,
                        prompt_type=PromptType.CMD,
                        cmd_category=CmdCategory.UNCATEGORIZED,
                        status=PromptPlanStatus.DRAFT,
                        tags=[source],  # Add source tag
                    )
                    prompts.append(prompt)

                except Exception as e:
                    errors.append(
                        LoadError(
                            filename=str(file),
                            error_message=str(e),
                            error_type=type(e).__name__,
                        )
                    )

        # Then cycle through the subdirs, create Prompt models and append
        for sub_dir in cmds_dir.iterdir():
            if sub_dir.is_dir():
                try:
                    cmd_category = CmdCategory(sub_dir.name.lower())

                    for file in sub_dir.iterdir():
                        try:
                            if file.is_file():
                                # Check if filename adheres to naming rules
                                current_filename = file.name
                                if not self.check_filename(current_filename):
                                    # Normalize the filename
                                    fixed_filename = self.normalize_filename(
                                        current_filename
                                    )

                                    # Create new file path with normalized name
                                    new_file_path = file.parent / fixed_filename

                                    # Rename the file on disk
                                    file.rename(new_file_path)

                                    # Update file reference to the new path
                                    file = new_file_path
                                    print(
                                        f"📝 Renamed: {current_filename} → {
                                            fixed_filename}"
                                    )

                                prompt_content = file.read_text()
                                prompt = self.new_prompt_model(
                                    prompt_content=prompt_content,
                                    name=file.name,
                                    prompt_type=PromptType.CMD,
                                    cmd_category=cmd_category,
                                    status=PromptPlanStatus.DRAFT,
                                    tags=[source],  # Add source tag
                                )
                                prompts.append(prompt)

                        except Exception as e:
                            errors.append(
                                LoadError(
                                    filename=str(file),
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )
                except ValueError:
                    # Skip directories that don't match valid CmdCategory values
                    continue

        return prompts, errors

    def load_cmds_from_disk(self) -&amp;gt; PromptLoadResult:
        """Load commands from both .claude and .gemini directories

        Returns:
            PromptLoadResult: Combined results from both directories
        """
        project_dir = Path(__file__).parent.parent
        claude_cmds_dir = project_dir / ".claude" / "commands"
        gemini_cmds_dir = project_dir / ".gemini" / "commands"

        all_prompts = []
        all_errors = []

        # Load from Claude directory
        claude_prompts, claude_errors = self._load_cmds_from_directory(
            claude_cmds_dir, "claude"
        )
        all_prompts.extend(claude_prompts)
        all_errors.extend(claude_errors)

        # Load from Gemini directory
        gemini_prompts, gemini_errors = self._load_cmds_from_directory(
            gemini_cmds_dir, "gemini"
        )
        all_prompts.extend(gemini_prompts)
        all_errors.extend(gemini_errors)

        return PromptLoadResult(
            loaded_prompts=all_prompts,
            errors=all_errors,
        )

    def load_plans_from_disk(self) -&amp;gt; PromptLoadResult:
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        status_mapping = {
            "drafts": PromptPlanStatus.DRAFT,
            "approved": PromptPlanStatus.APPROVED,
            "completed": PromptPlanStatus.COMPLETED,
        }

        prompts = []
        errors = []

        for subdir in plans_dir.iterdir():
            if subdir.is_dir() and subdir.name in status_mapping:
                cmd_category = None
                status = status_mapping[subdir.name]
                for file in subdir.iterdir():
                    try:
                        if file.is_file():
                            # Check if filename adheres to naming rules
                            current_filename = file.name
                            if not self.check_filename(current_filename):
                                # Normalize the filename
                                fixed_filename = self.normalize_filename(
                                    current_filename
                                )

                                # Create new file path with normalized name
                                new_file_path = file.parent / fixed_filename

                                # Rename the file on disk
                                file.rename(new_file_path)

                                # Update file reference to the new path
                                file = new_file_path
                                print(
                                    f"""📝 Renamed: {current_filename} → {
                                        fixed_filename}
                                      """
                                )

                            prompts.append(
                                self.new_prompt_model(
                                    prompt_content=file.read_text(),
                                    name=file.name,
                                    prompt_type=PromptType.PLAN,
                                    github_url=self.config.github_url,
                                    cmd_category=cmd_category,
                                    status=status,
                                    project=project_dir.name,
                                )
                            )

                    except Exception as e:
                        errors.append(
                            LoadError(
                                filename=str(file),
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

        return PromptLoadResult(loaded_prompts=prompts, errors=errors)

    def normalize_filename(self, filename: str) -&amp;gt; str:
        """Normalize filename to use underscores and ensure .md or .toml extension

        Args:
            filename: The original filename

        Returns:
            Normalized filename with underscores and .md or .toml extension
        """
        # Replace hyphens with underscores
        normalized = filename.replace("-", "_")

        # Check if it already has .md or .toml extension
        if normalized.endswith(".md") or normalized.endswith(".toml"):
            return normalized

        # If it has another extension, replace it with .md
        if "." in normalized:
            normalized = normalized.rsplit(".", 1)[0] + ".md"
        else:
            # No extension, add .md as default
            normalized = normalized + ".md"

        return normalized

    def check_filename(self, filename: str) -&amp;gt; bool:
        """Check if filename adheres to naming rules
        (underscores and .md or .toml extension)

        Args:
            filename: The filename to check

        Returns:
            bool: True if filename follows the rules, False otherwise
        """
        # Check if filename has .md or .toml extension
        if not (filename.endswith(".md") or filename.endswith(".toml")):
            return False

        # Check if filename contains hyphens (should use underscores)
        if "-" in filename:
            return False

        return True

    def new_prompt_model(
        self,
        prompt_content: str,
        name: str,
        prompt_type: PromptType,
        github_url: Optional[str] = None,
        cmd_category: Optional[CmdCategory] = None,
        status: PromptPlanStatus = PromptPlanStatus.DRAFT,
        project: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -&amp;gt; Prompt:
        if prompt_type == PromptType.CMD and not cmd_category:
            raise ValueError("CMD type prompts require a category")

        default_tags = []
        if cmd_category:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                default_tags.append(cmd_category)
            else:
                default_tags.append(cmd_category.value)
        default_tags.append(prompt_type.value)

        # Merge custom tags with default tags
        all_tags = default_tags + (tags if tags else [])

        prompt_data = PromptData(
            type=prompt_type,
            status=status,
            project=project,
            cmd_category=cmd_category,
            content=prompt_content,
            description=description,
            tags=all_tags,
        )

        content_hash = hashlib.sha256(prompt_content.encode("utf-8")).hexdigest()

        timestamp = datetime.now(timezone.utc)

        db_name = self.create_db_name(
            prompt_type=prompt_type,
            prompt_status=status,
            cmd_category=cmd_category,
            project_name=project,
            name=name,
        )

        prompt = Prompt(
            id=str(uuid.uuid4()),
            name=db_name,
            github_url=github_url,
            data=prompt_data,
            version=1,
            content_hash=content_hash,
            created_at=timestamp,
            updated_at=timestamp,
        )

        return prompt

    def create_db_name(
        self,
        prompt_type: PromptType,
        prompt_status: Optional[PromptPlanStatus],
        cmd_category: Optional[CmdCategory],
        project_name: Optional[str],
        name: str,
    ) -&amp;gt; str:
        # in the directory [project]/_docs/plans:
        # there are directories: draft, approved and completed
        # we model those as PromptPlanStatus -&amp;gt; see prompt_models.py
        if prompt_type == PromptType.PLAN:
            create_name = project_name + "_" + prompt_status.value + "_" + name
        if prompt_type == PromptType.CMD:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                create_name = cmd_category + "_" + name
            else:
                create_name = cmd_category.value + "_" + name

        return create_name

    def parse_db_name(self, db_name: str, prompt_type: PromptType) -&amp;gt; str:
        """Extract the original filename from the database name

        Args:
            db_name: The database name
            (e.g., 'collect-approved-update_function.md')
            prompt_type: The type of prompt(PLAN or CMD)

        Returns:
            The original filename(e.g., 'update_function.md')
        """
        # split the name to a list using '_' seperator
        ls = db_name.split("_")
        # rebuild filename from the list of split words
        filename = ""
        if prompt_type == PromptType.PLAN:
            # if prompt type is PLAN: then name will include the project
            # so we need to drop the first 2 words in the db_name
            # example: collect_completed_add_claude_sdk_processing.md
            # ls = [collect, completed, add, claude, sdk, processing.md]
            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

        if prompt_type == PromptType.CMD:
            # if prompt type is CMD: then name will only include the dir/type
            # so we only need to drop the first word in ls
            # example: tools_create_database.md
            # ls = [tools, create, database.md]
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

    def check_exists(self, name: str) -&amp;gt; Tuple[bool, str]:
        """Check if a prompt with the given name already exists

        Args:
            name: The prompt name to check

        Returns:
            Tuple[bool, str]: (exists, prompt_id)
            where exists is True if prompt exists,
            and prompt_id is the ID if found, empty string if not found
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT id FROM prompt WHERE name = ?", (name,))
        result = cursor.fetchone()

        if result:
            return (True, result["id"])  # Found: return True and the prompt ID
        else:
            # Not found: return False and empty string
            return (False, "")

    def save_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Initial prompt creation"
    ) -&amp;gt; PromptCreateResult:
        """Create a new prompt and initialize version history
        if the prompt doesn't exist, if it already exists then
        we call `update_prompt_in_db` with the update.

        Args:
            prompt: Prompt object to create
            change_summary: Description of this change
            (default: "Initial prompt creation")

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            # Validate prompt has required fields
            if not prompt.id or not prompt.name or not prompt.data:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id if prompt.id else "",
                    version=prompt.version if prompt.version else 1,
                    error_message="Prompt missing required fields",
                    error="ValidationError",
                )

            exists, prompt_id = self.check_exists(prompt.name)
            if exists:
                # get prompt from database using prompt_id from the version
                # retrieved from the database
                prompt_from_db = self.get_prompt_by_id(prompt_id)
                # if we don't have a prompt here then we return false
                if prompt_from_db is None:
                    return PromptCreateResult(
                        success=False,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"prompt retrieval failed for {
                            prompt_id}",
                        error="ValueError",
                    )

                # otherwise we have a prompt from the database call
                # and we need to compare hashes to see if there are changes
                if prompt.content_hash == prompt_from_db.content_hash:
                    # if they are the same then the version in the db is the
                    # same as the version on disk so we return success and
                    # do nothing else.
                    return PromptCreateResult(
                        success=True,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"""
                        prompt: {prompt.name} from disk is the same as db
                        """,
                        error="",
                    )

                else:
                    # if we get here then we have changes on disk that are more
                    # current than what is in the database

                    # IMPORTANT: We override the prompt.id here because the
                    # prompt exists already and we don't have a clean way of
                    # storing the uuid with the prompt on disk.
                    # When the prompt model is created from loading from disk,
                    # we DO generate a uuid for the model at that time just in
                    # case the prompt is newly generated from the disk and is
                    # not in the db
                    prompt.id = prompt_from_db.id

                    # Important to note that we will increment the version in
                    # `self.update_prompt_in_db`, we do not increment it here
                    return self.update_prompt_in_db(prompt)

            else:  # prompt doesn't exist in the database
                # if we make it here we have a new prompt and it
                # needs to be saved to the database for the first time
                prompt_jsonb = prompt.data.model_dump_json()

                # Create new cursor for this transaction
                cursor = self.conn.cursor()

                # insert prompt into prompt table
                cursor.execute(
                    """
                    INSERT INTO prompt(
                    id,
                    name,
                    data,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?,?)
                    """,
                    (
                        prompt.id,
                        prompt.name,
                        prompt_jsonb,
                        prompt.version,
                        prompt.content_hash,
                        prompt.created_at,
                        prompt.updated_at,
                        prompt.github_url,
                    ),
                )

                # insert initial version into prompt_history table
                cursor.execute(
                    """
                    INSERT INTO prompt_history(
                    id,
                    version,
                    data,
                    content_hash,
                    created_at,
                    archived_at,
                    change_summary,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                    """,
                    (
                        prompt.id,
                        prompt.version,
                        prompt_jsonb,
                        prompt.content_hash,
                        prompt.created_at,
                        datetime.now(timezone.utc),
                        change_summary,
                        prompt.github_url,
                    ),
                )
                self.conn.commit()

                return PromptCreateResult(
                    success=True, prompt_id=prompt.id, version=prompt.version
                )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def update_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Prompt updated from disk"
    ) -&amp;gt; PromptCreateResult:
        """Update an existing prompt and add to version history

        Args:
            prompt: Prompt object to update
            change_summary: Description of this change

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            cursor = self.conn.cursor()

            # first we get the existing prompt in the database
            current_prompt = self.get_prompt_by_id(prompt.id)
            if not current_prompt:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id,
                    version=prompt.version,
                    error_message=f"Prompt w id {prompt.id} not found",
                    error="NotFoundError",
                )
            # then we increment the version
            prompt.version = current_prompt.version + 1

            # we need to recalculate the hash for the udpated prompt
            # so we can properly compare for changes
            prompt.content_hash = hashlib.sha256(
                prompt.data.content.encode("utf-8")
            ).hexdigest()

            # process the PromptData model to to json
            prompt_jsonb = prompt.data.model_dump_json()

            # then we update the `updated_at` timestamp
            prompt.updated_at = datetime.now(timezone.utc)

            # Update prompt table
            # NOTE: when writing the the jsonb field `data` we use jsonb
            # when reading we use `json(data)`
            cursor.execute(
                """
                UPDATE prompt
                SET name = ?,
                    data = jsonb(?),
                    version = ?,
                    content_hash = ?,
                    updated_at = ?,
                    github_url = ?
                WHERE id = ?
                """,
                (
                    prompt.name,
                    prompt_jsonb,
                    prompt.version,
                    prompt.content_hash,
                    prompt.updated_at,
                    prompt.github_url,
                    prompt.id,
                ),
            )

            # Insert into the updated prompt into prompt_history
            cursor.execute(
                """
                INSERT INTO prompt_history(
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                """,
                (
                    prompt.id,
                    prompt.version,
                    prompt_jsonb,
                    prompt.content_hash,
                    prompt.created_at,
                    datetime.now(timezone.utc),
                    change_summary,
                    prompt.github_url,
                ),
            )

            self.conn.commit()

            return PromptCreateResult(
                success=True, prompt_id=prompt.id, version=prompt.version
            )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def get_prompt_by_id(self, prompt_id: str) -&amp;gt; Optional[Prompt]:
        """Get a prompt by its ID from the database

        Args:
            prompt_id: The ID of the prompt to retrieve

        Returns:
            Optional[Prompt]: The prompt if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE id = ?
            """,
            (prompt_id,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        # Parse the JSONB data back to PromptData
        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        # Create and return the Prompt object
        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def get_prompt_by_name(self, prompt_name: str) -&amp;gt; Optional[Prompt]:
        """
        Get a prompt by name from the database

        Args:
            prompt_name: The name of the prompt to retrieve.
            (should be unique)
        Returns:
            Optional[Prompt]: The prompt if found by name or None otherwise
        """

        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE name = ?
            """,
            (prompt_name,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def delete_prompt_by_id(self, prompt_id: str) -&amp;gt; PromptDeleteResult:
        cursor = self.conn.cursor()
        try:
            # archive final state in prompt_history table before deletion
            # we will not be deleting the version history of the prompt
            cursor.execute(
                """
                INSERT INTO prompt_history (
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                SELECT id, version, data, content_hash, created_at, ?, ?, github_url
                FROM prompt WHERE id = ?
            """,
                (datetime.now(timezone.utc), "DELETED - Final Version", prompt_id),
            )

            # Delete only from the prompt table
            cursor.execute("DELETE FROM prompt WHERE id = ?", (prompt_id,))
            deleted_row_count = cursor.rowcount

            self.conn.commit()

            return PromptDeleteResult(
                success=True,
                prompt_id=prompt_id,
                deleted=True,
                rows_affected=deleted_row_count,
            )

        except Exception as e:
            self.conn.rollback()
            return PromptDeleteResult(
                success=False,
                prompt_id=prompt_id,
                deleted=False,
                rows_affected=0,
                error_message=str(e),
                error_type=type(e).__name__,
            )

    def bulk_save_in_db(self, prompts: List[Prompt]) -&amp;gt; List[PromptCreateResult]:
        """
        Bulk load/save prompts into the database

        Args:
            plans: List of Plan objects to load into database

        Returns:
            PlanLoadResult: Summary of the loading operation
        """

        return [self.save_prompt_in_db(prompt) for prompt in prompts]

    def flatten_cmds_to_disk(self) -&amp;gt; List[PromptFlattenResult]:
        """Flatten all cmd_category prompts from database to disk directories

        Queries all CMD type prompts from database and writes them to:
        - .claude/commands/{category}/{filename}
        - .gemini/commands/{category}/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure command directories exist
            if not self.cmd_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create command directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all CMD type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;gt;&amp;gt; '$.type' = 'cmd'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No CMD prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    # `**` unpacks the dictionary into key words for pydantic
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.CMD)

                    # Get category, handle None/uncategorized case
                    if prompt.data.cmd_category:
                        if isinstance(prompt.data.cmd_category, str):
                            category = prompt.data.cmd_category
                        else:
                            category = prompt.data.cmd_category.value
                    else:
                        category = "uncategorized"

                    # Determine target directory based on tags
                    target_dirs = []
                    if prompt.data.tags:
                        if "claude" in prompt.data.tags:
                            target_dirs.append("claude")
                        if "gemini" in prompt.data.tags:
                            target_dirs.append("gemini")

                    # If no source tags found, skip this prompt
                    if not target_dirs:
                        errmsg = "No source tag (claude/gemini) found in tags"
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category=category,
                                error_message=errmsg,
                                error_type="MissingSourceTag",
                            )
                        )
                        continue

                    # Write to appropriate directories based on source tags
                    for target_dir in target_dirs:
                        try:
                            target_path = (
                                project_dir
                                / f".{target_dir}"
                                / "commands"
                                / category
                                / filename
                            )

                            # Ensure parent directory exists
                            target_path.parent.mkdir(parents=True, exist_ok=True)

                            # Write content to file
                            target_path.write_text(
                                prompt.data.content, encoding="utf-8"
                            )

                            results.append(
                                PromptFlattenResult(
                                    success=True,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=str(target_path),
                                    cmd_category=category,
                                    error_message="",
                                    error_type="",
                                )
                            )

                        except Exception as e:
                            results.append(
                                PromptFlattenResult(
                                    success=False,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=(
                                        str(target_path)
                                        if "target_path" in locals()
                                        else ""
                                    ),
                                    cmd_category=category,
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results

    def flatten_plans_to_disk(self) -&amp;gt; List[PromptFlattenResult]:
        """Flatten all plan prompts from database to disk directories

        Queries all PLAN type prompts from database and writes them to:
        - _docs/plans/drafts/{filename}
        - _docs/plans/approved/{filename}
        - _docs/plans/completed/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure plan directories exist
            if not self.plans_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create plan directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all PLAN type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&amp;gt;&amp;gt; '$.type' = 'plan'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No PLAN prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            # Status to directory mapping
            status_dir_mapping = {
                PromptPlanStatus.DRAFT.value: "drafts",
                PromptPlanStatus.APPROVED.value: "approved",
                PromptPlanStatus.COMPLETED.value: "completed",
            }

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Validate project name for PLAN type prompts
                    if not prompt.data.project:
                        # PLAN type must have a project name
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category="",
                                error_message="PLAN type prompt missing required project name",
                                error_type="MissingProjectError",
                            )
                        )
                        continue

                    # TODO: update this to use coordinate the github_url
                    if prompt.data.project != project_dir.name:
                        # Skip this prompt - it belongs to a different project
                        continue

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.PLAN)

                    # Get status directory
                    status_value = (
                        prompt.data.status.value
                        if hasattr(prompt.data.status, "value")
                        else str(prompt.data.status)
                    )
                    status_dir = status_dir_mapping.get(status_value, "drafts")

                    # Write to appropriate status directory
                    try:
                        target_path = (
                            project_dir / "_docs" / "plans" / status_dir / filename
                        )

                        # Ensure parent directory exists
                        target_path.parent.mkdir(parents=True, exist_ok=True)

                        # Write content to file
                        target_path.write_text(prompt.data.content, encoding="utf-8")

                        results.append(
                            PromptFlattenResult(
                                success=True,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=str(target_path),
                                cmd_category=status_dir,
                                error_message="",
                                error_type="",
                            )
                        )

                    except Exception as e:
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=(
                                    str(target_path)
                                    if "target_path" in locals()
                                    else ""
                                ),
                                cmd_category=status_dir,
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results
&lt;/file&gt;
  &lt;file path="repository/test_datetime_adapters.py"&gt;"""Test the custom datetime adapters for SQLite3 compatibility."""

import pytest
import warnings
from datetime import datetime, date
from repository.database import SQLite3Database
from repository import datetime_adapters


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db_path = ":memory:"  # Use in-memory database for tests
    db = SQLite3Database(db_path)

    # Create test table
    with db.get_connection() as conn:
        conn.execute(
            """
            CREATE TABLE test_dates (
                id INTEGER PRIMARY KEY,
                created_at TIMESTAMP,
                updated_at DATETIME,
                date_only DATE,
                description TEXT
            )
        """
        )
        yield conn


def test_datetime_storage_retrieval(test_db):
    """Test that datetime objects can be stored and retrieved without warnings."""

    # Capture warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Test data
        test_datetime = datetime(2025, 1, 13, 10, 30, 45)
        test_date = date(2025, 1, 13)

        # Insert test data
        test_db.execute(
            """
            INSERT INTO test_dates (created_at, updated_at, date_only, description)
            VALUES (?, ?, ?, ?)
        """,
            (test_datetime, test_datetime, test_date, "Test record"),
        )

        test_db.commit()

        # Retrieve data
        cursor = test_db.execute(
            """
            SELECT created_at, updated_at, date_only, description 
            FROM test_dates WHERE id = 1
        """
        )
        row = cursor.fetchone()

        # Verify no deprecation warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert (
            len(deprecation_warnings) == 0
        ), f"Found deprecation warnings: {[str(dw.message) for dw in deprecation_warnings]}"

        # Verify data integrity
        assert isinstance(row["created_at"], datetime)
        assert isinstance(row["updated_at"], datetime)
        assert isinstance(row["date_only"], date)
        assert row["created_at"] == test_datetime
        assert row["updated_at"] == test_datetime
        assert row["date_only"] == test_date


def test_datetime_iso_format(test_db):
    """Test that datetimes are stored in ISO format."""

    test_datetime = datetime(2025, 1, 13, 14, 30, 45)

    # Insert using our adapter
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, description)
        VALUES (?, ?)
    """,
        (test_datetime, "ISO format test"),
    )
    test_db.commit()

    # Read raw value (bypass converter)
    cursor = test_db.execute(
        """
        SELECT CAST(created_at AS TEXT) as raw_datetime 
        FROM test_dates WHERE description = 'ISO format test'
    """
    )
    row = cursor.fetchone()

    # Verify ISO format
    expected_iso = "2025-01-13T14:30:45"
    assert row["raw_datetime"] == expected_iso


def test_adapter_functions_directly():
    """Test adapter and converter functions directly."""

    # Test datetime adapter
    test_dt = datetime(2025, 1, 13, 10, 30, 45, 123456)
    adapted = datetime_adapters.adapt_datetime_iso(test_dt)
    assert adapted == "2025-01-13T10:30:45.123456"

    # Test date adapter
    test_date = date(2025, 1, 13)
    adapted_date = datetime_adapters.adapt_date_iso(test_date)
    assert adapted_date == "2025-01-13"

    # Test datetime converter
    iso_bytes = b"2025-01-13T10:30:45.123456"
    converted = datetime_adapters.convert_datetime_iso(iso_bytes)
    assert converted == test_dt

    # Test date converter
    date_bytes = b"2025-01-13"
    converted_date = datetime_adapters.convert_date_iso(date_bytes)
    assert converted_date == test_date

    # Test timestamp converter
    timestamp_bytes = b"1736765445"  # Unix timestamp for 2025-01-13 10:30:45 UTC
    converted_ts = datetime_adapters.convert_timestamp(timestamp_bytes)
    # Note: This will be in local timezone
    assert isinstance(converted_ts, datetime)


def test_timezone_naive_handling():
    """Test that timezone info is properly stripped."""

    # Create timezone-aware datetime
    from datetime import timezone

    tz_aware = datetime(2025, 1, 13, 10, 30, 45, tzinfo=timezone.utc)

    # Adapt should strip timezone
    adapted = datetime_adapters.adapt_datetime_iso(tz_aware)
    assert adapted == "2025-01-13T10:30:45"
    assert "+00:00" not in adapted  # No timezone offset in output


def test_multiple_datetime_operations(test_db):
    """Test multiple datetime operations to ensure no warnings."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Multiple inserts
        for i in range(5):
            dt = datetime.now()
            test_db.execute(
                """
                INSERT INTO test_dates (created_at, updated_at, description)
                VALUES (?, ?, ?)
            """,
                (dt, dt, f"Record {i}"),
            )

        test_db.commit()

        # Multiple selects
        cursor = test_db.execute("SELECT * FROM test_dates")
        rows = cursor.fetchall()

        # Verify all datetimes are properly converted
        for row in rows:
            if row["created_at"]:
                assert isinstance(row["created_at"], datetime)
            if row["updated_at"]:
                assert isinstance(row["updated_at"], datetime)

        # Check for warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert len(deprecation_warnings) == 0


def test_null_datetime_handling(test_db):
    """Test that NULL datetime values are handled correctly."""

    # Insert NULL values
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, updated_at, date_only, description)
        VALUES (NULL, NULL, NULL, 'Null test')
    """
    )
    test_db.commit()

    # Retrieve NULL values
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at, date_only 
        FROM test_dates WHERE description = 'Null test'
    """
    )
    row = cursor.fetchone()

    # Verify NULLs are preserved
    assert row["created_at"] is None
    assert row["updated_at"] is None
    assert row["date_only"] is None


def test_backwards_compatibility(test_db):
    """Test that existing ISO format strings are still readable."""

    # Manually insert ISO format strings (simulating old data)
    test_db.execute(
        """
        INSERT INTO test_dates (id, created_at, updated_at, description)
        VALUES (100, '2024-12-01T10:30:45', '2024-12-01T10:30:45', 'Old format')
    """
    )
    test_db.commit()

    # Read with our converters
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at 
        FROM test_dates WHERE id = 100
    """
    )
    row = cursor.fetchone()

    # Verify conversion works
    assert isinstance(row["created_at"], datetime)
    assert row["created_at"].year == 2024
    assert row["created_at"].month == 12
    assert row["created_at"].day == 1
    assert row["created_at"].hour == 10
    assert row["created_at"].minute == 30
    assert row["created_at"].second == 45
&lt;/file&gt;
  &lt;file path=".gemini/settings.json"&gt;{
	"mcpServers": {
		"collect": {
			"command": "uv",
			"args": [
				"run",
				"python",
				"collect.py"
			],
			"workingDirectory": "/Users/benjaminmetz/python/collect",
			"enabled": true
		}
	}
}
&lt;/file&gt;
  &lt;file path="dotfiles/.zshrc"&gt;GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"

export EDITOR="nvim"
export VISUAL="nvim"
export PATH="$PATH:$HOME/.local/bin"

# path to sqlite3
export PATH="/opt/homebrew/opt/sqlite/bin:$PATH"

# path to ripgrep
export PATH="$HOME/opt/homebrew/bin/rg:$PATH"

#python uv path
. "$HOME/.local/bin/env"

# path to scripts and zig and stuff
export PATH="$HOME/bin:$PATH"

# path to npm
export PATH="$(npm config get prefix)/bin:$PATH"
# path to zig
export PATH=$PATH:~/bin/zig/
export PATH="$(brew --prefix coreutils)/libexec/gnubin:$PATH"

# dependencies needed for gemini / google token processing
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$PKG_CONFIG_PATH"
export PATH="$(brew --prefix)/bin:$PATH"
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$(brew --prefix protobuf)/lib/pkgconfig:$PKG_CONFIG_PATH"

# shortcuts to project work
alias gowork='cd $HOME/go/src/github.com/metzben &amp;amp;&amp;amp; ls -lhG'
alias py='cd $HOME/python &amp;amp;&amp;amp; ls -l --color'
alias collect='cd $HOME/python/collect &amp;amp;&amp;amp; source .venv/bin/activate'
alias el='cd $HOME/go/src/github.com/metzben/elephnt &amp;amp;&amp;amp; ls -l --color'
alias tiny='cd $HOME/go/src/github.com/metzben/tinystack &amp;amp;&amp;amp; ls -lhG'
alias ai='cd $HOME/python/aiwork &amp;amp;&amp;amp; ls -lhG'
alias mcp='cd $HOME/python/mcpwork &amp;amp;&amp;amp; ls -lhG'
alias base='cd $HOME/base &amp;amp;&amp;amp; nvim .'
alias fta='cd $HOME/python/fastta &amp;amp;&amp;amp; nvim .'
alias indicators='cd $HOME/python/indicators &amp;amp;&amp;amp; ls -l'
alias mcpstart='cd $HOME/python/startermcp &amp;amp;&amp;amp; ls -l'
alias tools='cd ~/bin &amp;amp;&amp;amp; ls -l --color'
alias plans='cd _docs/plans &amp;amp;&amp;amp; tree -C -L 2'

# Database function - only works in collect directory
db() {
    if [[ "$PWD" == *"/collect" ]] || [[ "$PWD" == *"/collect/"* ]]; then
        sqlite3 data/collect.db
    else
        echo "Not in collect directory. This command only works in the collect project."
    fi
}

# claude ai shortcuts
alias ask='claude -p '
alias editmcp='nvim ~/Library/Application\ Support/Claude/claude_desktop_config.json'
alias rip='claude --dangerously-skip-permissions'
alias cmds='cd "$(git rev-parse --show-toplevel)/.claude/commands" &amp;amp;&amp;amp; ls -l --color'
alias gms='cd "$(git rev-parse --show-toplevel)/.gemini/commands" &amp;amp;&amp;amp; ls -l --color'

# git shortcuts
alias gs='git status'
alias gd='git diff --staged'
alias gc='git commit -m '
alias push='git push origin main'
alias ga='git add '
alias gb='git branch'
alias gwl='git worktree list'
alias rebase='git pull --rebase origin main'
alias pull='git pull origin main'

# Worktree navigation functions
cd1() {
    local project_name=$(basename "$(pwd)")
    local wt1_path="../${project_name}-wt1"
    
    if [[ -d "$wt1_path" ]]; then
        cd "$wt1_path"
        echo "Changed to worktree 1: $(pwd)"
    else
        echo "Worktree 1 not found: $wt1_path"
        echo "Run 'trees' to create worktrees first."
    fi
}

cd2() {
    local project_name=$(basename "$(pwd)")
    local wt2_path="../${project_name}-wt2"
    
    if [[ -d "$wt2_path" ]]; then
        cd "$wt2_path"
        echo "Changed to worktree 2: $(pwd)"
    else
        echo "Worktree 2 not found: $wt2_path"
        echo "Run 'trees' to create worktrees first."
    fi
}


checkport() {
    if [ -z "$1" ]; then
        echo "Usage: checkport &amp;lt;port_number&amp;gt;"
        return 1
    fi
    
    if lsof -i :$1 2&amp;gt;/dev/null; then
        echo "Port $1 is in use"
    else
        echo "Port $1 is available"
    fi
}

# uv shortcuts
alias env='source .venv/bin/activate'
alias da='deactivate'
alias ipy='uv run ipython'

# go shortcuts
alias run='go test -v -run'

# config shortcuts
alias src='source ~/.zshrc'
alias openz='nvim ~/.zshrc'
alias initlua='nvim $HOME/.config/nvim/init.lua'
alias ghconf='nvim $HOME/.config/ghostty/config'
alias oc='cursor .'

# misc shorty's
alias ll='ls -l --color'
alias tll='tree -C -L 2'
alias oc='cursor .'
alias onv='nvim .'
alias runz='zig run src/main.zig'
alias cperr="zig run src/main.zig 2&amp;gt;&amp;amp;1 | tee /dev/tty | awk '/error:/{found=1} found {print}' | pbcopy"

# ollama models
alias deep70='ollama run deepseek-r1:70b'
alias llama70='ollama run llama3.3'

# The next line updates PATH for the Google Cloud SDK.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc'; fi

# The next line enables shell command completion for gcloud.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc'; fi

alias auth='gcloud auth login'
alias auth2='gcloud auth application-default login'

export PS1='b@m %~ % '



# opencode
export PATH=/Users/benjaminmetz/.opencode/bin:$PATH
&lt;/file&gt;
  &lt;file path="dotfiles/nvim/init.lua"&gt;vim.opt.clipboard = "unnamedplus"

if vim.g.vscode then
	return
end

-- Set &amp;lt;space&amp;gt; as the leader key
-- See `:help mapleader`
--  NOTE: Must happen before plugins are loaded (otherwise wrong leader will be used)
vim.g.mapleader = " "
vim.g.maplocalleader = " "

-- have neovim honor the terminal opacity
vim.opt.termguicolors = true
--vim.cmd.colorscheme("catppuccin-mocha")

vim.cmd([[highlight Normal ctermbg=none guibg=none]])

-- Set to true if you have a Nerd Font installed and selected in the terminal
vim.g.have_nerd_font = true

-- [[ Setting options ]]
-- See `:help vim.opt`
-- NOTE: You can change these options as you wish!
--  For more options, you can see `:help option-list`

-- Make line numbers default
vim.opt.number = true
-- You can also add relative line numbers, to help with jumping.
--  Experiment for yourself to see if you like it!
vim.opt.relativenumber = true

-- Enable mouse mode, can be useful for resizing splits for example!
vim.opt.mouse = "a"

-- Don't show the mode, since it's already in the status line
vim.opt.showmode = true

-- Sync clipboard between OS and Neovim.
--  Schedule the setting after `UiEnter` because it can increase startup-time.
--  Remove this option if you want your OS clipboard to remain independent.
--  See `:help 'clipboard'`
vim.schedule(function()
	vim.opt.clipboard = "unnamedplus"
end)

-- Enable break indent
vim.opt.breakindent = true

-- Save undo history
vim.opt.undofile = true

-- Case-insensitive searching UNLESS \C or one or more capital letters in the search term
vim.opt.ignorecase = true
vim.opt.smartcase = true

-- Keep signcolumn on by default
vim.opt.signcolumn = "yes"

-- Decrease update time
vim.opt.updatetime = 250

-- Decrease mapped sequence wait time
-- Displays which-key popup sooner
vim.opt.timeoutlen = 300

-- Configure how new splits should be opened
vim.opt.splitright = true
vim.opt.splitbelow = true

-- Sets how neovim will display certain whitespace characters in the editor.
--  See `:help 'list'`
--  and `:help 'listchars'`
vim.opt.list = true
vim.opt.listchars = { tab = "» ", trail = "·", nbsp = "␣" }

-- Preview substitutions live, as you type!
vim.opt.inccommand = "split"

-- Show which line your cursor is on
vim.opt.cursorline = true

-- Minimal number of screen lines to keep above and below the cursor.
vim.opt.scrolloff = 15

vim.o.foldmethod = "expr"
vim.o.foldexpr = "nvim_treesitter#foldexpr()"
vim.o.foldenable = true
vim.o.foldlevel = 99 -- Keep folds open by default

--vim.cmd([[packadd packer.nvim]])

-- [[ Basic Keymaps ]]
--  See `:help vim.keymap.set()`

-- Clear highlights on search when pressing &amp;lt;Esc&amp;gt; in normal mode
--  See `:help hlsearch`
vim.keymap.set("n", "&amp;lt;Esc&amp;gt;", "&amp;lt;cmd&amp;gt;nohlsearch&amp;lt;CR&amp;gt;")
-- when in insert mode pressing j and j again will &amp;lt;Esc&amp;gt;'
vim.keymap.set("i", "&amp;lt;D-j&amp;gt;", "&amp;lt;Esc&amp;gt;", { noremap = true, silent = true })
vim.keymap.set("n", "a", "A", { noremap = true, silent = true })
vim.keymap.set("n", "4", "$", { noremap = true, silent = true })

-- Diagnostic keymaps
vim.keymap.set("n", "&amp;lt;leader&amp;gt;q", vim.diagnostic.setloclist, { desc = "Open diagnostic [Q]uickfix list" })
vim.keymap.set("n", "[d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })
vim.keymap.set("n", "]d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })

-- for people to discover. Otherwise, you normally need to press &amp;lt;C-\&amp;gt;&amp;lt;C-n&amp;gt;, which
-- is not what someone will guess without a bit more experience.
--
-- NOTE: This won't work in all terminal emulators/tmux/etc. Try your own mapping
-- or just use &amp;lt;C-\&amp;gt;&amp;lt;C-n&amp;gt; to exit terminal mode
vim.keymap.set("t", "&amp;lt;Esc&amp;gt;&amp;lt;Esc&amp;gt;", "&amp;lt;C-\\&amp;gt;&amp;lt;C-n&amp;gt;", { desc = "Exit terminal mode" })

-- Keybinds to make split navigation easier.
-- Use CTRL+&amp;lt;hjkl&amp;gt; to switch between windows
--
--  See `:help wincmd` for a list of all window commands
vim.keymap.set("n", "&amp;lt;C-h&amp;gt;", "&amp;lt;C-w&amp;gt;&amp;lt;C-h&amp;gt;", { desc = "Move focus to the left window" })
vim.keymap.set("n", "&amp;lt;C-l&amp;gt;", "&amp;lt;C-w&amp;gt;&amp;lt;C-l&amp;gt;", { desc = "Move focus to the right window" })
vim.keymap.set("n", "&amp;lt;C-j&amp;gt;", "&amp;lt;C-w&amp;gt;&amp;lt;C-j&amp;gt;", { desc = "Move focus to the lower window" })
vim.keymap.set("n", "&amp;lt;C-k&amp;gt;", "&amp;lt;C-w&amp;gt;&amp;lt;C-k&amp;gt;", { desc = "Move focus to the upper window" })

vim.api.nvim_create_autocmd("BufEnter", {
	pattern = "$HOME/go/src/github.com/metzben/*",
	callback = function()
		vim.cmd("colorscheme onedark")
	end,
})

-- Ensure Packer is loaded
vim.cmd([[packadd packer.nvim]])

-- [[ Basic Autocommands ]]
--  See `:help lua-guide-autocommands`

-- Highlight when yanking (copying) text
--  Try it with `yap` in normal mode
--  See `:help vim.highlight.on_yank()`
vim.api.nvim_create_autocmd("TextYankPost", {
	desc = "Highlight when yanking (copying) text",
	group = vim.api.nvim_create_augroup("kickstart-highlight-yank", { clear = true }),
	callback = function()
		vim.highlight.on_yank()
	end,
})

-- [[ Install `lazy.nvim` plugin manager ]]
--    See `:help lazy.nvim.txt` or https://github.com/folke/lazy.nvim for more info
local lazypath = vim.fn.stdpath("data") .. "/lazy/lazy.nvim"
if not (vim.uv or vim.loop).fs_stat(lazypath) then
	local lazyrepo = "https://github.com/folke/lazy.nvim.git"
	local out = vim.fn.system({ "git", "clone", "--filter=blob:none", "--branch=stable", lazyrepo, lazypath })
	if vim.v.shell_error ~= 0 then
		error("Error cloning lazy.nvim:\n" .. out)
	end
end ---@diagnostic disable-next-line: undefined-field
vim.opt.rtp:prepend(lazypath)

-- [[ Configure and install plugins ]]
--
--  To check the current status of your plugins, run
--    :Lazy
--
--  You can press `?` in this menu for help. Use `:q` to close the window
--
--  To update plugins you can run
--    :Lazy update
--
-- NOTE: Here is where you install your plugins.
require("lazy").setup({
	-- NOTE: Plugins can be added with a link (or for a github repo: 'owner/repo' link).
	"tpope/vim-sleuth", -- Detect tabstop and shiftwidth automatically

	-- NOTE: Plugins can also be added by using a table,
	-- with the first argument being the link and the following
	-- keys can be used to configure plugin behavior/loading/etc.
	--
	-- Use `opts = {}` to force a plugin to be loaded.
	--

	-- Here is a more advanced example where we pass configuration
	-- options to `gitsigns.nvim`. This is equivalent to the following Lua:
	--    require('gitsigns').setup({ ... })
	--
	-- See `:help gitsigns` to understand what the configuration keys do
	{ -- Adds git related signs to the gutter, as well as utilities for managing changes
		"lewis6991/gitsigns.nvim",
		opts = {
			signs = {
				add = { text = "+" },
				change = { text = "~" },
				delete = { text = "_" },
				topdelete = { text = "‾" },
				changedelete = { text = "~" },
			},
		},
	},

	-- NOTE: Plugins can also be configured to run Lua code when they are loaded.
	--
	-- This is often very useful to both group configuration, as well as handle
	-- lazy loading plugins that don't need to be loaded immediately at startup.
	--
	-- For example, in the following configuration, we use:
	--  event = 'VimEnter'
	--
	-- which loads which-key before all the UI elements are loaded. Events can be
	-- normal autocommands events (`:help autocmd-events`).
	--
	-- Then, because we use the `opts` key (recommended), the configuration runs
	-- after the plugin has been loaded as `require(MODULE).setup(opts)`.

	{ -- Useful plugin to show you pending keybinds.
		"folke/which-key.nvim",
		event = "VimEnter", -- Sets the loading event to 'VimEnter'
		opts = {
			icons = {
				-- set icon mappings to true if you have a Nerd Font
				mappings = vim.g.have_nerd_font,
				-- If you are using a Nerd Font: set icons.keys to an empty table which will use the
				-- default which-key.nvim defined Nerd Font icons, otherwise define a string table
				keys = vim.g.have_nerd_font and {} or {
					Up = "&amp;lt;Up&amp;gt; ",
					Down = "&amp;lt;Down&amp;gt; ",
					Left = "&amp;lt;Left&amp;gt; ",
					Right = "&amp;lt;Right&amp;gt; ",
					C = "&amp;lt;C-…&amp;gt; ",
					M = "&amp;lt;M-…&amp;gt; ",
					D = "&amp;lt;D-…&amp;gt; ",
					S = "&amp;lt;S-…&amp;gt; ",
					CR = "&amp;lt;CR&amp;gt; ",
					Esc = "&amp;lt;Esc&amp;gt; ",
					ScrollWheelDown = "&amp;lt;ScrollWheelDown&amp;gt; ",
					ScrollWheelUp = "&amp;lt;ScrollWheelUp&amp;gt; ",
					NL = "&amp;lt;NL&amp;gt; ",
					BS = "&amp;lt;BS&amp;gt; ",
					Space = "&amp;lt;Space&amp;gt; ",
					Tab = "&amp;lt;Tab&amp;gt; ",
					F1 = "&amp;lt;F1&amp;gt;",
					F2 = "&amp;lt;F2&amp;gt;",
					F3 = "&amp;lt;F3&amp;gt;",
					F4 = "&amp;lt;F4&amp;gt;",
					F5 = "&amp;lt;F5&amp;gt;",
					F6 = "&amp;lt;F6&amp;gt;",
					F7 = "&amp;lt;F7&amp;gt;",
					F8 = "&amp;lt;F8&amp;gt;",
					F9 = "&amp;lt;F9&amp;gt;",
					F10 = "&amp;lt;F10&amp;gt;",
					F11 = "&amp;lt;F11&amp;gt;",
					F12 = "&amp;lt;F12&amp;gt;",
				},
			},

			-- Document existing key chains
			spec = {
				{ "&amp;lt;leader&amp;gt;c", group = "[C]ode", mode = { "n", "x" } },
				{ "&amp;lt;leader&amp;gt;d", group = "[D]ocument" },
				{ "&amp;lt;leader&amp;gt;r", group = "[R]ename" },
				{ "&amp;lt;leader&amp;gt;s", group = "[S]earch" },
				{ "&amp;lt;leader&amp;gt;w", group = "[W]orkspace" },
				{ "&amp;lt;leader&amp;gt;t", group = "[T]oggle" },
				{ "&amp;lt;leader&amp;gt;h", group = "Git [H]unk", mode = { "n", "v" } },
			},
		},
	},

	-- NOTE: Plugins can specify dependencies.
	--
	-- The dependencies are proper plugin specifications as well - anything
	-- you do for a plugin at the top level, you can do for a dependency.
	--
	-- Use the `dependencies` key to specify the dependencies of a particular plugin

	{ -- Fuzzy Finder (files, lsp, etc)
		"nvim-telescope/telescope.nvim",
		event = "VimEnter",
		branch = "0.1.x",
		dependencies = {
			"nvim-lua/plenary.nvim",
			{ -- If encountering errors, see telescope-fzf-native README for installation instructions
				"nvim-telescope/telescope-fzf-native.nvim",

				-- `build` is used to run some command when the plugin is installed/updated.
				-- This is only run then, not every time Neovim starts up.
				build = "make",

				-- `cond` is a condition used to determine whether this plugin should be
				-- installed and loaded.
				cond = function()
					return vim.fn.executable("make") == 1
				end,
			},
			{ "nvim-telescope/telescope-ui-select.nvim" },

			-- Useful for getting pretty icons, but requires a Nerd Font.
			{ "nvim-tree/nvim-web-devicons", enabled = vim.g.have_nerd_font },
		},
		config = function()
			-- Telescope is a fuzzy finder that comes with a lot of different things that
			-- it can fuzzy find! It's more than just a "file finder", it can search
			-- many different aspects of Neovim, your workspace, LSP, and more!
			--
			-- The easiest way to use Telescope, is to start by doing something like:
			--  :Telescope help_tags
			--
			-- After running this command, a window will open up and you're able to
			-- type in the prompt window. You'll see a list of `help_tags` options and
			-- a corresponding preview of the help.
			--
			-- Two important keymaps to use while in Telescope are:
			--  - Insert mode: &amp;lt;c-/&amp;gt;
			--  - Normal mode: ?
			--
			-- This opens a window that shows you all of the keymaps for the current
			-- Telescope picker. This is really useful to discover what Telescope can
			-- do as well as how to actually do it!

			-- [[ Configure Telescope ]]
			-- See `:help telescope` and `:help telescope.setup()`
			require("telescope").setup({
				-- You can put your default mappings / updates / etc. in here
				--  All the info you're looking for is in `:help telescope.setup()`
				--
				-- defaults = {
				--   mappings = {
				--     i = { ['&amp;lt;c-enter&amp;gt;'] = 'to_fuzzy_refine' },
				--   },
				-- },
				-- pickers = {}
				extensions = {
					["ui-select"] = {
						require("telescope.themes").get_dropdown(),
					},
				},
			})

			-- Enable Telescope extensions if they are installed
			pcall(require("telescope").load_extension, "fzf")
			pcall(require("telescope").load_extension, "ui-select")

			-- See `:help telescope.builtin`
			local builtin = require("telescope.builtin")
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sh", builtin.help_tags, { desc = "[S]earch [H]elp" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sk", builtin.keymaps, { desc = "[S]earch [K]eymaps" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sf", function()
				builtin.find_files({ hidden = true, no_ignore = true })
			end, { desc = "[S]earch [F]iles" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;ss", builtin.builtin, { desc = "[S]earch [S]elect Telescope" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sw", builtin.grep_string, { desc = "[S]earch current [W]ord" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sg", builtin.live_grep, { desc = "[S]earch by [G]rep" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sd", builtin.diagnostics, { desc = "[S]earch [D]iagnostics" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sr", builtin.resume, { desc = "[S]earch [R]esume" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;s.", builtin.oldfiles, { desc = '[S]earch Recent Files ("." for repeat)' })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;&amp;lt;leader&amp;gt;", builtin.buffers, { desc = "[ ] Find existing buffers" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;af", builtin.current_buffer_fuzzy_find, { desc = "[S]earch in existing file" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;nf", "]m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;pf", "[m", { noremap = true, silent = true })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;r", vim.lsp.buf.rename, { desc = "LSP Rename" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;d", "&amp;lt;cmd&amp;gt;Telescope diagnostics&amp;lt;CR&amp;gt;")
			vim.api.nvim_set_keymap("n", "&amp;lt;leader&amp;gt;c", "~", { noremap = true, silent = true })
			vim.keymap.set(
				"n",
				"&amp;lt;leader&amp;gt;O",
				":put! _&amp;lt;CR&amp;gt;",
				{ desc = "Add blank line above without entering edit mode" }
			)

			vim.keymap.set("v", "&amp;lt;leader&amp;gt;/", ":norm I//&amp;lt;CR&amp;gt;", { desc = "Comment selected block" })
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;/", "gcc", { desc = "Toggle comment on current line" })
			-- Slightly advanced example of overriding default behavior and theme
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;[", function()
				-- You can pass additional configuration to Telescope to change the theme, layout, etc.
				builtin.current_buffer_fuzzy_find(require("telescope.themes").get_dropdown({
					winblend = 10,
					previewer = false,
				}))
			end, { desc = "[/] Fuzzily search in current buffer" })

			-- It's also possible to pass additional configuration options.
			--  See `:help telescope.builtin.live_grep()` for information about particular keys
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;s/", function()
				builtin.live_grep({
					grep_open_files = true,
					prompt_title = "Live Grep in Open Files",
				})
			end, { desc = "[S]earch [/] in Open Files" })

			-- Shortcut for searching your Neovim configuration files
			vim.keymap.set("n", "&amp;lt;leader&amp;gt;sn", function()
				builtin.find_files({ cwd = vim.fn.stdpath("config") })
			end, { desc = "[S]earch [N]eovim files" })
		end,
	},

	-- LSP Plugins
	{
		-- `lazydev` configures Lua LSP for your Neovim config, runtime and plugins
		-- used for completion, annotations and signatures of Neovim apis
		"folke/lazydev.nvim",
		ft = "lua",
		opts = {
			library = {
				-- Load luvit types when the `vim.uv` word is found
				{ path = "luvit-meta/library", words = { "vim%.uv" } },
			},
		},
	},
	{ "Bilal2453/luvit-meta", lazy = true },
	{
		-- Main LSP Configuration
		"neovim/nvim-lspconfig",
		dependencies = {
			-- Automatically install LSPs and related tools to stdpath for Neovim
			{ "williamboman/mason.nvim", config = true }, -- NOTE: Must be loaded before dependants
			"williamboman/mason-lspconfig.nvim",
			"WhoIsSethDaniel/mason-tool-installer.nvim",

			-- Useful status updates for LSP.
			-- NOTE: `opts = {}` is the same as calling `require('fidget').setup({})`
			{ "j-hui/fidget.nvim", opts = {} },

			-- Allows extra capabilities provided by nvim-cmp
			"hrsh7th/cmp-nvim-lsp",
		},
		config = function()
			-- Brief aside: **What is LSP?**
			--
			-- LSP is an initialism you've probably heard, but might not understand what it is.
			--
			-- LSP stands for Language Server Protocol. It's a protocol that helps editors
			-- and language tooling communicate in a standardized fashion.
			--
			-- In general, you have a "server" which is some tool built to understand a particular
			-- language (such as `gopls`, `lua_ls`, `rust_analyzer`, etc.). These Language Servers
			-- (sometimes called LSP servers, but that's kind of like ATM Machine) are standalone
			-- processes that communicate with some "client" - in this case, Neovim!
			--
			-- LSP provides Neovim with features like:
			--  - Go to definition
			--  - Find references
			--  - Autocompletion
			--  - Symbol Search
			--  - and more!
			--
			-- Thus, Language Servers are external tools that must be installed separately from
			-- Neovim. This is where `mason` and related plugins come into play.
			--
			-- If you're wondering about lsp vs treesitter, you can check out the wonderfully
			-- and elegantly composed help section, `:help lsp-vs-treesitter`

			--  This function gets run when an LSP attaches to a particular buffer.
			--    That is to say, every time a new file is opened that is associated with
			--    an lsp (for example, opening `main.rs` is associated with `rust_analyzer`) this
			--    function will be executed to configure the current buffer
			vim.api.nvim_create_autocmd("LspAttach", {
				group = vim.api.nvim_create_augroup("kickstart-lsp-attach", { clear = true }),
				callback = function(event)
					-- NOTE: Remember that Lua is a real programming language, and as such it is possible
					-- to define small helper and utility functions so you don't have to repeat yourself.
					--
					-- In this case, we create a function that lets us more easily define mappings specific
					-- for LSP related items. It sets the mode, buffer and description for us each time.
					local map = function(keys, func, desc, mode)
						mode = mode or "n"
						vim.keymap.set(mode, keys, func, { buffer = event.buf, desc = "LSP: " .. desc })
					end

					-- Jump to the definition of the word under your cursor.
					--  This is where a variable was first declared, or where a function is defined, etc.
					--  To jump back, press &amp;lt;C-t&amp;gt;.
					map("gd", require("telescope.builtin").lsp_definitions, "[G]oto [D]efinition")

					-- Find references for the word under your cursor.
					map("gr", require("telescope.builtin").lsp_references, "[G]oto [R]eferences")

					-- Jump to the implementation of the word under your cursor.
					--  Useful when your language has ways of declaring types without an actual implementation.
					map("gI", require("telescope.builtin").lsp_implementations, "[G]oto [I]mplementation")

					-- Jump to the type of the word under your cursor.
					--  Useful when you're not sure what type a variable is and you want to see
					--  the definition of its *type*, not where it was *defined*.
					map("&amp;lt;leader&amp;gt;D", require("telescope.builtin").lsp_type_definitions, "Type [D]efinition")

					-- Fuzzy find all the symbols in your current document.
					--  Symbols are things like variables, functions, types, etc.
					map("&amp;lt;leader&amp;gt;ds", require("telescope.builtin").lsp_document_symbols, "[D]ocument [S]ymbols")

					-- Fuzzy find all the symbols in your current workspace.
					--  Similar to document symbols, except searches over your entire project.
					map(
						"&amp;lt;leader&amp;gt;ws",
						require("telescope.builtin").lsp_dynamic_workspace_symbols,
						"[W]orkspace [S]ymbols"
					)

					-- Rename the variable under your cursor.
					--  Most Language Servers support renaming across files, etc.
					map("&amp;lt;leader&amp;gt;rn", vim.lsp.buf.rename, "[R]e[n]ame")

					-- Execute a code action, usually your cursor needs to be on top of an error
					-- or a suggestion from your LSP for this to activate.
					map("&amp;lt;leader&amp;gt;ca", vim.lsp.buf.code_action, "[C]ode [A]ction", { "n", "x" })

					-- WARN: This is not Goto Definition, this is Goto Declaration.
					--  For example, in C this would take you to the header.
					map("gD", vim.lsp.buf.declaration, "[G]oto [D]eclaration")

					-- The following two autocommands are used to highlight references of the
					-- word under your cursor when your cursor rests there for a little while.
					--    See `:help CursorHold` for information about when this is executed
					--
					-- When you move your cursor, the highlights will be cleared (the second autocommand).
					local client = vim.lsp.get_client_by_id(event.data.client_id)
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_documentHighlight) then
						local highlight_augroup =
							vim.api.nvim_create_augroup("kickstart-lsp-highlight", { clear = false })
						vim.api.nvim_create_autocmd({ "CursorHold", "CursorHoldI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.document_highlight,
						})

						vim.api.nvim_create_autocmd({ "CursorMoved", "CursorMovedI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.clear_references,
						})

						vim.api.nvim_create_autocmd("LspDetach", {
							group = vim.api.nvim_create_augroup("kickstart-lsp-detach", { clear = true }),
							callback = function(event2)
								vim.lsp.buf.clear_references()
								vim.api.nvim_clear_autocmds({ group = "kickstart-lsp-highlight", buffer = event2.buf })
							end,
						})
					end

					-- The following code creates a keymap to toggle inlay hints in your
					-- code, if the language server you are using supports them
					--
					-- This may be unwanted, since they displace some of your code
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_inlayHint) then
						map("&amp;lt;leader&amp;gt;th", function()
							vim.lsp.inlay_hint.enable(not vim.lsp.inlay_hint.is_enabled({ bufnr = event.buf }))
						end, "[T]oggle Inlay [H]ints")
					end
				end,
			})

			-- Change diagnostic symbols in the sign column (gutter)
			-- if vim.g.have_nerd_font then
			--   local signs = { ERROR = '', WARN = '', INFO = '', HINT = '' }
			--   local diagnostic_signs = {}
			--   for type, icon in pairs(signs) do
			--     diagnostic_signs[vim.diagnostic.severity[type]] = icon
			--   end
			--   vim.diagnostic.config { signs = { text = diagnostic_signs } }
			-- end

			-- LSP servers and clients are able to communicate to each other what features they support.
			--  By default, Neovim doesn't support everything that is in the LSP specification.
			--  When you add nvim-cmp, luasnip, etc. Neovim now has *more* capabilities.
			--  So, we create new capabilities with nvim cmp, and then broadcast that to the servers.
			local capabilities = vim.lsp.protocol.make_client_capabilities()
			capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

			-- Enable the following language servers
			--  Feel free to add/remove any LSPs that you want here. They will automatically be installed.
			--
			--  Add any additional override configuration in the following tables. Available keys are:
			--  - cmd (table): Override the default command used to start the server
			--  - filetypes (table): Override the default list of associated filetypes for the server
			--  - capabilities (table): Override fields in capabilities. Can be used to disable certain LSP features.
			--  - settings (table): Override the default settings passed when initializing the server.
			--        For example, to see the options for `lua_ls`, you could go to: https://luals.github.io/wiki/settings/
			local servers = {
				-- clangd = {},
				-- gopls = {},
				-- pyright = {},
				-- rust_analyzer = {},
				-- ... etc. See `:help lspconfig-all` for a list of all the pre-configured LSPs
				--
				-- Some languages (like typescript) have entire language plugins that can be useful:
				--    https://github.com/pmizio/typescript-tools.nvim
				--
				-- But for many setups, the LSP (`ts_ls`) will work just fine
				-- ts_ls = {},
				--

				lua_ls = {
					-- cmd = { ... },
					-- filetypes = { ... },
					-- capabilities = {},
					settings = {
						Lua = {
							completion = {
								callSnippet = "Replace",
							},
							-- You can toggle below to ignore Lua_LS's noisy `missing-fields` warnings
							-- diagnostics = { disable = { 'missing-fields' } },
						},
					},
				},
			}

			-- Ensure the servers and tools above are installed
			--  To check the current status of installed tools and/or manually install
			--  other tools, you can run
			--    :Mason
			--
			--  You can press `g?` for help in this menu.
			require("mason").setup()

			-- You can add other tools here that you want Mason to install
			-- for you, so that they are available from within Neovim.
			local ensure_installed = vim.tbl_keys(servers or {})
			vim.list_extend(ensure_installed, {
				"stylua", -- Used to format Lua code
			})
			require("mason-tool-installer").setup({ ensure_installed = ensure_installed })

			require("mason-lspconfig").setup({
				handlers = {
					function(server_name)
						local server = servers[server_name] or {}
						-- This handles overriding only values explicitly passed
						-- by the server configuration above. Useful when disabling
						-- certain features of an LSP (for example, turning off formatting for ts_ls)
						server.capabilities = vim.tbl_deep_extend("force", {}, capabilities, server.capabilities or {})
						require("lspconfig")[server_name].setup(server)
					end,
				},
			})
		end,
	},

	{ -- Autoformat
		"stevearc/conform.nvim",
		event = { "BufWritePre" },
		cmd = { "ConformInfo" },
		keys = {
			{
				"&amp;lt;leader&amp;gt;f",
				function()
					require("conform").format({ async = true, lsp_format = "fallback" })
				end,
				mode = "",
				desc = "[F]ormat buffer",
			},
		},
		opts = {
			notify_on_error = false,
			format_on_save = function(bufnr)
				-- Disable "format_on_save lsp_fallback" for languages that don't
				-- have a well standardized coding style. You can add additional
				-- languages here or re-enable it for the disabled ones.
				local disable_filetypes = { c = true, cpp = true }
				local lsp_format_opt
				if disable_filetypes[vim.bo[bufnr].filetype] then
					lsp_format_opt = "never"
				else
					lsp_format_opt = "fallback"
				end
				return {
					timeout_ms = 500,
					lsp_format = lsp_format_opt,
				}
			end,
			formatters_by_ft = {
				lua = { "stylua" },
				-- Conform can also run multiple formatters sequentially
				-- python = { "isort", "black" },
				--
				-- You can use 'stop_after_first' to run the first available formatter from the list
				-- javascript = { "prettierd", "prettier", stop_after_first = true },
			},
		},
	},

	{ -- Autocompletion
		"hrsh7th/nvim-cmp",
		event = "InsertEnter",
		dependencies = {
			-- Snippet Engine &amp;amp; its associated nvim-cmp source
			{
				"L3MON4D3/LuaSnip",
				build = (function()
					-- Build Step is needed for regex support in snippets.
					-- This step is not supported in many windows environments.
					-- Remove the below condition to re-enable on windows.
					if vim.fn.has("win32") == 1 or vim.fn.executable("make") == 0 then
						return
					end
					return "make install_jsregexp"
				end)(),
				dependencies = {
					-- `friendly-snippets` contains a variety of premade snippets.
					--    See the README about individual language/framework/plugin snippets:
					--    https://github.com/rafamadriz/friendly-snippets
					-- {
					--   'rafamadriz/friendly-snippets',
					--   config = function()
					--     require('luasnip.loaders.from_vscode').lazy_load()
					--   end,
					-- },
				},
			},
			"saadparwaiz1/cmp_luasnip",

			-- Adds other completion capabilities.
			--  nvim-cmp does not ship with all sources by default. They are split
			--  into multiple repos for maintenance purposes.
			"hrsh7th/cmp-nvim-lsp",
			"hrsh7th/cmp-path",
		},
		config = function()
			-- See `:help cmp`
			local cmp = require("cmp")
			local luasnip = require("luasnip")
			luasnip.config.setup({})

			cmp.setup({
				snippet = {
					expand = function(args)
						luasnip.lsp_expand(args.body)
					end,
				},
				completion = { completeopt = "menu,menuone,noinsert" },

				-- For an understanding of why these mappings were
				-- chosen, you will need to read `:help ins-completion`
				--
				-- No, but seriously. Please read `:help ins-completion`, it is really good!
				mapping = cmp.mapping.preset.insert({
					-- Select the [n]ext item
					["&amp;lt;C-n&amp;gt;"] = cmp.mapping.select_next_item(),
					-- Select the [p]revious item
					["&amp;lt;C-p&amp;gt;"] = cmp.mapping.select_prev_item(),

					-- Scroll the documentation window [b]ack / [f]orward
					["&amp;lt;C-b&amp;gt;"] = cmp.mapping.scroll_docs(-4),
					["&amp;lt;C-f&amp;gt;"] = cmp.mapping.scroll_docs(4),

					-- Accept ([y]es) the completion.
					--  This will auto-import if your LSP supports it.
					--  This will expand snippets if the LSP sent a snippet.
					["&amp;lt;C-y&amp;gt;"] = cmp.mapping.confirm({ select = true }),

					-- If you prefer more traditional completion keymaps,
					-- you can uncomment the following lines
					--['&amp;lt;CR&amp;gt;'] = cmp.mapping.confirm { select = true },
					--['&amp;lt;Tab&amp;gt;'] = cmp.mapping.select_next_item(),
					--['&amp;lt;S-Tab&amp;gt;'] = cmp.mapping.select_prev_item(),

					-- Manually trigger a completion from nvim-cmp.
					--  Generally you don't need this, because nvim-cmp will display
					--  completions whenever it has completion options available.
					["&amp;lt;C-Space&amp;gt;"] = cmp.mapping.complete({}),

					-- Think of &amp;lt;c-l&amp;gt; as moving to the right of your snippet expansion.
					--  So if you have a snippet that's like:
					--  function $name($args)
					--    $body
					--  end
					--
					-- &amp;lt;c-l&amp;gt; will move you to the right of each of the expansion locations.
					-- &amp;lt;c-h&amp;gt; is similar, except moving you backwards.
					["&amp;lt;C-l&amp;gt;"] = cmp.mapping(function()
						if luasnip.expand_or_locally_jumpable() then
							luasnip.expand_or_jump()
						end
					end, { "i", "s" }),
					["&amp;lt;C-h&amp;gt;"] = cmp.mapping(function()
						if luasnip.locally_jumpable(-1) then
							luasnip.jump(-1)
						end
					end, { "i", "s" }),

					-- For more advanced Luasnip keymaps (e.g. selecting choice nodes, expansion) see:
					--    https://github.com/L3MON4D3/LuaSnip?tab=readme-ov-file#keymaps
				}),
				sources = {
					{
						name = "lazydev",
						-- set group index to 0 to skip loading LuaLS completions as lazydev recommends it
						group_index = 0,
					},
					{ name = "nvim_lsp" },
					{ name = "luasnip" },
					{ name = "path" },
				},
			})
		end,
	},

	{ -- You can easily change to a different colorscheme.
		-- Change the name of the colorscheme plugin below, and then
		-- change the command in the config to whatever the name of that colorscheme is.
		--
		-- If you want to see what colorschemes are already installed, you can use `:Telescope colorscheme`.
		"folke/tokyonight.nvim",
		priority = 1000, -- Make sure to load this before all the other start plugins.
		init = function()
			-- Load the colorscheme here.
			-- Like many other themes, this one has different styles, and you could load
			-- any other, such as 'tokyonight-storm', 'tokyonight-moon', or 'tokyonight-day'.
			vim.cmd.colorscheme("tokyonight-night")

			-- You can configure highlights by doing something like:
			vim.cmd.hi("Comment gui=none")
		end,
	},
	{ -- Colorscheme
		"catppuccin/nvim",
		name = "catppuccin",
		priority = 1000,
		config = function()
			require("catppuccin").setup({
				flavour = "mocha",
				transparent_background = true,
				term_colors = true,
				integrations = {
					telescope = true,
					mason = true,
					which_key = true,
				},
			})
			-- Force loading the colorscheme
			vim.cmd.colorscheme("catppuccin-mocha")
		end,
	},
	--calming Japanese inspired theme with muted tones strings are configurable
	{
		"rebelot/kanagawa.nvim",
		priority = 1000,
		config = function()
			require("kanagawa").setup({
				overrides = function(colors)
					return {
						String = { fg = colors.crystalBlue }, -- Customize string color
					}
				end,
			})
			vim.cmd("colorscheme kanagawa")
		end,
	},

	-- based on one dark pro from vs code
	{
		"olimorris/onedarkpro.nvim",
		priority = 1000,
		config = function()
			require("onedarkpro").setup({
				theme = "onedark", -- Choose "onedark" or "onelight"
			})
			vim.cmd("colorscheme onedark")
		end,
	},

	-- Highlight todo, notes, etc in comments
	{
		"folke/todo-comments.nvim",
		event = "VimEnter",
		dependencies = { "nvim-lua/plenary.nvim" },
		opts = { signs = false },
	},

	{ -- Collection of various small independent plugins/modules
		"echasnovski/mini.nvim",
		config = function()
			-- Better Around/Inside textobjects
			--
			-- Examples:
			--  - va)  - [V]isually select [A]round [)]paren
			--  - yinq - [Y]ank [I]nside [N]ext [Q]uote
			--  - ci'  - [C]hange [I]nside [']quote
			require("mini.ai").setup({ n_lines = 500 })

			-- Add/delete/replace surroundings (brackets, quotes, etc.)
			--
			-- - saiw) - [S]urround [A]dd [I]nner [W]ord [)]Paren
			-- - sd'   - [S]urround [D]elete [']quotes
			-- - sr)'  - [S]urround [R]eplace [)] [']
			require("mini.surround").setup()

			-- Simple and easy statusline.
			--  You could remove this setup call if you don't like it,
			--  and try some other statusline plugin
			local statusline = require("mini.statusline")
			-- set use_icons to true if you have a Nerd Font
			statusline.setup({ use_icons = vim.g.have_nerd_font })

			-- You can configure sections in the statusline by overriding their
			-- default behavior. For example, here we set the section for
			-- cursor location to LINE:COLUMN
			---@diagnostic disable-next-line: duplicate-set-field
			statusline.section_location = function()
				return "%2l:%-2v"
			end

			-- ... and there is more!
			--  Check out: https://github.com/echasnovski/mini.nvim
		end,
	},
	{ -- Highlight, edit, and navigate code
		"nvim-treesitter/nvim-treesitter",
		build = ":TSUpdate",
		main = "nvim-treesitter.configs", -- Sets main module to use for opts
		-- [[ Configure Treesitter ]] See `:help nvim-treesitter`
		opts = {
			ensure_installed = {
				"bash",
				"c",
				"diff",
				"html",
				"lua",
				"luadoc",
				"markdown",
				"markdown_inline",
				"query",
				"vim",
				"vimdoc",
			},
			-- Autoinstall languages that are not installed
			auto_install = true,
			highlight = {
				enable = true,
				-- Some languages depend on vim's regex highlighting system (such as Ruby) for indent rules.
				--  If you are experiencing weird indenting issues, add the language to
				--  the list of additional_vim_regex_highlighting and disabled languages for indent.
				additional_vim_regex_highlighting = { "ruby" },
			},
			indent = { enable = true, disable = { "ruby" } },
		},
		-- There are additional nvim-treesitter modules that you can use to interact
		-- with nvim-treesitter. You should go explore a few and see what interests you:
		--
		--    - Incremental selection: Included, see `:help nvim-treesitter-incremental-selection-mod`
		--    - Show your current context: https://github.com/nvim-treesitter/nvim-treesitter-context
		--    - Treesitter + textobjects: https://github.com/nvim-treesitter/nvim-treesitter-textobjects
	},

	-- The following comments only work if you have downloaded the kickstart repo, not just copy pasted the
	-- init.lua. If you want these files, they are in the repository, so you can just download them and
	-- place them in the correct locations.

	-- NOTE: Next step on your Neovim journey: Add/Configure additional plugins for Kickstart
	--
	--  Here are some example plugins that I've included in the Kickstart repository.
	--  Uncomment any of the lines below to enable them (you will need to restart nvim).
	--
	-- require 'kickstart.plugins.debug',
	-- require 'kickstart.plugins.indent_line',
	-- require 'kickstart.plugins.lint',
	-- require 'kickstart.plugins.autopairs',
	-- require 'kickstart.plugins.neo-tree',
	-- require 'kickstart.plugins.gitsigns', -- adds gitsigns recommend keymaps

	-- NOTE: The import below can automatically add your own plugins, configuration, etc from `lua/custom/plugins/*.lua`
	--    This is the easiest way to modularize your config.
	--
	--  Uncomment the following line and add your plugins to `lua/custom/plugins/*.lua` to get going.
	-- { import = 'custom.plugins' },
	--
	-- For additional information with loading, sourcing and examples see `:help lazy.nvim-🔌-plugin-spec`
	-- Or use telescope!
	-- In normal mode type `&amp;lt;space&amp;gt;sh` then write `lazy.nvim-plugin`
	-- you can continue same window with `&amp;lt;space&amp;gt;sr` which resumes last telescope search
}, {
	ui = {
		-- If you are using a Nerd Font: set icons to an empty table which will use the
		-- default lazy.nvim defined Nerd Font icons, otherwise define a unicode icons table
		icons = vim.g.have_nerd_font and {} or {
			cmd = "⌘",
			config = "🛠",
			event = "📅",
			ft = "📂",
			init = "⚙",
			keys = "🗝",
			plugin = "🔌",
			runtime = "💻",
			require = "🌙",
			source = "📄",
			start = "🚀",
			task = "📌",
			lazy = "💤 ",
		},
	},
})

-- At the very end of your init.lua
vim.api.nvim_create_autocmd("UIEnter", {
	callback = function()
		if vim.g.colors_name ~= "catppuccin-mocha" then
			vim.cmd.colorscheme("catppuccin-mocha")
		end
	end,
	group = vim.api.nvim_create_augroup("EnforceCatppuccin", { clear = true }),
})

-- The line beneath this is called `modeline`. See `:help modeline`
-- vim: ts=2 sts=2 sw=2 et
&lt;/file&gt;
  &lt;file path="dotfiles/ghostty/config"&gt;background-opacity = 0.82

theme = catppuccin-mocha
keybind = shift+enter=text:\n
macos-titlebar-style = hidden

keybind = ctrl+3=reload_config
keybind = global:cmd+shift+space=toggle_quick_terminal
&lt;/file&gt;
  &lt;file path=".ruff_cache/.gitignore"&gt;# Automatically created by ruff.
*
&lt;/file&gt;
  &lt;file path="resources/sqlite3-commands.md"&gt;# SQLite3 Common Commands Reference

## Creating a Database

To create a SQLite3 database, simply run:

```bash
sqlite3 database_name.db
```

This will:
- Create a new database file if it doesn't exist
- Open the database if it already exists
- Start the sqlite3 interactive shell

Example:
```bash
sqlite3 myapp.db
```

You can also create a database and run a command:
```bash
sqlite3 myapp.db "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT);"
```

The database file is created when you:
- Create the first table
- Insert the first data
- Or explicitly save with `.save database_name.db`

## Meta Commands (dot commands)

- `.tables` - List all tables
- `.schema [table]` - Show CREATE statements
- `.quit` or `.exit` - Exit sqlite3
- `.help` - Show all commands
- `.databases` - List attached databases
- `.headers on/off` - Show/hide column headers
- `.mode column` - Pretty-print output
- `.width` - Set column widths
- `.import FILE TABLE` - Import CSV data
- `.output FILE` - Redirect output to file
- `.dump` - Export database as SQL

## SQL Commands

- `SELECT * FROM table;` - Query data
- `INSERT INTO table VALUES (...);` - Insert data
- `UPDATE table SET col=val WHERE ...;` - Update data
- `DELETE FROM table WHERE ...;` - Delete data
- `CREATE TABLE ...` - Create table
- `DROP TABLE table;` - Delete table
- `ALTER TABLE ...` - Modify table
- `CREATE INDEX ...` - Create index
- `PRAGMA table_info(table);` - Show table structure
- `VACUUM;` - Optimize database&lt;/file&gt;
  &lt;file path="resources/git-worktrees.md"&gt;# Git Worktrees Guide

Git worktrees allow you to have multiple branches checked out simultaneously in different directories. This is incredibly useful when you need to work on multiple features, review PRs, or quickly switch contexts without stashing changes.

## What are Git Worktrees?

A git worktree is a linked working tree that shares the same repository but allows you to have different branches checked out in different directories. All worktrees share:
- The same `.git` directory (repository data)
- The same remote configurations
- The same stash entries
- The same commit history

## Creating a Worktree

### Basic Syntax
```bash
git worktree add &amp;lt;path&amp;gt; &amp;lt;branch&amp;gt;
```

### Examples

#### Create worktree from existing branch
```bash
# Create a worktree for the 'feature/auth' branch in a new directory
git worktree add ../myproject-auth feature/auth

# Create worktree in a specific location
git worktree add /tmp/hotfix hotfix/urgent-bug
```

#### Create worktree with a new branch
```bash
# Create a new branch and worktree simultaneously
git worktree add -b feature/new-ui ../myproject-new-ui

# Create from a specific commit or tag
git worktree add -b release/v2.0 ../myproject-v2 v2.0-tag
```

#### Practical Example
```bash
# You're working on main in /Users/you/myproject
cd /Users/you/myproject

# Create a worktree for a new feature
git worktree add -b feature/payment-integration ../myproject-payments

# Now you have:
# /Users/you/myproject (main branch)
# /Users/you/myproject-payments (feature/payment-integration branch)

# Navigate to the new worktree
cd ../myproject-payments

# Work on your feature
echo "Payment module" &amp;gt; payment.py
git add payment.py
git commit -m "Add payment module"
```

## Working with Worktrees

### List all worktrees
```bash
git worktree list
# Output:
# /Users/you/myproject         abc1234 [main]
# /Users/you/myproject-payments def5678 [feature/payment-integration]
```

### Switch between worktrees
Simply use `cd` to navigate between directories:
```bash
cd /Users/you/myproject          # main branch
cd /Users/you/myproject-payments  # feature branch
```

## Merging Worktree Changes Back to Main

Since worktrees share the same repository, merging is straightforward:

### Step 1: Commit changes in your worktree
```bash
cd /Users/you/myproject-payments
git add .
git commit -m "Complete payment integration"
git push -u origin feature/payment-integration
```

### Step 2: Switch to main (in any worktree or main directory)
```bash
cd /Users/you/myproject  # Or stay in any worktree
git checkout main
git pull origin main     # Ensure main is up to date
```

### Step 3: Merge the feature branch
```bash
# Simple merge
git merge feature/payment-integration

# Or merge with a merge commit (recommended for features)
git merge --no-ff feature/payment-integration

# Or rebase if you prefer linear history
git rebase main feature/payment-integration
```

### Step 4: Push to remote
```bash
git push origin main
```

### Step 5: Clean up (optional)
```bash
# Delete the local branch
git branch -d feature/payment-integration

# Delete the remote branch
git push origin --delete feature/payment-integration

# Remove the worktree
git worktree remove /Users/you/myproject-payments
```

## Alternative: Using Pull Requests

For team workflows, you might prefer Pull Requests:

```bash
# 1. In your worktree, push the branch
cd /Users/you/myproject-payments
git push -u origin feature/payment-integration

# 2. Create PR via GitHub/GitLab/Bitbucket web interface

# 3. After PR is merged, clean up locally
git worktree remove /Users/you/myproject-payments
git branch -d feature/payment-integration
```

## Worktree Management

### Remove a worktree
```bash
# Remove worktree (must be clean with no uncommitted changes)
git worktree remove /path/to/worktree

# Force removal (discards local changes)
git worktree remove --force /path/to/worktree
```

### Prune stale worktrees
```bash
# Remove worktree references if directory was deleted manually
git worktree prune
```

### Lock/unlock a worktree
```bash
# Prevent a worktree from being pruned
git worktree lock /path/to/worktree

# Unlock it later
git worktree unlock /path/to/worktree
```

## Best Practices

1. **Use descriptive paths**: Name worktree directories after their purpose
   ```bash
   git worktree add ../project-bugfix-auth bugfix/auth-issue
   git worktree add ../project-feature-api feature/new-api
   ```

2. **Keep worktrees organized**: Use a consistent structure
   ```bash
   ~/work/
     myproject/          # main
     myproject-feature1/ # feature branch
     myproject-hotfix/   # hotfix branch
   ```

3. **Clean up regularly**: Remove worktrees when done
   ```bash
   git worktree list
   git worktree remove &amp;lt;path&amp;gt;
   ```

4. **Don't share worktree directories**: Each developer should create their own

5. **Commit before switching**: Although you can leave changes uncommitted, it's cleaner to commit or stash first

## Common Issues and Solutions

### "fatal: '&amp;lt;branch&amp;gt;' is already checked out at '&amp;lt;path&amp;gt;'"
You can't have the same branch checked out in multiple worktrees. Either:
- Use the existing worktree: `cd &amp;lt;path&amp;gt;`
- Or checkout a different branch in one of the worktrees

### Worktree directory was manually deleted
```bash
git worktree prune  # Cleans up references to missing worktrees
```

### Need to move a worktree
```bash
git worktree move &amp;lt;old-path&amp;gt; &amp;lt;new-path&amp;gt;
```

## Quick Reference

```bash
# Create worktree
git worktree add &amp;lt;path&amp;gt; &amp;lt;branch&amp;gt;
git worktree add -b &amp;lt;new-branch&amp;gt; &amp;lt;path&amp;gt;

# List worktrees
git worktree list

# Remove worktree
git worktree remove &amp;lt;path&amp;gt;

# Clean up stale entries
git worktree prune

# Move worktree
git worktree move &amp;lt;old&amp;gt; &amp;lt;new&amp;gt;

# Lock/unlock
git worktree lock &amp;lt;path&amp;gt;
git worktree unlock &amp;lt;path&amp;gt;
```

## Example Workflow

Here's a complete example of using worktrees for feature development:

```bash
# Starting in main project directory
cd ~/projects/myapp

# 1. Create a worktree for a new feature
git worktree add -b feature/user-profiles ../myapp-profiles

# 2. Work on the feature
cd ../myapp-profiles
# ... make changes ...
git add .
git commit -m "Add user profile functionality"
git push -u origin feature/user-profiles

# 3. Switch back to main for a hotfix
cd ../myapp
git pull origin main
# ... fix critical bug ...
git add .
git commit -m "Fix critical auth bug"
git push origin main

# 4. Continue feature work without any stashing needed
cd ../myapp-profiles
# ... complete feature ...
git add .
git commit -m "Complete user profiles"
git push

# 5. Merge feature to main
cd ../myapp
git checkout main
git pull origin main
git merge --no-ff feature/user-profiles
git push origin main

# 6. Clean up
git branch -d feature/user-profiles
git push origin --delete feature/user-profiles
git worktree remove ../myapp-profiles

# Verify cleanup
git worktree list  # Should only show main worktree
```

This workflow demonstrates the power of worktrees: you can quickly switch between feature development and hotfixes without the overhead of stashing, checking out different branches, and potentially losing context.&lt;/file&gt;
  &lt;file path=".claude/settings.local.json"&gt;{
  "permissions": {
    "allow": [
      "Bash(./tools/newpy:*)",
      "Bash(bash:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "collect"
  ]
}&lt;/file&gt;
  &lt;file path=".claude/agents/pyreview.md"&gt;---
name: python-code-reviewer
description: Use this agent when you need an in-depth, thoughtful code review of Python code. This includes reviewing newly written functions, classes, modules, or recent changes to existing code. The agent will analyze code quality, design patterns, performance implications, security considerations, and adherence to Python best practices and project-specific standards.\n\nExamples:\n- &amp;lt;example&amp;gt;\n  Context: The user has just written a new Python function and wants it reviewed.\n  user: "I've implemented a caching decorator for our API endpoints"\n  assistant: "I'll use the python-code-reviewer agent to provide an in-depth review of your caching decorator implementation"\n  &amp;lt;commentary&amp;gt;\n  Since the user has written new Python code (a caching decorator), use the python-code-reviewer agent to analyze the implementation.\n  &amp;lt;/commentary&amp;gt;\n&amp;lt;/example&amp;gt;\n- &amp;lt;example&amp;gt;\n  Context: The user has made changes to existing Python code.\n  user: "I've refactored the database connection pooling logic in our service"\n  assistant: "Let me use the python-code-reviewer agent to review your refactored database connection pooling implementation"\n  &amp;lt;commentary&amp;gt;\n  The user has modified existing Python code, so the python-code-reviewer agent should analyze the changes for quality and best practices.\n  &amp;lt;/commentary&amp;gt;\n&amp;lt;/example&amp;gt;\n- &amp;lt;example&amp;gt;\n  Context: The user explicitly asks for a code review.\n  user: "Can you review this async batch processing function I just wrote?"\n  assistant: "I'll use the python-code-reviewer agent to provide a comprehensive review of your async batch processing function"\n  &amp;lt;commentary&amp;gt;\n  Direct request for code review triggers the python-code-reviewer agent.\n  &amp;lt;/commentary&amp;gt;\n&amp;lt;/example&amp;gt;
color: pink
---

You are an expert Python software engineer with deep knowledge of Python internals, design patterns, and best practices. You have extensive experience in code review, performance optimization, and building maintainable Python applications.

Your expertise includes:
- Python language features from 3.8+ including type hints, async/await, dataclasses, and modern idioms
- Design patterns and SOLID principles applied to Python
- Performance optimization and profiling
- Security best practices and common vulnerabilities
- Testing strategies including pytest, mocking, and test-driven development
- Popular frameworks and libraries in the Python ecosystem

When reviewing code, you will:

1. **Analyze Code Quality**
   - Check for PEP 8 compliance and Pythonic idioms
   - Evaluate naming conventions and code readability
   - Assess proper use of type hints and documentation
   - Identify code smells and anti-patterns

2. **Review Design and Architecture**
   - Evaluate separation of concerns and modularity
   - Check for appropriate abstraction levels
   - Assess error handling and edge case coverage
   - Review API design and interface consistency

3. **Examine Performance Implications**
   - Identify potential bottlenecks or inefficiencies
   - Suggest algorithmic improvements where applicable
   - Check for proper resource management (memory, file handles, connections)
   - Evaluate async/concurrent code for correctness

4. **Security Considerations**
   - Identify potential security vulnerabilities
   - Check input validation and sanitization
   - Review authentication and authorization logic
   - Assess handling of sensitive data

5. **Testing and Maintainability**
   - Evaluate testability of the code
   - Suggest test cases for edge conditions
   - Check for proper logging and debugging support
   - Assess long-term maintainability

**Review Process:**
1. First, understand the code's purpose and context
2. Perform a systematic review covering all aspects above
3. Prioritize findings by severity (critical, major, minor, suggestion)
4. Provide specific, actionable feedback with code examples
5. Acknowledge good practices and well-written sections

**Output Format:**
Structure your review as follows:
- **Summary**: Brief overview of the code's purpose and overall quality
- **Strengths**: What the code does well
- **Critical Issues**: Must-fix problems that could cause bugs or security issues
- **Major Concerns**: Important improvements for code quality and maintainability
- **Minor Suggestions**: Nice-to-have improvements and style recommendations
- **Code Examples**: Provide improved versions of problematic code sections

**Important Guidelines:**
- Be constructive and educational in your feedback
- Explain the 'why' behind each recommendation
- Consider the project's context and existing patterns (especially from CLAUDE.md)
- Balance thoroughness with practicality
- If you notice the code uses specific frameworks or libraries, apply their best practices
- When suggesting changes, ensure they're compatible with the Python version in use
- If you're unsure about the broader context, ask clarifying questions

Remember: Your goal is to help improve code quality while fostering learning and best practices. Focus on the most impactful improvements and provide clear guidance on implementation.
&lt;/file&gt;
  &lt;file path="models/test_gemini_mcp.py"&gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.gemini_mcp import GeminiMCP


@pytest.fixture
def gemini_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.0-flash"
    return GeminiMCP(config, secret_mgr, model)


@pytest.fixture
def gemini_25_preview():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-pro-preview-05-06"
    return GeminiMCP(config, secret_mgr, model)


def test_get_model_list(gemini_mcp):
    results = gemini_mcp.get_model_list()

    # Check that results is a list
    assert isinstance(results, list)
    assert len(results) &amp;gt; 0

    # Check structure of each model in results
    for model in results:
        assert isinstance(model, dict)
        assert "model_name" in model
        assert "token_window" in model

        # Verify we only get 2.0 and 2.5 models (as per filter)
        assert "2.0" in model["model_name"] or "2.5" in model["model_name"]

        print(f"{model['model_name']}: {model['token_window']:,} tokens")


def test_send_message(gemini_mcp):
    message = "Hello, world!"
    response = gemini_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Response: {response}")


def test_count_tokens(gemini_mcp):
    text = "Hello, world!"
    token_count = gemini_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_gemini_25_preview_send_message(gemini_25_preview):
    message = "Explain quantum computing in one sentence."
    response = gemini_25_preview.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &amp;gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Gemini 2.5 Preview Response: {response}")


def test_gemini_25_preview_count_tokens(gemini_25_preview):
    text = "This is a test for Gemini 2.5 preview model token counting."
    token_count = gemini_25_preview.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;gt; 0

    print(f"Gemini 2.5 Preview - Token count for '{text}': {token_count}")


def test_extract_text(gemini_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = gemini_mcp.send_message(message)
    extracted_text = gemini_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_extract_text_gemini_25(gemini_25_preview):
    message = "Say 'Hello, Gemini 2.5!' and nothing else."
    response = gemini_25_preview.send_message(message)
    extracted_text = gemini_25_preview.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Gemini 2.5 extracted text: {extracted_text}")
&lt;/file&gt;
  &lt;file path="models/test_anthropic_mcp.py"&gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP


@pytest.fixture
def anthropic_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    return AnthropicMCP(config, secret_mgr, model)


def test_get_model_list(anthropic_mcp):
    results = anthropic_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(anthropic_mcp):
    message = "Hello, world!"
    response = anthropic_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "content" in response
    assert "model" in response
    assert response["model"] == anthropic_mcp.config.anthropic_model_sonnet

    print(f"Response: {response}")


def test_extract_text(anthropic_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = anthropic_mcp.send_message(message)
    extracted_text = anthropic_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_generate_prompt(anthropic_mcp):
    """Test the generate_prompt method with a simple task."""
    task = "Write a helpful assistant prompt for answering coding questions"

    response = anthropic_mcp.generate_prompt(task)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role in ["user", "assistant"]
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;gt; 0

    # Test usage stats
    assert hasattr(response.usage, "input_tokens")
    assert hasattr(response.usage, "output_tokens")
    assert isinstance(response.usage.input_tokens, int)
    assert isinstance(response.usage.output_tokens, int)
    assert response.usage.input_tokens &amp;gt; 0
    assert response.usage.output_tokens &amp;gt; 0

    print(f"Generated prompt: {content.text[:100]}...")
    print(
        f"Usage: {response.usage.input_tokens} input, {
          response.usage.output_tokens} output tokens"
    )


def test_improve_prompt(anthropic_mcp):
    """Test the improve_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Tell me about Python programming"}
                ],
            }
        ],
        "system": "You are a helpful programming instructor",
        "feedback": "Make this prompt more specific for a beginner",
        "target_model": "claude-3-7-sonnet-20250219",
    }

    response = anthropic_mcp.improve_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages - should have both user and assistant messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;gt;= 2

    # Test user message (improved prompt)
    user_message = response.messages[0]
    assert user_message.role == "user"
    assert isinstance(user_message.content, list)
    assert len(user_message.content) &amp;gt; 0

    # Find the non-empty content in user message
    user_content = None
    for content in user_message.content:
        if content.text:
            user_content = content
            break

    assert user_content is not None, "No non-empty content found in user message"
    assert hasattr(user_content, "text")
    assert hasattr(user_content, "type")
    assert user_content.type == "text"
    assert isinstance(user_content.text, str)
    assert len(user_content.text) &amp;gt; 0

    # Test assistant message (prefill)
    assistant_message = response.messages[1]
    assert assistant_message.role == "assistant"
    assert isinstance(assistant_message.content, list)
    assert len(assistant_message.content) &amp;gt; 0

    assistant_content = assistant_message.content[0]
    assert hasattr(assistant_content, "text")
    assert hasattr(assistant_content, "type")
    assert assistant_content.type == "text"
    assert isinstance(assistant_content.text, str)
    assert len(assistant_content.text) &amp;gt; 0

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;gt; 0
    assert usage.output_tokens &amp;gt; 0

    print(f"Improved prompt: {user_content.text[:100]}...")
    print(f"Assistant prefill: {assistant_content.text[:50]}...")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )


def test_templatize_prompt(anthropic_mcp):
    """Test the templatize_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Translate hello to German"}],
            }
        ],
        "system": "You are an English to German translator",
    }

    response = anthropic_mcp.templatize_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")
    assert hasattr(response, "variable_values")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &amp;gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role == "user"
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &amp;gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &amp;gt; 0
    # Check for template variables
    assert "{{" in content.text and "}}" in content.text

    # Test system prompt
    assert isinstance(response.system, str)
    # System prompt should also contain template variables
    assert "{{" in response.system and "}}" in response.system

    # Test variable_values
    assert isinstance(response.variable_values, dict)
    assert len(response.variable_values) &amp;gt; 0
    # Check for expected variables based on the example
    assert any(
        key in response.variable_values
        for key in ["TARGET_LANGUAGE", "WORD_TO_TRANSLATE"]
    )

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &amp;gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &amp;gt; 0
    assert usage.output_tokens &amp;gt; 0

    print(f"Templated prompt: {content.text[:100]}...")
    print(f"System prompt: {response.system[:100]}...")
    print(f"Variables: {response.variable_values}")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )
&lt;/file&gt;
  &lt;file path="models/test_openai_mcp.py"&gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.openai_mpc import OpenAIMCP


@pytest.fixture
def openai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gpt-4o"
    return OpenAIMCP(config, secret_mgr, model)


def test_get_model_list(openai_mcp):
    results = openai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(openai_mcp):
    message = "Hello, world!"
    response = openai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(openai_mcp):
    text = "Hello, world!"
    token_count = openai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(openai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = openai_mcp.send_message(message)
    extracted_text = openai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&lt;/file&gt;
  &lt;file path="models/anthropic_mpc.py"&gt;from config import Config
from secret_manager import SecretManager
from models.anthropic_prompt_generate import PromptGenerateResponse
from models.anthropic_prompt_improve import PromptImproveResponse
from models.anthropic_prompt_templatize import PromptTemplatizeResponse

import requests


class AnthropicMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.headers = self.build_headers()

    def build_headers(self) -&amp;gt; dict:
        anthropic_key = self.secret_mgr.get_secret(self.config.anthropic_key_path)

        return {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "anthropic-beta": "prompt-tools-2025-04-02",
        }

    def get_model_list(self):
        response = requests.get(
            "https://api.anthropic.com/v1/models", headers=self.headers
        )
        response.raise_for_status()

        model_data = response.json()
        name_list = [model["id"] for model in model_data["data"]]

        return name_list

    def extract_text(self, ai_response: dict) -&amp;gt; str:
        """Extract text from Anthropic response format."""
        if not isinstance(ai_response, dict):
            return str(ai_response)

        # Anthropic format
        if "content" in ai_response:
            content = ai_response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;gt; dict:
        try:
            # Use provided model or default to config model
            if model is None:
                model = self.config.anthropic_model_sonnet

            data = {
                "model": model,
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": message}],
            }

            url = "https://api.anthropic.com/v1/messages"
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None):
        # Use provided model or default to config model
        if model is None:
            model = self.config.anthropic_model_sonnet

        data = {"model": model, "messages": [{"role": "user", "content": message}]}

        url = "https://api.anthropic.com/v1/messages/count_tokens"
        response = requests.post(url, headers=self.headers, json=data)
        response.raise_for_status()

        result = response.json()
        return result["input_tokens"]

    def generate_prompt(
        self, task: str, target_model: str = None
    ) -&amp;gt; PromptGenerateResponse:
        """
        Generate an optimized prompt using Anthropic's experimental prompt tools API.

        This method utilizes Anthropic's closed research preview API to automatically
        generate high-quality prompts based on a task description. The API creates
        structured prompts suitable for use with Claude models.

        Args:
            task (str): Description of the prompt's purpose
                Example: "a chef for a meal prep planning service"
            target_model (str, optional): Target model for optimization
                Example: "claude-3-7-sonnet-20250219"

        Returns:
            PromptGenerateResponse: Response object containing:
                - messages: List of message objects for use with Messages API
                  - User message with generated prompt text
                  - Optional assistant message with response guidance
                - system: System prompt (currently always empty string)
                - usage: Token usage statistics (input/output tokens)

        Raises:
            RuntimeError: If API request fails or network issues occur
            ValueError: If required configuration/secrets are missing
            requests.HTTPError: If API returns error status codes

        Example:
            &amp;gt;&amp;gt;&amp;gt; response = anthropic_mcp.generate_prompt("a helpful programming assistant")
            &amp;gt;&amp;gt;&amp;gt; prompt_text = response.messages[0].content[0].text
            &amp;gt;&amp;gt;&amp;gt; print(f"Generated prompt: {prompt_text}")

        Note:
            - This is an experimental API in closed research preview
            - Access requires explicit invitation from Anthropic
            - Requires anthropic-beta header: "prompt-tools-2025-04-02"
            - No long-term support guarantees for experimental features
            - Designed primarily for prompt engineering platforms

        API Documentation:
            https://docs.anthropic.com/en/api/prompt-tools-generate
        """
        url = "https://api.anthropic.com/v1/experimental/generate_prompt"

        # Format the task string as a dict for the API
        data = {"task": task}
        if target_model:
            data["target_model"] = target_model

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            return PromptGenerateResponse(**response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def improve_prompt(self, data: dict) -&amp;gt; PromptImproveResponse:
        url = "https://api.anthropic.com/v1/experimental/improve_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptImproveResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def templatize_prompt(self, data: dict) -&amp;gt; PromptTemplatizeResponse:
        url = "https://api.anthropic.com/v1/experimental/templatize_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptTemplatizeResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to templatize prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in templatize_prompt: {e}")
&lt;/file&gt;
  &lt;file path="models/anthropic_prompt_generate.py"&gt;from typing import List, Optional
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: Optional[int] = None
    cache_read_input_tokens: Optional[int] = None
    service_tier: Optional[str] = None


class PromptGenerateResponse(BaseModel):
    """Response from Anthropic's prompt tools generate API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the generated prompt text,
    and may include an assistant message with a prefill."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: UsageStats
    """Token usage statistics for the generation."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;lt;generated prompt&amp;gt;", "type": "text"}],
                "role": "user",
            }
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptGenerateResponse(**example_json)
    print(f"Generated prompt: {response.messages[0].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&lt;/file&gt;
  &lt;file path="models/__init__.py" /&gt;
  &lt;file path="models/anthropic_prompt_improve.py"&gt;from typing import List
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [{"text": "&amp;lt;improved prompt&amp;gt;", "type": "text"}],
                "role": "user",
            },
            {
                "content": [{"text": "&amp;lt;assistant prefill&amp;gt;", "type": "text"}],
                "role": "assistant",
            },
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
&lt;/file&gt;
  &lt;file path="models/anthropic_prompt_templatize.py"&gt;from typing import List, Dict
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptTemplatizeResponse(BaseModel):
    """Response from Anthropic's prompt tools templatize API."""

    messages: List[Message]
    """List of message objects with templated variables."""

    system: str = ""
    """System prompt with templated variables."""

    usage: List[UsageStats]
    """Token usage statistics for the templatization."""

    variable_values: Dict[str, str]
    """Dictionary mapping template variable names to their extracted values."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the templatize endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}",
                        "type": "text",
                    }
                ],
                "role": "user",
            }
        ],
        "system": "You are a professional English to {{TARGET_LANGUAGE}} translator",
        "usage": [{"input_tokens": 490, "output_tokens": 661}],
        "variable_values": {"TARGET_LANGUAGE": "German", "WORD_TO_TRANSLATE": "hello"},
    }

    # Parse into Pydantic model
    response = PromptTemplatizeResponse(**example_json)
    print(f"Templated prompt: {response.messages[0].content[0].text}")
    print(f"System prompt: {response.system}")
    print(f"Variables: {response.variable_values}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
&lt;/file&gt;
  &lt;file path="models/xai_mcp.py"&gt;from config import Config
from secret_manager import SecretManager
import requests


class XaiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        try:
            response = requests.get("https://api.x.ai/v1/models", headers=headers)
            response.raise_for_status()
            data = response.json()

            name_list = [model["id"] for model in data["data"]]
            return name_list

        except Exception as e:
            print(f"Error fetching XAI models: {str(e)}")
            return []

    def extract_text(self, response) -&amp;gt; str:
        """Extract text from XAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # XAI format (same as OpenAI)
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, model: str = None, reasoning_effort: str = "high"
    ):
        try:
            xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

            headers = {
                "Authorization": f"Bearer {xai_key}",
                "Content-Type": "application/json",
            }

            # Use provided model or default to config model
            if model is None:
                model = "grok-3-mini-fast-latest"

            data = {
                "messages": [
                    {"role": "system", "content": self.config.grok_system_prompt},
                    {"role": "user", "content": message},
                ],
                "reasoning_effort": reasoning_effort,
                "model": model,
            }

            url = "https://api.x.ai/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to XAI: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, text: str, model: str = None):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        # Use provided model or default to config model
        if model is None:
            model = "grok-3-fast-latest"

        data = {"model": model, "text": text}

        url = "https://api.x.ai/v1/tokenize-text"
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()

        result = response.json()
        return len(result["token_ids"])
&lt;/file&gt;
  &lt;file path="models/openai_mpc.py"&gt;from config import Config
from secret_manager import SecretManager
import requests
import tiktoken


class OpenAIMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self) -&amp;gt; list:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {"Authorization": f"Bearer {openai_key}"}

            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            response.raise_for_status()

            model_data = response.json()
            name_list = [model["id"] for model in model_data["data"]]

            return name_list

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def extract_text(self, response) -&amp;gt; str:
        """Extract text from OpenAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # OpenAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;gt; dict:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {openai_key}",
            }

            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # Use max_completion_tokens for reasoning models (o3, o1 series)
            if model and ("o3" in model or "o1" in model):
                data = {
                    "model": model,
                    "max_completion_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }
            else:
                data = {
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }

            url = "https://api.openai.com/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;gt; int:
        try:
            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # OpenAI doesn't have a direct token counting API, so use tiktoken
            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(message))

        except Exception as e:
            raise RuntimeError(f"Failed to count tokens: {e}")
&lt;/file&gt;
  &lt;file path="models/gemini_mcp.py"&gt;from config import Config
from secret_manager import SecretManager
import requests
from fetcher import Fetcher
from mcp.server.fastmcp import Context
from typing import Dict, List


class GeminiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&amp;gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.api_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)
        self.base_url = self.config.gemini_base_url

    def get_model_list(self) -&amp;gt; Dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            base_url = self.config.gemini_base_url
            url = f"{base_url}models?key={gemini_key}"
            response = requests.get(url)
            response.raise_for_status()

            return self.filter_models(["2.0", "2.5"], response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def filter_models(
        self, versions: List[str], model_endpoint_response: Dict
    ) -&amp;gt; List[Dict]:
        """
        Filter models by version numbers and include token limits.

        Args:
            versions: List of version strings (e.g., ['2.0', '2.5'])

        Returns:
            List of dicts with model info including inputTokenLimit
        """
        filtered_models = []

        for model in model_endpoint_response["models"]:
            model_name = model["name"].split("/")[-1]

            for version in versions:
                if version in model_name:
                    model_to_tokencount = {
                        "model_name": model_name,
                        "token_window": model.get("inputTokenLimit", 0),
                    }
                    filtered_models.append(model_to_tokencount)

        filtered_models.sort(key=lambda x: x["token_window"], reverse=True)
        return filtered_models

    def extract_text(self, ai_response: dict) -&amp;gt; str:
        # Extract text from response
        if "candidates" in ai_response and len(ai_response["candidates"]) &amp;gt; 0:
            candidate = ai_response["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                if len(parts) &amp;gt; 0 and "text" in parts[0]:
                    return parts[0]["text"]
        return str(ai_response)

    async def build_prompt_from_url(
        self, url: str, prompt: str, ctx: Context = None
    ) -&amp;gt; str:

        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        concat = prompt + response

        ai_response = self.send_message(
            concat, max_tokens=1024, model="gemini-2.5-flash-preview-05-20"
        )

        return self.extract_text(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&amp;gt; dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:generateContent?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": message}]}],
                "generationConfig": {"maxOutputTokens": max_tokens},
            }

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&amp;gt; int:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            # Fix common model name errors
            if model == "gemini-2.5-pro-preview":
                model = "gemini-2.5-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:countTokens?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {"contents": [{"parts": [{"text": message}]}]}

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            result = response.json()
            return result["totalTokens"]

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to count tokens with Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in count_tokens: {e}")
&lt;/file&gt;
  &lt;file path="models/test_xai_mcp.py"&gt;import pytest
from config import Config
from secret_manager import SecretManager
from models.xai_mcp import XaiMCP


@pytest.fixture
def xai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "grok-3-mini-fast-latest"
    return XaiMCP(config, secret_mgr, model)


def test_get_model_list(xai_mcp):
    results = xai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &amp;gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(xai_mcp):
    message = "Hello, world!"
    response = xai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &amp;gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(xai_mcp):
    text = "Hello, world!"
    token_count = xai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &amp;gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(xai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = xai_mcp.send_message(message)
    extracted_text = xai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &amp;gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
&lt;/file&gt;
  &lt;file path="api/prompt_api.py"&gt;from fastapi import APIRouter, Depends, Request, HTTPException
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from config import Config

prompt_api_router = APIRouter()


def get_db_connection(request: Request):
    """Create database connection as a dependency using app state"""
    db_path = request.app.state.db_path
    db = SQLite3Database(db_path=db_path)
    conn = db.get_connection()
    try:
        yield conn
    finally:
        conn.close()


def get_prompt_service(conn=Depends(get_db_connection)):
    """Get prompt service instance with injected database connection"""

    config = Config()
    return PromptService(conn, config)


@prompt_api_router.get("/")
async def welcome() -&amp;gt; dict:
    return {"message": "Welcome to the prompt api service"}


@prompt_api_router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "prompt_api"}


@prompt_api_router.get("/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: str, prompt_service: PromptService = Depends(get_prompt_service)
):
    prompt = prompt_service.get_prompt_by_id(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="prompt not found")

    return prompt
&lt;/file&gt;
  &lt;file path="api/__init__.py"&gt;from .prompt_api import prompt_api_router

__all__ = ["prompt_api_router"]
&lt;/file&gt;
&lt;/source_code&gt;</file>
  <file path="CLAUDE.md">---
allowed-tools: Bash(tools/*)
description: scripts that can be perused and run where appropriate see the examples in this prompt
---

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

**Setup:**
```bash
uv sync
```

**Testing:**
### Use `pytest` for all testing in this project.
### When running all tests, use the Makefile and run test-fast:
### here is an example

```bash
make test-fast
```
### OR use the following bash command:
```bash
uv run pytest -v -n auto -m "not slow"
```


## IMPORTANT: Always Always use uv run when running tests
### Here is an example
```bash
uv run pytest test_collect.py::test_function_name -v -s
# Run specific test: pytest test_collect.py::test_function_name -v -s
```
**Code Quality:**
```bash
make lint     # Run ruff check
make format   # Run black formatter
make check    # Run all: lint, format, test
```

**Run MCP Server:**
```bash
uv run collect.py
```

**Plan Management:**
```bash
# Sync plans from filesystem to database
uv run -m repository.plan_service

# Test plan service functionality (uses separate test database)
uv run pytest repository/test_plan_service.py -v -s

# Test plan database operations
uv run pytest repository/test_plan_service.py::test_sync_plans -v -s

# Set up test database manually (optional - done automatically by tests)
uv run yoyo apply --config yoyo-test-plans.ini --batch

# Reset test database to clean state
rm data/test_plans.db &amp;&amp; uv run yoyo apply --config yoyo-test-plans.ini --batch

# Test database management utilities
uv run python repository/test_database_setup.py setup
uv run python repository/test_database_setup.py reset
uv run python repository/test_database_setup.py cleanup
```

## Planning System

This project uses a structured planning approach for feature development with plans organized in `_docs/plans/`. Plans progress through three stages:

### Plan Lifecycle
1. **`drafts/`** - Initial plans under development or consideration
2. **`approved/`** - Reviewed plans ready for implementation
3. **`completed/`** - Implemented plans with results documented

### Plan Document Format

#### Draft/Approved Plans Should Include:
```markdown
# Plan: [Clear Action-Oriented Title]

## Overview
Brief description of what needs to be implemented and why

## Implementation Steps
### 1. [Step Name]
Detailed implementation instructions including:
- Specific file locations and line numbers
- Function signatures with type hints
- Implementation pseudo-code or actual code
- Error handling approach

### 2. [Next Step]
...

## Key Features
- List of main features/capabilities
- Expected benefits

## Testing Considerations
- Test scenarios to implement
- Edge cases to handle
- Performance considerations

## Example Usage
```python
# Code examples demonstrating the feature
```
```

#### Completed Plans Add:
- **Status**: COMPLETED (YYYY-MM-DD)
- **Implementation Summary**: What was actually done
- **Results**: Outcomes, verification, test results
- **Files Modified**: List with ✅/❌ status indicators

### Plan Naming Conventions
- Use descriptive names with underscores: `add_improve_prompt.md`
- Start with action verbs: add, fix, implement, create, update
- Keep names concise but clear about the purpose

### Automated Plan Processing

The repository includes tools for automated plan implementation:

```python
# Build git worktrees for all approved plans
from mcp__collect__build_worktrees import build_worktrees
result = await build_worktrees(auto_process=True)  # Uses Claude Code SDK

# Sync plans between filesystem and database
from repository.plan_service import PlanService
service = PlanService(conn)
result = service.sync_plans()  # Loads plans into SQLite with JSONB
```

Branch names are automatically derived from plan filenames:
- `add_improve_prompt.md` → `feature/add-improve-prompt`
- Underscores become hyphens, `feature/` prefix added

### Plan Management Database

Plans are tracked in `data/plans.db` with:
- **plans** table: Current state with JSONB data field
- **plan_history**: Audit trail of changes
- **plan_metrics**: Analytics and performance data

Use the PlanService class to:
- Load plans from disk to database
- Track plan status changes
- Detect content changes via SHA256 hashing
- Query plans by status, tags, or content

## Architecture Overview

This is an MCP (Model Context Protocol) server that provides web content fetching and multi-model AI analysis tools. The architecture follows these key patterns:

### Core Structure
- **collect.py**: Main MCP server entry point with FastMCP tool definitions
- **fetcher.py**: Handles URL fetching and content processing with clipboard integration
- **config.py**: Environment-based configuration with dotenv support
- **secret_manager.py**: Google Cloud Secret Manager integration for API keys

### Models Package
The `models/` directory contains unified API wrappers for different AI providers:
- **anthropic_mpc.py**: Anthropic Claude API integration
- **openai_mpc.py**: OpenAI API integration  
- **gemini_mcp.py**: Google Gemini API integration
- **xai_mcp.py**: XAI/Grok API integration

Each model wrapper follows the same pattern: configuration injection, secret management, and standardized methods like `send_message()`, `count_tokens()`, and `get_model_list()`.

### Packages
Additional specialized packages provide focused functionality:
- **reviewer/**: Code review automation system
  - `code_review.py`: CodeReviewer class for analyzing code diffs
  - Supports both file-based and git diff reviews
  - Generates individual model reviews and consolidated summaries
- **repository/**: Plan management and database operations
  - `plan_service.py`: PlanService class for filesystem-to-database sync
  - `plan_models.py`: Pydantic models for plan data with JSONB support
  - `database.py`: SQLite connection management with custom datetime adapters
  - Supports plan lifecycle tracking and content change detection


### Key Features
- **Async token counting**: All providers support async token counting with proper chunking
- **Multi-model workflows**: Send content to all AI models concurrently via `multi_model_code_review()`
- **Content processing**: HTML-to-markdown conversion using readabilipy and markdownify
- **Automatic chunking**: Handles large content (&gt;25k tokens) with intelligent splitting
- **Code review system**: Automated code review via `run_code_review()` and `run_git_diff_review()` tools
- **Prompt engineering**: Generate optimized AI prompts using Anthropic's experimental API via `generate_prompt()`
- **Documentation extraction**: Intelligent section extraction from web docs using `get_docs()` with AI filtering
- **Clipboard integration**: Direct content copying with `copy_clipboard()` and automatic clipboard support in fetchers
- **Enhanced model features**: Gemini model listing with token limits, unified `extract_text()` methods across all providers

### Configuration
Environment variables are loaded from `.env` file:
- GCP_PROJECT_ID (required)
- API key paths for Google Cloud Secret Manager:
  - ANTHROPIC_API_KEY_PATH
  - OPENAI_API_KEY_PATH
  - GEMINI_API_KEY_PATH
  - XAI_API_KEY_PATH
- Default model names for each provider
- Code review model configurations

### Directory Structure
```
collect/
├── data/
│   ├── prompts.db      # Original prompts database
│   └── plans.db        # Plan management database
├── _docs/
│   └── plans/
│       ├── drafts/     # Plans under development
│       ├── approved/   # Plans ready for implementation
│       └── completed/  # Implemented plans with results
├── migrations/         # Database migrations for prompts.db
├── migrations-plans/   # Database migrations for plans.db
├── repository/         # Plan management system
│   ├── plan_service.py # Plan filesystem-to-database sync
│   ├── plan_models.py  # Pydantic models for plan data
│   └── database.py     # SQLite connection management
├── models/            # AI provider API wrappers
└── reviewer/          # Code review automation
```

### Testing Strategy
- **IMPORTANT**:  When writing and designing tests, we only want live direct integration tests. Please only create live direct integration testing. Please do not use mocks. 

## Rules
- **IMPORTANT**: YOU MUST always use `uv run` to run tests.

## Workflow Rules
- I do not want a pr created if I don't have a branch already

## Tools

###IMPORTANT: 
I have a directory from the main project directory called: tools/* wherein there scripts stored that you can use
use

- All of my tools in this directory are on my path and can be called directly.
- You use these tools and see what they do by simply calling the tool name with `--llm`

Example 1:
```bash
extract --llm
```

Example 2: 
```bash
createdb --llm
```


</file>
  <file path="fetcher.py">from typing import List
from mcp.server.fastmcp import Context
import pyperclip
import httpx
from models.anthropic_mpc import AnthropicMCP


class Fetcher:
    def __init__(self, ctx: Context = None) -&gt; None:
        self.ctx = ctx

    async def get(self, url: str) -&gt; str:
        """
        Fetch content from a single URL.

        Args:
            url: URL to fetch content from

        Returns:
            Content from the URL as a string
        """

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                content = response.text

                return content

            except httpx.HTTPError as e:
                return f"Error fetching {url}: {str(e)}"
            except Exception as e:
                return f"Error fetching {url}: {str(e)}"

    async def fetch_urls(self, urls: List[str]) -&gt; str:
        """
        Fetch content from multiple URLs and concatenate their responses.
        If token count exceeds 25000, content is split into chunks.

        Args:
            urls: List of URLs to fetch content from
            ctx: Optional context object for progress reporting

        Returns:
            Either concatenated content from all URLs as a string,
            or a list of content chunks if token count exceeds 25000
        """

        results = []

        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for i, url in enumerate(urls):
                if self.ctx:
                    self.ctx.info(f"Fetching content from {url}")
                    await self.ctx.report_progress(i, len(urls))

                try:
                    response = await client.get(url)
                    response.raise_for_status()

                    results.append(f"\n\n--- Content from {url} --\n\n")
                    results.append(response.text)

                except httpx.HTTPError as e:
                    results.append(f"\n\n --- Error fetching {url}: {str(e)} ---\n\n")
                except Exception as e:
                    results.append(f"\n\n--- error fetching {url}: {str(e)} ---\n\n")

        if self.ctx:
            self.ctx.info("all urls processed")
            await self.ctx.report_progress(len(urls), len(urls))

        content = "".join(results)

        # Copy original content to clipboard
        pyperclip.copy(content)

        # Otherwise return the original content
        return content

    async def chunk_by_token_count(text: str, max_tokens: int = 25000) -&gt; List[str]:
        """
        Split text into chunks that are each under the specified token count.

        Args:
            text: The text to chunk
            max_tokens: Maximum tokens per chunk

        Returns:
            List of text chunks, each under max_tokens
        """

        # If text is short enough, return as a single chunk
        anthropic_mcp = AnthropicMCP()
        token_count = await anthropic_mcp.count_tokens(text, None)
        if token_count &lt;= max_tokens:
            return [text]

        # Split text into paragraphs as a starting point
        paragraphs = text.split("\n\n")
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0

        for paragraph in paragraphs:
            paragraph_tokens = await anthropic_mcp.count_tokens(
                paragraph + "\n\n", None
            )

            # If adding this paragraph would exceed the limit,
            # start a new chunk
            if current_chunk_tokens + paragraph_tokens &gt; max_tokens:
                # If the paragraph alone exceeds the limit, we split it further
                if paragraph_tokens &gt; max_tokens:
                    # Split by sentences or just characters if needed
                    sentences = paragraph.split(". ")
                    for sentence in sentences:
                        sentence_tokens = await anthropic_mcp.count_tokens(
                            sentence + ". ", None
                        )
                        if current_chunk_tokens + sentence_tokens &gt; max_tokens:
                            if current_chunk:
                                chunks.append("".join(current_chunk))
                            current_chunk = [sentence + ". "]
                            current_chunk_tokens = sentence_tokens
                        else:
                            current_chunk.append(sentence + ". ")
                            current_chunk_tokens += sentence_tokens
                else:
                    # Save the current chunk and start a new one
                    chunks.append("".join(current_chunk))
                    current_chunk = [paragraph + "\n\n"]
                    current_chunk_tokens = paragraph_tokens
            else:
                # Add paragraph to current chunk
                current_chunk.append(paragraph + "\n\n")
                current_chunk_tokens += paragraph_tokens

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append("".join(current_chunk))

        return chunks
</file>
  <file path="reviewer/test_diff.md"># Test Code Review

## Diff

```diff
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,5 +1,8 @@
 def calculate_total(items):
+    if not items:
+        return 0
+        
     total = 0
     for item in items:
-        total += item.price
+        total += item.get('price', 0)
     return total
```</file>
  <file path="reviewer/code_review.py">import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from llmrunner import code_review_models_to_mcp, llmrunner, LLMRunnerResults


class CodeReviewer:
    """
    A class for performing multi-model code reviews on diff files.
    """

    def __init__(self, output_dir: str = "codereview"):
        """
        Initialize the CodeReviewer.

        Args:
            output_dir: Default directory for output files
        """
        self.output_dir = output_dir

    def extract_response_text(self, response: Any) -&gt; str:
        """Extract text from different model response formats."""
        if not isinstance(response, dict):
            return str(response)

        # Gemini format
        if "candidates" in response:
            candidates = response["candidates"]
            if candidates and "content" in candidates[0]:
                parts = candidates[0]["content"].get("parts", [])
                if parts and "text" in parts[0]:
                    return parts[0]["text"]

        # OpenAI/XAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        # Anthropic format
        if "content" in response:
            content = response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(response)

    def create_markdown_content(self, result, response_text: str) -&gt; str:
        """Create markdown content for a model result."""
        return f"""
            # Code Review - {result.model}

            **Model**: {result.model}
            **Timestamp**: {result.timestamp}
            **Duration**: {result.duration_seconds:.2f} seconds

            ---

            {response_text}

            ---
            *Generated by {result.model} via MCP Code Review Tool*
        """

    def write_error_file(
        self, output_dir: str, timestamp: str, failed_results: List
    ) -&gt; str:
        """Write error file for failed model results."""
        error_filename = f"errors_{timestamp}.md"
        error_filepath = os.path.join(output_dir, error_filename)

        error_content = f"""
            # Code Review Errors

            **Timestamp**: {timestamp}
            **Failed Models**: {len(failed_results)}

            ## Errors

        """
        for failed_result in failed_results:
            error_content += f"""
                ### {failed_result.model}
                - **Error**: {failed_result.error}
                - **Timestamp**: {failed_result.timestamp}

            """

        with open(error_filepath, "w", encoding="utf-8") as f:
            f.write(error_content)

        return error_filename

    def read_input_file(self, file_path: str) -&gt; str:
        """Read and return content from input file."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Input file {file_path} not found")
        except Exception as e:
            raise Exception(f"Error reading {file_path}: {str(e)}")

    def create_code_review_prompt(self, content: str) -&gt; str:
        """Create the code review prompt."""
        return f"""Please perform a comprehensive code review of the following diff/code changes:

{content}

## Code Review Instructions

Analyze the code changes thoroughly and provide:

### 1. **Overall Assessment**
- Brief summary of what changed and why
- Impact on the codebase (scope and significance)
- Alignment with best practices

### 2. **Issues Found**
Look for and report:
- **Security vulnerabilities** (injection, authentication, authorization, data exposure)
- **Bugs and logic errors** (edge cases, null checks, error handling)
- **Performance issues** (inefficient algorithms, memory leaks, blocking operations)
- **Code quality problems** (readability, maintainability, complexity)
- **Testing gaps** (missing tests, inadequate coverage)

### 3. **Suggestions for Improvement**
Provide specific, actionable recommendations:
- Code structure and organization
- Error handling improvements
- Performance optimizations
- Better naming and documentation
- Refactoring opportunities

### 4. **Positive Aspects**
Highlight what was done well:
- Good patterns and practices used
- Clear, readable code
- Proper error handling
- Well-structured logic

### 5. **Risk Assessment**
Evaluate potential risks:
- **High Risk**: Breaking changes, security issues, data corruption
- **Medium Risk**: Performance degradation, maintainability concerns
- **Low Risk**: Minor style issues, documentation gaps

## Summary Table
End with a concise table of findings:

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| ... | 🔴/🟡/🟢 | ... | ... |

Use emojis: 🔴 Critical, 🟡 Important, 🟢 Minor

Be thorough but concise. Focus on actionable feedback that improves code quality, security, and maintainability."""

    def create_summary(
        self, timestamp: str, from_file: str, results: LLMRunnerResults
    ) -&gt; Dict[str, Any]:
        """Create summary dictionary for the review session."""
        return {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": results.total_models,
            "successful_reviews": results.success_count,
            "failed_reviews": results.failure_count,
            "output_files": [],
        }

    def write_successful_results(
        self,
        results: LLMRunnerResults,
        output_dir: str,
        timestamp: str,
        summary: Dict[str, Any],
    ) -&gt; None:
        """Write markdown files for successful model results."""
        for result in results.successful_results:
            filename = f"{result.model}_{timestamp}.md"
            filepath = os.path.join(output_dir, filename)

            response_text = self.extract_response_text(result.response)
            markdown_content = self.create_markdown_content(result, response_text)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            summary["output_files"].append(filename)

    def write_summary_file(
        self, output_dir: str, timestamp: str, summary: Dict[str, Any]
    ) -&gt; str:
        """Write the summary JSON file."""
        summary_filename = f"summary_{timestamp}.json"
        summary_filepath = os.path.join(output_dir, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        return summary_filename

    async def review_code(
        self, from_file: str, to_file: Optional[str] = None
    ) -&gt; Dict[str, Any]:
        """
        Run code review on a diff file using multiple LLM models.

        Args:
            from_file: Path to the file containing the diff/code to review
            to_file: Directory name to write results to (uses default if None)

        Returns:
            Summary of the code review results
        """
        output_dir = to_file or self.output_dir

        # Read input file and create prompt
        content = self.read_input_file(from_file)
        prompt = self.create_code_review_prompt(content)

        # Run analysis with multiple models
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output directory and timestamp
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary
        summary = self.create_summary(timestamp, from_file, results)

        # Write successful results
        self.write_successful_results(results, output_dir, timestamp, summary)

        # Write error file if needed
        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        # Write summary file
        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }

    async def review_diff_from_git(
        self, to_file: Optional[str] = None, staged_only: bool = True
    ) -&gt; Dict[str, Any]:
        """
        Run code review on git diff output.

        Args:
            to_file: Directory name to write results to (uses default if None)
            staged_only: If True, review only staged changes;
            if False, review changes

        Returns:
            Summary of the code review results
        """
        import subprocess

        # Get git diff
        try:
            if staged_only:
                result = subprocess.run(
                    ["git", "diff", "--staged"],
                    capture_output=True,
                    text=True,
                    check=True,
                )
            else:
                result = subprocess.run(
                    ["git", "diff"], capture_output=True, text=True, check=True
                )

            if not result.stdout.strip():
                raise ValueError("No changes found in git diff")

            diff_content = result.stdout

        except subprocess.CalledProcessError as e:
            raise Exception(f"Git diff failed: {e}")
        except FileNotFoundError:
            raise Exception("Git not found. Make sure git is installed and in PATH")

        # Create prompt directly from diff content
        prompt = self.create_code_review_prompt(diff_content)

        # Run analysis
        output_dir = to_file or self.output_dir
        models_to_mcp = code_review_models_to_mcp()
        results = await llmrunner(prompt, models_to_mcp)

        # Setup output
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Create summary with git diff info
        summary = self.create_summary(timestamp, "git diff", results)
        summary["source"] = "git_diff_staged" if staged_only else "git_diff_all"

        # Write results
        self.write_successful_results(results, output_dir, timestamp, summary)

        if results.failed_results:
            error_filename = self.write_error_file(
                output_dir, timestamp, results.failed_results
            )
            summary["error_file"] = error_filename

        self.write_summary_file(output_dir, timestamp, summary)

        return {
            "status": "completed",
            "summary": summary,
            "output_directory": output_dir,
            "files_created": len(summary["output_files"])
            + (1 if "error_file" in summary else 0)
            + 1,
        }
</file>
  <file path="reviewer/test_code_review_live.py">import pytest
import os
import json
import tempfile
import shutil
from reviewer.code_review import CodeReviewer


class TestCodeReviewLiveIntegration:
    """Live integration tests for code review functionality with real API calls"""

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def test_diff_file(self):
        """Path to the test diff file"""
        return "reviewer/test_diff.md"

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_code_review_from_file(self, test_diff_file, temp_output_dir):
        """Test live code review using test_diff.md with all models"""
        # Verify test file exists
        assert os.path.exists(test_diff_file), f"Test file {test_diff_file} not found"

        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run the code review
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Verify basic result structure
        assert result["status"] == "completed"
        assert "summary" in result
        assert "output_directory" in result
        assert "files_created" in result
        assert result["output_directory"] == temp_output_dir

        # Verify summary data
        summary = result["summary"]
        assert summary["input_file"] == test_diff_file
        assert summary["total_models"] &gt;= 1
        assert (
            summary["successful_reviews"] + summary["failed_reviews"]
            == summary["total_models"]
        )
        assert "timestamp" in summary

        # Verify output files were created
        output_files = os.listdir(temp_output_dir)
        assert len(output_files) &gt;= 1  # At least summary file should exist

        # Verify summary file exists and is valid JSON
        summary_files = [f for f in output_files if f.startswith("summary_")]
        assert len(summary_files) == 1

        summary_path = os.path.join(temp_output_dir, summary_files[0])
        with open(summary_path, "r") as f:
            summary_data = json.load(f)

        assert summary_data["input_file"] == test_diff_file
        assert summary_data["total_models"] == summary["total_models"]

        # Verify individual model review files for successful models
        for output_file in summary["output_files"]:
            file_path = os.path.join(temp_output_dir, output_file)
            assert os.path.exists(file_path)

            # Verify file contains expected content
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                assert "Code Review" in content
                assert "**Model**:" in content
                assert "**Timestamp**:" in content
                assert "**Duration**:" in content
                assert "Generated by" in content

        # Print results for visibility
        print("\n✅ Live code review test completed successfully!")
        print("📊 Results:")
        print(f"  - Total models: {summary['total_models']}")
        print(f"  - Successful reviews: {summary['successful_reviews']}")
        print(f"  - Failed reviews: {summary['failed_reviews']}")
        print(f"  - Files created: {result['files_created']}")

        if summary["successful_reviews"] &gt; 0:
            print(f"  - Review files: {', '.join(summary['output_files'])}")

        if "error_file" in summary:
            print(f"  - Error file: {summary['error_file']}")

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_live_git_diff_review(self, temp_output_dir):
        """Test live git diff review functionality"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        try:
            # Try to run git diff review (may fail if no staged changes)
            result = await reviewer.review_diff_from_git(
                temp_output_dir, staged_only=False
            )

            # If successful, verify structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_all"

            print("\n✅ Git diff review completed!")
            print(
                f"📊 Results: {result['summary']['successful_reviews']} successful, {result['summary']['failed_reviews']} failed"
            )

        except ValueError as e:
            if "No changes found in git diff" in str(e):
                pytest.skip("No git changes found - test requires uncommitted changes")
            else:
                raise

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_code_review_error_handling(self, temp_output_dir):
        """Test error handling with non-existent file"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_output_dir)

    def test_code_review_prompt_generation(self):
        """Test that code review prompt is generated correctly"""
        reviewer = CodeReviewer()
        test_content = "Sample code content"

        prompt = reviewer.create_code_review_prompt(test_content)

        # Verify prompt contains required elements
        assert "comprehensive code review" in prompt
        assert test_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Security vulnerabilities" in prompt
        assert "Suggestions for Improvement" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt
        assert "Summary Table" in prompt
        assert "🔴" in prompt and "🟡" in prompt and "🟢" in prompt

    @pytest.mark.asyncio
    @pytest.mark.slow
    async def test_response_text_extraction_live(self, test_diff_file, temp_output_dir):
        """Test that response text extraction works with real API responses"""
        reviewer = CodeReviewer(output_dir=temp_output_dir)

        # Run a quick review to get real responses
        result = await reviewer.review_code(test_diff_file, temp_output_dir)

        # Test extraction on any successful results
        if result["summary"]["successful_reviews"] &gt; 0:
            # Read one of the output files to verify extraction worked
            first_output = result["summary"]["output_files"][0]
            file_path = os.path.join(temp_output_dir, first_output)

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Should contain actual review content, not just raw API response
            assert len(content) &gt; 100  # Should be substantial content
            assert "Code Review" in content
            assert not content.startswith('{"')  # Should not be raw JSON

            print(f"✅ Response extraction verified for {first_output}")


# Mark all tests in this class as slow integration tests
pytestmark = [pytest.mark.slow, pytest.mark.integration]
</file>
  <file path="reviewer/__init__.py"># Reviewer package for code review functionality
</file>
  <file path="reviewer/test_code_review.py">import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import Mock, patch, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


# Module-level fixtures available to all test classes
@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def reviewer(temp_dir):
    """Create a CodeReviewer instance with temp directory."""
    return CodeReviewer(output_dir=temp_dir)


@pytest.fixture
def sample_diff_content():
    """Sample diff content for testing."""
    return """
diff --git a/test.py b/test.py
index 1234567..abcdefg 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,6 @@
 def hello():
-    print("Hello")
+    print("Hello World")
+    return "greeting"
+
+def goodbye():
+    print("Goodbye")
"""


@pytest.fixture
def mock_model_result():
    """Create a mock successful model result."""
    return ModelResult(
        model="test-model",
        timestamp="2024-01-01T12:00:00",
        success=True,
        actual_model="test-model",
        duration_seconds=2.5,
        response={
            "choices": [
                {
                    "message": {
                        "content": "# Code Review\n\nThis is a test review response."
                    }
                }
            ]
        },
    )


@pytest.fixture
def mock_failed_result():
    """Create a mock failed model result."""
    return ModelResult(
        model="failed-model",
        timestamp="2024-01-01T12:00:00",
        success=False,
        error="API timeout error",
    )


@pytest.fixture
def mock_llm_results(mock_model_result, mock_failed_result):
    """Create mock LLMRunnerResults."""
    return LLMRunnerResults(
        successful_results=[mock_model_result],
        failed_results=[mock_failed_result],
        total_models=2,
        success_count=1,
        failure_count=1,
    )


class TestCodeReviewer:
    """Test suite for CodeReviewer class."""


class TestInitialization:
    """Test CodeReviewer initialization."""

    def test_default_initialization(self):
        """Test CodeReviewer with default parameters."""
        reviewer = CodeReviewer()
        assert reviewer.output_dir == "codereview"

    def test_custom_output_dir(self):
        """Test CodeReviewer with custom output directory."""
        custom_dir = "/tmp/custom_reviews"
        reviewer = CodeReviewer(output_dir=custom_dir)
        assert reviewer.output_dir == custom_dir


class TestExtractResponseText:
    """Test response text extraction from different model formats."""

    def test_extract_gemini_response(self, reviewer):
        """Test extracting text from Gemini response format."""
        gemini_response = {
            "candidates": [
                {"content": {"parts": [{"text": "This is a Gemini response"}]}}
            ]
        }
        result = reviewer.extract_response_text(gemini_response)
        assert result == "This is a Gemini response"

    def test_extract_openai_response(self, reviewer):
        """Test extracting text from OpenAI/XAI response format."""
        openai_response = {
            "choices": [{"message": {"content": "This is an OpenAI response"}}]
        }
        result = reviewer.extract_response_text(openai_response)
        assert result == "This is an OpenAI response"

    def test_extract_anthropic_response(self, reviewer):
        """Test extracting text from Anthropic response format."""
        anthropic_response = {"content": [{"text": "This is an Anthropic response"}]}
        result = reviewer.extract_response_text(anthropic_response)
        assert result == "This is an Anthropic response"

    def test_extract_non_dict_response(self, reviewer):
        """Test extracting text from non-dictionary response."""
        simple_response = "Simple string response"
        result = reviewer.extract_response_text(simple_response)
        assert result == "Simple string response"

    def test_extract_unknown_format(self, reviewer):
        """Test extracting text from unknown response format."""
        unknown_response = {"unknown": "format"}
        result = reviewer.extract_response_text(unknown_response)
        assert result == str(unknown_response)

    def test_extract_empty_gemini_response(self, reviewer):
        """Test extracting from empty Gemini response."""
        empty_response = {"candidates": []}
        result = reviewer.extract_response_text(empty_response)
        assert result == str(empty_response)


class TestMarkdownContent:
    """Test markdown content creation."""

    def test_create_markdown_content(self, reviewer, mock_model_result):
        """Test creating markdown content for a model result."""
        response_text = "Test review content"
        result = reviewer.create_markdown_content(mock_model_result, response_text)

        assert "# Code Review - test-model" in result
        assert "**Model**: test-model" in result
        assert "**Timestamp**: 2024-01-01T12:00:00" in result
        assert "**Duration**: 2.50 seconds" in result
        assert "Test review content" in result
        assert "*Generated by test-model via MCP Code Review Tool*" in result


class TestFileOperations:
    """Test file reading and writing operations."""

    def test_read_input_file_success(self, reviewer, temp_dir, sample_diff_content):
        """Test successfully reading an input file."""
        test_file = os.path.join(temp_dir, "test_diff.md")
        with open(test_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        result = reviewer.read_input_file(test_file)
        assert result == sample_diff_content

    def test_read_input_file_not_found(self, reviewer):
        """Test reading a non-existent file."""
        with pytest.raises(FileNotFoundError, match="Input file .* not found"):
            reviewer.read_input_file("/nonexistent/file.md")

    def test_write_error_file(self, reviewer, temp_dir, mock_failed_result):
        """Test writing error file for failed results."""
        timestamp = "20240101_120000"
        failed_results = [mock_failed_result]

        error_filename = reviewer.write_error_file(temp_dir, timestamp, failed_results)

        assert error_filename == "errors_20240101_120000.md"
        error_path = os.path.join(temp_dir, error_filename)
        assert os.path.exists(error_path)

        with open(error_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review Errors" in content
        assert "**Failed Models**: 1" in content
        assert "### failed-model" in content
        assert "API timeout error" in content

    def test_write_summary_file(self, reviewer, temp_dir):
        """Test writing summary JSON file."""
        timestamp = "20240101_120000"
        summary = {
            "timestamp": timestamp,
            "input_file": "test.md",
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": ["model1_20240101_120000.md"],
        }

        summary_filename = reviewer.write_summary_file(temp_dir, timestamp, summary)

        assert summary_filename == "summary_20240101_120000.json"
        summary_path = os.path.join(temp_dir, summary_filename)
        assert os.path.exists(summary_path)

        with open(summary_path, "r", encoding="utf-8") as f:
            loaded_summary = json.load(f)

        assert loaded_summary == summary


class TestPromptCreation:
    """Test code review prompt creation."""

    def test_create_code_review_prompt(self, reviewer, sample_diff_content):
        """Test creating a comprehensive code review prompt."""
        prompt = reviewer.create_code_review_prompt(sample_diff_content)

        assert "comprehensive code review" in prompt
        assert sample_diff_content in prompt
        assert "Overall Assessment" in prompt
        assert "Issues Found" in prompt
        assert "Suggestions" in prompt
        assert "Positive Aspects" in prompt
        assert "Risk Assessment" in prompt


class TestSummaryCreation:
    """Test summary creation and management."""

    def test_create_summary(self, reviewer, mock_llm_results):
        """Test creating a summary dictionary."""
        timestamp = "20240101_120000"
        from_file = "test.md"

        summary = reviewer.create_summary(timestamp, from_file, mock_llm_results)

        expected_summary = {
            "timestamp": timestamp,
            "input_file": from_file,
            "total_models": 2,
            "successful_reviews": 1,
            "failed_reviews": 1,
            "output_files": [],
        }

        assert summary == expected_summary

    def test_write_successful_results(self, reviewer, temp_dir, mock_llm_results):
        """Test writing successful model results to files."""
        timestamp = "20240101_120000"
        summary = {"output_files": []}

        reviewer.write_successful_results(
            mock_llm_results, temp_dir, timestamp, summary
        )

        # Check that file was created
        expected_filename = "test-model_20240101_120000.md"
        expected_path = os.path.join(temp_dir, expected_filename)
        assert os.path.exists(expected_path)

        # Check that summary was updated
        assert expected_filename in summary["output_files"]

        # Check file content
        with open(expected_path, "r", encoding="utf-8") as f:
            content = f.read()

        assert "# Code Review - test-model" in content
        assert "This is a test review response." in content


class TestReviewCode:
    """Test the main review_code method."""

    @pytest.mark.asyncio
    async def test_review_code_success(
        self, reviewer, temp_dir, sample_diff_content, mock_llm_results
    ):
        """Test successful code review execution."""
        # Create input file
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Mock dependencies
        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # Verify result structure
            assert result["status"] == "completed"
            assert result["output_directory"] == temp_dir
            # 1 success + 1 error + 1 summary
            assert result["files_created"] == 3

            # Verify files were created
            assert os.path.exists(
                os.path.join(temp_dir, "test-model_20240101_120000.md")
            )
            assert os.path.exists(os.path.join(temp_dir, "errors_20240101_120000.md"))
            assert os.path.exists(
                os.path.join(temp_dir, "summary_20240101_120000.json")
            )

    @pytest.mark.asyncio
    async def test_review_code_file_not_found(self, reviewer, temp_dir):
        """Test review_code with non-existent input file."""
        with pytest.raises(FileNotFoundError):
            await reviewer.review_code("/nonexistent/file.md", temp_dir)

    @pytest.mark.asyncio
    async def test_review_code_no_failures(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code with only successful results."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        # Create results with no failures
        success_only_results = LLMRunnerResults(
            successful_results=[
                ModelResult(
                    model="test-model",
                    timestamp="2024-01-01T12:00:00",
                    success=True,
                    actual_model="test-model",
                    duration_seconds=2.5,
                    response={"choices": [{"message": {"content": "Review content"}}]},
                )
            ],
            failed_results=[],
            total_models=1,
            success_count=1,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_only_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_code(input_file, temp_dir)

            # 1 success + 1 summary (no error file)
            assert result["files_created"] == 2
            assert "error_file" not in result["summary"]


class TestReviewDiffFromGit:
    """Test git diff review functionality."""

    @pytest.mark.asyncio
    async def test_review_diff_from_git_staged(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing staged git diff."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            # Setup mocks
            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess call
            mock_subprocess.assert_called_once_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"
            assert result["summary"]["input_file"] == "git diff"

    @pytest.mark.asyncio
    async def test_review_diff_from_git_all_changes(
        self, reviewer, temp_dir, mock_llm_results
    ):
        """Test reviewing all git changes."""
        mock_git_output = "diff --git a/file.py b/file.py\n+added line"

        with (
            patch("subprocess.run") as mock_subprocess,
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
        ):

            mock_subprocess.return_value.stdout = mock_git_output
            mock_subprocess.return_value.check_returncode = Mock()
            mock_models.return_value = Mock()
            mock_runner.return_value = mock_llm_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            result = await reviewer.review_diff_from_git(temp_dir, staged_only=False)

            mock_subprocess.assert_called_once_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

            assert result["summary"]["source"] == "git_diff_all"

    @pytest.mark.asyncio
    async def test_review_diff_no_changes(self, reviewer, temp_dir):
        """Test git diff with no changes."""
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value.stdout = ""
            mock_subprocess.return_value.check_returncode = Mock()

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_not_found(self, reviewer, temp_dir):
        """Test git diff when git is not installed."""
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    @pytest.mark.asyncio
    async def test_review_diff_git_error(self, reviewer, temp_dir):
        """Test git diff with git command error."""
        import subprocess

        with patch(
            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
        ):
            with pytest.raises(Exception, match="Git diff failed"):
                await reviewer.review_diff_from_git(temp_dir)


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_extract_response_malformed_gemini(self, reviewer):
        """Test extracting from malformed Gemini response."""
        malformed = {"candidates": [{"content": {"parts": []}}]}  # Empty parts
        result = reviewer.extract_response_text(malformed)
        assert result == str(malformed)

    def test_extract_response_malformed_openai(self, reviewer):
        """Test extracting from malformed OpenAI response."""
        malformed = {"choices": [{"message": {}}]}  # Missing content
        result = reviewer.extract_response_text(malformed)
        assert result == ""

    def test_extract_response_empty_anthropic(self, reviewer):
        """Test extracting from empty Anthropic response."""
        empty = {"content": []}
        result = reviewer.extract_response_text(empty)
        assert result == str(empty)

    @pytest.mark.asyncio
    async def test_review_code_with_default_output_dir(
        self, reviewer, temp_dir, sample_diff_content
    ):
        """Test review_code using default output directory from None."""
        input_file = os.path.join(temp_dir, "input_diff.md")
        with open(input_file, "w", encoding="utf-8") as f:
            f.write(sample_diff_content)

        success_results = LLMRunnerResults(
            successful_results=[],
            failed_results=[],
            total_models=0,
            success_count=0,
            failure_count=0,
        )

        with (
            patch("reviewer.code_review.code_review_models_to_mcp") as mock_models,
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_runner,
            patch("reviewer.code_review.datetime") as mock_datetime,
            patch("os.makedirs") as mock_makedirs,
        ):

            mock_models.return_value = Mock()
            mock_runner.return_value = success_results
            mock_datetime.now.return_value.strftime.return_value = "20240101_120000"

            # Test with None to_file (should use default)
            result = await reviewer.review_code(input_file, None)

            # Should use reviewer's default output_dir
            mock_makedirs.assert_called_with(temp_dir, exist_ok=True)
            assert result["output_directory"] == temp_dir


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>
  <file path="reviewer/test_code_review_integration.py">import pytest
import os
import json
import tempfile
import shutil
from unittest.mock import patch, MagicMock, AsyncMock
from reviewer.code_review import CodeReviewer
from llmrunner import LLMRunnerResults, ModelResult


class TestCodeReviewIntegration:
    """Test integration between .claude/commands/model_code_review.md and code_review.py"""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def sample_diff_content(self):
        """Sample diff content for testing"""
        return """diff --git a/src/auth.py b/src/auth.py
index 1234567..abcdefg 100644
--- a/src/auth.py
+++ b/src/auth.py
@@ -1,5 +1,10 @@
 def authenticate_user(username, password):
-    # Simple authentication
-    return username == "admin" and password == "secret"
+    # Improved authentication with validation
+    if not username or not password:
+        return False
+    
+    # TODO: Add proper password hashing
+    return username == "admin" and password == "secret123"
 
 def get_user_role(username):
     return "admin" if username == "admin" else "user"
"""

    @pytest.fixture
    def sample_diff_file(self, temp_dir, sample_diff_content):
        """Create sample diff file for testing"""
        diff_file = os.path.join(temp_dir, "test_diff.md")
        with open(diff_file, "w") as f:
            f.write(sample_diff_content)
        return diff_file

    @pytest.fixture
    def mock_llm_results(self):
        """Mock LLM runner results matching expected format"""
        successful_results = [
            ModelResult(
                model="claude-3-5-sonnet",
                success=True,
                response={
                    "content": [
                        {
                            "text": "## Code Review Analysis\n\n### Security Issues\n🔴 **Critical**: Hardcoded password in authentication logic\n\n### Recommendations\n- Implement proper password hashing\n- Add input validation"
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:00",
                duration_seconds=2.5,
                error=None,
            ),
            ModelResult(
                model="gpt-4-turbo",
                success=True,
                response={
                    "choices": [
                        {
                            "message": {
                                "content": "## Security Analysis\n\n🔴 **High Risk**: Authentication uses plaintext password comparison\n🟡 **Medium**: Missing input validation for empty credentials"
                            }
                        }
                    ]
                },
                timestamp="2024-01-01T12:00:05",
                duration_seconds=3.1,
                error=None,
            ),
        ]

        failed_results = [
            ModelResult(
                model="gemini-pro",
                success=False,
                response=None,
                timestamp="2024-01-01T12:00:10",
                duration_seconds=0,
                error="API rate limit exceeded",
            )
        ]

        return LLMRunnerResults(
            successful_results=successful_results,
            failed_results=failed_results,
            total_models=3,
            success_count=2,
            failure_count=1,
        )

    @pytest.mark.asyncio
    async def test_code_review_from_file_integration(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test the complete file-based code review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config", "gpt": "config"}

            # Initialize reviewer with temp directory
            reviewer = CodeReviewer(output_dir=temp_dir)

            # Run review (simulates mcp__collect__run_code_review)
            result = await reviewer.review_code(sample_diff_file, temp_dir)

            # Verify return structure matches command expectations
            assert result["status"] == "completed"
            assert "summary" in result
            assert "output_directory" in result
            assert "files_created" in result

            # Verify output files were created as documented in command
            files = os.listdir(temp_dir)

            # Should have individual model reviews
            claude_files = [f for f in files if f.startswith("claude-3-5-sonnet")]
            gpt_files = [f for f in files if f.startswith("gpt-4-turbo")]
            assert len(claude_files) == 1
            assert len(gpt_files) == 1

            # Should have summary file
            summary_files = [f for f in files if f.startswith("summary_")]
            assert len(summary_files) == 1

            # Should have errors file for failed models
            error_files = [f for f in files if f.startswith("errors_")]
            assert len(error_files) == 1

            # Verify summary JSON structure
            summary_file = os.path.join(temp_dir, summary_files[0])
            with open(summary_file, "r") as f:
                summary_data = json.load(f)

            assert summary_data["total_models"] == 3
            assert summary_data["successful_reviews"] == 2
            assert summary_data["failed_reviews"] == 1
            assert len(summary_data["output_files"]) == 2

    @pytest.mark.asyncio
    async def test_git_diff_review_integration(self, temp_dir, mock_llm_results):
        """Test git diff review workflow as described in Claude command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
            patch("subprocess.run") as mock_subprocess,
        ):

            # Setup mocks
            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {"claude": "config"}

            # Mock git diff output
            mock_subprocess.return_value = MagicMock(
                stdout="diff --git a/test.py b/test.py\n+def new_function():\n+    pass",
                returncode=0,
            )

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Test staged-only review (Option A from command)
            result = await reviewer.review_diff_from_git(temp_dir, staged_only=True)

            # Verify subprocess called with correct git command
            mock_subprocess.assert_called_with(
                ["git", "diff", "--staged"], capture_output=True, text=True, check=True
            )

            # Verify result structure
            assert result["status"] == "completed"
            assert result["summary"]["source"] == "git_diff_staged"

            # Test all changes review
            await reviewer.review_diff_from_git(temp_dir, staged_only=False)
            mock_subprocess.assert_called_with(
                ["git", "diff"], capture_output=True, text=True, check=True
            )

    def test_claude_command_workflow_documentation(self):
        """Verify that the Claude command documentation matches code_review.py capabilities"""
        reviewer = CodeReviewer()

        # Test that CodeReviewer has methods mentioned in command
        assert hasattr(
            reviewer, "review_code"
        ), "Should support file-based review (Option B)"
        assert hasattr(
            reviewer, "review_diff_from_git"
        ), "Should support git diff review (Option A)"

        # Test that review_code signature matches command usage
        import inspect

        sig = inspect.signature(reviewer.review_code)
        assert "from_file" in sig.parameters, "Should accept from_file parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

        # Test that review_diff_from_git signature matches command usage
        sig = inspect.signature(reviewer.review_diff_from_git)
        assert "staged_only" in sig.parameters, "Should accept staged_only parameter"
        assert "to_file" in sig.parameters, "Should accept to_file parameter"

    def test_output_file_naming_convention(
        self, temp_dir, sample_diff_file, mock_llm_results
    ):
        """Test that output files follow naming convention documented in command"""
        with (
            patch(
                "reviewer.code_review.llmrunner", new_callable=AsyncMock
            ) as mock_llmrunner,
            patch(
                "reviewer.code_review.code_review_models_to_mcp"
            ) as mock_models_config,
        ):

            mock_llmrunner.return_value = mock_llm_results
            mock_models_config.return_value = {}

            reviewer = CodeReviewer(output_dir=temp_dir)

            # Mock timestamp for predictable filenames
            with patch("reviewer.code_review.datetime") as mock_datetime:
                mock_datetime.now.return_value.strftime.return_value = "20241201_143052"

                # Run async test
                import asyncio

                asyncio.run(reviewer.review_code(sample_diff_file, temp_dir))

            files = os.listdir(temp_dir)

            # Verify naming matches documentation: {model}_YYYYMMDD_HHMMSS.md
            expected_patterns = [
                "claude-3-5-sonnet_20241201_143052.md",
                "gpt-4-turbo_20241201_143052.md",
                "summary_20241201_143052.json",
                "errors_20241201_143052.md",
            ]

            for pattern in expected_patterns:
                assert pattern in files, f"Expected file {pattern} not found in {files}"

    def test_prompt_structure_matches_command_requirements(self):
        """Test that code review prompt includes all sections mentioned in command"""
        reviewer = CodeReviewer()
        prompt = reviewer.create_code_review_prompt("sample code")

        # Verify prompt includes all required sections from command documentation
        required_sections = [
            "Overall Assessment",
            "Issues Found",
            "Security vulnerabilities",
            "Bugs and logic errors",
            "Performance issues",
            "Code quality problems",
            "Testing gaps",
            "Suggestions for Improvement",
            "Positive Aspects",
            "Risk Assessment",
            "Summary Table",
        ]

        for section in required_sections:
            assert section in prompt, f"Prompt missing required section: {section}"

        # Verify emoji risk indicators are included
        risk_emojis = ["🔴", "🟡", "🟢"]
        for emoji in risk_emojis:
            assert emoji in prompt, f"Prompt missing risk emoji: {emoji}"

    @pytest.mark.asyncio
    async def test_error_handling_matches_command_expectations(self, temp_dir):
        """Test error handling for scenarios mentioned in command troubleshooting"""
        reviewer = CodeReviewer(output_dir=temp_dir)

        # Test "No git changes found" scenario
        with patch("subprocess.run") as mock_subprocess:
            mock_subprocess.return_value = MagicMock(stdout="", returncode=0)

            with pytest.raises(ValueError, match="No changes found in git diff"):
                await reviewer.review_diff_from_git(temp_dir)

        # Test file not found scenario
        with pytest.raises(FileNotFoundError, match="Input file.*not found"):
            await reviewer.review_code("nonexistent_file.md", temp_dir)

        # Test git not available scenario
        with patch("subprocess.run", side_effect=FileNotFoundError("git not found")):
            with pytest.raises(Exception, match="Git not found"):
                await reviewer.review_diff_from_git(temp_dir)

    def test_response_text_extraction_supports_all_models(self):
        """Test that response extraction works for all model formats mentioned in command"""
        reviewer = CodeReviewer()

        # Test Anthropic Claude format
        anthropic_response = {"content": [{"text": "Claude review content"}]}
        assert (
            reviewer.extract_response_text(anthropic_response)
            == "Claude review content"
        )

        # Test OpenAI GPT format
        openai_response = {"choices": [{"message": {"content": "GPT review content"}}]}
        assert reviewer.extract_response_text(openai_response) == "GPT review content"

        # Test Google Gemini format
        gemini_response = {
            "candidates": [{"content": {"parts": [{"text": "Gemini review content"}]}}]
        }
        assert (
            reviewer.extract_response_text(gemini_response) == "Gemini review content"
        )

        # Test fallback for XAI Grok or unknown formats
        unknown_response = "Direct string response"
        assert (
            reviewer.extract_response_text(unknown_response) == "Direct string response"
        )
</file>
  <file path="reviewer/test_codereview_live/errors_20250601_085957.md">
            # Code Review Errors

            **Timestamp**: 20250601_085957
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.646644

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:56.833688

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.262231

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:59:57.434107

            </file>
  <file path="reviewer/test_codereview_live/summary_20250601_085957.json">{
  "timestamp": "20250601_085957",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_085957.md"
}</file>
  <file path="reviewer/test_codereview/errors_20250601_084959.md">
            # Code Review Errors

            **Timestamp**: 20250601_084959
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-gemini-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-gemini-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-gemini-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-gemini-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.675555

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-openai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-openai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-openai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-openai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:58.837903

            
                ### grok-3
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-xai-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-xai-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-xai-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-xai-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.045514

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 403 Permission denied on resource project your-anthropic-secret-path. [reason: "CONSUMER_INVALID"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "secretmanager.googleapis.com"
}
metadata {
  key: "containerInfo"
  value: "your-anthropic-secret-path"
}
metadata {
  key: "consumer"
  value: "projects/your-anthropic-secret-path"
}
, locale: "en-US"
message: "Permission denied on resource project your-anthropic-secret-path."
, links {
  description: "Google developers console"
  url: "https://console.developers.google.com"
}
]
                - **Timestamp**: 2025-06-01T08:49:59.245033

            </file>
  <file path="reviewer/test_codereview/summary_20250601_084601.json">{
  "timestamp": "20250601_084601",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084601.md"
}</file>
  <file path="reviewer/test_codereview/summary_20250601_084959.json">{
  "timestamp": "20250601_084959",
  "input_file": "/Users/benjaminmetz/python/collect/test_diff.md",
  "total_models": 4,
  "successful_reviews": 0,
  "failed_reviews": 4,
  "output_files": [],
  "error_file": "errors_20250601_084959.md"
}</file>
  <file path="reviewer/test_codereview/errors_20250601_084601.md">
            # Code Review Errors

            **Timestamp**: 20250601_084601
            **Failed Models**: 4

            ## Errors

        
                ### gemini-2.5-flash-preview-05-20
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:43:35.383459

            
                ### gpt-4o
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:44:24.400527

            
                ### grok-3
                - **Error**: Unexpected error in send_message: Timeout of 60.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                - **Timestamp**: 2025-06-01T08:45:01.153872

            
                ### claude-sonnet-4-20250514
                - **Error**: Unexpected error in send_message: 504 Deadline Exceeded
                - **Timestamp**: 2025-06-01T08:46:01.157505

            </file>
  <file path="reviewer/live_test_codereview/grok-3-mini-fast-latest_20250601_091219.md">
            # Code Review - grok-3-mini-fast-latest

            **Model**: grok-3-mini-fast-latest
            **Timestamp**: 2025-06-01T09:11:32.476330
            **Duration**: 17.72 seconds

            ---

            Below is a comprehensive code review of the provided diff for the `calculate_total` function in `test.py`, based on the instructions. I'll address each section step by step, focusing on actionable feedback to improve code quality, security, and maintainability.

---

### 1. **Overall Assessment**

- **Brief summary of what changed and why**: The changes enhance the robustness of the `calculate_total` function. An explicit check for an empty `items` list was added to return 0, preventing potential runtime errors. Additionally, the price access was modified from direct attribute access (`item.price`) to a dictionary-based approach (`item.get('price', 0)`), likely to handle cases where `items` are dictionaries rather than custom objects. This suggests a shift in data structure (e.g., from object-oriented to dictionary-based data), possibly to accommodate more flexible input formats or to avoid `AttributeError` when `price` is missing.

- **Impact on the codebase**: The scope is narrow, affecting only this function, but the significance is moderate. It improves error handling and reduces the risk of crashes in edge cases (e.g., empty lists or missing keys), making the code more resilient. However, if this function is used across the application, the change could introduce inconsistencies if other parts of the codebase still expect `items` to be objects with attributes. Overall, it aligns with defensive programming principles.

- **Alignment with best practices**: The changes are positive, as they address common pitfalls like unhandled edge cases and attribute access errors. Using `dict.get()` with a default value follows best practices for handling optional keys, promoting safer code. However, the function could benefit from additional improvements in type safety, documentation, and testing to fully align with modern Python standards (e.g., PEP 8, type hints).

---

### 2. **Issues Found**

I analyzed the diff for potential security, bugs, performance, code quality, and testing issues. Here's a breakdown:

- **Security vulnerabilities**: 
  - No significant security issues were identified. This function performs a simple summation and doesn't involve user input, network operations, or sensitive data handling, so risks like injection or data exposure are low. However, if `items` comes from an untrusted source (e.g., user input or external API), the function could be vulnerable to malicious data (e.g., if `items` contains non-numeric values). This isn't directly addressed in the change.

- **Bugs and logic errors**:
  - **Edge case handling**: The added empty list check is good, but the function now assumes `items` elements are dictionaries (due to `item.get()`). If `items` contains non-dictionary types (e.g., custom objects or other data structures), a `AttributeError` could still occur when calling `get()`. For example, if `items` includes strings or lists, the code would fail. The original code had a similar issue with attribute access, but this change doesn't fully resolve type mismatches.
  - **Error handling**: No broader error handling is present (e.g., for non-numeric `price` values). If a dictionary has a `price` key with a non-numeric value (e.g., a string), a `TypeError` could occur during addition. The `get()` method defaults to 0 for missing keys, which is helpful, but it doesn't validate the type or value of existing keys.
  - **Logic errors**: The function works for valid inputs but could return incorrect results if `items` is not a list (e.g., a string or None). The iteration (`for item in items`) assumes `items` is iterable, which could raise a `TypeError` if it's not.

- **Performance issues**:
  - No major performance concerns. The function uses a linear O(n) loop, which is efficient for most use cases. The addition of the empty check and `get()` method adds negligible overhead. However, if `items` is very large, the function could benefit from Python's built-in `sum()` function for slight optimization.

- **Code quality problems**:
  - **Readability and maintainability**: The code is straightforward, but the shift from attribute access to dictionary access could confuse maintainers if the data structure isn't documented. There's no type hinting, docstring, or comments, making it harder to understand the function's intent or constraints. Code style is mostly fine, but the indentation in the diff looks consistent.
  - **Complexity**: Low cyclomatic complexity, which is good. However, the function could be simplified using Python's `sum()` with a generator expression, reducing code duplication.

- **Testing gaps**:
  - No tests are provided or referenced in the diff, so it's unclear if the changes are covered. Potential gaps include:
    - Tests for edge cases like empty lists, missing keys, non-dictionary items, or invalid data types.
    - Regression tests to ensure the function still works for valid inputs (e.g., lists of dictionaries or objects).
    - The added changes improve robustness, but without tests, there's no assurance that they don't introduce new bugs. Unit tests should cover various scenarios to achieve good code coverage.

---

### 3. **Suggestions for Improvement**

Here are specific, actionable recommendations to enhance the code:

- **Code structure and organization**:
  - Add type hints to clarify expected input and output types, e.g., `def calculate_total(items: list[dict]) -&gt; int`. This would make the function more explicit about assuming dictionary-based items and help catch type errors early with tools like mypy.
  - Consider refactoring the function to handle both dictionary and object-based inputs if needed, e.g., by checking the type of each item and using `getattr(item, 'price', 0)` for objects or `item.get('price', 0)` for dicts. This would make the function more flexible.

- **Error handling improvements**:
  - Add input validation at the start of the function, e.g., check if `items` is iterable and raise a custom error (e.g., `ValueError`) if not. Also, handle potential non-numeric `price` values by adding a type check or conversion, e.g., `total += float(item.get('price', 0))` to avoid `TypeError`.
  - For better robustness, use a try-except block around the loop to catch unexpected errors and log them or return a default value, but avoid overusing exceptions for performance reasons.

- **Performance optimizations**:
  - Replace the manual loop with a more concise and potentially faster approach using `sum()`, e.g., `return sum(item.get('price', 0) for item in items or [])`. This handles the empty case automatically and reduces code lines. The `or []` ensures that if `items` is None, it defaults to an empty list, avoiding errors.

- **Better naming and documentation**:
  - Add a docstring to the function describing its purpose, parameters, return value, and any assumptions (e.g., that items are dictionaries). Example:
    ```
    def calculate_total(items):
        """
        Calculate the total price from a list of items.

        Args:
            items (list): A list of dictionaries, each containing a 'price' key.

        Returns:
            int: The sum of all prices, or 0 if no items are provided.

        Raises:
            ValueError: If items is not iterable or contains invalid data.
        """
        # ... rest of the code
    ```
  - Improve variable naming if needed; `total` is fine, but `items` could be more descriptive (e.g., `item_list`) if the context isn't clear.

- **Refactoring opportunities**:
  - Simplify the code using `sum()` as mentioned earlier. If this function is part of a larger module, consider extracting price retrieval into a separate helper function (e.g., `get_price(item)`) to handle different data types, improving reusability and reducing duplication.

---

### 4. **Positive Aspects**

The changes demonstrate good engineering practices and improve the code's reliability:

- **Good patterns and practices**: Adding the empty list check and using `dict.get()` with a default value is a solid example of defensive programming. It prevents common errors like `AttributeError` or index errors, making the function more robust without overcomplicating the logic.
- **Clear, readable code**: The updated logic is straightforward and easy to follow. The changes maintain simplicity while addressing potential issues, which enhances maintainability.
- **Proper error handling**: The use of `get('price', 0)` elegantly handles missing keys, reducing the need for explicit try-except blocks and improving code flow.
- **Well-structured logic**: The function remains concise and focused on a single responsibility (summing prices), adhering to the single-responsibility principle.

Overall, the diff shows thoughtful improvements that make the code more resilient to real-world inputs.

---

### 5. **Risk Assessment**

- **High Risk**: None identified. The changes are localized and don't introduce breaking changes or security threats, but improper handling of data types could lead to runtime errors in certain scenarios.
- **Medium Risk**: Potential type mismatches (e.g., if `items` elements are not dictionaries) could cause errors or incorrect results. Additionally, lack of testing might lead to undetected bugs in production.
- **Low Risk**: Minor issues like missing documentation or code style inconsistencies, which don't affect functionality but could impact long-term maintainability.

---

### Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential type mismatch for `items` elements | 🟡 Important | Assumes items are dictionaries; could raise errors if elements are custom objects or other types. | Add type hints and input validation, e.g., check if item is dict or use a flexible getter function. |
| Missing error handling for non-numeric prices or non-iterable inputs | 🟡 Important | Could raise `TypeError` if `price` is not a number or if `items` is not iterable. | Add type checks or use safe conversions, e.g., `float(item.get('price', 0))`, and validate `items` at the start. |
| Lack of documentation and docstrings | 🟢 Minor | No explanation of function behavior, parameters, or assumptions, reducing readability. | Add a descriptive docstring and consider inline comments for complex logic. |
| Testing gaps for edge cases | 🟡 Important | No visible tests for empty lists, missing keys, or invalid inputs, risking undetected regressions. | Implement unit tests covering various scenarios, e.g., using pytest with cases for empty, valid, and invalid inputs. |
| Opportunity for code simplification | 🟢 Minor | Manual loop could be replaced with `sum()` for conciseness and performance. | Refactor to use `sum(item.get('price', 0) for item in items or [])` to handle edges automatically. |

This review provides a balanced, actionable critique to help refine the code. If you have additional context (e.g., the rest of the codebase or testing framework), I can refine this further!

            ---
            *Generated by grok-3-mini-fast-latest via MCP Code Review Tool*
        </file>
  <file path="reviewer/live_test_codereview/summary_20250601_091219.json">{
  "timestamp": "20250601_091219",
  "input_file": "test_diff.md",
  "total_models": 4,
  "successful_reviews": 4,
  "failed_reviews": 0,
  "output_files": [
    "gemini-2.0-flash_20250601_091219.md",
    "o3-mini-2025-01-31_20250601_091219.md",
    "grok-3-mini-fast-latest_20250601_091219.md",
    "claude-opus-4-20250514_20250601_091219.md"
  ]
}</file>
  <file path="reviewer/live_test_codereview/o3-mini-2025-01-31_20250601_091219.md">
            # Code Review - o3-mini-2025-01-31

            **Model**: o3-mini-2025-01-31
            **Timestamp**: 2025-06-01T09:11:23.235642
            **Duration**: 9.24 seconds

            ---

            Below is a comprehensive review of the code diff:

──────────────────────────────
1. Overall Assessment
──────────────────────────────
• Summary of Changes:
 – An early exit is added to handle the case where the items list is empty (returning 0).
 – Instead of directly accessing an attribute (item.price), the code now uses the dictionary “get” method (item.get('price', 0)) to retrieve the price value.
 – This indicates a shift in expectation from an object with a price attribute to a dictionary-like object where "price" is a key.
  
• Impact on the Codebase:
 – The changes are localized to the calculate_total function in test.py.
 – The modifications improve robustness when items is empty and when an item does not contain a "price" key.
 – It may affect other parts of the system if they pass objects with a price attribute rather than dictionaries; integration testing is advised.
  
• Alignment with Best Practices:
 – Handling an empty list immediately is a good practice to avoid unnecessary computation.
 – Using item.get with a default value enhances fault tolerance for missing keys.
 – Code readability is maintained, though further documentation could be added.

──────────────────────────────
2. Issues Found
──────────────────────────────
• Security Vulnerabilities:
 – No apparent security concerns such as injection risks or data exposures. The function’s scope is narrow and does not involve external inputs.

• Bugs and Logic Errors:
 – Type Expectations: Changing from attribute access to dictionary access may break callers expecting objects with attributes. Confirm that all items are dictionaries.
 – Implicit Assumptions: The code assumes every item is a dict-like object. There’s no type-checking, so if an unexpected type is encountered, it might raise an AttributeError or TypeError.
 – Early Exit:
  • The early return for empty items is clear, but if items is None (i.e., not provided), it might lead to unexpected None errors unless the caller guarantees a list.

• Performance Issues:
 – The function’s algorithm is simple and works in O(n) time. There are no performance bottlenecks even for larger lists.
 – Memory usage is minimal.

• Code Quality Problems:
 – Readability is high; however, a brief comment explaining the change in expected item format (from object to dict) could help future maintainers.
 

            ---
            *Generated by o3-mini-2025-01-31 via MCP Code Review Tool*
        </file>
  <file path="reviewer/live_test_codereview/claude-opus-4-20250514_20250601_091219.md">
            # Code Review - claude-opus-4-20250514

            **Model**: claude-opus-4-20250514
            **Timestamp**: 2025-06-01T09:11:50.195387
            **Duration**: 29.30 seconds

            ---

            ## Code Review: calculate_total Function Enhancement

### 1. **Overall Assessment**

**Summary of Changes:**
- Added null/empty check for the `items` parameter
- Changed from attribute access (`item.price`) to dictionary access with default value (`item.get('price', 0)`)

**Impact:**
- Improves robustness by handling edge cases (empty input and missing price fields)
- Changes the expected data structure from objects with price attributes to dictionaries
- Prevents potential AttributeError and KeyError exceptions

**Best Practices Alignment:**
- ✅ Defensive programming with input validation
- ✅ Graceful handling of missing data
- ⚠️ Inconsistent data structure assumption (mixing objects and dictionaries)

### 2. **Issues Found**

**Code Quality Problems:**
1. **Inconsistent Data Model**: The change from `item.price` to `item.get('price', 0)` suggests a shift from objects to dictionaries, but this isn't documented or type-hinted
2. **Silent Failure**: Items without prices are silently treated as having price 0, which might mask data quality issues
3. **Missing Type Hints**: No type annotations to clarify expected input/output types
4. **No Documentation**: Function lacks docstring explaining purpose, parameters, and return value

**Potential Logic Issues:**
1. **Type Assumption**: Code assumes items are dictionaries but doesn't validate this
2. **Numeric Type Assumption**: No validation that 'price' values are numeric

### 3. **Suggestions for Improvement**

```python
from typing import List, Dict, Union, Optional
import logging

def calculate_total(items: Optional[List[Dict[str, Union[int, float]]]]) -&gt; Union[int, float]:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dictionaries with 'price' key, or None/empty list
        
    Returns:
        Total price as int or float. Returns 0 for empty/None input.
        
    Raises:
        TypeError: If price value is not numeric
        ValueError: If price value is negative
    """
    if not items:
        return 0
    
    total = 0
    for idx, item in enumerate(items):
        if not isinstance(item, dict):
            raise TypeError(f"Item at index {idx} is not a dictionary")
            
        price = item.get('price', None)
        if price is None:
            logging.warning(f"Item at index {idx} has no price field")
            continue
            
        if not isinstance(price, (int, float)):
            raise TypeError(f"Price for item at index {idx} is not numeric: {price}")
            
        if price &lt; 0:
            raise ValueError(f"Price for item at index {idx} is negative: {price}")
            
        total += price
        
    return total
```

**Alternative approach for mixed data types:**
```python
def calculate_total(items: Optional[List[Union[Dict, object]]]) -&gt; Union[int, float]:
    """Handle both dictionary and object inputs."""
    if not items:
        return 0
    
    total = 0
    for item in items:
        if isinstance(item, dict):
            price = item.get('price', 0)
        elif hasattr(item, 'price'):
            price = item.price
        else:
            price = 0
            
        total += price
        
    return total
```

### 4. **Positive Aspects**

- ✅ **Good defensive programming**: Adding the empty check prevents errors on None/empty inputs
- ✅ **Graceful degradation**: Using `.get()` with default prevents KeyError exceptions
- ✅ **Maintains backward compatibility**: Function signature remains unchanged
- ✅ **Simple and readable**: The logic remains straightforward and easy to understand

### 5. **Risk Assessment**

**Medium Risk** 🟡
- **Breaking Change**: Switching from attribute to dictionary access could break existing code that passes objects
- **Data Quality**: Silent handling of missing prices could hide data issues in production
- **Type Safety**: Lack of validation could lead to runtime errors with unexpected data types

**Mitigation Strategies:**
1. Add comprehensive unit tests covering all data type scenarios
2. Implement logging for items with missing prices
3

            ---
            *Generated by claude-opus-4-20250514 via MCP Code Review Tool*
        </file>
  <file path="reviewer/live_test_codereview/gemini-2.0-flash_20250601_091219.md">
            # Code Review - gemini-2.0-flash

            **Model**: gemini-2.0-flash
            **Timestamp**: 2025-06-01T09:11:16.427900
            **Duration**: 6.81 seconds

            ---

            ## Test Code Review

### 1. **Overall Assessment**

The diff introduces two key changes to the `calculate_total` function:

1.  A check for an empty `items` list, returning 0 in that case.
2.  A change in how the price is accessed: from `item.price` to `item.get('price', 0)`.

The first change handles a potential edge case, preventing errors when the input is empty. The second change makes the code more robust by handling cases where an item might not have a `price` attribute directly, but rather stores it as a dictionary key. These changes enhance the robustness and reliability of the function. The scope is relatively small, impacting only the `calculate_total` function. These changes generally align with best practices for defensive programming and error handling.

### 2. **Issues Found**

*   **Potential Type Error (🟡)**:  While `item.get('price', 0)` handles the absence of the 'price' key, it assumes the value associated with the 'price' key (if it exists) will be a number that can be added to `total`. If `item['price']` exists but is a string (e.g., "unknown"), a `TypeError` would still occur.
*   **Limited Input Validation (🟡)**: The code assumes that each `item` in `items` is a dictionary. It doesn't validate that `items` is even a list, or that each element within it is a dictionary-like object.

### 3. **Suggestions for Improvement**

*   **Type Validation and Error Handling (Important):**  Implement more robust type validation, either with `isinstance` checks or using a try-except block:

    ```python
    def calculate_total(items):
        if not items:
            return 0

        total = 0
        for item in items:
            try:
                price = item.get('price', 0)
                if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
                total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.
        return total
    ```

*   **Consider a dedicated Item class (Minor):** If the structure of `items` is fixed (i.e., always containing dictionaries with a 'price'), consider defining a dedicated `Item` class with a `price` attribute. This would improve code readability and maintainability.

*   **Add input validation (Minor):** Assert that `items` is a list and each element is either a dictionary or an object with a `get` method.

    ```python
    def calculate_total(items):
        if not isinstance(items, list):
            raise TypeError("items must be a list")

        if not items:
            return 0

        total = 0
        for item in items:
            if not hasattr(item, 'get') and not isinstance(item, dict):
                raise TypeError("Each item must be a dictionary or an object with a 'get' method")
              if not isinstance(price, (int, float)):
                    raise ValueError(f"Price must be a number, but got {type(price)}")
              total += price
            except (TypeError, ValueError) as e:
                print(f"Error processing item: {item}. Error: {e}") # Or raise the exception, depending on desired behavior
                # Handle the error, perhaps by skipping the item or logging the error.

        return total
    ```

### 4. **Positive Aspects**

*   **Handles Empty Input (🟢):** The addition of the `if not items` check is a good practice for handling edge cases and preventing potential errors.
*   **Robust Price Access (🟢):** Using `item.get('price', 0)` is a good way to handle cases where the `price` attribute may not be directly available, providing a default value of 0 if the key is missing.

### 5. **Risk Assessment**

*   **Medium Risk**:  The lack of explicit type validation for the `price` can still lead to runtime errors. Implementing the suggested improvement involving the `try-except` block significantly mitigates this risk.

## Summary Table

| Issue | Severity | Description | Suggested Fix |
|-------|----------|-------------|---------------|
| Potential Type Error | 🟡 |  If `item

            ---
            *Generated by gemini-2.0-flash via MCP Code Review Tool*
        
</file>
  <file path="migrations/20250810_02_add-github-url-to-prompt-history.sql">-- Add github_url column to prompt_history table to track project association in historical records
-- depends: 20250810_01_add-projects-table

-- Add github_url column to prompt_history table
ALTER TABLE prompt_history ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Add index for efficient queries by github_url
CREATE INDEX IF NOT EXISTS idx_prompt_history_github_url ON prompt_history(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_history_github_url;
-- ALTER TABLE prompt_history DROP COLUMN github_url;</file>
  <file path="migrations/20250727_01_create-prompt-tables.sql">-- Create prompt tables for prompt storage, versioning, and metrics
-- depends: 

-- Current prompt table
CREATE TABLE IF NOT EXISTS prompt (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    data JSONB NOT NULL,
    version INTEGER DEFAULT 1,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Historical versions table
CREATE TABLE IF NOT EXISTS prompt_history (
    id TEXT,
    version INTEGER,
    data JSONB NOT NULL,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL,
    archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    change_summary TEXT,
    PRIMARY KEY (id, version)
);

-- Metrics time-series table (optimized for prompt tracking)
CREATE TABLE IF NOT EXISTS prompt_metrics (
    prompt_id TEXT,
    version INTEGER,
    metric_name TEXT,
    step INTEGER,
    value REAL,
    timestamp TIMESTAMP,
    PRIMARY KEY (prompt_id, version, metric_name, step)
);

-- Performance-critical indexes
CREATE INDEX IF NOT EXISTS idx_prompt_hash ON prompt(content_hash);
CREATE INDEX IF NOT EXISTS idx_prompt_updated ON prompt(updated_at);
CREATE INDEX IF NOT EXISTS idx_prompt_history_created ON prompt_history(created_at);
CREATE INDEX IF NOT EXISTS idx_prompt_metrics_time ON prompt_metrics(timestamp);

-- Expression indexes on JSONB fields for common queries
CREATE INDEX IF NOT EXISTS idx_prompt_status ON prompt(data -&gt;&gt; '$.status');
CREATE INDEX IF NOT EXISTS idx_prompt_type ON prompt(data -&gt;&gt; '$.type');</file>
  <file path="migrations/20250810_01_add-projects-table.sql">-- Add projects table and update prompt table with project reference
-- depends: 20250727_01_create-prompt-tables

-- Projects table creation with github_url as primary key
CREATE TABLE IF NOT EXISTS projects (
    github_url TEXT PRIMARY KEY,
    description TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add github_url to prompt table
ALTER TABLE prompt ADD COLUMN github_url TEXT REFERENCES projects(github_url) ON DELETE SET NULL;

-- Index for github_url foreign key
CREATE INDEX IF NOT EXISTS idx_prompt_github_url ON prompt(github_url);

-- Down migration (rollback)
-- DROP INDEX IF EXISTS idx_prompt_github_url;
-- ALTER TABLE prompt DROP COLUMN github_url;
-- DROP TABLE IF EXISTS projects;</file>
  <file path="repository/test_database.py">import pytest
import sqlite3
import os

from repository.database import SQLite3Database


@pytest.fixture
def test_db():
    """Create a test database instance"""
    test_db_path = "test_collect.db"
    db = SQLite3Database(db_path=test_db_path)
    yield db
    # Cleanup
    if os.path.exists(test_db_path):
        os.remove(test_db_path)


def test_database_connection_basic(test_db):
    """Test basic database connection establishment"""
    with test_db.get_connection() as conn:
        assert conn is not None
        assert isinstance(conn, sqlite3.Connection)
        # Test basic query
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_connection_read_only(test_db):
    """Test read-only database connection"""
    with test_db.get_connection(read_only=True) as conn:
        assert conn is not None
        # Should be able to read
        cursor = conn.execute("SELECT 1")
        result = cursor.fetchone()
        assert result[0] == 1


def test_database_row_factory(test_db):
    """Test that Row factory is properly configured"""
    with test_db.get_connection() as conn:
        cursor = conn.execute("SELECT 1 as test_col")
        row = cursor.fetchone()
        # Should be able to access by column name
        assert row["test_col"] == 1


def test_database_pragma_settings(test_db):
    """Test that PRAGMA settings are applied correctly"""
    with test_db.get_connection() as conn:
        # Check foreign keys
        cursor = conn.execute("PRAGMA foreign_keys")
        assert cursor.fetchone()[0] == 1

        # Check journal mode
        cursor = conn.execute("PRAGMA journal_mode")
        assert cursor.fetchone()[0] == "wal"

        # Check synchronous mode
        cursor = conn.execute("PRAGMA synchronous")
        assert cursor.fetchone()[0] == 1  # NORMAL = 1


def test_database_context_manager_cleanup():
    """Test that database connections are properly closed"""
    test_db_path = "test_cleanup.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with db.get_connection() as conn:
            conn.execute("SELECT 1")

        # Connection should be closed after context manager exits
        # We can't directly test if it's closed, but we can verify
        # that we can create a new connection successfully
        with db.get_connection() as conn:
            assert conn is not None

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)


def test_database_error_handling():
    """Test error handling and rollback"""
    test_db_path = "test_error.db"
    db = SQLite3Database(db_path=test_db_path)

    try:
        with pytest.raises(sqlite3.Error):
            with db.get_connection() as conn:
                # This should cause an error
                conn.execute("INVALID SQL STATEMENT")

    finally:
        if os.path.exists(test_db_path):
            os.remove(test_db_path)
</file>
  <file path="repository/database.py">import sqlite3

from contextlib import contextmanager
from typing import Generator


class SQLite3Database:
    def __init__(self, db_path: str = "../data/collect.db"):
        self.db_path = db_path

    # Decorator that converts this generator function into a context manager
    @contextmanager
    def get_connection(
        self, read_only: bool = False
    ) -&gt; Generator[sqlite3.Connection, None, None]:
        """Context manager for database connections"""
        # Setup phase: runs when entering 'with' block
        # Enable PARSE_DECLTYPES to use our custom datetime converters
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.row_factory = sqlite3.Row  # enables column access by name

        # Connection optimizations
        conn.execute("PRAGMA foreign_keys = ON")  # enables foreign key support
        if not read_only:
            # enables better concurrency
            conn.execute("PRAGMA journal_mode = WAL")
            conn.execute("PRAGMA synchronous = NORMAL")  # Faster writes
        conn.execute("PRAGMA cache_size = -64000")  # 64MB cache
        # Use memory for temp tables
        conn.execute("PRAGMA temp_store = MEMORY")
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB memory-mapped I/O

        try:
            yield conn  # Pauses here, returns conn to 'with' statement
            # Code inside 'with' block runs here
            if not read_only:
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            # Cleanup phase: always runs when exiting 'with' block
            conn.close()
</file>
  <file path="repository/test_prompt_service.py">import pytest
from typing import List
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from repository.prompt_models import Prompt, PromptType, PromptPlanStatus, CmdCategory
from config import Config


@pytest.fixture
def prompt_service():
    """
    ## How It Works

    1. **`with db.get_connection() as conn:`**
       - Opens a database connection using a context manager
       - The `as conn` assigns the connection to the variable
       - When the `with` block exits, `conn.close()` is automatically called

    2. **`cmd_service = CmdsService(conn)`**
       - Creates the service object with the database connection
       - The service can now execute database operations

    3. **`yield cmd_service`**
       - This is pytest fixture syntax that provides the service to the test
       - `yield` pauses execution here while the test runs
       - After the test completes, execution resumes after the `yield`

    4. **Automatic cleanup**
       - When the test finishes, the `with` block exits
       - Database connection is automatically closed
       - Resources are freed

    This pattern ensures **deterministic cleanup** -
    the database connection will always be properly closed regardless of
    whether the test passes or fails.
    """
    config = Config()
    db = SQLite3Database(config.db_path)
    with db.get_connection() as conn:
        cmd_service = PromptService(conn, config)

        yield cmd_service


def test_check_dirs(prompt_service: PromptService):
    result = prompt_service.cmd_check_dirs()
    assert result is True


def test_load_cmds_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_cmds_from_disk()
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def test_load_plans_from_disk(prompt_service: PromptService):
    load_results = prompt_service.load_plans_from_disk()

    print(f"\nTotal plans loaded: {len(load_results.loaded_prompts)}")
    # Assert no errors occurred during loading
    assert (
        load_results.errors is None or len(load_results.errors) == 0
    ), f"Expected no errors, but found {
        len(load_results.errors) if load_results.errors else 0} errors"


def create_test_prompts(prompt_service: PromptService) -&gt; List[Prompt]:
    prompt_content = """
    this is a test prompt for testing database persistence... blah blah
    """

    def new_cmd_prompt(prompt_content: str) -&gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.CMD,
            cmd_category=CmdCategory.PYTHON,
            status=PromptPlanStatus.DRAFT,
            project="collect",
            description="A basic test prompt",
            tags=["test", "python", "cmd"],
        )

    def new_plan_prompt(prompt_content: str) -&gt; Prompt:
        return prompt_service.new_prompt_model(
            prompt_content=prompt_content,
            name="test_prompt.md",
            prompt_type=PromptType.PLAN,
            cmd_category=None,
            status=PromptPlanStatus.APPROVED,
            project="collect",
            description="A basic prd prompt",
            tags=["test", "python", "plan"],
        )

    return [new_cmd_prompt(prompt_content), new_plan_prompt(prompt_content)]


def test_save_prompt_in_db(prompt_service: PromptService):
    # create test cmd and plan prompt types
    pls = create_test_prompts(prompt_service)
    cmd_prompt = pls[0]
    plan_prompt = pls[1]

    try:
        # save test prompts in sqlite and verify success
        cmd_result = prompt_service.save_prompt_in_db(cmd_prompt)
        print(f"cmd_result: {cmd_result}")
        assert cmd_result.success is not False

        plan_result = prompt_service.save_prompt_in_db(plan_prompt)
        print(f"plan_result: {plan_result}")
        assert plan_result.success is not False

        # retrieve the saved test prompts from sqlite and verify they
        # match the original test cmd and plan prompts
        print(f"Retrieving cmd prompt with id: {cmd_prompt.id}")
        retrieved_cmd = prompt_service.get_prompt_by_id(cmd_prompt.id)
        print(f"Retrieved cmd: {retrieved_cmd}")
        assert retrieved_cmd is not None

        retrieved_plan = prompt_service.get_prompt_by_id(plan_prompt.id)
        assert retrieved_plan is not None

        # update prompt and increment the version
        updated_text = cmd_prompt.data.content + "UPDATED TEXT"
        cmd_prompt.data.content = updated_text

        update_result = prompt_service.update_prompt_in_db(cmd_prompt)
        assert update_result.success is True

        # retrieve the updated prompt again from the prompt table and
        # validate the changes were persisted/updated
        retrieved_prompt = prompt_service.get_prompt_by_id(cmd_prompt.id)
        assert retrieved_prompt.data.content == updated_text

        # retrieve the prompt by name
        # and validate correct prompt retrieval
        retrieved_prompt_by_name = prompt_service.get_prompt_by_name(cmd_prompt.name)
        assert retrieved_prompt_by_name is not None
        assert retrieved_prompt_by_name.id == cmd_prompt.id

    finally:
        # Clean up test data - this will ALWAYS run, even if test fails
        print("\nCleaning up test prompts...")

        cmd_cleanup = delete_prompt_completely(prompt_service, cmd_prompt.id)
        print(f"CMD cleanup result: {cmd_cleanup}")

        plan_cleanup = delete_prompt_completely(prompt_service, plan_prompt.id)
        print(f"PLAN cleanup result: {plan_cleanup}")


def delete_prompt_completely(prompt_service: PromptService, prompt_id: str):
    """
    DELETE a prompt from tables: prompt, prompt_history and prompt_metrics
    THIS IS FOR INTEGRATION TESTING ONLY - as production code should reserve
    history
    """
    cursor = prompt_service.conn.cursor()
    try:
        # start transaction
        cursor.execute("BEGIN TRANSACTION")

        # delete from prompt_history first (due to composite primary key)
        cursor.execute(
            """
                       DELETE FROM prompt_history
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_history_rows_deleted = cursor.rowcount

        # delete from prompt_metrics table if any exist
        cursor.execute(
            """
                       DELETE FROM prompt_metrics
                       WHERE prompt_id = ?
                       """,
            (prompt_id,),
        )
        prompt_metrics_rows_deleted = cursor.rowcount

        # delete from prompt table (we do this last)
        cursor.execute(
            """
                       DELETE FROM prompt
                       WHERE id = ?
                       """,
            (prompt_id,),
        )
        prompt_rows_deleted = cursor.rowcount

        prompt_service.conn.commit()
        return {
            "success": True,
            "prompt_rows": prompt_rows_deleted,
            "prompt_history_rows": prompt_history_rows_deleted,
            "prompt_metrics_rows": prompt_metrics_rows_deleted,
        }

    except Exception as e:
        prompt_service.conn.rollback()
        return {"success": False, "error": str(e)}


def test_prompt_loading(prompt_service: PromptService):
    cmds = prompt_service.load_cmds_from_disk()
    print(f"\nTotal commands loaded: {len(cmds.loaded_prompts)}")
    assert len(cmds.errors) == 0

    plans = prompt_service.load_plans_from_disk()
    print(f"\nTotal plans loaded: {len(plans.loaded_prompts)}")
    assert len(plans.errors) == 0

    prompts = cmds.loaded_prompts + plans.loaded_prompts

    results = prompt_service.bulk_save_in_db(prompts)

    bad_results = [result for result in results if not result.success]
    good_results = [result for result in results if result.success]

    print(f"\nGood Result count: {len(good_results)}")
    print(f"\nBad Result count: {len(bad_results)}")
</file>
  <file path="repository/prompt_models.py">from pydantic import BaseModel, Field
from datetime import datetime
from enum import Enum
from typing import Optional, List
from config import Config


class PromptPlanStatus(str, Enum):
    """Plan status types"""

    DRAFT = "draft"
    APPROVED = "approved"
    COMPLETED = "completed"


class PromptType(str, Enum):
    """Prompt types"""

    PLAN = "plan"
    CMD = "cmd"


def create_cmd_category_enum():
    """Create CmdCategory enum dynamically from config"""
    try:
        config = Config()
        subdirs = config.command_subdirs
    except Exception:
        # Fallback to default subdirs if config fails
        subdirs = ["archive", "go", "js", "mcp", "python", "tools"]

    # Build enum members dictionary
    members = {}
    for subdir in subdirs:
        members[subdir.upper()] = subdir

    # Always include UNCATEGORIZED as fallback
    members["UNCATEGORIZED"] = "uncategorized"

    # Create enum using the functional API with type=str for JSON serialization
    return Enum("CmdCategory", members, type=str)


# Create the enum instance
CmdCategory = create_cmd_category_enum()


class Project(BaseModel):
    github_url: str
    description: str
    created_at: datetime
    updated_at: datetime


class PromptData(BaseModel):
    """Structured data for prompt JSONB field"""

    type: PromptType
    status: PromptPlanStatus
    project: Optional[str]
    cmd_category: Optional[CmdCategory]
    content: str  # This is the prompt content, in markdown
    description: Optional[str] = None
    # using 'claude' or 'gemini' here to specify the dir it will write to
    # .claude/commands and .gemini/commands respectively
    tags: List[str] = Field(default_factory=list)


class Prompt(BaseModel):
    id: str
    name: str
    github_url: Optional[str]
    data: PromptData  # Structured JSONB data
    version: int
    content_hash: str
    created_at: datetime
    updated_at: datetime


class PromptCreate(BaseModel):
    id: str
    name: str
    data: PromptData
    content_hash: str
    version: Optional[int] = 1


class LoadError(BaseModel):
    filename: str
    error_message: str
    error_type: str


class PromptCreateResult(BaseModel):
    """Result of creating a new prompt"""

    success: bool
    prompt_id: str
    version: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptLoadResult(BaseModel):
    """Result of loading prompts into database"""

    loaded_prompts: List[Prompt]
    errors: Optional[List[LoadError]] = None


class PromptDeleteResult(BaseModel):
    success: bool
    prompt_id: str
    deleted: bool
    rows_affected: int
    error_message: Optional[str] = None
    error_type: Optional[str] = None


class PromptFlattenResult(BaseModel):
    """Result of flattening a prompt to disk"""

    success: bool
    prompt_id: str
    prompt_name: str
    file_path: str
    cmd_category: str
    error_message: Optional[str] = None
    error_type: Optional[str] = None
</file>
  <file path="repository/datetime_adapters.py">"""Custom datetime adapters for SQLite3 to avoid Python 3.12 deprecation warnings.

This module provides custom adapters and converters for datetime objects when
working with SQLite databases in Python 3.12+, replacing the deprecated default
adapters.
"""

import datetime
import sqlite3


def adapt_datetime_iso(val):
    """Adapt datetime.datetime to timezone-naive ISO 8601 format.

    Args:
        val: datetime.datetime object to adapt

    Returns:
        str: ISO 8601 formatted datetime string
    """
    return val.replace(tzinfo=None).isoformat()


def adapt_date_iso(val):
    """Adapt datetime.date to ISO 8601 date format.

    Args:
        val: datetime.date object to adapt

    Returns:
        str: ISO 8601 formatted date string
    """
    return val.isoformat()


def convert_datetime_iso(val):
    """Convert ISO 8601 datetime string to datetime.datetime object.

    Args:
        val: bytes object containing ISO 8601 datetime string

    Returns:
        datetime.datetime: Parsed datetime object
    """
    return datetime.datetime.fromisoformat(val.decode())


def convert_date_iso(val):
    """Convert ISO 8601 date string to datetime.date object.

    Args:
        val: bytes object containing ISO 8601 date string

    Returns:
        datetime.date: Parsed date object
    """
    return datetime.date.fromisoformat(val.decode())


def convert_timestamp(val):
    """Convert Unix timestamp to datetime.datetime object.

    Args:
        val: bytes object containing Unix timestamp

    Returns:
        datetime.datetime: Datetime object from timestamp
    """
    return datetime.datetime.fromtimestamp(int(val))


def register_adapters():
    """Register all custom datetime adapters and converters with sqlite3.

    This function should be called once at application startup to configure
    SQLite to use our custom datetime handling instead of the deprecated
    default handlers.
    """
    # Register adapters (Python -&gt; SQLite)
    sqlite3.register_adapter(datetime.datetime, adapt_datetime_iso)
    sqlite3.register_adapter(datetime.date, adapt_date_iso)

    # Register converters (SQLite -&gt; Python)
    sqlite3.register_converter("TIMESTAMP", convert_datetime_iso)
    sqlite3.register_converter("DATETIME", convert_datetime_iso)
    sqlite3.register_converter("DATE", convert_date_iso)


# Automatically register adapters when module is imported
register_adapters()
</file>
  <file path="repository/prompt_service.py">import sqlite3
from pathlib import Path
import uuid
import json
from datetime import datetime, timezone
import hashlib
from typing import Optional, List, Tuple
from repository.prompt_models import (
    PromptLoadResult,
    LoadError,
    CmdCategory,
    PromptType,
    PromptPlanStatus,
    PromptData,
    Prompt,
    PromptCreateResult,
    PromptDeleteResult,
    PromptFlattenResult,
)
from config import Config


class PromptService:
    def __init__(self, conn: sqlite3.Connection, config: Config):
        self.conn = conn
        self.config = config
        self.plans_check_dirs()
        self.cmd_check_dirs()

    def plans_check_dirs(self) -&gt; bool:
        """Check if all required plan directories exist, create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        # Required directories
        required_dirs = [
            plans_dir,
            plans_dir / "drafts",
            plans_dir / "approved",
            plans_dir / "completed",
        ]

        missing_dirs = []
        created_dirs = []

        for dir_path in required_dirs:
            if not dir_path.exists():
                missing_dirs.append(dir_path)

        if missing_dirs:
            print("📁 Creating missing plan directories:")
            for missing_dir in missing_dirs:
                try:
                    missing_dir.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(missing_dir)
                    print(
                        f"   ✅ Created: {
                            missing_dir.relative_to(project_dir)}"
                    )
                except Exception as e:
                    print(
                        f"   ❌ Failed to create {
                            missing_dir.relative_to(project_dir)}: {e}"
                    )
                    return False

            if created_dirs:
                print(
                    f"📁 Successfully created {
                        len(created_dirs)} directories"
                )
        else:
            print("✅ All required plan directories exist")

        return True

    def cmd_check_dirs(self) -&gt; bool:
        """Check if all required command directories exist,
           create them if missing

        Returns:
            bool: True if all directories exist or were created successfully,
            False on error
        """
        project_dir = Path(__file__).parent.parent
        claude_dir = project_dir / ".claude"
        gemini_dir = project_dir / ".gemini"

        # Get subdirectories from config
        config = Config()
        subdirs = config.command_subdirs

        # Build required directories
        required_dirs = {
            "claude": [claude_dir / "commands"]
            + [claude_dir / "commands" / subdir for subdir in subdirs],
            "gemini": [gemini_dir / "commands"]
            + [gemini_dir / "commands" / subdir for subdir in subdirs],
        }

        # Check for missing directories by type
        missing_by_type = {"claude": [], "gemini": []}
        for dir_type, dirs in required_dirs.items():
            for dir_path in dirs:
                if not dir_path.exists():
                    missing_by_type[dir_type].append(dir_path)

        # Count total missing
        total_missing = sum(len(dirs) for dirs in missing_by_type.values())

        if total_missing == 0:
            print("✅ All required command directories exist")
            return True

        # Create missing directories
        print(f"📁 Creating {total_missing} missing command directories:")
        created_count = 0
        failed = False

        for dir_type, missing_dirs in missing_by_type.items():
            if missing_dirs:
                print(f"\n   {dir_type.title()} directories:")
                for missing_dir in missing_dirs:
                    try:
                        missing_dir.mkdir(parents=True, exist_ok=True)
                        created_count += 1
                        print(
                            f"   ✅ Created: {
                                missing_dir.relative_to(project_dir)}"
                        )
                    except Exception as e:
                        print(
                            f"   ❌ Failed to create {
                                missing_dir.relative_to(project_dir)}: {e}"
                        )
                        failed = True

        if created_count &gt; 0:
            print(f"\n📁 Successfully created {created_count} directories")

        return not failed

    def _load_cmds_from_directory(
        self, cmds_dir: Path, source: str
    ) -&gt; Tuple[List[Prompt], List[LoadError]]:
        """Load commands from a specific directory

        Args:
            cmds_dir: Path to the commands directory
            source: Source identifier ('claude' or 'gemini')

        Returns:
            Tuple of (prompts list, errors list)
        """
        prompts = []
        errors = []

        if not cmds_dir.exists():
            return prompts, errors

        # Loop through the files in cmds dir and load prompts first
        for file in cmds_dir.iterdir():
            if file.is_file():
                try:
                    # Check if filename adheres to naming rules
                    current_filename = file.name
                    if not self.check_filename(current_filename):
                        # Only rename files during explicit operations, not during loading
                        # Skip file renaming when just loading/reading files
                        print(
                            f"⚠️  File {
                                current_filename} doesn't follow naming convention but will not be renamed during load operation"
                        )

                    prompt_content = file.read_text()
                    prompt = self.new_prompt_model(
                        prompt_content=prompt_content,
                        name=file.name,
                        prompt_type=PromptType.CMD,
                        cmd_category=CmdCategory.UNCATEGORIZED,
                        status=PromptPlanStatus.DRAFT,
                        tags=[source],  # Add source tag
                    )
                    prompts.append(prompt)

                except Exception as e:
                    errors.append(
                        LoadError(
                            filename=str(file),
                            error_message=str(e),
                            error_type=type(e).__name__,
                        )
                    )

        # Then cycle through the subdirs, create Prompt models and append
        for sub_dir in cmds_dir.iterdir():
            if sub_dir.is_dir():
                try:
                    cmd_category = CmdCategory(sub_dir.name.lower())

                    for file in sub_dir.iterdir():
                        try:
                            if file.is_file():
                                # Check if filename adheres to naming rules
                                current_filename = file.name
                                if not self.check_filename(current_filename):
                                    # Normalize the filename
                                    fixed_filename = self.normalize_filename(
                                        current_filename
                                    )

                                    # Create new file path with normalized name
                                    new_file_path = file.parent / fixed_filename

                                    # Rename the file on disk
                                    file.rename(new_file_path)

                                    # Update file reference to the new path
                                    file = new_file_path
                                    print(
                                        f"📝 Renamed: {current_filename} → {
                                            fixed_filename}"
                                    )

                                prompt_content = file.read_text()
                                prompt = self.new_prompt_model(
                                    prompt_content=prompt_content,
                                    name=file.name,
                                    prompt_type=PromptType.CMD,
                                    cmd_category=cmd_category,
                                    status=PromptPlanStatus.DRAFT,
                                    tags=[source],  # Add source tag
                                )
                                prompts.append(prompt)

                        except Exception as e:
                            errors.append(
                                LoadError(
                                    filename=str(file),
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )
                except ValueError:
                    # Skip directories that don't match valid CmdCategory values
                    continue

        return prompts, errors

    def load_cmds_from_disk(self) -&gt; PromptLoadResult:
        """Load commands from both .claude and .gemini directories

        Returns:
            PromptLoadResult: Combined results from both directories
        """
        project_dir = Path(__file__).parent.parent
        claude_cmds_dir = project_dir / ".claude" / "commands"
        gemini_cmds_dir = project_dir / ".gemini" / "commands"

        all_prompts = []
        all_errors = []

        # Load from Claude directory
        claude_prompts, claude_errors = self._load_cmds_from_directory(
            claude_cmds_dir, "claude"
        )
        all_prompts.extend(claude_prompts)
        all_errors.extend(claude_errors)

        # Load from Gemini directory
        gemini_prompts, gemini_errors = self._load_cmds_from_directory(
            gemini_cmds_dir, "gemini"
        )
        all_prompts.extend(gemini_prompts)
        all_errors.extend(gemini_errors)

        return PromptLoadResult(
            loaded_prompts=all_prompts,
            errors=all_errors,
        )

    def load_plans_from_disk(self) -&gt; PromptLoadResult:
        project_dir = Path(__file__).parent.parent
        plans_dir = project_dir / "_docs" / "plans"

        status_mapping = {
            "drafts": PromptPlanStatus.DRAFT,
            "approved": PromptPlanStatus.APPROVED,
            "completed": PromptPlanStatus.COMPLETED,
        }

        prompts = []
        errors = []

        for subdir in plans_dir.iterdir():
            if subdir.is_dir() and subdir.name in status_mapping:
                cmd_category = None
                status = status_mapping[subdir.name]
                for file in subdir.iterdir():
                    try:
                        if file.is_file():
                            # Check if filename adheres to naming rules
                            current_filename = file.name
                            if not self.check_filename(current_filename):
                                # Normalize the filename
                                fixed_filename = self.normalize_filename(
                                    current_filename
                                )

                                # Create new file path with normalized name
                                new_file_path = file.parent / fixed_filename

                                # Rename the file on disk
                                file.rename(new_file_path)

                                # Update file reference to the new path
                                file = new_file_path
                                print(
                                    f"""📝 Renamed: {current_filename} → {
                                        fixed_filename}
                                      """
                                )

                            prompts.append(
                                self.new_prompt_model(
                                    prompt_content=file.read_text(),
                                    name=file.name,
                                    prompt_type=PromptType.PLAN,
                                    github_url=self.config.github_url,
                                    cmd_category=cmd_category,
                                    status=status,
                                    project=project_dir.name,
                                )
                            )

                    except Exception as e:
                        errors.append(
                            LoadError(
                                filename=str(file),
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

        return PromptLoadResult(loaded_prompts=prompts, errors=errors)

    def normalize_filename(self, filename: str) -&gt; str:
        """Normalize filename to use underscores and ensure .md or .toml extension

        Args:
            filename: The original filename

        Returns:
            Normalized filename with underscores and .md or .toml extension
        """
        # Replace hyphens with underscores
        normalized = filename.replace("-", "_")

        # Check if it already has .md or .toml extension
        if normalized.endswith(".md") or normalized.endswith(".toml"):
            return normalized

        # If it has another extension, replace it with .md
        if "." in normalized:
            normalized = normalized.rsplit(".", 1)[0] + ".md"
        else:
            # No extension, add .md as default
            normalized = normalized + ".md"

        return normalized

    def check_filename(self, filename: str) -&gt; bool:
        """Check if filename adheres to naming rules
        (underscores and .md or .toml extension)

        Args:
            filename: The filename to check

        Returns:
            bool: True if filename follows the rules, False otherwise
        """
        # Check if filename has .md or .toml extension
        if not (filename.endswith(".md") or filename.endswith(".toml")):
            return False

        # Check if filename contains hyphens (should use underscores)
        if "-" in filename:
            return False

        return True

    def new_prompt_model(
        self,
        prompt_content: str,
        name: str,
        prompt_type: PromptType,
        github_url: Optional[str] = None,
        cmd_category: Optional[CmdCategory] = None,
        status: PromptPlanStatus = PromptPlanStatus.DRAFT,
        project: Optional[str] = None,
        description: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ) -&gt; Prompt:
        if prompt_type == PromptType.CMD and not cmd_category:
            raise ValueError("CMD type prompts require a category")

        default_tags = []
        if cmd_category:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                default_tags.append(cmd_category)
            else:
                default_tags.append(cmd_category.value)
        default_tags.append(prompt_type.value)

        # Merge custom tags with default tags
        all_tags = default_tags + (tags if tags else [])

        prompt_data = PromptData(
            type=prompt_type,
            status=status,
            project=project,
            cmd_category=cmd_category,
            content=prompt_content,
            description=description,
            tags=all_tags,
        )

        content_hash = hashlib.sha256(prompt_content.encode("utf-8")).hexdigest()

        timestamp = datetime.now(timezone.utc)

        db_name = self.create_db_name(
            prompt_type=prompt_type,
            prompt_status=status,
            cmd_category=cmd_category,
            project_name=project,
            name=name,
        )

        prompt = Prompt(
            id=str(uuid.uuid4()),
            name=db_name,
            github_url=github_url,
            data=prompt_data,
            version=1,
            content_hash=content_hash,
            created_at=timestamp,
            updated_at=timestamp,
        )

        return prompt

    def create_db_name(
        self,
        prompt_type: PromptType,
        prompt_status: Optional[PromptPlanStatus],
        cmd_category: Optional[CmdCategory],
        project_name: Optional[str],
        name: str,
    ) -&gt; str:
        # in the directory [project]/_docs/plans:
        # there are directories: draft, approved and completed
        # we model those as PromptPlanStatus -&gt; see prompt_models.py
        if prompt_type == PromptType.PLAN:
            create_name = project_name + "_" + prompt_status.value + "_" + name
        if prompt_type == PromptType.CMD:
            # Handle both enum and string values
            if isinstance(cmd_category, str):
                create_name = cmd_category + "_" + name
            else:
                create_name = cmd_category.value + "_" + name

        return create_name

    def parse_db_name(self, db_name: str, prompt_type: PromptType) -&gt; str:
        """Extract the original filename from the database name

        Args:
            db_name: The database name
            (e.g., 'collect-approved-update_function.md')
            prompt_type: The type of prompt(PLAN or CMD)

        Returns:
            The original filename(e.g., 'update_function.md')
        """
        # split the name to a list using '_' seperator
        ls = db_name.split("_")
        # rebuild filename from the list of split words
        filename = ""
        if prompt_type == PromptType.PLAN:
            # if prompt type is PLAN: then name will include the project
            # so we need to drop the first 2 words in the db_name
            # example: collect_completed_add_claude_sdk_processing.md
            # ls = [collect, completed, add, claude, sdk, processing.md]
            for word in ls[2:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

        if prompt_type == PromptType.CMD:
            # if prompt type is CMD: then name will only include the dir/type
            # so we only need to drop the first word in ls
            # example: tools_create_database.md
            # ls = [tools, create, database.md]
            for word in ls[1:]:
                if not word.endswith(".md"):
                    filename = filename + word + "_"
                else:
                    filename = filename + word
            return filename

    def check_exists(self, name: str) -&gt; Tuple[bool, str]:
        """Check if a prompt with the given name already exists

        Args:
            name: The prompt name to check

        Returns:
            Tuple[bool, str]: (exists, prompt_id)
            where exists is True if prompt exists,
            and prompt_id is the ID if found, empty string if not found
        """
        cursor = self.conn.cursor()
        cursor.execute("SELECT id FROM prompt WHERE name = ?", (name,))
        result = cursor.fetchone()

        if result:
            return (True, result["id"])  # Found: return True and the prompt ID
        else:
            # Not found: return False and empty string
            return (False, "")

    def save_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Initial prompt creation"
    ) -&gt; PromptCreateResult:
        """Create a new prompt and initialize version history
        if the prompt doesn't exist, if it already exists then
        we call `update_prompt_in_db` with the update.

        Args:
            prompt: Prompt object to create
            change_summary: Description of this change
            (default: "Initial prompt creation")

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            # Validate prompt has required fields
            if not prompt.id or not prompt.name or not prompt.data:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id if prompt.id else "",
                    version=prompt.version if prompt.version else 1,
                    error_message="Prompt missing required fields",
                    error="ValidationError",
                )

            exists, prompt_id = self.check_exists(prompt.name)
            if exists:
                # get prompt from database using prompt_id from the version
                # retrieved from the database
                prompt_from_db = self.get_prompt_by_id(prompt_id)
                # if we don't have a prompt here then we return false
                if prompt_from_db is None:
                    return PromptCreateResult(
                        success=False,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"prompt retrieval failed for {
                            prompt_id}",
                        error="ValueError",
                    )

                # otherwise we have a prompt from the database call
                # and we need to compare hashes to see if there are changes
                if prompt.content_hash == prompt_from_db.content_hash:
                    # if they are the same then the version in the db is the
                    # same as the version on disk so we return success and
                    # do nothing else.
                    return PromptCreateResult(
                        success=True,
                        prompt_id=prompt.id,
                        version=prompt.version,
                        error_message=f"""
                        prompt: {prompt.name} from disk is the same as db
                        """,
                        error="",
                    )

                else:
                    # if we get here then we have changes on disk that are more
                    # current than what is in the database

                    # IMPORTANT: We override the prompt.id here because the
                    # prompt exists already and we don't have a clean way of
                    # storing the uuid with the prompt on disk.
                    # When the prompt model is created from loading from disk,
                    # we DO generate a uuid for the model at that time just in
                    # case the prompt is newly generated from the disk and is
                    # not in the db
                    prompt.id = prompt_from_db.id

                    # Important to note that we will increment the version in
                    # `self.update_prompt_in_db`, we do not increment it here
                    return self.update_prompt_in_db(prompt)

            else:  # prompt doesn't exist in the database
                # if we make it here we have a new prompt and it
                # needs to be saved to the database for the first time
                prompt_jsonb = prompt.data.model_dump_json()

                # Create new cursor for this transaction
                cursor = self.conn.cursor()

                # insert prompt into prompt table
                cursor.execute(
                    """
                    INSERT INTO prompt(
                    id,
                    name,
                    data,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?,?)
                    """,
                    (
                        prompt.id,
                        prompt.name,
                        prompt_jsonb,
                        prompt.version,
                        prompt.content_hash,
                        prompt.created_at,
                        prompt.updated_at,
                        prompt.github_url,
                    ),
                )

                # insert initial version into prompt_history table
                cursor.execute(
                    """
                    INSERT INTO prompt_history(
                    id,
                    version,
                    data,
                    content_hash,
                    created_at,
                    archived_at,
                    change_summary,
                    github_url
                    )
                    VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                    """,
                    (
                        prompt.id,
                        prompt.version,
                        prompt_jsonb,
                        prompt.content_hash,
                        prompt.created_at,
                        datetime.now(timezone.utc),
                        change_summary,
                        prompt.github_url,
                    ),
                )
                self.conn.commit()

                return PromptCreateResult(
                    success=True, prompt_id=prompt.id, version=prompt.version
                )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def update_prompt_in_db(
        self, prompt: Prompt, change_summary: str = "Prompt updated from disk"
    ) -&gt; PromptCreateResult:
        """Update an existing prompt and add to version history

        Args:
            prompt: Prompt object to update
            change_summary: Description of this change

        Returns:
            PromptCreateResult: Success/failure with details
        """
        try:
            cursor = self.conn.cursor()

            # first we get the existing prompt in the database
            current_prompt = self.get_prompt_by_id(prompt.id)
            if not current_prompt:
                return PromptCreateResult(
                    success=False,
                    prompt_id=prompt.id,
                    version=prompt.version,
                    error_message=f"Prompt w id {prompt.id} not found",
                    error="NotFoundError",
                )
            # then we increment the version
            prompt.version = current_prompt.version + 1

            # we need to recalculate the hash for the udpated prompt
            # so we can properly compare for changes
            prompt.content_hash = hashlib.sha256(
                prompt.data.content.encode("utf-8")
            ).hexdigest()

            # process the PromptData model to to json
            prompt_jsonb = prompt.data.model_dump_json()

            # then we update the `updated_at` timestamp
            prompt.updated_at = datetime.now(timezone.utc)

            # Update prompt table
            # NOTE: when writing the the jsonb field `data` we use jsonb
            # when reading we use `json(data)`
            cursor.execute(
                """
                UPDATE prompt
                SET name = ?,
                    data = jsonb(?),
                    version = ?,
                    content_hash = ?,
                    updated_at = ?,
                    github_url = ?
                WHERE id = ?
                """,
                (
                    prompt.name,
                    prompt_jsonb,
                    prompt.version,
                    prompt.content_hash,
                    prompt.updated_at,
                    prompt.github_url,
                    prompt.id,
                ),
            )

            # Insert into the updated prompt into prompt_history
            cursor.execute(
                """
                INSERT INTO prompt_history(
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                VALUES(?, ?, jsonb(?), ?, ?, ?, ?, ?)
                """,
                (
                    prompt.id,
                    prompt.version,
                    prompt_jsonb,
                    prompt.content_hash,
                    prompt.created_at,
                    datetime.now(timezone.utc),
                    change_summary,
                    prompt.github_url,
                ),
            )

            self.conn.commit()

            return PromptCreateResult(
                success=True, prompt_id=prompt.id, version=prompt.version
            )

        except Exception as e:
            self.conn.rollback()
            return PromptCreateResult(
                success=False,
                prompt_id=prompt.id,
                version=prompt.version,
                error_message=str(e),
                error=type(e).__name__,
            )

    def get_prompt_by_id(self, prompt_id: str) -&gt; Optional[Prompt]:
        """Get a prompt by its ID from the database

        Args:
            prompt_id: The ID of the prompt to retrieve

        Returns:
            Optional[Prompt]: The prompt if found, None otherwise
        """
        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE id = ?
            """,
            (prompt_id,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        # Parse the JSONB data back to PromptData
        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        # Create and return the Prompt object
        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def get_prompt_by_name(self, prompt_name: str) -&gt; Optional[Prompt]:
        """
        Get a prompt by name from the database

        Args:
            prompt_name: The name of the prompt to retrieve.
            (should be unique)
        Returns:
            Optional[Prompt]: The prompt if found by name or None otherwise
        """

        cursor = self.conn.cursor()
        cursor.execute(
            """
            SELECT
            id,
            name,
            json(data) as data_json,
            version,
            content_hash,
            created_at,
            updated_at,
            github_url

            FROM prompt
            WHERE name = ?
            """,
            (prompt_name,),
        )

        row = cursor.fetchone()
        if not row:
            return None

        data_dict = json.loads(row["data_json"])
        prompt_data = PromptData(**data_dict)

        return Prompt(
            id=row["id"],
            name=row["name"],
            github_url=row["github_url"],
            data=prompt_data,
            version=row["version"],
            content_hash=row["content_hash"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def delete_prompt_by_id(self, prompt_id: str) -&gt; PromptDeleteResult:
        cursor = self.conn.cursor()
        try:
            # archive final state in prompt_history table before deletion
            # we will not be deleting the version history of the prompt
            cursor.execute(
                """
                INSERT INTO prompt_history (
                id,
                version,
                data,
                content_hash,
                created_at,
                archived_at,
                change_summary,
                github_url)
                SELECT id, version, data, content_hash, created_at, ?, ?, github_url
                FROM prompt WHERE id = ?
            """,
                (datetime.now(timezone.utc), "DELETED - Final Version", prompt_id),
            )

            # Delete only from the prompt table
            cursor.execute("DELETE FROM prompt WHERE id = ?", (prompt_id,))
            deleted_row_count = cursor.rowcount

            self.conn.commit()

            return PromptDeleteResult(
                success=True,
                prompt_id=prompt_id,
                deleted=True,
                rows_affected=deleted_row_count,
            )

        except Exception as e:
            self.conn.rollback()
            return PromptDeleteResult(
                success=False,
                prompt_id=prompt_id,
                deleted=False,
                rows_affected=0,
                error_message=str(e),
                error_type=type(e).__name__,
            )

    def bulk_save_in_db(self, prompts: List[Prompt]) -&gt; List[PromptCreateResult]:
        """
        Bulk load/save prompts into the database

        Args:
            plans: List of Plan objects to load into database

        Returns:
            PlanLoadResult: Summary of the loading operation
        """

        return [self.save_prompt_in_db(prompt) for prompt in prompts]

    def flatten_cmds_to_disk(self) -&gt; List[PromptFlattenResult]:
        """Flatten all cmd_category prompts from database to disk directories

        Queries all CMD type prompts from database and writes them to:
        - .claude/commands/{category}/{filename}
        - .gemini/commands/{category}/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure command directories exist
            if not self.cmd_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create command directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all CMD type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&gt;&gt; '$.type' = 'cmd'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No CMD prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    # `**` unpacks the dictionary into key words for pydantic
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.CMD)

                    # Get category, handle None/uncategorized case
                    if prompt.data.cmd_category:
                        if isinstance(prompt.data.cmd_category, str):
                            category = prompt.data.cmd_category
                        else:
                            category = prompt.data.cmd_category.value
                    else:
                        category = "uncategorized"

                    # Determine target directory based on tags
                    target_dirs = []
                    if prompt.data.tags:
                        if "claude" in prompt.data.tags:
                            target_dirs.append("claude")
                        if "gemini" in prompt.data.tags:
                            target_dirs.append("gemini")

                    # If no source tags found, skip this prompt
                    if not target_dirs:
                        errmsg = "No source tag (claude/gemini) found in tags"
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category=category,
                                error_message=errmsg,
                                error_type="MissingSourceTag",
                            )
                        )
                        continue

                    # Write to appropriate directories based on source tags
                    for target_dir in target_dirs:
                        try:
                            target_path = (
                                project_dir
                                / f".{target_dir}"
                                / "commands"
                                / category
                                / filename
                            )

                            # Ensure parent directory exists
                            target_path.parent.mkdir(parents=True, exist_ok=True)

                            # Write content to file
                            target_path.write_text(
                                prompt.data.content, encoding="utf-8"
                            )

                            results.append(
                                PromptFlattenResult(
                                    success=True,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=str(target_path),
                                    cmd_category=category,
                                    error_message="",
                                    error_type="",
                                )
                            )

                        except Exception as e:
                            results.append(
                                PromptFlattenResult(
                                    success=False,
                                    prompt_id=prompt.id,
                                    prompt_name=prompt.name,
                                    file_path=(
                                        str(target_path)
                                        if "target_path" in locals()
                                        else ""
                                    ),
                                    cmd_category=category,
                                    error_message=str(e),
                                    error_type=type(e).__name__,
                                )
                            )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results

    def flatten_plans_to_disk(self) -&gt; List[PromptFlattenResult]:
        """Flatten all plan prompts from database to disk directories

        Queries all PLAN type prompts from database and writes them to:
        - _docs/plans/drafts/{filename}
        - _docs/plans/approved/{filename}
        - _docs/plans/completed/{filename}

        Returns:
            List[PromptFlattenResult]: Individual results for each file written
        """
        results = []

        try:
            # Ensure plan directories exist
            if not self.plans_check_dirs():
                results.append(
                    PromptFlattenResult(
                        success=False,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="Failed to create plan directories",
                        error_type="DirectoryError",
                    )
                )
                return results

            # Query all PLAN type prompts from database
            cursor = self.conn.cursor()
            cursor.execute(
                """
                SELECT
                    id,
                    name,
                    json(data) as data_json,
                    version,
                    content_hash,
                    created_at,
                    updated_at,
                    github_url
                FROM prompt
                WHERE data -&gt;&gt; '$.type' = 'plan'
                ORDER BY name
            """
            )

            rows = cursor.fetchall()

            if not rows:
                results.append(
                    PromptFlattenResult(
                        success=True,
                        prompt_id="",
                        prompt_name="",
                        file_path="",
                        cmd_category="",
                        error_message="No PLAN prompts found in database",
                        error_type="",
                    )
                )
                return results

            project_dir = Path(__file__).parent.parent

            # Status to directory mapping
            status_dir_mapping = {
                PromptPlanStatus.DRAFT.value: "drafts",
                PromptPlanStatus.APPROVED.value: "approved",
                PromptPlanStatus.COMPLETED.value: "completed",
            }

            for row in rows:
                try:
                    # Parse the JSONB data back to PromptData
                    data_dict = json.loads(row["data_json"])
                    prompt_data = PromptData(**data_dict)

                    # Create Prompt object
                    prompt = Prompt(
                        id=row["id"],
                        name=row["name"],
                        github_url=row["github_url"],
                        data=prompt_data,
                        version=row["version"],
                        content_hash=row["content_hash"],
                        created_at=row["created_at"],
                        updated_at=row["updated_at"],
                    )

                    # Validate project name for PLAN type prompts
                    if not prompt.data.project:
                        # PLAN type must have a project name
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path="",
                                cmd_category="",
                                error_message="PLAN type prompt missing required project name",
                                error_type="MissingProjectError",
                            )
                        )
                        continue

                    # TODO: update this to use coordinate the github_url
                    if prompt.data.project != project_dir.name:
                        # Skip this prompt - it belongs to a different project
                        continue

                    # Get original filename from database name
                    filename = self.parse_db_name(prompt.name, PromptType.PLAN)

                    # Get status directory
                    status_value = (
                        prompt.data.status.value
                        if hasattr(prompt.data.status, "value")
                        else str(prompt.data.status)
                    )
                    status_dir = status_dir_mapping.get(status_value, "drafts")

                    # Write to appropriate status directory
                    try:
                        target_path = (
                            project_dir / "_docs" / "plans" / status_dir / filename
                        )

                        # Ensure parent directory exists
                        target_path.parent.mkdir(parents=True, exist_ok=True)

                        # Write content to file
                        target_path.write_text(prompt.data.content, encoding="utf-8")

                        results.append(
                            PromptFlattenResult(
                                success=True,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=str(target_path),
                                cmd_category=status_dir,
                                error_message="",
                                error_type="",
                            )
                        )

                    except Exception as e:
                        results.append(
                            PromptFlattenResult(
                                success=False,
                                prompt_id=prompt.id,
                                prompt_name=prompt.name,
                                file_path=(
                                    str(target_path)
                                    if "target_path" in locals()
                                    else ""
                                ),
                                cmd_category=status_dir,
                                error_message=str(e),
                                error_type=type(e).__name__,
                            )
                        )

                except Exception as e:
                    results.append(
                        PromptFlattenResult(
                            success=False,
                            prompt_id=row.get("id", ""),
                            prompt_name=row.get("name", ""),
                            file_path="",
                            cmd_category="",
                            error_message=f"Failed to process prompt: {
                                str(e)}",
                            error_type=type(e).__name__,
                        )
                    )

        except Exception as e:
            results.append(
                PromptFlattenResult(
                    success=False,
                    prompt_id="",
                    prompt_name="",
                    file_path="",
                    cmd_category="",
                    error_message=f"Database query failed: {str(e)}",
                    error_type=type(e).__name__,
                )
            )

        return results
</file>
  <file path="repository/test_datetime_adapters.py">"""Test the custom datetime adapters for SQLite3 compatibility."""

import pytest
import warnings
from datetime import datetime, date
from repository.database import SQLite3Database
from repository import datetime_adapters


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db_path = ":memory:"  # Use in-memory database for tests
    db = SQLite3Database(db_path)

    # Create test table
    with db.get_connection() as conn:
        conn.execute(
            """
            CREATE TABLE test_dates (
                id INTEGER PRIMARY KEY,
                created_at TIMESTAMP,
                updated_at DATETIME,
                date_only DATE,
                description TEXT
            )
        """
        )
        yield conn


def test_datetime_storage_retrieval(test_db):
    """Test that datetime objects can be stored and retrieved without warnings."""

    # Capture warnings
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Test data
        test_datetime = datetime(2025, 1, 13, 10, 30, 45)
        test_date = date(2025, 1, 13)

        # Insert test data
        test_db.execute(
            """
            INSERT INTO test_dates (created_at, updated_at, date_only, description)
            VALUES (?, ?, ?, ?)
        """,
            (test_datetime, test_datetime, test_date, "Test record"),
        )

        test_db.commit()

        # Retrieve data
        cursor = test_db.execute(
            """
            SELECT created_at, updated_at, date_only, description 
            FROM test_dates WHERE id = 1
        """
        )
        row = cursor.fetchone()

        # Verify no deprecation warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert (
            len(deprecation_warnings) == 0
        ), f"Found deprecation warnings: {[str(dw.message) for dw in deprecation_warnings]}"

        # Verify data integrity
        assert isinstance(row["created_at"], datetime)
        assert isinstance(row["updated_at"], datetime)
        assert isinstance(row["date_only"], date)
        assert row["created_at"] == test_datetime
        assert row["updated_at"] == test_datetime
        assert row["date_only"] == test_date


def test_datetime_iso_format(test_db):
    """Test that datetimes are stored in ISO format."""

    test_datetime = datetime(2025, 1, 13, 14, 30, 45)

    # Insert using our adapter
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, description)
        VALUES (?, ?)
    """,
        (test_datetime, "ISO format test"),
    )
    test_db.commit()

    # Read raw value (bypass converter)
    cursor = test_db.execute(
        """
        SELECT CAST(created_at AS TEXT) as raw_datetime 
        FROM test_dates WHERE description = 'ISO format test'
    """
    )
    row = cursor.fetchone()

    # Verify ISO format
    expected_iso = "2025-01-13T14:30:45"
    assert row["raw_datetime"] == expected_iso


def test_adapter_functions_directly():
    """Test adapter and converter functions directly."""

    # Test datetime adapter
    test_dt = datetime(2025, 1, 13, 10, 30, 45, 123456)
    adapted = datetime_adapters.adapt_datetime_iso(test_dt)
    assert adapted == "2025-01-13T10:30:45.123456"

    # Test date adapter
    test_date = date(2025, 1, 13)
    adapted_date = datetime_adapters.adapt_date_iso(test_date)
    assert adapted_date == "2025-01-13"

    # Test datetime converter
    iso_bytes = b"2025-01-13T10:30:45.123456"
    converted = datetime_adapters.convert_datetime_iso(iso_bytes)
    assert converted == test_dt

    # Test date converter
    date_bytes = b"2025-01-13"
    converted_date = datetime_adapters.convert_date_iso(date_bytes)
    assert converted_date == test_date

    # Test timestamp converter
    timestamp_bytes = b"1736765445"  # Unix timestamp for 2025-01-13 10:30:45 UTC
    converted_ts = datetime_adapters.convert_timestamp(timestamp_bytes)
    # Note: This will be in local timezone
    assert isinstance(converted_ts, datetime)


def test_timezone_naive_handling():
    """Test that timezone info is properly stripped."""

    # Create timezone-aware datetime
    from datetime import timezone

    tz_aware = datetime(2025, 1, 13, 10, 30, 45, tzinfo=timezone.utc)

    # Adapt should strip timezone
    adapted = datetime_adapters.adapt_datetime_iso(tz_aware)
    assert adapted == "2025-01-13T10:30:45"
    assert "+00:00" not in adapted  # No timezone offset in output


def test_multiple_datetime_operations(test_db):
    """Test multiple datetime operations to ensure no warnings."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Multiple inserts
        for i in range(5):
            dt = datetime.now()
            test_db.execute(
                """
                INSERT INTO test_dates (created_at, updated_at, description)
                VALUES (?, ?, ?)
            """,
                (dt, dt, f"Record {i}"),
            )

        test_db.commit()

        # Multiple selects
        cursor = test_db.execute("SELECT * FROM test_dates")
        rows = cursor.fetchall()

        # Verify all datetimes are properly converted
        for row in rows:
            if row["created_at"]:
                assert isinstance(row["created_at"], datetime)
            if row["updated_at"]:
                assert isinstance(row["updated_at"], datetime)

        # Check for warnings
        deprecation_warnings = [
            warning for warning in w if issubclass(warning.category, DeprecationWarning)
        ]
        assert len(deprecation_warnings) == 0


def test_null_datetime_handling(test_db):
    """Test that NULL datetime values are handled correctly."""

    # Insert NULL values
    test_db.execute(
        """
        INSERT INTO test_dates (created_at, updated_at, date_only, description)
        VALUES (NULL, NULL, NULL, 'Null test')
    """
    )
    test_db.commit()

    # Retrieve NULL values
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at, date_only 
        FROM test_dates WHERE description = 'Null test'
    """
    )
    row = cursor.fetchone()

    # Verify NULLs are preserved
    assert row["created_at"] is None
    assert row["updated_at"] is None
    assert row["date_only"] is None


def test_backwards_compatibility(test_db):
    """Test that existing ISO format strings are still readable."""

    # Manually insert ISO format strings (simulating old data)
    test_db.execute(
        """
        INSERT INTO test_dates (id, created_at, updated_at, description)
        VALUES (100, '2024-12-01T10:30:45', '2024-12-01T10:30:45', 'Old format')
    """
    )
    test_db.commit()

    # Read with our converters
    cursor = test_db.execute(
        """
        SELECT created_at, updated_at 
        FROM test_dates WHERE id = 100
    """
    )
    row = cursor.fetchone()

    # Verify conversion works
    assert isinstance(row["created_at"], datetime)
    assert row["created_at"].year == 2024
    assert row["created_at"].month == 12
    assert row["created_at"].day == 1
    assert row["created_at"].hour == 10
    assert row["created_at"].minute == 30
    assert row["created_at"].second == 45
</file>
  <file path=".gemini/settings.json">{
	"mcpServers": {
		"collect": {
			"command": "uv",
			"args": [
				"run",
				"python",
				"collect.py"
			],
			"workingDirectory": "/Users/benjaminmetz/python/collect",
			"enabled": true
		}
	}
}
</file>
  <file path="dotfiles/.zshrc">GHOSTTY_CONFIG_DIR="$HOME/.config/ghostty"

export EDITOR="nvim"
export VISUAL="nvim"
export PATH="$PATH:$HOME/.local/bin"

# path to sqlite3
export PATH="/opt/homebrew/opt/sqlite/bin:$PATH"

# path to ripgrep
export PATH="$HOME/opt/homebrew/bin/rg:$PATH"

#python uv path
. "$HOME/.local/bin/env"

# path to scripts and zig and stuff
export PATH="$HOME/bin:$PATH"

# path to npm
export PATH="$(npm config get prefix)/bin:$PATH"
# path to zig
export PATH=$PATH:~/bin/zig/
export PATH="$(brew --prefix coreutils)/libexec/gnubin:$PATH"

# dependencies needed for gemini / google token processing
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$PKG_CONFIG_PATH"
export PATH="$(brew --prefix)/bin:$PATH"
export PKG_CONFIG_PATH="$(brew --prefix sentencepiece)/lib/pkgconfig:$(brew --prefix protobuf)/lib/pkgconfig:$PKG_CONFIG_PATH"

# shortcuts to project work
alias gowork='cd $HOME/go/src/github.com/metzben &amp;&amp; ls -lhG'
alias py='cd $HOME/python &amp;&amp; ls -l --color'
alias collect='cd $HOME/python/collect &amp;&amp; source .venv/bin/activate'
alias el='cd $HOME/go/src/github.com/metzben/elephnt &amp;&amp; ls -l --color'
alias tiny='cd $HOME/go/src/github.com/metzben/tinystack &amp;&amp; ls -lhG'
alias ai='cd $HOME/python/aiwork &amp;&amp; ls -lhG'
alias mcp='cd $HOME/python/mcpwork &amp;&amp; ls -lhG'
alias base='cd $HOME/base &amp;&amp; nvim .'
alias fta='cd $HOME/python/fastta &amp;&amp; nvim .'
alias indicators='cd $HOME/python/indicators &amp;&amp; ls -l'
alias mcpstart='cd $HOME/python/startermcp &amp;&amp; ls -l'
alias tools='cd ~/bin &amp;&amp; ls -l --color'
alias plans='cd _docs/plans &amp;&amp; tree -C -L 2'

# Database function - only works in collect directory
db() {
    if [[ "$PWD" == *"/collect" ]] || [[ "$PWD" == *"/collect/"* ]]; then
        sqlite3 data/collect.db
    else
        echo "Not in collect directory. This command only works in the collect project."
    fi
}

# claude ai shortcuts
alias ask='claude -p '
alias editmcp='nvim ~/Library/Application\ Support/Claude/claude_desktop_config.json'
alias rip='claude --dangerously-skip-permissions'
alias cmds='cd "$(git rev-parse --show-toplevel)/.claude/commands" &amp;&amp; ls -l --color'
alias gms='cd "$(git rev-parse --show-toplevel)/.gemini/commands" &amp;&amp; ls -l --color'

# git shortcuts
alias gs='git status'
alias gd='git diff --staged'
alias gc='git commit -m '
alias push='git push origin main'
alias ga='git add '
alias gb='git branch'
alias gwl='git worktree list'
alias rebase='git pull --rebase origin main'
alias pull='git pull origin main'

# Worktree navigation functions
cd1() {
    local project_name=$(basename "$(pwd)")
    local wt1_path="../${project_name}-wt1"
    
    if [[ -d "$wt1_path" ]]; then
        cd "$wt1_path"
        echo "Changed to worktree 1: $(pwd)"
    else
        echo "Worktree 1 not found: $wt1_path"
        echo "Run 'trees' to create worktrees first."
    fi
}

cd2() {
    local project_name=$(basename "$(pwd)")
    local wt2_path="../${project_name}-wt2"
    
    if [[ -d "$wt2_path" ]]; then
        cd "$wt2_path"
        echo "Changed to worktree 2: $(pwd)"
    else
        echo "Worktree 2 not found: $wt2_path"
        echo "Run 'trees' to create worktrees first."
    fi
}


checkport() {
    if [ -z "$1" ]; then
        echo "Usage: checkport &lt;port_number&gt;"
        return 1
    fi
    
    if lsof -i :$1 2&gt;/dev/null; then
        echo "Port $1 is in use"
    else
        echo "Port $1 is available"
    fi
}

# uv shortcuts
alias env='source .venv/bin/activate'
alias da='deactivate'
alias ipy='uv run ipython'

# go shortcuts
alias run='go test -v -run'

# config shortcuts
alias src='source ~/.zshrc'
alias openz='nvim ~/.zshrc'
alias initlua='nvim $HOME/.config/nvim/init.lua'
alias ghconf='nvim $HOME/.config/ghostty/config'
alias oc='cursor .'

# misc shorty's
alias ll='ls -l --color'
alias tll='tree -C -L 2'
alias oc='cursor .'
alias onv='nvim .'
alias runz='zig run src/main.zig'
alias cperr="zig run src/main.zig 2&gt;&amp;1 | tee /dev/tty | awk '/error:/{found=1} found {print}' | pbcopy"

# ollama models
alias deep70='ollama run deepseek-r1:70b'
alias llama70='ollama run llama3.3'

# The next line updates PATH for the Google Cloud SDK.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/path.zsh.inc'; fi

# The next line enables shell command completion for gcloud.
if [ -f '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc' ]; then . '/Users/benjaminmetz/google-cloud-sdk/completion.zsh.inc'; fi

alias auth='gcloud auth login'
alias auth2='gcloud auth application-default login'

export PS1='b@m %~ % '



# opencode
export PATH=/Users/benjaminmetz/.opencode/bin:$PATH
</file>
  <file path="dotfiles/nvim/init.lua">vim.opt.clipboard = "unnamedplus"

if vim.g.vscode then
	return
end

-- Set &lt;space&gt; as the leader key
-- See `:help mapleader`
--  NOTE: Must happen before plugins are loaded (otherwise wrong leader will be used)
vim.g.mapleader = " "
vim.g.maplocalleader = " "

-- have neovim honor the terminal opacity
vim.opt.termguicolors = true
--vim.cmd.colorscheme("catppuccin-mocha")

vim.cmd([[highlight Normal ctermbg=none guibg=none]])

-- Set to true if you have a Nerd Font installed and selected in the terminal
vim.g.have_nerd_font = true

-- [[ Setting options ]]
-- See `:help vim.opt`
-- NOTE: You can change these options as you wish!
--  For more options, you can see `:help option-list`

-- Make line numbers default
vim.opt.number = true
-- You can also add relative line numbers, to help with jumping.
--  Experiment for yourself to see if you like it!
vim.opt.relativenumber = true

-- Enable mouse mode, can be useful for resizing splits for example!
vim.opt.mouse = "a"

-- Don't show the mode, since it's already in the status line
vim.opt.showmode = true

-- Sync clipboard between OS and Neovim.
--  Schedule the setting after `UiEnter` because it can increase startup-time.
--  Remove this option if you want your OS clipboard to remain independent.
--  See `:help 'clipboard'`
vim.schedule(function()
	vim.opt.clipboard = "unnamedplus"
end)

-- Enable break indent
vim.opt.breakindent = true

-- Save undo history
vim.opt.undofile = true

-- Case-insensitive searching UNLESS \C or one or more capital letters in the search term
vim.opt.ignorecase = true
vim.opt.smartcase = true

-- Keep signcolumn on by default
vim.opt.signcolumn = "yes"

-- Decrease update time
vim.opt.updatetime = 250

-- Decrease mapped sequence wait time
-- Displays which-key popup sooner
vim.opt.timeoutlen = 300

-- Configure how new splits should be opened
vim.opt.splitright = true
vim.opt.splitbelow = true

-- Sets how neovim will display certain whitespace characters in the editor.
--  See `:help 'list'`
--  and `:help 'listchars'`
vim.opt.list = true
vim.opt.listchars = { tab = "» ", trail = "·", nbsp = "␣" }

-- Preview substitutions live, as you type!
vim.opt.inccommand = "split"

-- Show which line your cursor is on
vim.opt.cursorline = true

-- Minimal number of screen lines to keep above and below the cursor.
vim.opt.scrolloff = 15

vim.o.foldmethod = "expr"
vim.o.foldexpr = "nvim_treesitter#foldexpr()"
vim.o.foldenable = true
vim.o.foldlevel = 99 -- Keep folds open by default

--vim.cmd([[packadd packer.nvim]])

-- [[ Basic Keymaps ]]
--  See `:help vim.keymap.set()`

-- Clear highlights on search when pressing &lt;Esc&gt; in normal mode
--  See `:help hlsearch`
vim.keymap.set("n", "&lt;Esc&gt;", "&lt;cmd&gt;nohlsearch&lt;CR&gt;")
-- when in insert mode pressing j and j again will &lt;Esc&gt;'
vim.keymap.set("i", "&lt;D-j&gt;", "&lt;Esc&gt;", { noremap = true, silent = true })
vim.keymap.set("n", "a", "A", { noremap = true, silent = true })
vim.keymap.set("n", "4", "$", { noremap = true, silent = true })

-- Diagnostic keymaps
vim.keymap.set("n", "&lt;leader&gt;q", vim.diagnostic.setloclist, { desc = "Open diagnostic [Q]uickfix list" })
vim.keymap.set("n", "[d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })
vim.keymap.set("n", "]d", vim.diagnostic.goto_prev, { desc = "Go to previous [D]iagnostic message" })

-- for people to discover. Otherwise, you normally need to press &lt;C-\&gt;&lt;C-n&gt;, which
-- is not what someone will guess without a bit more experience.
--
-- NOTE: This won't work in all terminal emulators/tmux/etc. Try your own mapping
-- or just use &lt;C-\&gt;&lt;C-n&gt; to exit terminal mode
vim.keymap.set("t", "&lt;Esc&gt;&lt;Esc&gt;", "&lt;C-\\&gt;&lt;C-n&gt;", { desc = "Exit terminal mode" })

-- Keybinds to make split navigation easier.
-- Use CTRL+&lt;hjkl&gt; to switch between windows
--
--  See `:help wincmd` for a list of all window commands
vim.keymap.set("n", "&lt;C-h&gt;", "&lt;C-w&gt;&lt;C-h&gt;", { desc = "Move focus to the left window" })
vim.keymap.set("n", "&lt;C-l&gt;", "&lt;C-w&gt;&lt;C-l&gt;", { desc = "Move focus to the right window" })
vim.keymap.set("n", "&lt;C-j&gt;", "&lt;C-w&gt;&lt;C-j&gt;", { desc = "Move focus to the lower window" })
vim.keymap.set("n", "&lt;C-k&gt;", "&lt;C-w&gt;&lt;C-k&gt;", { desc = "Move focus to the upper window" })

vim.api.nvim_create_autocmd("BufEnter", {
	pattern = "$HOME/go/src/github.com/metzben/*",
	callback = function()
		vim.cmd("colorscheme onedark")
	end,
})

-- Ensure Packer is loaded
vim.cmd([[packadd packer.nvim]])

-- [[ Basic Autocommands ]]
--  See `:help lua-guide-autocommands`

-- Highlight when yanking (copying) text
--  Try it with `yap` in normal mode
--  See `:help vim.highlight.on_yank()`
vim.api.nvim_create_autocmd("TextYankPost", {
	desc = "Highlight when yanking (copying) text",
	group = vim.api.nvim_create_augroup("kickstart-highlight-yank", { clear = true }),
	callback = function()
		vim.highlight.on_yank()
	end,
})

-- [[ Install `lazy.nvim` plugin manager ]]
--    See `:help lazy.nvim.txt` or https://github.com/folke/lazy.nvim for more info
local lazypath = vim.fn.stdpath("data") .. "/lazy/lazy.nvim"
if not (vim.uv or vim.loop).fs_stat(lazypath) then
	local lazyrepo = "https://github.com/folke/lazy.nvim.git"
	local out = vim.fn.system({ "git", "clone", "--filter=blob:none", "--branch=stable", lazyrepo, lazypath })
	if vim.v.shell_error ~= 0 then
		error("Error cloning lazy.nvim:\n" .. out)
	end
end ---@diagnostic disable-next-line: undefined-field
vim.opt.rtp:prepend(lazypath)

-- [[ Configure and install plugins ]]
--
--  To check the current status of your plugins, run
--    :Lazy
--
--  You can press `?` in this menu for help. Use `:q` to close the window
--
--  To update plugins you can run
--    :Lazy update
--
-- NOTE: Here is where you install your plugins.
require("lazy").setup({
	-- NOTE: Plugins can be added with a link (or for a github repo: 'owner/repo' link).
	"tpope/vim-sleuth", -- Detect tabstop and shiftwidth automatically

	-- NOTE: Plugins can also be added by using a table,
	-- with the first argument being the link and the following
	-- keys can be used to configure plugin behavior/loading/etc.
	--
	-- Use `opts = {}` to force a plugin to be loaded.
	--

	-- Here is a more advanced example where we pass configuration
	-- options to `gitsigns.nvim`. This is equivalent to the following Lua:
	--    require('gitsigns').setup({ ... })
	--
	-- See `:help gitsigns` to understand what the configuration keys do
	{ -- Adds git related signs to the gutter, as well as utilities for managing changes
		"lewis6991/gitsigns.nvim",
		opts = {
			signs = {
				add = { text = "+" },
				change = { text = "~" },
				delete = { text = "_" },
				topdelete = { text = "‾" },
				changedelete = { text = "~" },
			},
		},
	},

	-- NOTE: Plugins can also be configured to run Lua code when they are loaded.
	--
	-- This is often very useful to both group configuration, as well as handle
	-- lazy loading plugins that don't need to be loaded immediately at startup.
	--
	-- For example, in the following configuration, we use:
	--  event = 'VimEnter'
	--
	-- which loads which-key before all the UI elements are loaded. Events can be
	-- normal autocommands events (`:help autocmd-events`).
	--
	-- Then, because we use the `opts` key (recommended), the configuration runs
	-- after the plugin has been loaded as `require(MODULE).setup(opts)`.

	{ -- Useful plugin to show you pending keybinds.
		"folke/which-key.nvim",
		event = "VimEnter", -- Sets the loading event to 'VimEnter'
		opts = {
			icons = {
				-- set icon mappings to true if you have a Nerd Font
				mappings = vim.g.have_nerd_font,
				-- If you are using a Nerd Font: set icons.keys to an empty table which will use the
				-- default which-key.nvim defined Nerd Font icons, otherwise define a string table
				keys = vim.g.have_nerd_font and {} or {
					Up = "&lt;Up&gt; ",
					Down = "&lt;Down&gt; ",
					Left = "&lt;Left&gt; ",
					Right = "&lt;Right&gt; ",
					C = "&lt;C-…&gt; ",
					M = "&lt;M-…&gt; ",
					D = "&lt;D-…&gt; ",
					S = "&lt;S-…&gt; ",
					CR = "&lt;CR&gt; ",
					Esc = "&lt;Esc&gt; ",
					ScrollWheelDown = "&lt;ScrollWheelDown&gt; ",
					ScrollWheelUp = "&lt;ScrollWheelUp&gt; ",
					NL = "&lt;NL&gt; ",
					BS = "&lt;BS&gt; ",
					Space = "&lt;Space&gt; ",
					Tab = "&lt;Tab&gt; ",
					F1 = "&lt;F1&gt;",
					F2 = "&lt;F2&gt;",
					F3 = "&lt;F3&gt;",
					F4 = "&lt;F4&gt;",
					F5 = "&lt;F5&gt;",
					F6 = "&lt;F6&gt;",
					F7 = "&lt;F7&gt;",
					F8 = "&lt;F8&gt;",
					F9 = "&lt;F9&gt;",
					F10 = "&lt;F10&gt;",
					F11 = "&lt;F11&gt;",
					F12 = "&lt;F12&gt;",
				},
			},

			-- Document existing key chains
			spec = {
				{ "&lt;leader&gt;c", group = "[C]ode", mode = { "n", "x" } },
				{ "&lt;leader&gt;d", group = "[D]ocument" },
				{ "&lt;leader&gt;r", group = "[R]ename" },
				{ "&lt;leader&gt;s", group = "[S]earch" },
				{ "&lt;leader&gt;w", group = "[W]orkspace" },
				{ "&lt;leader&gt;t", group = "[T]oggle" },
				{ "&lt;leader&gt;h", group = "Git [H]unk", mode = { "n", "v" } },
			},
		},
	},

	-- NOTE: Plugins can specify dependencies.
	--
	-- The dependencies are proper plugin specifications as well - anything
	-- you do for a plugin at the top level, you can do for a dependency.
	--
	-- Use the `dependencies` key to specify the dependencies of a particular plugin

	{ -- Fuzzy Finder (files, lsp, etc)
		"nvim-telescope/telescope.nvim",
		event = "VimEnter",
		branch = "0.1.x",
		dependencies = {
			"nvim-lua/plenary.nvim",
			{ -- If encountering errors, see telescope-fzf-native README for installation instructions
				"nvim-telescope/telescope-fzf-native.nvim",

				-- `build` is used to run some command when the plugin is installed/updated.
				-- This is only run then, not every time Neovim starts up.
				build = "make",

				-- `cond` is a condition used to determine whether this plugin should be
				-- installed and loaded.
				cond = function()
					return vim.fn.executable("make") == 1
				end,
			},
			{ "nvim-telescope/telescope-ui-select.nvim" },

			-- Useful for getting pretty icons, but requires a Nerd Font.
			{ "nvim-tree/nvim-web-devicons", enabled = vim.g.have_nerd_font },
		},
		config = function()
			-- Telescope is a fuzzy finder that comes with a lot of different things that
			-- it can fuzzy find! It's more than just a "file finder", it can search
			-- many different aspects of Neovim, your workspace, LSP, and more!
			--
			-- The easiest way to use Telescope, is to start by doing something like:
			--  :Telescope help_tags
			--
			-- After running this command, a window will open up and you're able to
			-- type in the prompt window. You'll see a list of `help_tags` options and
			-- a corresponding preview of the help.
			--
			-- Two important keymaps to use while in Telescope are:
			--  - Insert mode: &lt;c-/&gt;
			--  - Normal mode: ?
			--
			-- This opens a window that shows you all of the keymaps for the current
			-- Telescope picker. This is really useful to discover what Telescope can
			-- do as well as how to actually do it!

			-- [[ Configure Telescope ]]
			-- See `:help telescope` and `:help telescope.setup()`
			require("telescope").setup({
				-- You can put your default mappings / updates / etc. in here
				--  All the info you're looking for is in `:help telescope.setup()`
				--
				-- defaults = {
				--   mappings = {
				--     i = { ['&lt;c-enter&gt;'] = 'to_fuzzy_refine' },
				--   },
				-- },
				-- pickers = {}
				extensions = {
					["ui-select"] = {
						require("telescope.themes").get_dropdown(),
					},
				},
			})

			-- Enable Telescope extensions if they are installed
			pcall(require("telescope").load_extension, "fzf")
			pcall(require("telescope").load_extension, "ui-select")

			-- See `:help telescope.builtin`
			local builtin = require("telescope.builtin")
			vim.keymap.set("n", "&lt;leader&gt;sh", builtin.help_tags, { desc = "[S]earch [H]elp" })
			vim.keymap.set("n", "&lt;leader&gt;sk", builtin.keymaps, { desc = "[S]earch [K]eymaps" })
			vim.keymap.set("n", "&lt;leader&gt;sf", function()
				builtin.find_files({ hidden = true, no_ignore = true })
			end, { desc = "[S]earch [F]iles" })
			vim.keymap.set("n", "&lt;leader&gt;ss", builtin.builtin, { desc = "[S]earch [S]elect Telescope" })
			vim.keymap.set("n", "&lt;leader&gt;sw", builtin.grep_string, { desc = "[S]earch current [W]ord" })
			vim.keymap.set("n", "&lt;leader&gt;sg", builtin.live_grep, { desc = "[S]earch by [G]rep" })
			vim.keymap.set("n", "&lt;leader&gt;sd", builtin.diagnostics, { desc = "[S]earch [D]iagnostics" })
			vim.keymap.set("n", "&lt;leader&gt;sr", builtin.resume, { desc = "[S]earch [R]esume" })
			vim.keymap.set("n", "&lt;leader&gt;s.", builtin.oldfiles, { desc = '[S]earch Recent Files ("." for repeat)' })
			vim.keymap.set("n", "&lt;leader&gt;&lt;leader&gt;", builtin.buffers, { desc = "[ ] Find existing buffers" })
			vim.keymap.set("n", "&lt;leader&gt;af", builtin.current_buffer_fuzzy_find, { desc = "[S]earch in existing file" })
			vim.keymap.set("n", "&lt;leader&gt;nf", "]m", { noremap = true, silent = true })
			vim.keymap.set("n", "&lt;leader&gt;pf", "[m", { noremap = true, silent = true })
			vim.keymap.set("n", "&lt;leader&gt;r", vim.lsp.buf.rename, { desc = "LSP Rename" })
			vim.keymap.set("n", "&lt;leader&gt;d", "&lt;cmd&gt;Telescope diagnostics&lt;CR&gt;")
			vim.api.nvim_set_keymap("n", "&lt;leader&gt;c", "~", { noremap = true, silent = true })
			vim.keymap.set(
				"n",
				"&lt;leader&gt;O",
				":put! _&lt;CR&gt;",
				{ desc = "Add blank line above without entering edit mode" }
			)

			vim.keymap.set("v", "&lt;leader&gt;/", ":norm I//&lt;CR&gt;", { desc = "Comment selected block" })
			vim.keymap.set("n", "&lt;leader&gt;/", "gcc", { desc = "Toggle comment on current line" })
			-- Slightly advanced example of overriding default behavior and theme
			vim.keymap.set("n", "&lt;leader&gt;[", function()
				-- You can pass additional configuration to Telescope to change the theme, layout, etc.
				builtin.current_buffer_fuzzy_find(require("telescope.themes").get_dropdown({
					winblend = 10,
					previewer = false,
				}))
			end, { desc = "[/] Fuzzily search in current buffer" })

			-- It's also possible to pass additional configuration options.
			--  See `:help telescope.builtin.live_grep()` for information about particular keys
			vim.keymap.set("n", "&lt;leader&gt;s/", function()
				builtin.live_grep({
					grep_open_files = true,
					prompt_title = "Live Grep in Open Files",
				})
			end, { desc = "[S]earch [/] in Open Files" })

			-- Shortcut for searching your Neovim configuration files
			vim.keymap.set("n", "&lt;leader&gt;sn", function()
				builtin.find_files({ cwd = vim.fn.stdpath("config") })
			end, { desc = "[S]earch [N]eovim files" })
		end,
	},

	-- LSP Plugins
	{
		-- `lazydev` configures Lua LSP for your Neovim config, runtime and plugins
		-- used for completion, annotations and signatures of Neovim apis
		"folke/lazydev.nvim",
		ft = "lua",
		opts = {
			library = {
				-- Load luvit types when the `vim.uv` word is found
				{ path = "luvit-meta/library", words = { "vim%.uv" } },
			},
		},
	},
	{ "Bilal2453/luvit-meta", lazy = true },
	{
		-- Main LSP Configuration
		"neovim/nvim-lspconfig",
		dependencies = {
			-- Automatically install LSPs and related tools to stdpath for Neovim
			{ "williamboman/mason.nvim", config = true }, -- NOTE: Must be loaded before dependants
			"williamboman/mason-lspconfig.nvim",
			"WhoIsSethDaniel/mason-tool-installer.nvim",

			-- Useful status updates for LSP.
			-- NOTE: `opts = {}` is the same as calling `require('fidget').setup({})`
			{ "j-hui/fidget.nvim", opts = {} },

			-- Allows extra capabilities provided by nvim-cmp
			"hrsh7th/cmp-nvim-lsp",
		},
		config = function()
			-- Brief aside: **What is LSP?**
			--
			-- LSP is an initialism you've probably heard, but might not understand what it is.
			--
			-- LSP stands for Language Server Protocol. It's a protocol that helps editors
			-- and language tooling communicate in a standardized fashion.
			--
			-- In general, you have a "server" which is some tool built to understand a particular
			-- language (such as `gopls`, `lua_ls`, `rust_analyzer`, etc.). These Language Servers
			-- (sometimes called LSP servers, but that's kind of like ATM Machine) are standalone
			-- processes that communicate with some "client" - in this case, Neovim!
			--
			-- LSP provides Neovim with features like:
			--  - Go to definition
			--  - Find references
			--  - Autocompletion
			--  - Symbol Search
			--  - and more!
			--
			-- Thus, Language Servers are external tools that must be installed separately from
			-- Neovim. This is where `mason` and related plugins come into play.
			--
			-- If you're wondering about lsp vs treesitter, you can check out the wonderfully
			-- and elegantly composed help section, `:help lsp-vs-treesitter`

			--  This function gets run when an LSP attaches to a particular buffer.
			--    That is to say, every time a new file is opened that is associated with
			--    an lsp (for example, opening `main.rs` is associated with `rust_analyzer`) this
			--    function will be executed to configure the current buffer
			vim.api.nvim_create_autocmd("LspAttach", {
				group = vim.api.nvim_create_augroup("kickstart-lsp-attach", { clear = true }),
				callback = function(event)
					-- NOTE: Remember that Lua is a real programming language, and as such it is possible
					-- to define small helper and utility functions so you don't have to repeat yourself.
					--
					-- In this case, we create a function that lets us more easily define mappings specific
					-- for LSP related items. It sets the mode, buffer and description for us each time.
					local map = function(keys, func, desc, mode)
						mode = mode or "n"
						vim.keymap.set(mode, keys, func, { buffer = event.buf, desc = "LSP: " .. desc })
					end

					-- Jump to the definition of the word under your cursor.
					--  This is where a variable was first declared, or where a function is defined, etc.
					--  To jump back, press &lt;C-t&gt;.
					map("gd", require("telescope.builtin").lsp_definitions, "[G]oto [D]efinition")

					-- Find references for the word under your cursor.
					map("gr", require("telescope.builtin").lsp_references, "[G]oto [R]eferences")

					-- Jump to the implementation of the word under your cursor.
					--  Useful when your language has ways of declaring types without an actual implementation.
					map("gI", require("telescope.builtin").lsp_implementations, "[G]oto [I]mplementation")

					-- Jump to the type of the word under your cursor.
					--  Useful when you're not sure what type a variable is and you want to see
					--  the definition of its *type*, not where it was *defined*.
					map("&lt;leader&gt;D", require("telescope.builtin").lsp_type_definitions, "Type [D]efinition")

					-- Fuzzy find all the symbols in your current document.
					--  Symbols are things like variables, functions, types, etc.
					map("&lt;leader&gt;ds", require("telescope.builtin").lsp_document_symbols, "[D]ocument [S]ymbols")

					-- Fuzzy find all the symbols in your current workspace.
					--  Similar to document symbols, except searches over your entire project.
					map(
						"&lt;leader&gt;ws",
						require("telescope.builtin").lsp_dynamic_workspace_symbols,
						"[W]orkspace [S]ymbols"
					)

					-- Rename the variable under your cursor.
					--  Most Language Servers support renaming across files, etc.
					map("&lt;leader&gt;rn", vim.lsp.buf.rename, "[R]e[n]ame")

					-- Execute a code action, usually your cursor needs to be on top of an error
					-- or a suggestion from your LSP for this to activate.
					map("&lt;leader&gt;ca", vim.lsp.buf.code_action, "[C]ode [A]ction", { "n", "x" })

					-- WARN: This is not Goto Definition, this is Goto Declaration.
					--  For example, in C this would take you to the header.
					map("gD", vim.lsp.buf.declaration, "[G]oto [D]eclaration")

					-- The following two autocommands are used to highlight references of the
					-- word under your cursor when your cursor rests there for a little while.
					--    See `:help CursorHold` for information about when this is executed
					--
					-- When you move your cursor, the highlights will be cleared (the second autocommand).
					local client = vim.lsp.get_client_by_id(event.data.client_id)
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_documentHighlight) then
						local highlight_augroup =
							vim.api.nvim_create_augroup("kickstart-lsp-highlight", { clear = false })
						vim.api.nvim_create_autocmd({ "CursorHold", "CursorHoldI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.document_highlight,
						})

						vim.api.nvim_create_autocmd({ "CursorMoved", "CursorMovedI" }, {
							buffer = event.buf,
							group = highlight_augroup,
							callback = vim.lsp.buf.clear_references,
						})

						vim.api.nvim_create_autocmd("LspDetach", {
							group = vim.api.nvim_create_augroup("kickstart-lsp-detach", { clear = true }),
							callback = function(event2)
								vim.lsp.buf.clear_references()
								vim.api.nvim_clear_autocmds({ group = "kickstart-lsp-highlight", buffer = event2.buf })
							end,
						})
					end

					-- The following code creates a keymap to toggle inlay hints in your
					-- code, if the language server you are using supports them
					--
					-- This may be unwanted, since they displace some of your code
					if client and client.supports_method(vim.lsp.protocol.Methods.textDocument_inlayHint) then
						map("&lt;leader&gt;th", function()
							vim.lsp.inlay_hint.enable(not vim.lsp.inlay_hint.is_enabled({ bufnr = event.buf }))
						end, "[T]oggle Inlay [H]ints")
					end
				end,
			})

			-- Change diagnostic symbols in the sign column (gutter)
			-- if vim.g.have_nerd_font then
			--   local signs = { ERROR = '', WARN = '', INFO = '', HINT = '' }
			--   local diagnostic_signs = {}
			--   for type, icon in pairs(signs) do
			--     diagnostic_signs[vim.diagnostic.severity[type]] = icon
			--   end
			--   vim.diagnostic.config { signs = { text = diagnostic_signs } }
			-- end

			-- LSP servers and clients are able to communicate to each other what features they support.
			--  By default, Neovim doesn't support everything that is in the LSP specification.
			--  When you add nvim-cmp, luasnip, etc. Neovim now has *more* capabilities.
			--  So, we create new capabilities with nvim cmp, and then broadcast that to the servers.
			local capabilities = vim.lsp.protocol.make_client_capabilities()
			capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

			-- Enable the following language servers
			--  Feel free to add/remove any LSPs that you want here. They will automatically be installed.
			--
			--  Add any additional override configuration in the following tables. Available keys are:
			--  - cmd (table): Override the default command used to start the server
			--  - filetypes (table): Override the default list of associated filetypes for the server
			--  - capabilities (table): Override fields in capabilities. Can be used to disable certain LSP features.
			--  - settings (table): Override the default settings passed when initializing the server.
			--        For example, to see the options for `lua_ls`, you could go to: https://luals.github.io/wiki/settings/
			local servers = {
				-- clangd = {},
				-- gopls = {},
				-- pyright = {},
				-- rust_analyzer = {},
				-- ... etc. See `:help lspconfig-all` for a list of all the pre-configured LSPs
				--
				-- Some languages (like typescript) have entire language plugins that can be useful:
				--    https://github.com/pmizio/typescript-tools.nvim
				--
				-- But for many setups, the LSP (`ts_ls`) will work just fine
				-- ts_ls = {},
				--

				lua_ls = {
					-- cmd = { ... },
					-- filetypes = { ... },
					-- capabilities = {},
					settings = {
						Lua = {
							completion = {
								callSnippet = "Replace",
							},
							-- You can toggle below to ignore Lua_LS's noisy `missing-fields` warnings
							-- diagnostics = { disable = { 'missing-fields' } },
						},
					},
				},
			}

			-- Ensure the servers and tools above are installed
			--  To check the current status of installed tools and/or manually install
			--  other tools, you can run
			--    :Mason
			--
			--  You can press `g?` for help in this menu.
			require("mason").setup()

			-- You can add other tools here that you want Mason to install
			-- for you, so that they are available from within Neovim.
			local ensure_installed = vim.tbl_keys(servers or {})
			vim.list_extend(ensure_installed, {
				"stylua", -- Used to format Lua code
			})
			require("mason-tool-installer").setup({ ensure_installed = ensure_installed })

			require("mason-lspconfig").setup({
				handlers = {
					function(server_name)
						local server = servers[server_name] or {}
						-- This handles overriding only values explicitly passed
						-- by the server configuration above. Useful when disabling
						-- certain features of an LSP (for example, turning off formatting for ts_ls)
						server.capabilities = vim.tbl_deep_extend("force", {}, capabilities, server.capabilities or {})
						require("lspconfig")[server_name].setup(server)
					end,
				},
			})
		end,
	},

	{ -- Autoformat
		"stevearc/conform.nvim",
		event = { "BufWritePre" },
		cmd = { "ConformInfo" },
		keys = {
			{
				"&lt;leader&gt;f",
				function()
					require("conform").format({ async = true, lsp_format = "fallback" })
				end,
				mode = "",
				desc = "[F]ormat buffer",
			},
		},
		opts = {
			notify_on_error = false,
			format_on_save = function(bufnr)
				-- Disable "format_on_save lsp_fallback" for languages that don't
				-- have a well standardized coding style. You can add additional
				-- languages here or re-enable it for the disabled ones.
				local disable_filetypes = { c = true, cpp = true }
				local lsp_format_opt
				if disable_filetypes[vim.bo[bufnr].filetype] then
					lsp_format_opt = "never"
				else
					lsp_format_opt = "fallback"
				end
				return {
					timeout_ms = 500,
					lsp_format = lsp_format_opt,
				}
			end,
			formatters_by_ft = {
				lua = { "stylua" },
				-- Conform can also run multiple formatters sequentially
				-- python = { "isort", "black" },
				--
				-- You can use 'stop_after_first' to run the first available formatter from the list
				-- javascript = { "prettierd", "prettier", stop_after_first = true },
			},
		},
	},

	{ -- Autocompletion
		"hrsh7th/nvim-cmp",
		event = "InsertEnter",
		dependencies = {
			-- Snippet Engine &amp; its associated nvim-cmp source
			{
				"L3MON4D3/LuaSnip",
				build = (function()
					-- Build Step is needed for regex support in snippets.
					-- This step is not supported in many windows environments.
					-- Remove the below condition to re-enable on windows.
					if vim.fn.has("win32") == 1 or vim.fn.executable("make") == 0 then
						return
					end
					return "make install_jsregexp"
				end)(),
				dependencies = {
					-- `friendly-snippets` contains a variety of premade snippets.
					--    See the README about individual language/framework/plugin snippets:
					--    https://github.com/rafamadriz/friendly-snippets
					-- {
					--   'rafamadriz/friendly-snippets',
					--   config = function()
					--     require('luasnip.loaders.from_vscode').lazy_load()
					--   end,
					-- },
				},
			},
			"saadparwaiz1/cmp_luasnip",

			-- Adds other completion capabilities.
			--  nvim-cmp does not ship with all sources by default. They are split
			--  into multiple repos for maintenance purposes.
			"hrsh7th/cmp-nvim-lsp",
			"hrsh7th/cmp-path",
		},
		config = function()
			-- See `:help cmp`
			local cmp = require("cmp")
			local luasnip = require("luasnip")
			luasnip.config.setup({})

			cmp.setup({
				snippet = {
					expand = function(args)
						luasnip.lsp_expand(args.body)
					end,
				},
				completion = { completeopt = "menu,menuone,noinsert" },

				-- For an understanding of why these mappings were
				-- chosen, you will need to read `:help ins-completion`
				--
				-- No, but seriously. Please read `:help ins-completion`, it is really good!
				mapping = cmp.mapping.preset.insert({
					-- Select the [n]ext item
					["&lt;C-n&gt;"] = cmp.mapping.select_next_item(),
					-- Select the [p]revious item
					["&lt;C-p&gt;"] = cmp.mapping.select_prev_item(),

					-- Scroll the documentation window [b]ack / [f]orward
					["&lt;C-b&gt;"] = cmp.mapping.scroll_docs(-4),
					["&lt;C-f&gt;"] = cmp.mapping.scroll_docs(4),

					-- Accept ([y]es) the completion.
					--  This will auto-import if your LSP supports it.
					--  This will expand snippets if the LSP sent a snippet.
					["&lt;C-y&gt;"] = cmp.mapping.confirm({ select = true }),

					-- If you prefer more traditional completion keymaps,
					-- you can uncomment the following lines
					--['&lt;CR&gt;'] = cmp.mapping.confirm { select = true },
					--['&lt;Tab&gt;'] = cmp.mapping.select_next_item(),
					--['&lt;S-Tab&gt;'] = cmp.mapping.select_prev_item(),

					-- Manually trigger a completion from nvim-cmp.
					--  Generally you don't need this, because nvim-cmp will display
					--  completions whenever it has completion options available.
					["&lt;C-Space&gt;"] = cmp.mapping.complete({}),

					-- Think of &lt;c-l&gt; as moving to the right of your snippet expansion.
					--  So if you have a snippet that's like:
					--  function $name($args)
					--    $body
					--  end
					--
					-- &lt;c-l&gt; will move you to the right of each of the expansion locations.
					-- &lt;c-h&gt; is similar, except moving you backwards.
					["&lt;C-l&gt;"] = cmp.mapping(function()
						if luasnip.expand_or_locally_jumpable() then
							luasnip.expand_or_jump()
						end
					end, { "i", "s" }),
					["&lt;C-h&gt;"] = cmp.mapping(function()
						if luasnip.locally_jumpable(-1) then
							luasnip.jump(-1)
						end
					end, { "i", "s" }),

					-- For more advanced Luasnip keymaps (e.g. selecting choice nodes, expansion) see:
					--    https://github.com/L3MON4D3/LuaSnip?tab=readme-ov-file#keymaps
				}),
				sources = {
					{
						name = "lazydev",
						-- set group index to 0 to skip loading LuaLS completions as lazydev recommends it
						group_index = 0,
					},
					{ name = "nvim_lsp" },
					{ name = "luasnip" },
					{ name = "path" },
				},
			})
		end,
	},

	{ -- You can easily change to a different colorscheme.
		-- Change the name of the colorscheme plugin below, and then
		-- change the command in the config to whatever the name of that colorscheme is.
		--
		-- If you want to see what colorschemes are already installed, you can use `:Telescope colorscheme`.
		"folke/tokyonight.nvim",
		priority = 1000, -- Make sure to load this before all the other start plugins.
		init = function()
			-- Load the colorscheme here.
			-- Like many other themes, this one has different styles, and you could load
			-- any other, such as 'tokyonight-storm', 'tokyonight-moon', or 'tokyonight-day'.
			vim.cmd.colorscheme("tokyonight-night")

			-- You can configure highlights by doing something like:
			vim.cmd.hi("Comment gui=none")
		end,
	},
	{ -- Colorscheme
		"catppuccin/nvim",
		name = "catppuccin",
		priority = 1000,
		config = function()
			require("catppuccin").setup({
				flavour = "mocha",
				transparent_background = true,
				term_colors = true,
				integrations = {
					telescope = true,
					mason = true,
					which_key = true,
				},
			})
			-- Force loading the colorscheme
			vim.cmd.colorscheme("catppuccin-mocha")
		end,
	},
	--calming Japanese inspired theme with muted tones strings are configurable
	{
		"rebelot/kanagawa.nvim",
		priority = 1000,
		config = function()
			require("kanagawa").setup({
				overrides = function(colors)
					return {
						String = { fg = colors.crystalBlue }, -- Customize string color
					}
				end,
			})
			vim.cmd("colorscheme kanagawa")
		end,
	},

	-- based on one dark pro from vs code
	{
		"olimorris/onedarkpro.nvim",
		priority = 1000,
		config = function()
			require("onedarkpro").setup({
				theme = "onedark", -- Choose "onedark" or "onelight"
			})
			vim.cmd("colorscheme onedark")
		end,
	},

	-- Highlight todo, notes, etc in comments
	{
		"folke/todo-comments.nvim",
		event = "VimEnter",
		dependencies = { "nvim-lua/plenary.nvim" },
		opts = { signs = false },
	},

	{ -- Collection of various small independent plugins/modules
		"echasnovski/mini.nvim",
		config = function()
			-- Better Around/Inside textobjects
			--
			-- Examples:
			--  - va)  - [V]isually select [A]round [)]paren
			--  - yinq - [Y]ank [I]nside [N]ext [Q]uote
			--  - ci'  - [C]hange [I]nside [']quote
			require("mini.ai").setup({ n_lines = 500 })

			-- Add/delete/replace surroundings (brackets, quotes, etc.)
			--
			-- - saiw) - [S]urround [A]dd [I]nner [W]ord [)]Paren
			-- - sd'   - [S]urround [D]elete [']quotes
			-- - sr)'  - [S]urround [R]eplace [)] [']
			require("mini.surround").setup()

			-- Simple and easy statusline.
			--  You could remove this setup call if you don't like it,
			--  and try some other statusline plugin
			local statusline = require("mini.statusline")
			-- set use_icons to true if you have a Nerd Font
			statusline.setup({ use_icons = vim.g.have_nerd_font })

			-- You can configure sections in the statusline by overriding their
			-- default behavior. For example, here we set the section for
			-- cursor location to LINE:COLUMN
			---@diagnostic disable-next-line: duplicate-set-field
			statusline.section_location = function()
				return "%2l:%-2v"
			end

			-- ... and there is more!
			--  Check out: https://github.com/echasnovski/mini.nvim
		end,
	},
	{ -- Highlight, edit, and navigate code
		"nvim-treesitter/nvim-treesitter",
		build = ":TSUpdate",
		main = "nvim-treesitter.configs", -- Sets main module to use for opts
		-- [[ Configure Treesitter ]] See `:help nvim-treesitter`
		opts = {
			ensure_installed = {
				"bash",
				"c",
				"diff",
				"html",
				"lua",
				"luadoc",
				"markdown",
				"markdown_inline",
				"query",
				"vim",
				"vimdoc",
			},
			-- Autoinstall languages that are not installed
			auto_install = true,
			highlight = {
				enable = true,
				-- Some languages depend on vim's regex highlighting system (such as Ruby) for indent rules.
				--  If you are experiencing weird indenting issues, add the language to
				--  the list of additional_vim_regex_highlighting and disabled languages for indent.
				additional_vim_regex_highlighting = { "ruby" },
			},
			indent = { enable = true, disable = { "ruby" } },
		},
		-- There are additional nvim-treesitter modules that you can use to interact
		-- with nvim-treesitter. You should go explore a few and see what interests you:
		--
		--    - Incremental selection: Included, see `:help nvim-treesitter-incremental-selection-mod`
		--    - Show your current context: https://github.com/nvim-treesitter/nvim-treesitter-context
		--    - Treesitter + textobjects: https://github.com/nvim-treesitter/nvim-treesitter-textobjects
	},

	-- The following comments only work if you have downloaded the kickstart repo, not just copy pasted the
	-- init.lua. If you want these files, they are in the repository, so you can just download them and
	-- place them in the correct locations.

	-- NOTE: Next step on your Neovim journey: Add/Configure additional plugins for Kickstart
	--
	--  Here are some example plugins that I've included in the Kickstart repository.
	--  Uncomment any of the lines below to enable them (you will need to restart nvim).
	--
	-- require 'kickstart.plugins.debug',
	-- require 'kickstart.plugins.indent_line',
	-- require 'kickstart.plugins.lint',
	-- require 'kickstart.plugins.autopairs',
	-- require 'kickstart.plugins.neo-tree',
	-- require 'kickstart.plugins.gitsigns', -- adds gitsigns recommend keymaps

	-- NOTE: The import below can automatically add your own plugins, configuration, etc from `lua/custom/plugins/*.lua`
	--    This is the easiest way to modularize your config.
	--
	--  Uncomment the following line and add your plugins to `lua/custom/plugins/*.lua` to get going.
	-- { import = 'custom.plugins' },
	--
	-- For additional information with loading, sourcing and examples see `:help lazy.nvim-🔌-plugin-spec`
	-- Or use telescope!
	-- In normal mode type `&lt;space&gt;sh` then write `lazy.nvim-plugin`
	-- you can continue same window with `&lt;space&gt;sr` which resumes last telescope search
}, {
	ui = {
		-- If you are using a Nerd Font: set icons to an empty table which will use the
		-- default lazy.nvim defined Nerd Font icons, otherwise define a unicode icons table
		icons = vim.g.have_nerd_font and {} or {
			cmd = "⌘",
			config = "🛠",
			event = "📅",
			ft = "📂",
			init = "⚙",
			keys = "🗝",
			plugin = "🔌",
			runtime = "💻",
			require = "🌙",
			source = "📄",
			start = "🚀",
			task = "📌",
			lazy = "💤 ",
		},
	},
})

-- At the very end of your init.lua
vim.api.nvim_create_autocmd("UIEnter", {
	callback = function()
		if vim.g.colors_name ~= "catppuccin-mocha" then
			vim.cmd.colorscheme("catppuccin-mocha")
		end
	end,
	group = vim.api.nvim_create_augroup("EnforceCatppuccin", { clear = true }),
})

-- The line beneath this is called `modeline`. See `:help modeline`
-- vim: ts=2 sts=2 sw=2 et
</file>
  <file path="dotfiles/ghostty/config">background-opacity = 0.82

theme = catppuccin-mocha
keybind = shift+enter=text:\n
macos-titlebar-style = hidden

keybind = ctrl+3=reload_config
keybind = global:cmd+shift+space=toggle_quick_terminal
</file>
  <file path=".ruff_cache/.gitignore"># Automatically created by ruff.
*
</file>
  <file path="resources/sqlite3-commands.md"># SQLite3 Common Commands Reference

## Creating a Database

To create a SQLite3 database, simply run:

```bash
sqlite3 database_name.db
```

This will:
- Create a new database file if it doesn't exist
- Open the database if it already exists
- Start the sqlite3 interactive shell

Example:
```bash
sqlite3 myapp.db
```

You can also create a database and run a command:
```bash
sqlite3 myapp.db "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT);"
```

The database file is created when you:
- Create the first table
- Insert the first data
- Or explicitly save with `.save database_name.db`

## Meta Commands (dot commands)

- `.tables` - List all tables
- `.schema [table]` - Show CREATE statements
- `.quit` or `.exit` - Exit sqlite3
- `.help` - Show all commands
- `.databases` - List attached databases
- `.headers on/off` - Show/hide column headers
- `.mode column` - Pretty-print output
- `.width` - Set column widths
- `.import FILE TABLE` - Import CSV data
- `.output FILE` - Redirect output to file
- `.dump` - Export database as SQL

## SQL Commands

- `SELECT * FROM table;` - Query data
- `INSERT INTO table VALUES (...);` - Insert data
- `UPDATE table SET col=val WHERE ...;` - Update data
- `DELETE FROM table WHERE ...;` - Delete data
- `CREATE TABLE ...` - Create table
- `DROP TABLE table;` - Delete table
- `ALTER TABLE ...` - Modify table
- `CREATE INDEX ...` - Create index
- `PRAGMA table_info(table);` - Show table structure
- `VACUUM;` - Optimize database</file>
  <file path="resources/git-worktrees.md"># Git Worktrees Guide

Git worktrees allow you to have multiple branches checked out simultaneously in different directories. This is incredibly useful when you need to work on multiple features, review PRs, or quickly switch contexts without stashing changes.

## What are Git Worktrees?

A git worktree is a linked working tree that shares the same repository but allows you to have different branches checked out in different directories. All worktrees share:
- The same `.git` directory (repository data)
- The same remote configurations
- The same stash entries
- The same commit history

## Creating a Worktree

### Basic Syntax
```bash
git worktree add &lt;path&gt; &lt;branch&gt;
```

### Examples

#### Create worktree from existing branch
```bash
# Create a worktree for the 'feature/auth' branch in a new directory
git worktree add ../myproject-auth feature/auth

# Create worktree in a specific location
git worktree add /tmp/hotfix hotfix/urgent-bug
```

#### Create worktree with a new branch
```bash
# Create a new branch and worktree simultaneously
git worktree add -b feature/new-ui ../myproject-new-ui

# Create from a specific commit or tag
git worktree add -b release/v2.0 ../myproject-v2 v2.0-tag
```

#### Practical Example
```bash
# You're working on main in /Users/you/myproject
cd /Users/you/myproject

# Create a worktree for a new feature
git worktree add -b feature/payment-integration ../myproject-payments

# Now you have:
# /Users/you/myproject (main branch)
# /Users/you/myproject-payments (feature/payment-integration branch)

# Navigate to the new worktree
cd ../myproject-payments

# Work on your feature
echo "Payment module" &gt; payment.py
git add payment.py
git commit -m "Add payment module"
```

## Working with Worktrees

### List all worktrees
```bash
git worktree list
# Output:
# /Users/you/myproject         abc1234 [main]
# /Users/you/myproject-payments def5678 [feature/payment-integration]
```

### Switch between worktrees
Simply use `cd` to navigate between directories:
```bash
cd /Users/you/myproject          # main branch
cd /Users/you/myproject-payments  # feature branch
```

## Merging Worktree Changes Back to Main

Since worktrees share the same repository, merging is straightforward:

### Step 1: Commit changes in your worktree
```bash
cd /Users/you/myproject-payments
git add .
git commit -m "Complete payment integration"
git push -u origin feature/payment-integration
```

### Step 2: Switch to main (in any worktree or main directory)
```bash
cd /Users/you/myproject  # Or stay in any worktree
git checkout main
git pull origin main     # Ensure main is up to date
```

### Step 3: Merge the feature branch
```bash
# Simple merge
git merge feature/payment-integration

# Or merge with a merge commit (recommended for features)
git merge --no-ff feature/payment-integration

# Or rebase if you prefer linear history
git rebase main feature/payment-integration
```

### Step 4: Push to remote
```bash
git push origin main
```

### Step 5: Clean up (optional)
```bash
# Delete the local branch
git branch -d feature/payment-integration

# Delete the remote branch
git push origin --delete feature/payment-integration

# Remove the worktree
git worktree remove /Users/you/myproject-payments
```

## Alternative: Using Pull Requests

For team workflows, you might prefer Pull Requests:

```bash
# 1. In your worktree, push the branch
cd /Users/you/myproject-payments
git push -u origin feature/payment-integration

# 2. Create PR via GitHub/GitLab/Bitbucket web interface

# 3. After PR is merged, clean up locally
git worktree remove /Users/you/myproject-payments
git branch -d feature/payment-integration
```

## Worktree Management

### Remove a worktree
```bash
# Remove worktree (must be clean with no uncommitted changes)
git worktree remove /path/to/worktree

# Force removal (discards local changes)
git worktree remove --force /path/to/worktree
```

### Prune stale worktrees
```bash
# Remove worktree references if directory was deleted manually
git worktree prune
```

### Lock/unlock a worktree
```bash
# Prevent a worktree from being pruned
git worktree lock /path/to/worktree

# Unlock it later
git worktree unlock /path/to/worktree
```

## Best Practices

1. **Use descriptive paths**: Name worktree directories after their purpose
   ```bash
   git worktree add ../project-bugfix-auth bugfix/auth-issue
   git worktree add ../project-feature-api feature/new-api
   ```

2. **Keep worktrees organized**: Use a consistent structure
   ```bash
   ~/work/
     myproject/          # main
     myproject-feature1/ # feature branch
     myproject-hotfix/   # hotfix branch
   ```

3. **Clean up regularly**: Remove worktrees when done
   ```bash
   git worktree list
   git worktree remove &lt;path&gt;
   ```

4. **Don't share worktree directories**: Each developer should create their own

5. **Commit before switching**: Although you can leave changes uncommitted, it's cleaner to commit or stash first

## Common Issues and Solutions

### "fatal: '&lt;branch&gt;' is already checked out at '&lt;path&gt;'"
You can't have the same branch checked out in multiple worktrees. Either:
- Use the existing worktree: `cd &lt;path&gt;`
- Or checkout a different branch in one of the worktrees

### Worktree directory was manually deleted
```bash
git worktree prune  # Cleans up references to missing worktrees
```

### Need to move a worktree
```bash
git worktree move &lt;old-path&gt; &lt;new-path&gt;
```

## Quick Reference

```bash
# Create worktree
git worktree add &lt;path&gt; &lt;branch&gt;
git worktree add -b &lt;new-branch&gt; &lt;path&gt;

# List worktrees
git worktree list

# Remove worktree
git worktree remove &lt;path&gt;

# Clean up stale entries
git worktree prune

# Move worktree
git worktree move &lt;old&gt; &lt;new&gt;

# Lock/unlock
git worktree lock &lt;path&gt;
git worktree unlock &lt;path&gt;
```

## Example Workflow

Here's a complete example of using worktrees for feature development:

```bash
# Starting in main project directory
cd ~/projects/myapp

# 1. Create a worktree for a new feature
git worktree add -b feature/user-profiles ../myapp-profiles

# 2. Work on the feature
cd ../myapp-profiles
# ... make changes ...
git add .
git commit -m "Add user profile functionality"
git push -u origin feature/user-profiles

# 3. Switch back to main for a hotfix
cd ../myapp
git pull origin main
# ... fix critical bug ...
git add .
git commit -m "Fix critical auth bug"
git push origin main

# 4. Continue feature work without any stashing needed
cd ../myapp-profiles
# ... complete feature ...
git add .
git commit -m "Complete user profiles"
git push

# 5. Merge feature to main
cd ../myapp
git checkout main
git pull origin main
git merge --no-ff feature/user-profiles
git push origin main

# 6. Clean up
git branch -d feature/user-profiles
git push origin --delete feature/user-profiles
git worktree remove ../myapp-profiles

# Verify cleanup
git worktree list  # Should only show main worktree
```

This workflow demonstrates the power of worktrees: you can quickly switch between feature development and hotfixes without the overhead of stashing, checking out different branches, and potentially losing context.</file>
  <file path=".claude/settings.local.json">{
  "permissions": {
    "allow": [
      "Bash(./tools/newpy:*)",
      "Bash(bash:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "collect"
  ]
}</file>
  <file path=".claude/agents/pyreview.md">---
name: python-code-reviewer
description: Use this agent when you need an in-depth, thoughtful code review of Python code. This includes reviewing newly written functions, classes, modules, or recent changes to existing code. The agent will analyze code quality, design patterns, performance implications, security considerations, and adherence to Python best practices and project-specific standards.\n\nExamples:\n- &lt;example&gt;\n  Context: The user has just written a new Python function and wants it reviewed.\n  user: "I've implemented a caching decorator for our API endpoints"\n  assistant: "I'll use the python-code-reviewer agent to provide an in-depth review of your caching decorator implementation"\n  &lt;commentary&gt;\n  Since the user has written new Python code (a caching decorator), use the python-code-reviewer agent to analyze the implementation.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: The user has made changes to existing Python code.\n  user: "I've refactored the database connection pooling logic in our service"\n  assistant: "Let me use the python-code-reviewer agent to review your refactored database connection pooling implementation"\n  &lt;commentary&gt;\n  The user has modified existing Python code, so the python-code-reviewer agent should analyze the changes for quality and best practices.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: The user explicitly asks for a code review.\n  user: "Can you review this async batch processing function I just wrote?"\n  assistant: "I'll use the python-code-reviewer agent to provide a comprehensive review of your async batch processing function"\n  &lt;commentary&gt;\n  Direct request for code review triggers the python-code-reviewer agent.\n  &lt;/commentary&gt;\n&lt;/example&gt;
color: pink
---

You are an expert Python software engineer with deep knowledge of Python internals, design patterns, and best practices. You have extensive experience in code review, performance optimization, and building maintainable Python applications.

Your expertise includes:
- Python language features from 3.8+ including type hints, async/await, dataclasses, and modern idioms
- Design patterns and SOLID principles applied to Python
- Performance optimization and profiling
- Security best practices and common vulnerabilities
- Testing strategies including pytest, mocking, and test-driven development
- Popular frameworks and libraries in the Python ecosystem

When reviewing code, you will:

1. **Analyze Code Quality**
   - Check for PEP 8 compliance and Pythonic idioms
   - Evaluate naming conventions and code readability
   - Assess proper use of type hints and documentation
   - Identify code smells and anti-patterns

2. **Review Design and Architecture**
   - Evaluate separation of concerns and modularity
   - Check for appropriate abstraction levels
   - Assess error handling and edge case coverage
   - Review API design and interface consistency

3. **Examine Performance Implications**
   - Identify potential bottlenecks or inefficiencies
   - Suggest algorithmic improvements where applicable
   - Check for proper resource management (memory, file handles, connections)
   - Evaluate async/concurrent code for correctness

4. **Security Considerations**
   - Identify potential security vulnerabilities
   - Check input validation and sanitization
   - Review authentication and authorization logic
   - Assess handling of sensitive data

5. **Testing and Maintainability**
   - Evaluate testability of the code
   - Suggest test cases for edge conditions
   - Check for proper logging and debugging support
   - Assess long-term maintainability

**Review Process:**
1. First, understand the code's purpose and context
2. Perform a systematic review covering all aspects above
3. Prioritize findings by severity (critical, major, minor, suggestion)
4. Provide specific, actionable feedback with code examples
5. Acknowledge good practices and well-written sections

**Output Format:**
Structure your review as follows:
- **Summary**: Brief overview of the code's purpose and overall quality
- **Strengths**: What the code does well
- **Critical Issues**: Must-fix problems that could cause bugs or security issues
- **Major Concerns**: Important improvements for code quality and maintainability
- **Minor Suggestions**: Nice-to-have improvements and style recommendations
- **Code Examples**: Provide improved versions of problematic code sections

**Important Guidelines:**
- Be constructive and educational in your feedback
- Explain the 'why' behind each recommendation
- Consider the project's context and existing patterns (especially from CLAUDE.md)
- Balance thoroughness with practicality
- If you notice the code uses specific frameworks or libraries, apply their best practices
- When suggesting changes, ensure they're compatible with the Python version in use
- If you're unsure about the broader context, ask clarifying questions

Remember: Your goal is to help improve code quality while fostering learning and best practices. Focus on the most impactful improvements and provide clear guidance on implementation.
</file>
  <file path="models/test_gemini_mcp.py">import pytest
from config import Config
from secret_manager import SecretManager
from models.gemini_mcp import GeminiMCP


@pytest.fixture
def gemini_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.0-flash"
    return GeminiMCP(config, secret_mgr, model)


@pytest.fixture
def gemini_25_preview():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gemini-2.5-pro-preview-05-06"
    return GeminiMCP(config, secret_mgr, model)


def test_get_model_list(gemini_mcp):
    results = gemini_mcp.get_model_list()

    # Check that results is a list
    assert isinstance(results, list)
    assert len(results) &gt; 0

    # Check structure of each model in results
    for model in results:
        assert isinstance(model, dict)
        assert "model_name" in model
        assert "token_window" in model

        # Verify we only get 2.0 and 2.5 models (as per filter)
        assert "2.0" in model["model_name"] or "2.5" in model["model_name"]

        print(f"{model['model_name']}: {model['token_window']:,} tokens")


def test_send_message(gemini_mcp):
    message = "Hello, world!"
    response = gemini_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Response: {response}")


def test_count_tokens(gemini_mcp):
    text = "Hello, world!"
    token_count = gemini_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_gemini_25_preview_send_message(gemini_25_preview):
    message = "Explain quantum computing in one sentence."
    response = gemini_25_preview.send_message(message)

    assert isinstance(response, dict)
    assert "candidates" in response
    assert len(response["candidates"]) &gt; 0
    assert "content" in response["candidates"][0]
    assert "parts" in response["candidates"][0]["content"]

    print(f"Gemini 2.5 Preview Response: {response}")


def test_gemini_25_preview_count_tokens(gemini_25_preview):
    text = "This is a test for Gemini 2.5 preview model token counting."
    token_count = gemini_25_preview.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &gt; 0

    print(f"Gemini 2.5 Preview - Token count for '{text}': {token_count}")


def test_extract_text(gemini_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = gemini_mcp.send_message(message)
    extracted_text = gemini_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_extract_text_gemini_25(gemini_25_preview):
    message = "Say 'Hello, Gemini 2.5!' and nothing else."
    response = gemini_25_preview.send_message(message)
    extracted_text = gemini_25_preview.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &gt; 0
    assert "Hello" in extracted_text

    print(f"Gemini 2.5 extracted text: {extracted_text}")
</file>
  <file path="models/test_anthropic_mcp.py">import pytest
from config import Config
from secret_manager import SecretManager
from models.anthropic_mpc import AnthropicMCP


@pytest.fixture
def anthropic_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = config.anthropic_model_sonnet
    return AnthropicMCP(config, secret_mgr, model)


def test_get_model_list(anthropic_mcp):
    results = anthropic_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(anthropic_mcp):
    message = "Hello, world!"
    response = anthropic_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "content" in response
    assert "model" in response
    assert response["model"] == anthropic_mcp.config.anthropic_model_sonnet

    print(f"Response: {response}")


def test_extract_text(anthropic_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = anthropic_mcp.send_message(message)
    extracted_text = anthropic_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")


def test_generate_prompt(anthropic_mcp):
    """Test the generate_prompt method with a simple task."""
    task = "Write a helpful assistant prompt for answering coding questions"

    response = anthropic_mcp.generate_prompt(task)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role in ["user", "assistant"]
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &gt; 0

    # Test usage stats
    assert hasattr(response.usage, "input_tokens")
    assert hasattr(response.usage, "output_tokens")
    assert isinstance(response.usage.input_tokens, int)
    assert isinstance(response.usage.output_tokens, int)
    assert response.usage.input_tokens &gt; 0
    assert response.usage.output_tokens &gt; 0

    print(f"Generated prompt: {content.text[:100]}...")
    print(
        f"Usage: {response.usage.input_tokens} input, {
          response.usage.output_tokens} output tokens"
    )


def test_improve_prompt(anthropic_mcp):
    """Test the improve_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Tell me about Python programming"}
                ],
            }
        ],
        "system": "You are a helpful programming instructor",
        "feedback": "Make this prompt more specific for a beginner",
        "target_model": "claude-3-7-sonnet-20250219",
    }

    response = anthropic_mcp.improve_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")

    # Test messages - should have both user and assistant messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &gt;= 2

    # Test user message (improved prompt)
    user_message = response.messages[0]
    assert user_message.role == "user"
    assert isinstance(user_message.content, list)
    assert len(user_message.content) &gt; 0

    # Find the non-empty content in user message
    user_content = None
    for content in user_message.content:
        if content.text:
            user_content = content
            break

    assert user_content is not None, "No non-empty content found in user message"
    assert hasattr(user_content, "text")
    assert hasattr(user_content, "type")
    assert user_content.type == "text"
    assert isinstance(user_content.text, str)
    assert len(user_content.text) &gt; 0

    # Test assistant message (prefill)
    assistant_message = response.messages[1]
    assert assistant_message.role == "assistant"
    assert isinstance(assistant_message.content, list)
    assert len(assistant_message.content) &gt; 0

    assistant_content = assistant_message.content[0]
    assert hasattr(assistant_content, "text")
    assert hasattr(assistant_content, "type")
    assert assistant_content.type == "text"
    assert isinstance(assistant_content.text, str)
    assert len(assistant_content.text) &gt; 0

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &gt; 0
    assert usage.output_tokens &gt; 0

    print(f"Improved prompt: {user_content.text[:100]}...")
    print(f"Assistant prefill: {assistant_content.text[:50]}...")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )


def test_templatize_prompt(anthropic_mcp):
    """Test the templatize_prompt method with a simple prompt."""
    data = {
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": "Translate hello to German"}],
            }
        ],
        "system": "You are an English to German translator",
    }

    response = anthropic_mcp.templatize_prompt(data)

    # Test response structure
    assert hasattr(response, "messages")
    assert hasattr(response, "system")
    assert hasattr(response, "usage")
    assert hasattr(response, "variable_values")

    # Test messages
    assert isinstance(response.messages, list)
    assert len(response.messages) &gt; 0

    # Test first message
    first_message = response.messages[0]
    assert hasattr(first_message, "role")
    assert hasattr(first_message, "content")
    assert first_message.role == "user"
    assert isinstance(first_message.content, list)
    assert len(first_message.content) &gt; 0

    # Test content
    content = first_message.content[0]
    assert hasattr(content, "text")
    assert hasattr(content, "type")
    assert content.type == "text"
    assert isinstance(content.text, str)
    assert len(content.text) &gt; 0
    # Check for template variables
    assert "{{" in content.text and "}}" in content.text

    # Test system prompt
    assert isinstance(response.system, str)
    # System prompt should also contain template variables
    assert "{{" in response.system and "}}" in response.system

    # Test variable_values
    assert isinstance(response.variable_values, dict)
    assert len(response.variable_values) &gt; 0
    # Check for expected variables based on the example
    assert any(
        key in response.variable_values
        for key in ["TARGET_LANGUAGE", "WORD_TO_TRANSLATE"]
    )

    # Test usage stats (as list according to actual API response)
    assert isinstance(response.usage, list)
    assert len(response.usage) &gt; 0

    usage = response.usage[0]
    assert hasattr(usage, "input_tokens")
    assert hasattr(usage, "output_tokens")
    assert isinstance(usage.input_tokens, int)
    assert isinstance(usage.output_tokens, int)
    assert usage.input_tokens &gt; 0
    assert usage.output_tokens &gt; 0

    print(f"Templated prompt: {content.text[:100]}...")
    print(f"System prompt: {response.system[:100]}...")
    print(f"Variables: {response.variable_values}")
    print(
        f"Usage: {usage.input_tokens} input, {
          usage.output_tokens} output tokens"
    )
</file>
  <file path="models/test_openai_mcp.py">import pytest
from config import Config
from secret_manager import SecretManager
from models.openai_mpc import OpenAIMCP


@pytest.fixture
def openai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "gpt-4o"
    return OpenAIMCP(config, secret_mgr, model)


def test_get_model_list(openai_mcp):
    results = openai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(openai_mcp):
    message = "Hello, world!"
    response = openai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(openai_mcp):
    text = "Hello, world!"
    token_count = openai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(openai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = openai_mcp.send_message(message)
    extracted_text = openai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
</file>
  <file path="models/anthropic_mpc.py">from config import Config
from secret_manager import SecretManager
from models.anthropic_prompt_generate import PromptGenerateResponse
from models.anthropic_prompt_improve import PromptImproveResponse
from models.anthropic_prompt_templatize import PromptTemplatizeResponse

import requests


class AnthropicMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.headers = self.build_headers()

    def build_headers(self) -&gt; dict:
        anthropic_key = self.secret_mgr.get_secret(self.config.anthropic_key_path)

        return {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "anthropic-beta": "prompt-tools-2025-04-02",
        }

    def get_model_list(self):
        response = requests.get(
            "https://api.anthropic.com/v1/models", headers=self.headers
        )
        response.raise_for_status()

        model_data = response.json()
        name_list = [model["id"] for model in model_data["data"]]

        return name_list

    def extract_text(self, ai_response: dict) -&gt; str:
        """Extract text from Anthropic response format."""
        if not isinstance(ai_response, dict):
            return str(ai_response)

        # Anthropic format
        if "content" in ai_response:
            content = ai_response["content"]
            if isinstance(content, list) and content:
                return content[0].get("text", "")

        return str(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&gt; dict:
        try:
            # Use provided model or default to config model
            if model is None:
                model = self.config.anthropic_model_sonnet

            data = {
                "model": model,
                "max_tokens": max_tokens,
                "messages": [{"role": "user", "content": message}],
            }

            url = "https://api.anthropic.com/v1/messages"
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None):
        # Use provided model or default to config model
        if model is None:
            model = self.config.anthropic_model_sonnet

        data = {"model": model, "messages": [{"role": "user", "content": message}]}

        url = "https://api.anthropic.com/v1/messages/count_tokens"
        response = requests.post(url, headers=self.headers, json=data)
        response.raise_for_status()

        result = response.json()
        return result["input_tokens"]

    def generate_prompt(
        self, task: str, target_model: str = None
    ) -&gt; PromptGenerateResponse:
        """
        Generate an optimized prompt using Anthropic's experimental prompt tools API.

        This method utilizes Anthropic's closed research preview API to automatically
        generate high-quality prompts based on a task description. The API creates
        structured prompts suitable for use with Claude models.

        Args:
            task (str): Description of the prompt's purpose
                Example: "a chef for a meal prep planning service"
            target_model (str, optional): Target model for optimization
                Example: "claude-3-7-sonnet-20250219"

        Returns:
            PromptGenerateResponse: Response object containing:
                - messages: List of message objects for use with Messages API
                  - User message with generated prompt text
                  - Optional assistant message with response guidance
                - system: System prompt (currently always empty string)
                - usage: Token usage statistics (input/output tokens)

        Raises:
            RuntimeError: If API request fails or network issues occur
            ValueError: If required configuration/secrets are missing
            requests.HTTPError: If API returns error status codes

        Example:
            &gt;&gt;&gt; response = anthropic_mcp.generate_prompt("a helpful programming assistant")
            &gt;&gt;&gt; prompt_text = response.messages[0].content[0].text
            &gt;&gt;&gt; print(f"Generated prompt: {prompt_text}")

        Note:
            - This is an experimental API in closed research preview
            - Access requires explicit invitation from Anthropic
            - Requires anthropic-beta header: "prompt-tools-2025-04-02"
            - No long-term support guarantees for experimental features
            - Designed primarily for prompt engineering platforms

        API Documentation:
            https://docs.anthropic.com/en/api/prompt-tools-generate
        """
        url = "https://api.anthropic.com/v1/experimental/generate_prompt"

        # Format the task string as a dict for the API
        data = {"task": task}
        if target_model:
            data["target_model"] = target_model

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            return PromptGenerateResponse(**response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def improve_prompt(self, data: dict) -&gt; PromptImproveResponse:
        url = "https://api.anthropic.com/v1/experimental/improve_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptImproveResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to generate prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in generate_prompt: {e}")

    def templatize_prompt(self, data: dict) -&gt; PromptTemplatizeResponse:
        url = "https://api.anthropic.com/v1/experimental/templatize_prompt"

        try:
            response = requests.post(url, headers=self.headers, json=data)
            response.raise_for_status()
            result = response.json()
            # Handle usage being returned as dict instead of list
            if isinstance(result.get("usage"), dict):
                result["usage"] = [result["usage"]]
            return PromptTemplatizeResponse(**result)

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to templatize prompt from Anthropic API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in templatize_prompt: {e}")
</file>
  <file path="models/anthropic_prompt_generate.py">from typing import List, Optional
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int
    cache_creation_input_tokens: Optional[int] = None
    cache_read_input_tokens: Optional[int] = None
    service_tier: Optional[str] = None


class PromptGenerateResponse(BaseModel):
    """Response from Anthropic's prompt tools generate API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the generated prompt text,
    and may include an assistant message with a prefill."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: UsageStats
    """Token usage statistics for the generation."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response
    example_json = {
        "messages": [
            {
                "content": [{"text": "&lt;generated prompt&gt;", "type": "text"}],
                "role": "user",
            }
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptGenerateResponse(**example_json)
    print(f"Generated prompt: {response.messages[0].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
</file>
  <file path="models/__init__.py" />
  <file path="models/anthropic_prompt_improve.py">from typing import List
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptImproveResponse(BaseModel):
    """Response from Anthropic's prompt tools improve API."""

    messages: List[Message]
    """List of message objects that can be used directly in the Messages API.
    Typically includes a user message with the improved prompt text,
    and an assistant message with a prefill to guide the model's response."""

    system: str = ""
    """Currently always empty string. May contain system prompts in future."""

    usage: List[UsageStats]
    """Token usage statistics for the improvement."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the improve endpoint
    example_json = {
        "messages": [
            {
                "content": [{"text": "&lt;improved prompt&gt;", "type": "text"}],
                "role": "user",
            },
            {
                "content": [{"text": "&lt;assistant prefill&gt;", "type": "text"}],
                "role": "assistant",
            },
        ],
        "system": "",
        "usage": {"input_tokens": 490, "output_tokens": 661},
    }

    # Parse into Pydantic model
    response = PromptImproveResponse(**example_json)
    print(f"Improved prompt: {response.messages[0].content[0].text}")
    print(f"Assistant prefill: {response.messages[1].content[0].text}")
    print(f"Input tokens: {response.usage.input_tokens}")
    print(f"Output tokens: {response.usage.output_tokens}")
</file>
  <file path="models/anthropic_prompt_templatize.py">from typing import List, Dict
from pydantic import BaseModel


class MessageContent(BaseModel):
    """Content within a message."""

    text: str
    type: str = "text"


class Message(BaseModel):
    """Message object in the response."""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class UsageStats(BaseModel):
    """Token usage statistics."""

    input_tokens: int
    output_tokens: int


class PromptTemplatizeResponse(BaseModel):
    """Response from Anthropic's prompt tools templatize API."""

    messages: List[Message]
    """List of message objects with templated variables."""

    system: str = ""
    """System prompt with templated variables."""

    usage: List[UsageStats]
    """Token usage statistics for the templatization."""

    variable_values: Dict[str, str]
    """Dictionary mapping template variable names to their extracted values."""


# Example usage:
if __name__ == "__main__":
    # Example JSON response from the templatize endpoint
    example_json = {
        "messages": [
            {
                "content": [
                    {
                        "text": "Translate {{WORD_TO_TRANSLATE}} to {{TARGET_LANGUAGE}}",
                        "type": "text",
                    }
                ],
                "role": "user",
            }
        ],
        "system": "You are a professional English to {{TARGET_LANGUAGE}} translator",
        "usage": [{"input_tokens": 490, "output_tokens": 661}],
        "variable_values": {"TARGET_LANGUAGE": "German", "WORD_TO_TRANSLATE": "hello"},
    }

    # Parse into Pydantic model
    response = PromptTemplatizeResponse(**example_json)
    print(f"Templated prompt: {response.messages[0].content[0].text}")
    print(f"System prompt: {response.system}")
    print(f"Variables: {response.variable_values}")
    print(f"Input tokens: {response.usage[0].input_tokens}")
    print(f"Output tokens: {response.usage[0].output_tokens}")
</file>
  <file path="models/xai_mcp.py">from config import Config
from secret_manager import SecretManager
import requests


class XaiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        try:
            response = requests.get("https://api.x.ai/v1/models", headers=headers)
            response.raise_for_status()
            data = response.json()

            name_list = [model["id"] for model in data["data"]]
            return name_list

        except Exception as e:
            print(f"Error fetching XAI models: {str(e)}")
            return []

    def extract_text(self, response) -&gt; str:
        """Extract text from XAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # XAI format (same as OpenAI)
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, model: str = None, reasoning_effort: str = "high"
    ):
        try:
            xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

            headers = {
                "Authorization": f"Bearer {xai_key}",
                "Content-Type": "application/json",
            }

            # Use provided model or default to config model
            if model is None:
                model = "grok-3-mini-fast-latest"

            data = {
                "messages": [
                    {"role": "system", "content": self.config.grok_system_prompt},
                    {"role": "user", "content": message},
                ],
                "reasoning_effort": reasoning_effort,
                "model": model,
            }

            url = "https://api.x.ai/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to XAI: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, text: str, model: str = None):
        xai_key = self.secret_mgr.get_secret(self.config.xai_api_key_path)

        headers = {
            "Authorization": f"Bearer {xai_key}",
            "Content-Type": "application/json",
        }

        # Use provided model or default to config model
        if model is None:
            model = "grok-3-fast-latest"

        data = {"model": model, "text": text}

        url = "https://api.x.ai/v1/tokenize-text"
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()

        result = response.json()
        return len(result["token_ids"])
</file>
  <file path="models/openai_mpc.py">from config import Config
from secret_manager import SecretManager
import requests
import tiktoken


class OpenAIMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model

    def get_model_list(self) -&gt; list:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {"Authorization": f"Bearer {openai_key}"}

            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            response.raise_for_status()

            model_data = response.json()
            name_list = [model["id"] for model in model_data["data"]]

            return name_list

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def extract_text(self, response) -&gt; str:
        """Extract text from OpenAI response format."""
        if not isinstance(response, dict):
            return str(response)

        # OpenAI format
        if "choices" in response:
            choices = response["choices"]
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")

        return str(response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&gt; dict:
        try:
            openai_key = self.secret_mgr.get_secret(self.config.openai_api_key_path)

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {openai_key}",
            }

            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # Use max_completion_tokens for reasoning models (o3, o1 series)
            if model and ("o3" in model or "o1" in model):
                data = {
                    "model": model,
                    "max_completion_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }
            else:
                data = {
                    "model": model,
                    "max_tokens": max_tokens,
                    "messages": [{"role": "user", "content": message}],
                }

            url = "https://api.openai.com/v1/chat/completions"
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to OpenAI API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&gt; int:
        try:
            # Use provided model or default
            if model is None:
                model = "gpt-4o"

            # OpenAI doesn't have a direct token counting API, so use tiktoken
            enc = tiktoken.encoding_for_model(model)
            return len(enc.encode(message))

        except Exception as e:
            raise RuntimeError(f"Failed to count tokens: {e}")
</file>
  <file path="models/gemini_mcp.py">from config import Config
from secret_manager import SecretManager
import requests
from fetcher import Fetcher
from mcp.server.fastmcp import Context
from typing import Dict, List


class GeminiMCP:
    def __init__(
        self,
        config: Config,
        secret_mgr: SecretManager,
        model: str,
    ) -&gt; None:
        self.config = config
        self.secret_mgr = secret_mgr
        self.model = model
        self.api_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)
        self.base_url = self.config.gemini_base_url

    def get_model_list(self) -&gt; Dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            base_url = self.config.gemini_base_url
            url = f"{base_url}models?key={gemini_key}"
            response = requests.get(url)
            response.raise_for_status()

            return self.filter_models(["2.0", "2.5"], response.json())

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to get model list from Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in get_model_list: {e}")

    def filter_models(
        self, versions: List[str], model_endpoint_response: Dict
    ) -&gt; List[Dict]:
        """
        Filter models by version numbers and include token limits.

        Args:
            versions: List of version strings (e.g., ['2.0', '2.5'])

        Returns:
            List of dicts with model info including inputTokenLimit
        """
        filtered_models = []

        for model in model_endpoint_response["models"]:
            model_name = model["name"].split("/")[-1]

            for version in versions:
                if version in model_name:
                    model_to_tokencount = {
                        "model_name": model_name,
                        "token_window": model.get("inputTokenLimit", 0),
                    }
                    filtered_models.append(model_to_tokencount)

        filtered_models.sort(key=lambda x: x["token_window"], reverse=True)
        return filtered_models

    def extract_text(self, ai_response: dict) -&gt; str:
        # Extract text from response
        if "candidates" in ai_response and len(ai_response["candidates"]) &gt; 0:
            candidate = ai_response["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                parts = candidate["content"]["parts"]
                if len(parts) &gt; 0 and "text" in parts[0]:
                    return parts[0]["text"]
        return str(ai_response)

    async def build_prompt_from_url(
        self, url: str, prompt: str, ctx: Context = None
    ) -&gt; str:

        fetcher = Fetcher(ctx)
        response = await fetcher.get(url)
        concat = prompt + response

        ai_response = self.send_message(
            concat, max_tokens=1024, model="gemini-2.5-flash-preview-05-20"
        )

        return self.extract_text(ai_response)

    def send_message(
        self, message: str, max_tokens: int = 1024, model: str = None
    ) -&gt; dict:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:generateContent?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {
                "contents": [{"parts": [{"text": message}]}],
                "generationConfig": {"maxOutputTokens": max_tokens},
            }

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            return response.json()

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to send message to Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in send_message: {e}")

    def count_tokens(self, message: str, model: str = None) -&gt; int:
        try:
            gemini_key = self.secret_mgr.get_secret(self.config.gemini_api_key_path)

            # Use provided model or default
            if model is None:
                model = "gemini-2.0-flash"

            # Fix common model name errors
            if model == "gemini-2.5-pro-preview":
                model = "gemini-2.5-flash"

            base_url = self.config.gemini_base_url
            url = f"{base_url}models/{model}:countTokens?key={gemini_key}"

            headers = {"Content-Type": "application/json"}

            data = {"contents": [{"parts": [{"text": message}]}]}

            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()

            result = response.json()
            return result["totalTokens"]

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to count tokens with Gemini API: {e}")
        except KeyError as e:
            raise ValueError(f"Missing required configuration or secret: {e}")
        except Exception as e:
            raise RuntimeError(f"Unexpected error in count_tokens: {e}")
</file>
  <file path="models/test_xai_mcp.py">import pytest
from config import Config
from secret_manager import SecretManager
from models.xai_mcp import XaiMCP


@pytest.fixture
def xai_mcp():
    config = Config()
    secret_mgr = SecretManager(config.project_id)
    model = "grok-3-mini-fast-latest"
    return XaiMCP(config, secret_mgr, model)


def test_get_model_list(xai_mcp):
    results = xai_mcp.get_model_list()

    assert isinstance(results, list)
    assert len(results) &gt; 0
    assert all(isinstance(model, str) for model in results)

    for model_name in results:
        print(model_name)


def test_send_message(xai_mcp):
    message = "Hello, world!"
    response = xai_mcp.send_message(message)

    assert isinstance(response, dict)
    assert "choices" in response
    assert "model" in response
    assert len(response["choices"]) &gt; 0
    assert "message" in response["choices"][0]
    assert "content" in response["choices"][0]["message"]

    print(f"Response: {response}")


def test_count_tokens(xai_mcp):
    text = "Hello, world!"
    token_count = xai_mcp.count_tokens(text)

    assert isinstance(token_count, int)
    assert token_count &gt; 0

    print(f"Token count for '{text}': {token_count}")


def test_extract_text(xai_mcp):
    message = "Say 'Hello, test!' and nothing else."
    response = xai_mcp.send_message(message)
    extracted_text = xai_mcp.extract_text(response)

    assert isinstance(extracted_text, str)
    assert len(extracted_text) &gt; 0
    assert "Hello" in extracted_text

    print(f"Extracted text: {extracted_text}")
</file>
  <file path="api/prompt_api.py">from fastapi import APIRouter, Depends, Request, HTTPException
from repository.database import SQLite3Database
from repository.prompt_service import PromptService
from config import Config

prompt_api_router = APIRouter()


def get_db_connection(request: Request):
    """Create database connection as a dependency using app state"""
    db_path = request.app.state.db_path
    db = SQLite3Database(db_path=db_path)
    conn = db.get_connection()
    try:
        yield conn
    finally:
        conn.close()


def get_prompt_service(conn=Depends(get_db_connection)):
    """Get prompt service instance with injected database connection"""

    config = Config()
    return PromptService(conn, config)


@prompt_api_router.get("/")
async def welcome() -&gt; dict:
    return {"message": "Welcome to the prompt api service"}


@prompt_api_router.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "prompt_api"}


@prompt_api_router.get("/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: str, prompt_service: PromptService = Depends(get_prompt_service)
):
    prompt = prompt_service.get_prompt_by_id(prompt_id)
    if not prompt:
        raise HTTPException(status_code=404, detail="prompt not found")

    return prompt
</file>
  <file path="api/__init__.py">from .prompt_api import prompt_api_router

__all__ = ["prompt_api_router"]
</file>
</source_code>